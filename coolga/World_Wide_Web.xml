<?xml version="1.0" encoding="UTF-8" standalone="no"?><documents><document><year>2004</year><authors>Rajeev R. Raje1 | Snehasis Mukhopadhyay1| Michael Boyles1| Artur Papiez1| Nila Patel1| Mathew Palakal1 | Javed Mostafa2</authors><title>A bidding mechanism for Web-based agents involved in information classification</title><content>The impact of the World Wide Web on providing an easy information access is clearly evident in all aspects of today''s life. As attractive as the information availability is, due to its sheer volume, it creates an information-overload on users. Agent-based collaborative filtering is a technique used to effectively counter this burden. For agents to collaborate successfully, and maintain an overall progress of the entire interconnected environment, a governing mechanism is necessary. In this article, we present an economic framework based on a bidding mechanism for agents to communicate and negotiate with each other, thereby achieving collaborative information classification. D-SIFTER, a system developed using this economic framework, is described along with various experiments and their results.</content></document><document><year>2004</year><authors>Mengchi Liu1  | Tok Wang Ling1 </authors><title>A Conceptual Model and Rule-Based Query Language for HTML</title><content>Most documents available over the Web conform to the HTML specification. Such documents are hierarchically structured in nature. The existing data models for the Web either fail to capture the hierarchical structure within the documents or can only provide a very low level representation of such hierarchical structure. How to represent and query HTML documents at a higher level is an important issue. In this paper, we first propose a novel conceptual model for HTML. This conceptual model has only a few simple constructs but is able to represent the complex hierarchical structure within HTML documents at a level that is close to human conceptualization/visualization of the documents. We also describe how to convert HTML documents based on this conceptual model. Using the conceptual model and conversion method, one can capture the essence (i.e., semistructure) of HTML documents in a natural and simple way. Based on this conceptual model, we then present a rule&amp;#x2013;based language to query HTML documents over the Internet. This language provides a simple but very powerful way to query both intra&amp;#x2013;document structures and inter&amp;#x2013;document structures and allows the query results to be restructured. Being rule&amp;#x2013;based, it naturally supports negation and recursion and therefore is more expressive than SQL&amp;#x2013;based languages. A logical semantics is also provided.</content></document><document><year>2004</year><authors>Robert Tolksdorf1</authors><title>A framework for multiple coordination languages</title><content>The Web has become a world of services offered by Web agents. Although they provide attractive value in isolated operation, collaboration amongst Web agents from different origins could lead to more sophisticated services. Enabling for such an interworking would be the integration of platforms on which these agents operate. We take two coordination languages, namely Linda and KQML, and develop a model and framework which allows it to describe and implement both languages.</content></document><document><year>2004</year><authors>Lekha Chaisorn1 | Tat-Seng Chua1  | Chin-Hui Lee1 </authors><title>A Multi-Modal Approach to Story Segmentation for News Video</title><content>This research proposes a two-level, multi-modal framework to perform the segmentation and classification of news video into single-story semantic units. The video is analyzed at the shot and story unit (or scene) levels using a variety of features and techniques. At the shot level, we employ Decision Trees technique to classify the shots into one of 13 predefined categories or mid-level features. At the scene/story level, we perform the HMM (Hidden Markov Models) analysis to locate story boundaries. Our initial results indicate that we could achieve a high accuracy of over 95% for shot classification, and over 89% in F1 measure on scene/story boundary detection. Detailed analysis reveals that HMM is effective in identifying dominant features, which helps in locating story boundaries. Our eventual goal is to support the retrieval of news video at story unit level, together with associated texts retrieved from related news sites on the web.</content></document><document><year>2004</year><authors>Longzhuang Li1  | Yi Shang1</authors><title>A new method for automatic performance comparison of search engines</title><content>In this paper, we present a new method for automatically comparing the performance, such as precision, of search engines. Based on queries randomly selected from a specific domain of interest, the method uses robots to automatically query the target search engines, evaluates the relevance of the returned links to the query either automatically based on the vector space model or manually, and then applies statistic measures, including the probability of win and the Friedman statistic, to compare the performance of search engines. We show the experimental results of the new method on three search engines, AltaVista, Google, and InfoSeek. The method arrived at the same performance comparison result in applying either the automatic relevance evaluation method or the manual method. In addition, our results show that the probability of win is a better metric than the Friedman statistic in performance comparison. The advantage of the new method is that it is fast, flexible, consistent, and can adapt to the fast changing search engines.</content></document><document><year>2004</year><authors>Cristina Schmidt1  | Manish Parashar1 </authors><title>A Peer-to-Peer Approach to Web Service Discovery</title><content>Web Services are emerging as a dominant paradigm for constructing and composing distributed business applications and enabling enterprise-wide interoperability. A critical factor to the overall utility of Web Services is a scalable, flexible and robust discovery mechanism. This paper presents a Peer-to-Peer (P2P) indexing system and associated P2P storage that supports large-scale, decentralized, real-time search capabilities. The presented system supports complex queries containing partial keywords and wildcards. Furthermore, it guarantees that all existing data elements matching a query will be found with bounded costs in terms of number of messages and number of nodes involved. The key innovation is a dimension reducing indexing scheme that effectively maps the multidimensional information space to physical peers. The design and an experimental evaluation of the system are presented.</content></document><document><year>2004</year><authors>Gianluigi Greco1 | Sergio Greco1  | Ester Zumpano1 </authors><title>A Probabilistic Approach for Distillation and Ranking of Web Pages</title><content>A great number of recent papers have investigated the possibility of introducing more effective and efficient algorithms for search engines. In traditional search engines the resulting ranking is carried out using textual information only and, as showed by several works, they are not very useful for extracting relevant information. Present research, instead, takes a new approach, called Topic Distillation, whose main task is finding relevant documents using a different similarity criterion: retrieved documents are those related to the query topic, but which do not necessarily contain the query string. Current algorithms for topic distillation first compute a base set containing all the relevant pages and then, by applying an iterative procedure, obtain the authoritative pages. In this paper, we present a different approach which computes the authoritative pages by analyzing the structure of the base set. The technique applies a statistical approach to the co-citation matrix (of the base set) to find the most co-cited pages and combines a link analysis approach with the content page evaluation. Several experiments have shown the validity of our approach.</content></document><document><year>2004</year><authors>Jiangchuan Liu1  | Bo Li2 </authors><title>A QoS-Based Joint Scheduling and Caching Algorithm for Multimedia Objects</title><content>With the development of the broadband Internet, multimedia services have been widely deployed and contributed to a significant amount of todays Internet traffic. Like normal web objects (e.g., HTML pages and images), media objects can benefit from proxy caching; yet their unique features such as huge size and high bandwidth demand imply that conventional proxy caching strategies have to be substantially revised. Moreover, in the current Internet, clients are highly heterogeneous; it is necessary to differentiate their Quality-of-Service (QoS) requirements in streaming. However, the presence of an intermediate proxy in a streaming system poses great challenges to designers. This paper proposes a novel QoS-based algorithm for media streaming with proxy caching. We employ layered coding and transmission, and jointly consider the problems of caching and scheduling to improve the QoS for the clients. We derive general and effective solutions to the problems and evaluate their performance under various configurations. The results demonstrate that the proposed algorithm can accommodate diverse QoS demands from the clients, and yet satisfy stringent resource limits.</content></document><document><year>2004</year><authors>Sergio Flesca1 | Filippo Furfaro1  | Sergio Greco1 </authors><title>A Query Language for XML Based on Graph Grammars</title><content>In this paper we present a graphical query language for XML. The language, based on a simple form of graph grammars, permits us to extract data and reorganize information in a new structure. As with most of the current query languages for XML, queries consist of two parts: one extracting a subgraph and one constructing the output graph. The semantics of queries is given in terms of graph grammars. The use of graph grammars makes it possible to define, in a simple way, the structural properties of both the subgraph that has to be extracted and the graph that has to be constructed. We provide an example-driven comparison of our language w.r.t. other XML query languages, and show the effectiveness and simplicity of our approach.</content></document><document><year>2004</year><authors>A. S. Z. Belloum1 | E. C. Kaletas1 | A. W. van Halderen1 | H. Afsarmanesh1 | L. O. Hertzberger1  | A. J. H. Peddemors2 </authors><title>A Scalable Web Server Architecture</title><content>This paper describes a scalable architecture for Web servers designed to cope with the ongoing increase of the Internet requirements. In the paper, first the drawbacks of the traditional Web server architecture are discussed, and the need for an innovative solution is described. The proposed design addresses two of the parameters that can dramatically impact the performance of Web servers: (1) the need for a powerful data management system to cope with the increase in the complexity of users'' requests; and (2) an efficient caching mechanism to reduce the amount of redundant traffic. In this direction, a scalable solution based on distributed database technology to replace the file system is described, and performance test results of the system are provided. This architecture is further extended by a collaborative caching system that builds up an adaptive hierarchy of caches for Web servers, which allows them to keep up with the changes in the traffic generated by the applications they are running. Finally, some improvements to the proposed architecture are addressed.</content></document><document><year>2004</year><authors>V. Varadharajan1 | D. Foster1</authors><title>A Security Architecture for Mobile Agent Based Applications</title><content>This paper describes a security architecture for mobile agent based systems. It defines the notion of a security-enhanced agent and outlines security management components in agent platform bases and considers secure migration of agents from one base to another. The security enhanced agent carries a passport that contains its security credentials and some related security code. Then we describe how authentication, integrity and confidentiality, and access control are achieved using the agent's passport and the security infrastructure in the agent bases. We then discuss the application of the security model in roaming mobile agents and consider the types of access control policies that can be specified using the security enhanced agents and the policy base in the agent platforms. Finally we describe the security infrastructure that implements the proposed security services and outline the development of a secure agent based application using the proposed architecture.</content></document><document><year>2004</year><authors>Yan Wang1 | Kian-Lee Tan1  | Jian Ren1 </authors><title>A Study of Building Internet Marketplaces on the Basis of Mobile Agents for Parallel Processing</title><content>In this paper, we propose a framework of Internet marketplaces on the basis of mobile agents. It not only simulates real commercial activities by consumers, agents and merchants, but also provides an environment for parallel processing. The latter is particularly important as more shops (sites) can be searched in real time to provide consumers with better choices. Meanwhile, if the number of mobile agents is very large and the dispatch is processed in a serial way, it can become a bottleneck that impacts the efficiency as a whole. In this paper, we also present and discuss several hierarchical dispatch models where the dispatch of multiple mobile agents can be processed in parallel over different hosts. We study these models analytically and empirically. The conducted experiments show that, in comparison with several serial mobile agent models, parallel mobile agent models can improve the performance significantly. In addition, in the best case for the parallel dispatch model, the time complexity for dispatching n mobile agents is O(log2n).</content></document><document><year>2004</year><authors>Giovanna Di Marzo Serugendo1 | Murhimanya Muhugusa2 | Christian F. Tschudin3</authors><title>A survey of theories for mobile agents</title><content>This paper presents a comparative survey of formalisms related to mobile agents. It describes the -calculus and its extensions, the Ambient calculus, Petri nets, Actors, and the family of generative communication languages. Each of these formalisms defines a mathematical framework that can be used to reason about mobile code; they vary greatly in their expressiveness, in the mechanisms they provide to specify mobile code based applications and in their practical usefulness for the validation and the verification of such applications. In this paper we show how these formalisms can be used to represent the mobility and communication aspects of two mobile code environments: Obliq and Messengers. We compare and classify the different formalisms with respect to mobility and discuss some shortcomings and desirable extensions. We also point to other emerging concepts in formalisms for mobile code systems.</content></document><document><year>2004</year><authors>Ana Maria de Carvalho Moura1 | Maria Luiza Machado Campos1 | Cassia Maria Barreto1</authors><title>A survey on metadata for describing and retrieving Internet resources</title><content>Metadata, or information that makes data useful, have been considered by the database community basically as data in dictionaries used to control database management systems operations. More recently, metadata have been used to describe digital resources available across networks. This paper presents a survey of the state of the art concerning the use and importance of metadata, focusing on different standards and models found in the literature, describing how they serve as a basis for integrating heterogeneous resources on the Web and for developing more sophisticated search mechanisms.</content></document><document><year>2004</year><authors>Domenico Rosaci1| Giorgio Terracina2 | Domenico Ursino1</authors><title>A Technique for Extracting Sub-source Similarities from Information Sources Having Different Formats</title><content>In this paper we propose a semi-automatic technique for deriving the similarity degree between two portions of heterogeneous information sources (hereafter, sub-sources). The proposed technique consists in two phases: the first one selects the most promising pairs of sub-sources, whereas the second one computes the similarity degree relative to each promising pair. We show that the detection of sub-source similarities is a special case (and a very interesting one, for semi-structured information sources) of the more general problem of Scheme Match. In addition, we present a real example case to clarify the proposed technique, a set of experiments we have conducted to verify the quality of its results, a discussion about its computational complexity and its classification in the context of related literature. Finally, we discuss some possible applications which can benefit by derived similarities.</content></document><document><year>2004</year><authors>Huamin Chen1  | Arun Iyengar2 </authors><title>A Tiered System for Serving Differentiated Content</title><content>Contemporary Web sites typically consist of front&amp;#x2013;end Web servers, application servers, and back-end information systems such as database servers. There has been limited research on how to provide overload control and service differentiation for the back-end systems. In this paper we propose an architecture called tiered service (TS) for these purposes. In TS, there are several heterogeneous back-end systems to serve the Web applications. The Web applications communicate with a routing intermediary to intelligently route the queries to the appropriate back-end servers based on various policies such as client profiles and server load. In our system the back ends may store different qualities of data; lower quality data typically requires less overhead to serve. The main contributions of this paper include (i) a tiered content replication scheme that replicates tiered qualities of data on heterogeneous back ends with different capacity to satisfy clients with diverse requirements for latency and quality of data, and (ii) an application-transparent query routing architecture that automatically routes the queries to the appropriate back ends. The architecture was implemented in our test bed, and its performance was benchmarked. The experimental results demonstrate that TS offers significant performance improvement.</content></document><document><year>2004</year><authors>Stijn Dekeyser1| Jan Hidders2 | Jan Paredaens2</authors><title>A Transaction Model for XML Databases</title><content>The hierarchical and semistructured nature of XML data may cause complicated update behavior. Updates should not be limited to entire document trees, but should ideally involve subtrees and even individual elements. Providing a suitable scheduling algorithm for semistructured data can significantly improve collaboration systems that store their data&amp;#x2014;e.g., word processing documents or vector graphics&amp;#x2014;as XML documents. In this paper we show that concurrency control mechanisms in CVS, relational, and object-oriented database systems are inadequate for collaborative systems based on semistructured data. We therefore propose two new locking schemes based on path locks which are tightly coupled to the document instance. We also introduce two scheduling algorithms that can both be used with any of the two proposed path lock schemes. We prove that both schedulers guarantee serializability, and show that the conflict rules are necessary.</content></document><document><year>2004</year><authors>Chung-Ming Huang1  | Tz-Heng Hsu1</authors><title>A User-Aware Prefetching Mechanism for Video Streaming</title><content>The randomly and unpredictable user behaviors during a multimedia presentation may cause the long retrieval latency in the client&amp;#x2013;server connection. To accommodate the above problem, we propose a prefetching scheme that using the association rules from the data mining technique. The data mining technique can provide some priority information such as the support, confidence, and association rules which can be utilized for prefetching continuous media. Thus, using the data mining technique, the proposed prefetching policy can predict user behaviors and evaluate segments that may be accessed in near future. The proposed prefetching scheme was implemented and tested on synthetic data to estimate its effectiveness. Performance experiments show that the proposed prefetching scheme is effective in improving the latency reduction, even for small cache sizes.</content></document><document><year>2004</year><authors>Stephen G. Eick1| Audris Mockus1| Todd L. Graves1 | Alan F. Karr2</authors><title>A Web laboratory for software data analysis</title><content>We describe two prototypical elements of a World Wide Webbased system for visualization and analysis of data produced in the software development process. Our system incorporates interactive applets and visualization techniques into Web pages. A particularly powerful example of such an applet, SeeSoftTM, can display thousands of lines of text on a single screen, allowing detection of patterns not discernible directly from the text. In our system, Live Documents replace static statistical tables in ordinary documents by dynamic Webbased documents, in effect allowing the reader to customize the document as it is read. Use of the Web provides several advantages. The tools access data from a very large central data base, instead of requiring that it be downloaded; this ensures that readers are always working with the most uptodate version of the data, and relieves readers of the responsibility of preparing data for their use. The tools encourage collaborative research, as one researcher''s observations can easily be replicated and studied in greater detail by other team members. We have found this particularly useful while studying software data as part of a team that includes researchers in computer science, software engineering, and statistics, as well as development managers. Live documents will also help the Web revolutionize scientific publication, as papers published on the Web can contain Java applets that permit readers to confirm the conclusions reached by the authors'' statistical analyses.</content></document><document><year>2004</year><authors>Minsoo Lee1 | Stanley Y. W. Su2  | Herman Lam2 </authors><title>A Web-Based Knowledge Network for Supporting Emerging Internet Applications</title><content>Although the Internet and the World Wide Web technologies have gained a tremendous amount of popularity among people and organizations, the network that these technologies created is not much more than a multimedia data network. It provides tools and services for people to browse and search for data but does not provide the facilities for automatically delivering the relevant information for supporting decision&amp;#x2013;making to the right people or applications at the right time. Nor does it provide the means for users to enter and share their knowledge that would be useful for making the right decisions. In this work, we introduce the concept of a Web&amp;#x2013;based knowledge network, which allows users and organizations to publish, not only their multimedia data, but also their knowledge in terms of events, parameterized event filters, customizable rules and triggers that are associated with their data and application systems. Operations on the data and application systems may post events over the Internet to trigger the processing of rules defined by both information providers and consumers. The knowledge network is constructed by a number of replicable software components, which can be installed at various network sites. They, together with the existing Web servers, form a network of knowledge Web servers.</content></document><document><year>2004</year><authors>Yi Shang1  | Hongchi Shi1 </authors><title>A Webbased multiagent system for interpreting medical images</title><content>A difficult problem in medical image interpretation is that for every image type such as xray and every body organ such as heart, there exist specific solutions that do not allow for generalization. Just collecting all the specific solutions will not achieve the vision of a computerized physician. To address this problem, we develop an intelligent agent approach based on the concept of active fusion and agentoriented programming. The advantage of agentoriented programming is that it combines the benefits of objectoriented programming and expert system. Following this approach, we develop a Webbased multiagent system for interpreting medical images. The system is composed of two major types of intelligent agents: radiologist agents and patient representative agents. A radiologist agent decomposes the image interpretation task into smaller subtasks, uses multiple agents to solve the subtasks, and combines the solutions to the subtasks intelligently to solve the image interpretation problem. A patient representative agent takes questions from the user (usually a patient) through a Webbased interface, asks for multiple opinions from radiologist agents in interpreting a given set of images, and then integrates the opinions for the user. In addition, a patient representative agent can answer questions based on the information in a medical information database. To maximize the satisfaction that patients receive, the patient representative agents must be as informative and timely as communicating with a human. With an efficient pseudonatural language processing, a knowledge base in XML, and user communication through Microsoft Agent, the patient representative agents can answer questions effectively.</content></document><document><year>2004</year><authors>L.S.K. Chong1 | S.C. Hui1| C.K. Yeo1 | S. Foo1</authors><title>A WWWassisted fax system for Internet faxtofax communication</title><content>This paper describes a WWWassisted fax system (WAX) that is developed to provide reliable and enhanced Internet faxtofax communication. It integrates the easytouse WWW interface with conventional faxing procedures, resulting in an Internet fax system which not only circumvents the cost of long distance fax charges but also adds enhanced functionality not otherwise possible. The WAX system comprises two gateways, namely, the FaxIn and the FaxOut Gateways. The FaxIn Gateway accepts fax messages over Public Switched Telephone Network (PSTN) and stores them in a transit database. The system interfaces with the user over the WWW to provide access to his stored faxes, with the basic ability to send them out over the Internet to recipients. The FaxOut Gateway receives fax files from the FaxIn Gateway through the Internet and transmits them out to the intended recipients via the local PSTN. WAX users do not require any additional hardware except for a fax machine and a personal computer with Internet connectivity to gain access to WAX via any WWW browser. In addition, WAX provides a host of other enhanced features such as the ability to construct minifaxes from a single incoming fax as well as dynamically attach cover notes to outgoing faxes.</content></document><document><year>2004</year><authors>Thorsten Fiebig1  | Guido Moerkotte2 </authors><title>Algebraic XML Construction and its Optimization in Natix</title><content>While using an algebra that acts on sets of variable bindings for evaluating XML queries, the problem of constructing XML from these bindings arises. One approach is to define a powerful operator that is able to perform a complex construction of a representation of the XML result document. The drawback is that such an operator in its generality is hard to implement and disables algebraic optimization since it has to be executed last in the plan. Therefore we suggest to construct XML documents by special query execution plans called construction plans built from simple, easy to implement and efficient operators. The paper proposes four simple algebraic operators needed for XML document construction. Further, we introduce an optimizing translation algorithm of construction clauses into algebraic expressions and briefly point out algebraic optimizations enabled by our approach.</content></document><document><year>2004</year><authors>Ricardo Bianchini1  | Enrique V. Carrera1 </authors><title>Analytical and experimental evaluation of cluster-based network servers</title><content>In this paper we use analytic modeling and simulation to evaluate network servers implemented on clusters of workstations. More specifically, we model the potential benefits of locality-conscious request distribution within the cluster and evaluate the performance of a cluster-based server (called L2S) we designed in light of our experience with the model. Our most important modeling results show that locality-conscious distribution on a 16-node cluster can increase server throughput with respect to a locality-oblivious server by up to 5-fold, depending on the average size of the files requested and on the size of the server's working set. Our simulation results demonstrate that L2S achieves throughput that is within 28% of the full potential of locality-conscious distribution on 16 nodes, outperforming and significantly outscaling the best-known locality-conscious server. Based on our results and on the fact that the files serviced by network servers are becoming larger and more numerous, we conclude that our locality-conscious network server should prove very useful for its performance, scalability, and availability properties.</content></document><document><year>2004</year><authors>Muriel Jourdan1 | C&amp;eacute cile Roisin1| Laurent Tardif1 | Lionel Villard1</authors><title>Authoring SMIL documents by direct manipulations during presentation</title><content>This paper presents SmilEditor &amp;#x2013; an authoring environment to write multimedia documents encoded in SMIL. The main feature of SmilEditor is to strongly integrate the presentation view, in which the document is executed, with the editing process. In this view objects can be selected to perform a wide set of editing actions ranging from attributes setting to direct spatial or temporal editions. This way of editing a multimedia document is close to the wellknown WYSIWYG paradigm used by usual wordprocessors. Moreover, in order to help the author to specify the temporal organisation of documents, SmilEditor provides an execution report displayed through a timeline view. This view also contains information which helps the author to understand why such execution occurred. These multiple and synchronised views aim at covering the various needs for authoring multimedia documents.</content></document><document><year>2004</year><authors>Somchai Chatvichienchai1 | Mizuho Iwaihara1  | Yahiko Kambayashi1 </authors><title>Authorization Translation for XML Document Transformation</title><content>XML access control models proposed in the literature enforce access restrictions directly on the structure and content of an XML document. Therefore access authorization rules (authorizations, for short), which specify access rights of users on information within an XML document, must be revised if they do not match with changed structure of the XML document. In this paper, we present two authorization translation problems. The first is a problem of translating instance-level authorizations for an XML document. The second is a problem of translating schema-level authorizations for a collection of XML documents conforming to a DTD. For the first problem, we propose an algorithm that translates instance-level authorizations of a source XML document into those for a transformed XML document by using instance-tree mapping from the transformed document instance to the source document instance. For the second problem, we propose an algorithm that translates value-independent schema-level authorizations of non-recursive source DTD into those for a non-recursive target DTD by using schema-tree mapping from the target DTD to the source DTD. The goal of authorization translation is to preserve authorization equivalence at instance node level of the source document. The XML access control models use path expressions of XPath to locate data in XML documents. We define property of the path expressions (called node-reducible path expressions) that we can transform schema-level authorizations of value-independent type by schema-tree mapping. To compute authorizations on instances of schema elements of the target DTD, we need to identify the schema elements whose instances are located by a node-reducible path expression of a value-independent schema-level authorization. We give an algorithm that carries out path fragment containment test to identify the schema elements whose instances are located by a node-reducible path expression.</content></document><document><year>2004</year><authors>Yuqing Song1 | Wei Wang2  | Aidong Zhang2 </authors><title>Automatic Annotation and Retrieval of Images</title><content>Although a variety of techniques have been developed for content-based image retrieval (CBIR), automatic image retrieval by semantics still remains a challenging problem. We propose a novel approach for semantics-based image annotation and retrieval. Our approach is based on the monotonic tree model. The branches of the monotonic tree of an image, termed as structural elements, are classified and clustered based on their low level features such as color, spatial location, coarseness, and shape. Each cluster corresponds to some semantic feature. The category keywords indicating the semantic features are automatically annotated to the images. Based on the semantic features extracted from images, high-level (semantics-based) querying and browsing of images can be achieved. We apply our scheme to analyze scenery features. Experiments show that semantic features, such as sky, building, trees, water wave, placid water, and ground, can be effectively retrieved and located in images.</content></document><document><year>2004</year><authors>Robert C. Seacord1  | Scott A. Hissam1</authors><title>Browsers for distributed systems: Universal paradigm or siren''s song?</title><content>Webbased browsers are quickly becoming ubiquitous in the workplace. Software development managers are quick to incorporate browsers into a broad range of software development projects, often inappropriately. The purpose of this paper is to examine the technical issues relevant to incorporating browsers as a component of a commercial offtheshelf (COTS)based solution. Issues examined include portability, performance, functionality, security, human factors, distribution, installation, upgrading, componentbased development, runtime configuration management, and licensing.</content></document><document><year>2004</year><authors>Alfons Kemper1  | Christian Wiesner2 </authors><title>Building Scalable Electronic Market Places Using HyperQuery-Based Distributed Query Processing</title><content>Flexible distributed query processing capabilities are an important prerequisite for building scalable Internet applications, such as electronic Business-to-Business (B2B) market places. Architecting an electronic market place in a conventional data warehouse-like approach by integrating all the data from all participating enterprises in one centralized repository incurs severe problems: stale data, data security threats, administration overhead, inflexibility during query processing, etc. In this paper we present a new framework for dynamic distributed query processing based on so-called HyperQueries which are essentially query evaluation sub-plans sitting behind hyperlinks. Our approach facilitates the pre-materialization of static data at the market place whereas the dynamic data remains at the data sources. In contrast to traditional data integration systems, our approach executes essential (dynamic) parts of the data-integrating views at the data sources. The other, more static parts of the data are integrated &amp;agrave; priori at the central portal, e.g., the market place. The portal serves as an intermediary between clients and data providers which execute their sub-queries referenced via hyperlinks. The hyperlinks are embedded as attribute values within data objects of the intermediarys database. Retrieving such a virtual object will execute the referenced HyperQuery in order to materialize the missing data. We illustrate the flexibility of this distributed query processing architecture in the context of B2B electronic market places with an example derived from the car manufacturing industry.Based on these HyperQueries, we propose a reference architecture for building scalable and dynamic electronic market places. All administrative tasks in such a distributed B2B market place are modeled as Web services and are initiated decentrally by the participants. Thus, sensitive data remains under the full control of the data providers. We describe optimization and implementation issues to obtain an efficient and highly flexible data integration platform for electronic market places. All proposed techniques have been fully implemented in our QueryFlow prototype system which served as the platform for our performance evaluation.</content></document><document><year>2004</year><authors>Ravi Iyer1 </authors><title>Characterization and Evaluation of Cache Hierarchies for Web Servers</title><content>As Internet usage continues to expand rapidly, careful attention needs to be paid to the design of Internet servers for achieving high performance and end-user satisfaction. Currently, the memory system continues to remain a significant performance bottleneck for Internet servers employing multi-GHz processors. In this paper, our aim is two-fold: (1) to characterize the cache/memory performance of web server workloads and (2) to propose and evaluate cache design alternatives for future web servers. We chose SPECweb99 as the representative web server workload and our entire characterization and evaluation methodology is based on our CASPER simulation framework. We begin by exploring the processor cache design space for single and dual-processor servers. Based on our observations, we then evaluate other cache hierarchy alternatives such as chipset caches, coherence filters and decompressed page stores. We show the sensitivity of these components to basic organization parameters such as cache size, line size and degree of associativity. We also present the performance implications of routing memory requests initiated by I/O devices through these caches. Based on detailed simulation data and its implications on system level performance, this paper shows that chipset caches have significant potential for improving future web server performance.</content></document><document><year>2004</year><authors>Dian Tjondronegoro1  | Yi-Ping Phoebe Chen1 </authors><title>Content-Based Indexing and Retrieval Using MPEG-7 and X-Query in Video Data Management Systems</title><content>Current advances in multimedia technology enable ease of capturing and encoding digital video. As a result, video data is rapidly growing and becoming very important in our life. It is because video can transfer a large amount of knowledge by providing combination of text, graphics, or even images. Despite the vast growth of video, the effectiveness of its usage is very limited due to the lack of a complete technology for the organization and retrieval of video data. To date, there is no perfect solution for a complete video data-management technology, which can fully capture the content of video and index the video parts according to the contents, so that users can intuitively retrieve specific video segments. We have found that successful content-based video data-management systems depend on three most important components: key-segments extraction, content descriptions and video retrieval. While it is almost impossible for current computer technology to perceive the content of the video to identify correctly its key-segments, the system can understand more accurately the content of a specific video type by identifying the typical events that happens just before or after the key-segments (specific-domain-approach). Thus, we have proposed a concept of customisable video segmentation module, which integrates the suitable segmentation techniques for the current type of video. The identified key-segments are then described using standard video content descriptions to enable content-based retrievals. For retrieval, we have implemented XQuery, which currently is the most recent XML query language and the most powerful compared to older languages, such as XQL and XML-QL.</content></document><document><year>2004</year><authors>P. Ciancarini1  | D. Rossi1</authors><title>Coordinating Java agents over the WWW</title><content>We introduce Jada, a programming toolkit for coordinating agents written in Java. Coordination among either concurrent threads or distributed Java objects is achieved via shared object spaces. By exchanging objects through object spaces, Java agents or applets can exchange data or synchronize their actions over the Internet, a LAN, a single host, or even inside a Javaenabled browser. The access to an object space is performed using a set of methods of an ObjectSpace object. Such operations inspired by the Linda language are powerful enough to solve several coordination problems. Moreover, we show how Jada can be used as a coordination kernel for more complex coordination architectures.</content></document><document><year>2004</year><authors>Paolo Atzeni1 | Paolo Merialdo1  | Giansalvatore Mecca2 </authors><title>Data-Intensive Web Sites: Design and Maintenance</title><content>A methodology for designing and maintaining data&amp;#x2013;intensive Web sites is introduced. Leveraging on ideas well established in the database field, the approach heavily relies on the use of models for the description of Web sites. The design process is composed of two intertwined activities: database design and hypertext design. Each of these is further divided in a conceptual phase and a logical phase, based on specific data models. The methodology strongly supports site maintenance: in fact, the various models provide a concise description of the site structure; they allow to reason about the overall organization of pages in the site and possibly to restructure it.</content></document><document><year>2004</year><authors>Samuel T. Chanson1  | Tin-Wo Cheung1</authors><title>Design and Implementation of a PKI-Based End-to-End Secure Infrastructure for Mobile E-Commerce</title><content>The popularity of handheld mobile devices and deployment of the public key infrastructure in many parts of the world have led to the development of electronic commerce on mobile devices. For the current version of mobile phones, the main challenge is the limited computing capacity on these devices for PKI-based end-to-end secure transactions. This paper presents a new architecture and protocol for authentication and key exchange as well as the supporting infrastructure that is suitable for the mobile phone environment. The system requirements and our solutions in addressing these requirements in the restrictive environment are discussed. An evaluation of the system performance is also included. The system has been implemented and is supporting some real-life applications.</content></document><document><year>2004</year><authors>K. Y. Leung1 | Eric W. M. Wong1  | K. H. Yeung1 </authors><title>Designing Efficient and Robust Caching Algorithms for Streaming-on-Demand Services on the Internet</title><content>Content Delivery Networks (CDN) have been used on the Internet to cache media content so as to reduce the load on the original media server, network congestion, and latency. Due to the large size of media content compared to normal web objects, current caching algorithms used in the Internet are no longer suitable. This paper presents a high-performance prefetch system that accommodates user time-varying behavior. A hybrid caching technique, which combines prefetch and replacement algorithms, is also introduced. The robustness of the cache system against imperfect user request information is evaluated using three request noise models. Two prefetch performance indices are also presented to help content administrators in deciding when to update the user request profile for caching algorithms.</content></document><document><year>2004</year><authors>Shihui Zheng1 | Aoying Zhou1| Long Zhang1  | Hongjun Lu2 </authors><title>DVQ: Towards Visual Query Processing of XML Database Systems</title><content>XML has been recognized as a promising language for data exchange over the Internet. A number of query languages have been proposed for querying XML data. Most of those languages are path-expression based. One difficulty in forming path-expression based queries is that users have to know the structure of XML data against which the queries were issued. In this paper, we describe a DTD-driven visual query interface for XML database systems. With such an interface, a user can easily form path-expression based queries by clicking elements in the DTD tree displayed on the screen and supplying conditions if necessary. The interface and the query generation process are described in detail.</content></document><document><year>2004</year><authors>Sham Prasher1 | Xiaofang Zhou1  | Masaru Kitsuregawa2 </authors><title>Dynamic Multi-Resolution Spatial Object Derivation for Mobile and WWW Applications</title><content>Online geographic information systems provide the means to extract a subset of desired spatial information from a larger remote repository. Data retrieved representing real-world geographic phenomena are then manipulated to suit the specific needs of an end-user. Often this extraction requires the derivation of representations of objects specific to a particular resolution or scale from a single original stored version. Currently standard spatial data handling techniques cannot support the multi-resolution representation of such features in a database. In this paper a methodology to store and retrieve versions of spatial objects at different resolutions with respect to scale using standard database primitives and SQL is presented. The technique involves heavy fragmentation of spatial features that allows dynamic simplification into scale-specific object representations customised to the display resolution of the end-user's device. Experimental results comparing the new approach to traditional R-Tree indexing and external object simplification reveal the former performs notably better for mobile and WWW applications where client-side resources are limited and retrieved data loads are kept relatively small.</content></document><document><year>2004</year><authors>Hye-Young Paik1 | Boualem Benatallah1  | Rachid Hamadi1 </authors><title>Dynamic Restructuring of E-Catalog Communities Based on User Interaction Patterns</title><content>Since e-catalogs are dynamic, autonomous, and heterogeneous, the integration of a potentially large number of dynamic e-catalogs is a delicate and time-consuming task. In this paper, we describe the design and the implementation of a system through which existing on-line product catalogs can be integrated, and the resulting integrated catalogs can be continuously adapted and personalized within a dynamic environment. The integration framework originates from a previous project on integration of Web data, called WebFINDIT. Using the framework, we propose a methodology for adaptation of integrated catalogs based on the observation of customers'' interaction patterns.</content></document><document><year>2004</year><authors>Jeffrey Xu Yu1 | Daofeng Luo2| Xiaofeng Meng2  | Hongjun Lu3 </authors><title>Dynamically Updating XML Data: Numbering Scheme Revisited</title><content>Almost all existing approaches use certain numbering scheme to encode XML elements to facilitate query processing when XML data is stored in databases. For example, under the most popular region-based numbering scheme, the starting and ending positions of an element in a document are used as the code to identify the element so that the ancestor/descendant relationship between two elements can be determined by merely examining their codes. While such numbering scheme can greatly improve query performance, renumbering large amount of elements caused by updates becomes a performance bottleneck if XML documents are frequently updated. Unfortunately, no satisfactory work has been reported for efficient update of XML data. In this paper, we first formalize the XML data update problem by defining the basic operators to support most XML update queries. We then present a new numbering scheme that not only requires minimal code-length in comparison with existing numbering schema but also improves update performance when XML data is frequently updated at arbitrary positions. The fundamental difference between our new scheme and existing ones is that, instead of maintaining the explicit codes for elements, we only store the necessary information and generate the codes when they are needed in query processing. In addition to present the basic scheme, we also discuss some optimization techniques to further reduce the update cost. Results of a comprehensive performance study are provided to show the advantages of the new scheme.</content></document><document><year>2004</year><authors>Qing Li1 | M. Tamer Ozsu2</authors><title>Editorial: Introduction to Web Media Information Systems</title><content>Without Abstract</content></document><document><year>2004</year><authors>Osman Balci</authors><title>Editorial introduction</title><content>Without Abstract</content></document><document><year>2004</year><authors>A. J. M. Traina1 | C. Traina1 | J. M. Bueno1 | F. J. T. Chino1  | P. Azevedo-Marques2 </authors><title>Efficient Content-Based Image Retrieval through Metric Histograms</title><content>This paper presents a new and efficient method for content-based image retrieval employing the color distribution of images. This new method, called metric histogram, takes advantage of the correlation among adjacent bins of histograms, reducing the dimensionality of the feature vectors extracted from images, leading to faster and more flexible indexing and retrieval processes. The proposed technique works on each image independently from the others in the dataset, therefore there is no pre-defined number of color regions in the resulting histogram. Thus, it is not possible to use traditional comparison algorithms such as Euclidean or Manhattan distances. To allow the comparison of images through the new feature vectors given by metric histograms, a new metric distance function MHD( ) is also proposed. This paper shows the improvements in timing and retrieval discrimination obtained using metric histograms over traditional ones, even when using images with different spatial resolution or thumbnails. The experimental evaluation of the new method, for answering similarity queries over two representative image databases, shows that the metric histograms surpass the retrieval ability of traditional histograms because they are invariant on geometrical and brightness image transformations, and answer the queries up to 10 times faster than the traditional ones.</content></document><document><year>2004</year><authors>L. Y. Cao1  | M. T. &amp;Ouml zsu1 </authors><title>Evaluation of Strong Consistency Web Caching Techniques</title><content>The growth of the World Wide Web (WWW or Web) and its increasing use in all types of business have created bottlenecks that lead to high network and server overload and, eventually, high client latency. Web Caching has become an important topic of research, in the hope that these problems can be addressed by appropriate caching techniques. Conventional wisdom holds that strong cache consistency, with (almost) transactional consistency guarantees, may neither be necessary for Web applications, nor suitable due to its high overhead. However, as business transactions on the Web become more popular, strong consistency will be increasingly necessary. Consequently, it is important to have a comprehensive understanding of the performance behavior of these protocols. The existing studies, unfortunately, are ad hoc and the results cannot be compared across different studies. In this paper we evaluate the performance of different categories of cache consistency algorithms using a standard benchmark: TPC-W, which is the Web commerce benchmark. Our experiments show that we could still enforce strong cache consistency without much overhead, and Invalidation, as an event-driven strong cache consistency algorithm, is most suitable for online e-business. We also evaluate the optimum deployment of caches and find that proxy-side cache has a 30&amp;#x2013;35% performance advantage over client-side cache with regard to system throughput.</content></document><document><year>2004</year><authors>Elisa Bertino1| Tok Wang Ling2 | Umesh Dayal3</authors><title>Guest Editors' Introduction</title><content>Without Abstract</content></document><document><year>2004</year><authors>L.A. Carr1| D. De Roure1| W. Hall1 | G. Hill1</authors><title>Implementing an open link service for the World Wide Web</title><content>Links are the key element for changing a text into a hypertext, and yet the WWW provides limited linking facilities. Modeled on Open Hypermedia research the Distributed Link Service provides an independent system of link services for the World Wide Web and allows authors to create configurable navigation pathways for collections of WWW resources. This is achieved by adding links to documents as they are delivered from a WWW server, and by allowing the users to choose the sets of links that they will see according to their interests. This paper describes the development of the link service, the facilities that it adds for users of the WWW and its specific use in an Electronic Libraries project.</content></document><document><year>2004</year><authors>Ling Liu1 | Wei Tang1 | David Buttler1  | Calton Pu1 </authors><title>Information Monitoring on the Web: A Scalable Solution</title><content>This paper presents WebCQ, a continual query system for large-scale Web information monitoring. WebCQ is designed to discover and detect changes to Web pages efficiently, and to notify users of interesting changes with personalized messages. Users'' Web page monitoring requests are modeled as continual queries on the Web and referred to as Web page sentinels. The system consists of five main components: a change detection robot that discovers and detects changes, a proxy cache service that reduces the communication traffics to the original information provider on the remote server, a trigger evaluation tool that can filter only the changes that match certain thresholds, a personalized change presentation tool that highlights Web page changes, and a change notification service that displays and delivers interesting changes and fresh information to the right users at the right time. This paper describes the WebCQ system with an emphasis on the general issues in designing and engineering a large-scale information change monitoring system on the Web. There are two main contributions. First, we present the mechanisms that WebCQ provides to support various types of Web page sentinels for finding and displaying interesting changes to Web pages. The large collection of sentinel types allows WebCQ to efficiently locate and monitor a wide range of changes in Web pages. The second contribution is the development of sentinel grouping techniques for efficient and scalable processing of large number of concurrently running triggers and Web page sentinels. We report our initial experimental results showing the effectiveness of the proposed solutions.</content></document><document><year>2004</year><authors>Carlos E. Palau1 | Juan C. Guerri1 | Manuel Esteve1</authors><title>Instructional uses of the WWW: An evaluation tool</title><content>The development of the World Wide Web (WWW) and the subsequent introduction of different browsers, with their extensions, has changed the Internet from a textonly communications tool to a powerful multimedia platform whose potential applications are increasing day by day. Research efforts in the computer aided education field are represented by a broad spectrum of applications, from the virtual classroom to remote courses. In these environments, visualising the progress of students in a certain course is an important part of the learning process. At the Universidad Politecnica de Valencia we are experimenting with WWW based software tools and networks for computer aided learning. This research process has resulted in the development of a teacher''s authoring tool and an evaluation application; both developed using Java and based on Internet browsers. With this evaluation tool, teachers can easily create questions of different types &amp;#x2013; which are stored on a database. These questions can later be used to compose different exams or exercises for different students depending on the course and the objective of the examinations. The advantages include an improvement in the fulfillment of the teacher''s duties; an increase in the responsiveness of the exam results to the level of student understanding; and the potential for using the application in distance learning and training.</content></document><document><year>2004</year><authors>Hongjun Lu1  | Ling Feng1</authors><title>Integrating database and World Wide Web technologies</title><content>Integrating database and World Wide Web technologies is another topic where industrial and practical activities lead ahead of academic ones. The purpose of this article is to survey the related activities from database people''s view and stimulate the interests among the database community. It covers three aspects. First, the efforts that apply established database techniques to retrieving Web information are summarized. These efforts aim to overcome the inadequacy of file system technology on which the Web is based, so that information can be retrieved easily and quickly from the Web. Second, various approaches to interfacing databases via the Web are discussed, with examples of accomplished prototypes and commercial products showing recent advances. Finally, some possible extensions to the traditional database techniques are investigated for building fully fledged Webbased database applications.</content></document><document><year>2004</year><authors>Qiang Yang1  | Henry Hanning Zhang2 </authors><title>Integrating Web Prefetching and Caching Using Prediction Models</title><content>Web caching and prefetching have been studied in the past separately. In this paper, we present an integrated architecture for Web object caching and prefetching. Our goal is to design a prefetching system that can work with an existing Web caching system in a seamless manner. In this integrated architecture, a certain amount of caching space is reserved for prefetching. To empower the prefetching engine, a Web-object prediction model is built by mining the frequent paths from past Web log data. We show that the integrated architecture improves the performance over Web caching alone, and present our analysis on the tradeoff between the reduced latency and the potential increase in network load.</content></document><document><year>2004</year><authors>Nicky Joshi1 | Kushal Thakore1  | Stanley Y. W. Su1 </authors><title>IntelliBid: An Event-Trigger-Rule-Based Auction System over the Internet</title><content>This paper presents the design and implementation of an Event-Trigger-Rule-Based auction system called IntelliBid. A network of Knowledge Web Servers, each consisting of a Web server, an Event-Trigger-Rule (ETR) Server, an Event Engine, a Knowledge Profile Manager, and Bid Servers and their proxies constitutes IntelliBid. Together, they provide auction-related services to the creator of an auction site and the bidders and suppliers of products. IntelliBid offers a number of desirable features. First and foremost is the flexibility offered to bidders for defining their own rules to control their bids in an automatic bidding process, which frees the bidders from having to be on-line to place bids. By using different rules, the bidders can apply different bidding strategies. Second, it furnishes valuable statistical information about past auctions to both suppliers (or sellers) and bidders. The information can assist a bidder in bidding and a seller in setting a reasonable base price and/or the minimum incremental price. Third, since rules that control the automatic bidding are installed and processed by the ETR servers installed at bidders' individual sites, bidders' privacy and security are safeguarded. The statistical information that is released by IntelliBid only depicts the trend of the bidding prices of a product. The information about bidders is kept completely secret, thus safeguarding the privacy of the bidders. Fourth, IntelliBid's event, event filtering and event notification mechanisms keep both bidders and suppliers timely informed of auction events so that they or their software system can take the proper actions in the auction process. Fifth, any registered user of IntelliBid, bidder or supplier, can monitor the bids placed to any product being auctioned in IntelliBid. Sixth, IntelliBid allows bidders to do both on-line (or manual) bidding and automatic bidding. It also allows a bidder to participate in several auctions at the same time, in both manual and automated modes. The bidding of a product can depend on the result of the bidding of another product. Last, but not least, IntelliBid allows a person or organization to play both the role of bidder and the role of supplier simultaneously. The Profile Manager keeps the information as a bidder and information as a supplier separately. Moreover, IntelliBid's architecture uses a parallel event management system to do event registration and notification. This paper also reports the result of a performance study on the implication of using such a parallel system to achieve scalability.</content></document><document><year>2004</year><authors>H. Lilian Tang1</authors><title>Knowledge Elicitation and Semantic Representation for the Heterogeneous Web</title><content>This paper presents methods and principles for knowledge elicitation and semantics definitions for images and text, respectively, and furthermore introduces a semantic representation scheme that fuses the semantic information extracted from image and text to facilitate intelligent indexing and retrieval for multimedia collection as well as media transformation through their semantic meanings. The method can be deployed for WWW applications such as telemedicine or virtual gallery.</content></document><document><year>2004</year><authors>Claus Brabr|1 | Anders M&amp;oslash ller1 | Steffan Olesen1  | Michael I. Schwartzbach1 </authors><title>Language-Based Caching of Dynamically Generated HTML</title><content>Increasingly, HTML documents are dynamically generated by interactive Web services. To ensure that the client is presented with the newest versions of such documents it is customary to disable client caching causing a seemingly inevitable performance penalty. In the  system, dynamic HTML documents are composed of higher-order templates that are plugged together to construct complete documents. We show how to exploit this feature to provide an automatic fine-grained caching of document templates, based on the service source code. A  service transmits not the full HTML document but instead a compact JavaScript recipe for a client-side construction of the document based on a static collection of fragments that can be cached by the browser in the usual manner. We compare our approach with related techniques and demonstrate on a number of realistic benchmarks that the size of the transmitted data and the latency may be reduced significantly.</content></document><document><year>2004</year><authors>D. Calvanese1 | T. Catarci1  | G. Santucci1 </authors><title>LAURIN: A Distributed Digital Library of Newspaper Clippings</title><content>Among the wide range of digital libraries, an interesting, yet quite neglected, subclass is constituted by those exclusively dealing with newspaper clippings. Compared with book&amp;#x2013;oriented digital libraries, clipping libraries are more difficult to seize, since they are wide and unstructured, and the subjects and content of a clipping are completely heterogeneous. LAURIN is an EU&amp;#x2013;funded project involving seventeen participants from several countries, including two software companies and a large group of libraries, whose main purpose is to set up a network of digitalized newspaper clipping archives that can be easily accessed through the Internet, for searching and retrieving clippings. The project also provides the libraries with models and methodologies to be used for scanning, digitalizing, storing, indexing, and making accessible newspaper clippings. This paper concentrates on the main architectural features of the LAURIN distributed system, exposing the peculiarities deriving from the diverse users and tasks it supports.</content></document><document><year>2004</year><authors>Allan Heydon1  | Marc Najork1</authors><title>Mercator: A scalable, extensible Web crawler</title><content>This paper describes Mercator, a scalable, extensible Web crawler written entirely in Java. Scalable Web crawlers are an important component of many Web services, but their design is not welldocumented in the literature. We enumerate the major components of any scalable Web crawler, comment on alternatives and tradeoffs in their design, and describe the particular components used in Mercator. We also describe Mercator''s support for extensibility and customizability. Finally, we comment on Mercator''s performance, which we have found to be comparable to that of other crawlers for which performance numbers have been published.</content></document><document><year>2004</year><authors>Brian F. Cooper1| 2 | Neal Sample1 | Michael J. Franklin1 | Joshua Olshansky1  | Moshe Shadmon1 </authors><title>Middle-Tier Extensible Data Management</title><content>Current data management solutions are largely optimized for intra-enterprise, client&amp;#x2013;server applications. They depend on predictability, predefined structure, and universal administrative control, and cannot easily cope with change and lack of structure. However, modern e-commerce applications are dynamic, unpredictable, organic, and decentralized, and require adaptability. eXtensible Data Management (XDM) is a new approach that enables rapid development and deployment of networked, data-intensive services by providing semantically-rich, high-performance middle-tier data management, and allows heterogeneous data from different sources to be accessed in a uniform manner. Here, we discuss how middle tier extensible data management can benefit an enterprise, and present technical details and examples from the Index Fabric, an XDM engine we have implemented.</content></document><document><year>2004</year><authors>Danny B. Lange1  | Mitsuru Oshima2 </authors><title>Mobile agents with Java: The Aglet API</title><content>Java, the language that changed the Web overnight, offers some unique capabilities that are fueling the development of mobile agent systems. In this article we will show what exactly it is that makes Java such a powerful tool for mobile agent development. We will also draw attention to some shortcomings in Java language systems that have implications for the conceptual design and use of Java-based mobile agent systems. Last, but not least, we will introduce the aglet &amp;#x2013; a Java-based agile agent. We will give an overview of the aglet and, its application programming interface, and present a real-world example of its use in electronic commerce.</content></document><document><year>2004</year><authors>Raymond K. Wong1| Franky Lam1 | M. A. Orgun2</authors><title>Modelling and Manipulating Multidimensional Data in Semistructured Databases</title><content>Multidimensional information is pervasive in many computer applications including time series, spatial information, data warehousing, and visual data. While semistructured data or XML is becoming more and more popular for information integration and exchange, not much research work has been done in the design and implementation of semistructured database system to manage multidimensional information efficiently. In this paper, dimension operators have been defined based on a multidimensional logic which we call ML(). It can be used in applications such as multidimensional spreadsheets and multidimensional databases usually found in decision suport systems and data warehouses. Finally, a multidimensional XML database system has been prototyped and described in detail. Technologies such as XSL are used to transform or visualise data from different dimensions.</content></document><document><year>2004</year><authors>J. Baumann1| F. Hohl1| K. Rothermel1 | M. Stra&amp;szlig er1</authors><title>Mole &amp;#x2013; Concepts of a mobile agent system</title><content>Due to its salient properties, mobile agent technology has received a rapidly growing attention over the last few years. Many developments of mobile agent systems are under way in both academic and industrial environments. In addition, there are already various efforts to standardize mobile agent facilities and architectures. Mole is the first mobile agent system that has been developed in the Java language. The first version was finished in 1995, and since then Mole has been constantly improved. Mole provides a stable environment for the development and usage of mobile agents in the area of distributed applications. In this paper we describe the basic concepts of a mobile agent system, i.e., mobility, communication and security, discuss different implementation techniques, present the decisions made in Mole and give an overview of the system services implemented in Mole.</content></document><document><year>2004</year><authors>Yibei Ling1 | Shigang Chen2  | Xiaola Lin3 </authors><title>On the Performance Regularity of Web Servers</title><content>The performance regularity is concerned with the overall performance behavior of a system in the full spectrum of working area. Such a performance characteristic is generally overlooked and does not receive proper attention. The aim of this paper is twofold. First, it raises awareness of the importance of the performance regularity of a Web server. Secondly, it introduces the Gini performance coefficient (GPC) as a scale-invariant metric for measuring the performance regularity. In this paper, we present the theorems that relate the performance regularity of a Web server to the GPC, thereby providing a quantitative yardstick that complements the system capacity metric such as maximum throughput for measuring the system performance. To illustrate the use of the proposed approach, we calculate the values of GPC for several representative systems that were used in the public SPECweb96 benchmark study. The results are completely in line with our theoretical analysis.</content></document><document><year>2004</year><authors>Zhixiang Chen1 | Ada Wai-Chee Fu2  | Frank Chi-Hung Tong3 </authors><title>Optimal Algorithms for Finding User Access Sessions from Very Large Web Logs</title><content>Although efficient identification of user access sessions from very large web logs is an unavoidable data preparation task for the success of higher level web log mining, little attention has been paid to algorithmic study of this problem. In this paper we consider two types of user access sessions, interval sessions and gap sessions. We design two efficient algorithms for finding respectively those two types of sessions with the help of some proposed structures. We present theoretical analysis of the algorithms and prove that both algorithms have optimal time complexity and certain error-tolerant properties as well. We conduct empirical performance analysis of the algorithms with web logs ranging from 100 megabytes to 500 megabytes. The empirical analysis shows that the algorithms just take several seconds more than the baseline time, i.e., the time needed for reading the web log once sequentially from disk to RAM, testing whether each user access record is valid or not, and writing each valid user access record back to disk. The empirical analysis also shows that our algorithms are substantially faster than the sorting based session finding algorithms. Finally, optimal algorithms for finding user access sessions from distributed web logs are also presented.</content></document><document><year>2004</year><authors>Oren Unger1  | Israel Cidon2 </authors><title>Optimal Content Location in Multicast Based Overlay Networks with Content Updates</title><content>The architecture of overlay networks should support high-performance and high-scalability at low costs. This becomes more crucial when communication, storage costs as well as service latencies grow with the exploding amounts of data exchanged and with the size and span of the overlay network. For that end, multicast methodologies can be used to deliver content from regional servers to end users, as well as for the timely and economical synchronization of content among the distributed servers. Another important architectural problem is the efficient allocation of objects to servers to minimize storage, delivery and update costs. In this work, we suggest a multicast based architecture and address the optimal allocation and replication of dynamic objects that are both consumed and updated. Our model network includes consumers which are served using multicast or unicast transmissions and media sources (that may be also consumers) which update the objects using multicast communication. General costs are associated with distribution (download) and update traffic as well as the storage of objects in the servers. Optimal object allocation algorithms for tree networks are presented with complexities of O(N) and O(N2) in case of multicast and unicast distribution respectively. To our knowledge, the model of multicast distribution combined with multicast updates has not been analytically dealt before, despite its popularity in the industry.</content></document><document><year>2004</year><authors>Victor Safronov1  | Manish Parashar1 </authors><title>Optimizing Web Servers Using Page Rank Prefetching for Clustered Accesses</title><content>This paper presents a Page Rank based prefetching technique for accesses to Web page clusters. The approach uses the link structure of a requested page to determine the most important linked pages and to identify the page(s) to be prefetched. The underlying premise of our approach is that in the case of cluster accesses, the next pages requested by users of the Web server are typically based on the current and previous pages requested. Furthermore, if the requested pages have a lot of links to some important page, that page has a higher probability of being the next one requested. An experimental evaluation of the prefetching mechanism is presented using real server logs. The results show that the Page-Rank based scheme does better than random prefetching for clustered accesses, with hit rates of 90% in some cases.</content></document><document><year>2004</year><authors>Christos Bouras1| 2  | Agisilaos Konidaris1| 2 </authors><title>Performance Evaluation of a Hybrid Run-Time Management Policy for Data Intensive Web Sites</title><content>The issues of performance, response efficiency and data consistency are among the most important for data intensive Web sites. In order to deal with these issues we analyze and evaluate a hybrid run-time management policy that may be applied to data intensive Web sites. Our research relies on the performance evaluation of experimental client/server configurations. We propose a hybrid Web site run-time management policy that may apply to different Web site request patterns and data update frequencies. A run-time management policy is viewed as a Web page materialization policy that can adapt to different conditions at run-time. We define a concept that we have named the Compromise Factor (CF), to achieve the relationship between current server conditions and the materialization policy. The issue of Web and database data consistency is the driving force behind our approach. In some cases though, we prove that certain compromises to consistency can be beneficial to Web server performance and at the same time be unnoticeable to users. We first present a general a comparative cost model for the hybrid management policy and three other related and popular Web management policies. We then evaluate the performance of all the approaches. The results of our evaluation show that the concept of the CF may be beneficial to Web servers in terms of performance.</content></document><document><year>2004</year><authors>Linus W. Kwong1  | Yiu-Kai Ng1 </authors><title>Performing Binary-Categorization on Multiple-Record Web Documents Using Information Retrieval Models and Application Ontologies</title><content>To retrieve Web documents of interest, most of the Web users rely on Web search engines. All existing search engines provide query facility for users to search for the desired documents using search-engine keywords. However, when a search engine retrieves a long list of Web documents, the user might need to browse through each retrieved document in order to determine which document is of interest. We observe that there are two kinds of problems involved in the retrieval of Web documents: (1) an inappropriate selection of keywords specified by the user; and (2) poor precision in the retrieved Web documents. In solving these problems, we propose an automatic binary-categorization method that is applicable for recognizing multiple-record Web documents of interest, which appear often in advertisement Web pages. Our categorization method uses application ontologies and is based on two information retrieval models, the Vector Space Model (VSM) and the Clustering Model (CM). We analyze and cull Web documents to just those applicable to a particular application ontology. The culling analysis (i) uses CM to find a virtual centroid for the records in a Web document, (ii) computes a vector in a multi-dimensional space for this centroid, and (iii) compares the vector with the predefined ontology vector of the same multi-dimensional space using VSM, which we consider the magnitudes of the vectors, as well as the angle between them. Our experimental results show that we have achieved an average of 90% recall and 97% precision in recognizing Web documents belonged to the same category (i.e., domain of interest). Thus our categorization discards very few documents it should have kept and keeps very few it should have discarded.</content></document><document><year>2004</year><authors>Claus Brabr|1 | Anders M&amp;oslash ller1 | Mikkel Ricky1  | Michael I. Schwartzbach1 </authors><title>PowerForms: Declarative client-side form field validation</title><content>All uses of HTML forms may benefit from validation of the specified input field values. Simple validation matches individual values against specified formats, while more advanced validation may involve interdependencies of form fields. There is currently no standard for specifying or implementing such validation. Today, CGI programmers often use Perl libraries for simple server-side validation or program customized JavaScript solutions for client-side validation. We present PowerForms, which is an add-on to HTML forms that allows a purely declarative specification of input formats and sophisticated interdependencies of form fields. While our work may be seen as inspiration for a future extension of HTML, it is also available for CGI programmers today through a preprocessor that translates a PowerForms document into a combination of standard HTML and JavaScript that works on all combinations of platforms and browsers. The definitions of PowerForms formats are syntactically disjoint from the form itself, which allows a modular development where the form is perhaps automatically generated by other tools and the formats and interdependencies are added separately. PowerForms has a clean semantics defined through a fixed-point process that resolves the interdependencies between all field values. Text fields are equipped with status icons (by default traffic lights) that continuously reflect the validity of the text that has been entered so far, thus providing immediate feed-back for the user. For other GUI components the available options are dynamically filtered to present only the allowed values. PowerForms are integrated into the  system for generating interactive Web services, but is also freely available in an Open Source distribution as a stand-alone package.</content></document><document><year>2004</year><authors>Yi Shang1  | Longzhuang Li1 </authors><title>Precision Evaluation of Search Engines</title><content>In this paper, we present a general approach for statistically evaluating precision of search engines on the Web. Search engines are evaluated in two steps based on a large number of sample queries: (a) computing relevance scores of hits from each search engine, and (b) ranking the search engines based on statistical comparison of the relevance scores. In computing relevance scores of hits, we study four relevance scoring algorithms. Three of them are variations of algorithms widely used in the traditional information retrieval field. They are cover density ranking, Okapi similarity measurement, and vector space model algorithms. In addition, we develop a new three-level scoring algorithm to mimic commonly used manual approaches. In ranking the search engines in terms of precision, we apply a statistical metric called probability of win. In our experiments, six popular search engines, AltaVista, Fast, Google, Go, iWon, and NorthernLight, were evaluated based on queries from two domains of interest: parallel and distributed processing, and knowledge and data engineering. The first query set contains 1726 queries collected from the index terms of papers published in the IEEE Transactions on Knowledge and Data Engineering. The second set contains 1383 queries collected from the index terms of papers published in the IEEE Transactions on Parallel and Distributed Systems. Search engines were queried and compared in two different search modes: the default search mode and the exact phrase search mode. Our experimental results show that these six search engines performed differently under different search modes and scoring methods. Overall, Google was the best. NorthernLight was mostly second in the default search mode, whereas iWon was mostly second in the exact phrase search mode.</content></document><document><year>2004</year><authors>Yi-Hung Wu1  | Arbee L. P. Chen1 </authors><title>Prediction of Web Page Accesses by Proxy Server Log</title><content>As the population of web users grows, the variety of user behaviors on accessing information also grows, which has a great impact on the network utilization. Recently, many efforts have been made to analyze user behaviors on the WWW. In this paper, we represent user behaviors by sequences of consecutive web page accesses, derived from the access log of a proxy server. Moreover, the frequent sequences are discovered and organized as an index. Based on the index, we propose a scheme for predicting user requests and a proxy-based framework for prefetching web pages. We perform experiments on real data. The results show that our approach makes the predictions with a high degree of accuracy with little overhead. In the experiments, the best hit ratio of the prediction achieves 75.69%, while the longest time to make a prediction only requires 2.3 ms.</content></document><document><year>2004</year><authors>Christos Bouras1 | Agisilaos Konidaris1| 2  | Dionysios Kostoulas1 </authors><title>Predictive Prefetching on the Web and Its Potential Impact in the Wide Area</title><content>The rapid increase of World Wide Web users and the development of services with high bandwidth requirements have caused the substantial increase of response times for users on the Internet. Web latency would be significantly reduced, if browser, proxy or Web server software could make predictions about the pages that a user is most likely to request next, while the user is viewing the current page, and prefetch their content.In this paper we study Predictive Prefetching on a totally new Web system architecture. This is a system that provides two levels of caching before information reaches the clients. This work analyses prefetching on a Wide Area Network with the above mentioned characteristics. We first provide a structured overview of predictive prefetching and show its wide applicability to various computer systems. The WAN that we refer to is the GRNET academic network in Greece. We rely on log files collected at the network's Transparent cache (primary caching point), located at GRNET's edge connection to the Internet. We present the parameters that are most important for prefetching on GRNET's architecture and provide preliminary results of an experimental study, quantifying the benefits of prefetching on the WAN. Our experimental study includes the evaluation of two prediction algorithms: an n most popular document algorithm and a variation of the PPM (Prediction by Partial Matching) prediction algorithm. Our analysis clearly shows that Predictive prefetching can improve Web response times inside the GRNET WAN without substantial increase in network traffic due to prefetching.</content></document><document><year>2004</year><authors>Preface</authors><title/></document><document><year>2009</year><authors>Guilherme T. de Assis1 | Alberto H. F. Laender1 | Marcos AndrГ© GonГ§alves1  | Altigran S. da Silva2 </authors><title>A Genre-Aware Approach to Focused Crawling      </title><content>Focused crawlers have as their main goal to crawl Web pages that are relevant to a specific topic or user interest, playing         an important role for a great variety of applications. In general, they work by trying to find and crawl all kinds of pages         deemed as related to an implicitly declared topic. However, users are often not simply interested in any document about a         topic, but instead they may want only documents of a given type or genre on that topic to be retrieved. In this article, we         describe an approach to focused crawling that exploits not only content-related information but also genre information present         in Web pages to guide the crawling process. This approach has been designed to address situations in which the specific topic         of interest can be expressed by specifying two sets of terms, the first describing genre aspects of the desired pages and         the second related to the subject or content of these pages, thus requiring no training or any kind of preprocessing. The         effectiveness, efficiency and scalability of the proposed approach are demonstrated by a set of experiments involving the         crawling of pages related to syllabi of computer science courses, job offers in the computer science field and sale offers         of computer equipments. These experiments show that focused crawlers constructed according to our genre-aware approach achieve         levels of F1 superior to 88%, requiring the analysis of no more than 65% of the visited pages in order to find 90% of the         relevant pages. In addition, we experimentally analyze the impact of term selection on our approach and evaluate a proposed         strategy for semi-automatic generation of such terms. This analysis shows that a small set of terms selected by an expert         or a set of terms specified by a typical user familiar with the topic is usually enough to produce good results and that such         a semi-automatic strategy is very effective in supporting the task of selecting the sets of terms required to guide a crawling         process.      </content></document><document><year>2009</year><authors>Qing Li1 | Jing Chen1  | Yipu Wu1 </authors><title>Algorithm for Extracting Loosely Structured Data Records Through Digging Strict Patterns      </title><content>Extracting loosely structured data records (LSDRs) has wide applications in many domains, such as forum pattern recognition,         Weblogs data analysis, and books and news review analysis. Yet currently existing methods only work well for strongly structured         data records (SDRs). In this paper, we propose to address the problem of extracting LSDRs through mining strict patterns.         In our method, we utilize both content feature and tag tree feature to recognize the LSDRs, and propose a new algorithm to         extract the Data Records (DRs) automatically. The experimental results demonstrate that our algorithm is able to effectively         extract LSDRs with higher precision and recall.      </content></document><document><year>2009</year><authors>Fang Wei1 | Weining Qian2 | Chen Wang3  | Aoying Zhou1| 2 </authors><title>Detecting Overlapping Community Structures in Networks      </title><content>Community structure has been recognized as an important statistical feature of networked systems over the past decade. A lot         of work has been done to discover isolated communities from a network, and the focus was on developing of algorithms with         high quality and good performance. However, there is less work done on the discovery of overlapping community structure, even         though it could better capture the nature of network in some real-world applications. For example, people are always provided         with varying characteristics and interests, and are able to join very different communities in their social network. In this         context, we present a novel overlapping community structures detecting algorithm which first finds the seed sets by the spectral         partition and then extends them with a special random walks technique. At every expansion step, the modularity function Q         is chosen to measure the expansion structures. The function has become one of the popular standards in community detecting         and is defined in Newman and Girvan (Phys. Rev. 69:026113, 2004). We also give a theoretic analysis to the whole expansion process and prove that our algorithm gets the best community structures         greedily. Extensive experiments are conducted in real-world networks with various sizes. The results show that overlapping         is important to find the complete community structures and our method outperforms the C-means in quality.      </content></document><document><year>2009</year><authors>Seung-Kyun Han1| Dongmin Shin2| Jae-Yoon Jung3 | Jonghun Park4 </authors><title>Exploring the Relationship between Keywords and Feed Elements in Blog Post Search      </title><content>Blogs are increasingly accepted as a useful means to proliferate a variety of information on the web. As the popularity of         blogs grows rapidly, a number of blog search engines have appeared recently to help users access and discover blog posts efficiently.         Nevertheless, existing approaches tend to focus on ranking the blog posts according to their recency or popularity only, leaving         the problem of retrieving more topic relevant posts to a user&amp;#8217;s query largely unexplored. In this paper, we present a novel         blog ranking framework, called PTRank, that improves search quality by taking account of relevance feedback from users as         well as various information available from RSS feeds. A neural network method is employed to learn ranking functions that         provide a relevance score between a keyword and a blog post. Extensive experiments on real blog data have been conducted to         validate the proposed ranking framework for blog post search, and the results indicate that PTRank performs significantly         better than the existing popular approach.      </content></document><document><year>2009</year><authors>Xin Yang1 | Peifeng Xiang1  | Yuanchun Shi1 </authors><title>Finding User&amp;#8217;s Interest Blocks using Significant Implicit Evidence for Web Browsing on Small Screen Devices      </title><content>Recent researches on improving the efficiency and user experience of Web browsing on handhelds are seeking to solve the problem         by re-authoring Web pages or making adaptations and recommendations according to user preference. Their basis is a good understanding         of the relationship between user behaviors and user preference. We propose a practical method to find user&amp;#8217;s interest blocks         by machine learning using the combination of significant implicit evidences, which is extracted from four aspects of user         behaviors: display time, viewing information items, scrolling and link selection. We also develop a customized Web browser         for small screen devices to collect user behaviors accurately. For evaluation, we conduct an on-line user study and make statistical         analysis based on the dataset, which shows that most types of the suggested implicit evidences are significant, and viewing         information items is the least indicative aspect of user behaviors. The dataset is then processed off-line to find user&amp;#8217;s         interest blocks using the proposed method. Experimental results demonstrate the effectiveness of finding user&amp;#8217;s interest blocks         by machine learning using the combination of significant implicit evidences. Further analysis reveals the great effect of         users and moderate effect of Websites on the usefulness of significant implicit evidences.      </content></document><document><year>2009</year><authors>Hyunyoung Kil1 | Seog-Chan Oh2 | Ergin Elmacioglu3 | Wonhong Nam1  | Dongwon Lee1 </authors><title>Graph Theoretic Topological Analysis of Web Service Networks      </title><content>Using graph theory, we analyze the topological landscape of web service networks formed by real-world data set, either downloaded         from web service repositories or crawled by a search engine. We first propose a flexible framework to study syntactic web         service matchmaking in a unified manner. Under the framework, then, the data set is analyzed from diverse perspectives and         granularity. By and large, the data set is shown to exhibit small world network well and power-law-like distribution to some         extent. Finally, using random graph theory, we demonstrate how to accurately estimate the size of the giant component of such         web service networks.      </content></document><document><year>2009</year><authors>Karane Vieira1 | AndrГ© Luiz da Costa Carvalho1 | Klessius Berlt1 | Edleno S. de Moura1 | Altigran S. da Silva1  | Juliana Freire2 </authors><title>On Finding Templates on Web Collections      </title><content>Templates are pieces of HTML code common to a set of web pages usually adopted by content providers to enhance the uniformity         of layout and navigation of theirs Web sites. They are usually generated using authoring/publishing tools or by programs that         build HTML pages to publish content from a database. In spite of their usefulness, the content of templates can negatively         affect the quality of results produced by systems that automatically process information available in web sites, such as search         engines, clustering and automatic categorization programs. Further, the information available in templates is redundant and         thus processing and storing such information just once for a set of pages may save computational resources. In this paper,         we present and evaluate methods for detecting templates considering a scenario where multiple templates can be found in a         collection of Web pages. Most of previous work have studied template detection algorithms in a scenario where the collection         has just a single template. The scenario with multiple templates is more realistic and, as it is discussed here, it raises         important questions that may require extensions and adjustments in previously proposed template detection algorithms. We show         how to apply and evaluate two template detection algorithms in this scenario, creating solutions for detecting multiple templates.         The methods studied partitions the input collection into clusters that contain common HTML paths and share a high number of         HTML nodes and then apply a single-template detection procedure over each cluster. We also propose a new algorithm for single         template detection based on a restricted form of bottom-up tree-mapping that requires only small set of pages to correctly         identify a template and which has a worst-case linear complexity. Our experimental results over a representative set of Web         pages show that our approach is efficient and scalable while obtaining accurate results.      </content></document><document><year>2009</year><authors>Eitan Frachtenberg1 </authors><title>Reducing Query Latencies in Web Search Using Fine-Grained Parallelism      </title><content>Semantic Web search is a new application of recent advances in information retrieval (IR), natural language processing, artificial         intelligence, and other fields. The Powerset group in Microsoft develops a semantic search engine that aims to answer queries         not only by matching keywords, but by actually matching meaning in queries to meaning in Web documents. Compared to typical         keyword search, semantic search can pose additional engineering challenges for the back-end and infrastructure designs. Of         these, the main challenge addressed in this paper is how to lower query latencies to acceptable, interactive levels. Index-based         semantic search requires more data processing, such as numerous synonyms, hypernyms, multiple linguistic readings, and other         semantic information, both on queries and in the index. In addition, some of the algorithms can be super-linear, such as matching         co-references across a document. Consequently, many semantic queries can run significantly slower than the same keyword query.         Users, however, have grown to expect Web search engines to provide near-instantaneous results, and a slow search engine could         be deemed unusable even if it provides highly relevant results. It is therefore imperative for any search engine to meet its         users&amp;#8217; interactivity expectations, or risk losing them. Our approach to tackle this challenge is to exploit data parallelism         in slow search queries to reduce their latency in multi-core systems. Although all search engines are designed to exploit         parallelism, at the single-node level this usually translates to throughput-oriented task parallelism. This paper focuses         on the engineering of two latency-oriented approaches (coarse- and fine-grained) and compares them to the task-parallel approach.         We use Powerset&amp;#8217;s deployed search engine to evaluate the various factors that affect parallel performance: workload, overhead,         load balancing, and resource contention. We also discuss heuristics to selectively control the degree of parallelism and consequent         overhead on a query-by-query level. Our experimental results show that using fine-grained parallelism with these dynamic heuristics         can significantly reduce query latencies compared to fixed, coarse-granularity parallelization schemes. Although these results         were obtained on, and optimized for, Powerset&amp;#8217;s semantic search, they can be readily generalized to a wide class of inverted-index         search engines.      </content></document><document><year>2009</year><authors>Shenghua Bao1 | Bohai Yang1 | Ben Fei2 | Shengliang Xu1 | Zhong Su2  | Yong Yu1 </authors><title>Social Propagation: Boosting Social Annotations for Web Mining      </title><content>This paper is concerned with the problem of boosting social annotations using propagation, which is also called social propagation. In particular, we focus on propagating social annotations of web pages (e.g., annotations in Del.icio.us). Social annotations         are novel resources and valuable in many web applications, including web search and browsing. Although they are developing         fast, social annotations of web pages cover only a small proportion (&amp;lt;0.1%) of the World Wide Web. To alleviate the low coverage         of annotations, a general propagation model based on Random Surfer is proposed. Specifically, four steps are included, namely         basic propagation, multiple-annotation propagation, multiple-link-type propagation, and constraint-guided propagation. The         model is evaluated on a dataset of 40,422 web pages randomly sampled from 100 most popular English sites and ten famous academic         sites. Each page&amp;#8217;s annotations are obtained by querying the history interface of Del.icio.us. Experimental results show that         the proposed model is very effective in increasing the coverage of annotations while still preserving novel properties of         social annotations. Applications of propagated annotations on web search and classification further verify the effectiveness         of the model.      </content></document><document><year>2009</year><authors>Takeharu Eda1 | Masatoshi Yoshikawa2 | Toshio Uchiyama1  | Tadasu Uchiyama1 </authors><title>The Effectiveness of Latent Semantic Analysis for Building Up a Bottom-up Taxonomy from Folksonomy Tags      </title><content>In this paper, we evaluate the effectiveness of a semantic smoothing technique to organize folksonomy tags. Folksonomy tags         have no explicit relations and vary because they form uncontrolled vocabulary. We discriminates so-called subjective tags         like &amp;#8220;cool&amp;#8221; and &amp;#8220;fun&amp;#8221; from folksonomy tags without any extra knowledge other than folksonomy triples and use the level of         tag generalization to form the objective tags into a hierarchy. We verify that entropy of folksonomy tags is an effective         measure for discriminating subjective folksonomy tags. Our hierarchical tag allocation method guarantees the number of children         nodes and increases the number of available paths to a target node compared to an existing tree allocation method for folksonomy         tags.      </content></document><document><year>2009</year><authors>Nicoletta Di Blas1 | Franca Garzotto1  | Caterina Poggi2 </authors><title>Web Engineering at the Frontier of the Web 2.0: Design Patterns for Online 3D Shared Spaces      </title><content>Online 3D Shared Spaces (3DSSs) can be regarded as a frontier of the Web 2.0, where users as participants contribute to create a meaningful, engaging experience. Like other complex web applications, the development and evolution         of high-quality 3DSS applications requires methodological support&amp;#8212;through models, methods, and principles. Yet, the application         of structured, engineered approaches to this domain is largely unexplored. The purpose of this paper is to contribute to bridging         Web Engineering to the 3DSS world by means of design patterns. We present five patterns that focus on two factors deemed necessary for effective experiences in a 3DSS: Presence (i.e. the feeling of &amp;#8220;being there&amp;#8221;, typical of &amp;#8220;virtual worlds&amp;#8221;) and Long-Term Engagement (typical of successful Web 2.0 communities). The patterns presented in the paper distil our large-scale experiences with         3DSSs (that have involved so far over 9,000 youngsters from 3 continents) and are discussed in the light of existing literature.      </content></document><document><year>2009</year><authors>Boualem Benatallah1 | Fabio Casati2 | Dimitrios Georgakopoulos3  | Claude Godart4 </authors><title>WISE 2007 Extended Best Papers      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Piyanath Mangkorntong1  | Fethi A. Rabhi2 </authors><title>A Domain-Driven Approach for Detecting Event Patterns in E-Markets      </title><content>An e-market can be thought of as a distributed event system where an event is generated every time the market&amp;#8217;s state changes         in response to a number of human or computing agents. The paper describes a practical application of event processing in an         e-market context through conventional Event Processing Systems (EPSs). A new EPS architecture that allows the integration         of several existing EPSs under a unified domain-specific user interface and execution environment is proposed. We assess the         performance of a prototype system for a case study in financial market surveillance and its ability to provide a common interface         for two existing EPSs&amp;#8211;SMARTS and Coral8. A discussion on the experimental results and the issues arising from the proposed         EPS architecture are also provided.      </content></document><document><year>2008</year><authors>Liu Wenyin1 | Tianyong Hao1 | Wei Chen1  | Min Feng1 </authors><title>A Web-Based Platform for User-Interactive Question-Answering      </title><content>A user-interactive question-answering (QA) platform named BuyAns (at www.buyans.com) is presented. The platform is a special kind of online community and mainly features a rewarding scheme for answering questions         among all users, a pattern-based user interface (UI) for questioning and answering, and a pattern-based representation and         storage scheme for accumulated question-answer pairs. The system actually proposes and promotes a C2C business model for exchanging         and commercializing knowledge from ordinary people. It can also be used as an incentive and collaborative approach to knowledge         acquisition. Driven by the business model, prompt and quality answers are quickly accumulated. Due to the patterns used, accurate         answers can be provided automatically for repeated questions. Facilitating features and technologies, including user modeling,         reputation management, and answer clustering and fusion, are also developed and briefly described. Preliminary user studies         show the potential attraction of the system to its users as well as reasonable usability and user-satisfaction. We anticipate         hot applications of such a system in the Web 2.0 era.      </content></document><document><year>2008</year><authors>Hsuan Pu Chang1| Timothy K. Shih1| Qing Li2 | Chun-Chia Wang3| Te-Hua Wang4 | Louis R. Chao1</authors><title>An Adaptive Caching Strategy for m-Learning Based on SCORM Sequencing and Navigation      </title><content>One of the main problems encountered in the usage of mobile devices as a learning platform is the presence of an impermanent         network environment due to insufficient coverage or link failure in wireless communication. On the other hand, a persistent         connection is usually offered by cellular phones using a telecommunication protocol, but with a relatively weak computing         power and very limited network bandwidth which makes m-learning a time-consuming process. Moreover, learning contents are         currently composed of various multimedia resources that induce long latency to display on handheld devices such as smartphones         with GPRS. Recently, a lot of m-learning systems and contents have conformed to the Sharable Content Object Reference Model         (SCORM) since it was introduced by ADL in the late 90s. The Sequencing and Navigation (S&amp;amp;N) specification is an important         part of SCORM. S&amp;amp;N is defined to prescribe the intended student learning sequence by instructors. In this paper, we propose         an adaptive course caching strategy based on the S&amp;amp;N specification in an m-learning environment. The system automatically         switches to the corresponding course caching strategies, namely, the virtual memory management (VMM) mode and caching on disk         (COD) mode, according to the current networking capability. The proposed mechanism is implemented on an m-learning system&amp;#8212;Pocket         SCORM&amp;#8212;which received the 2005 Brandon Hall Excellence in Learning Award in the USA. Our simulation and experiments demonstrate         that the proposed course caching strategy ultimately reduces the latency during the learning process and decreases the requests         for Internet reconnection.      </content></document><document><year>2008</year><authors>Coral Calero1 | AngГ©lica Caro2  | Mario Piattini1 </authors><title>An Applicable Data Quality Model for Web Portal Data Consumers      </title><content>Web portals have emerged as an important means by which to access data on the worldwide. The people that use these applications         need to ensure that the data recovered is suitable for the task at hand. That is, they need to know the level of quality of         the data obtained. This paper introduces the PoDQA (Portal Data Quality Assessment) tool which implements PDQM, a Portal Data         Quality Model, which is centered upon the data consumer perspective. Thus, the measurement of data quality is carried out         by using the point of view of data consumers. Our work aims to fill the lack of specific proposals for the DQ evaluation in         Web portals and tools that put these proposals into practice. The paper illustrate how PoDQA tool works and how it can be         used by data consumers in order to, for example, discover the data quality of a specific web portal. PoDQA also suggests several         corrective maintenance activities for users who are interested in the improvement of the data quality of their Web portals.      </content></document><document><year>2008</year><authors>Wenbin Li1| 2| Ning Zhong2| 3 | Yiyu Yao2| 4 | Jiming Liu2| 5</authors><title>An Operable Email Based Intelligent Personal Assistant      </title><content>The recent phenomena of email-function-overloading and email-centricness in daily life and business have created new problems         to users. There is a practical need for developing a software assistant to facilitate the management of personal and organizational         emails, and to enable users to complete their email-centric jobs or tasks smoothly. This paper presents the status, goals,         and key technical elements of an Email-Centric Intelligent Personal Assistant, called ECIPA. ECIPA provides various assisting         functions, including automated and cost-sensitive spam filtering based on corresponding analysis, ontology-mediated email         classification, query and archiving. ECIPA can learn from dynamic user behaviors to effectively sort and automatically respond         email. Techniques developed in Web Intelligence (WI) are adopted to implement ECIPA. In order to facilitate cooperation of         ECIPAs of different users, the concept of operable email, an extension of traditional email with an operable form, is introduced. ECIPA can in fact be viewed as a family of collaborative agents working together on the operable email.      </content></document><document><year>2008</year><authors>Saikat Mukherjee1  | I. V. Ramakrishnan2 </authors><title>Automated Semantic Analysis of Schematic Data      </title><content>Content in numerous Web data sources, designed primarily for human consumption, are not directly amenable to machine processing.         Automated semantic analysis of such content facilitates their transformation into machine-processable and richly structured         semantically annotated data. This paper describes a learning-based technique for semantic analysis of schematic data which         are characterized by being template-generated from backend databases. Starting with a seed set of hand-labeled instances of         semantic concepts in a set of Web pages, the technique learns statistical models of these concepts using light-weight content         features. These models direct the annotation of diverse Web pages possessing similar content semantics. The principles behind         the technique find application in information retrieval and extraction problems. Focused Web browsing activities require only         selective fragments of particular Web pages but are often performed using bookmarks which fetch the contents of the entire         page. This results in information overload for users of constrained interaction modality devices such as small-screen handheld         devices. Fine-grained information extraction from Web pages, which are typically performed using page specific and syntactic         expressions known as wrappers, suffer from lack of scalability and robustness. We report on the application of our technique         in developing semantic bookmarks for retrieving targeted browsing content and semantic wrappers for robust and scalable information         extraction from Web pages sharing a semantic domain.      </content></document><document><year>2008</year><authors>Osamu Masutani1  | Hirotoshi Iwasaki1 </authors><title>BEIRA: An Area-based User Interface for Map Services      </title><content>This paper introduces BEIRA, an area-based map user interface for location-based contents. Recently, various web map services         are widely used to search for location-based contents. However, browsing a large number of contents that are arranged on a         map as points may be troublesome. We tackle this issue by using area-based representations instead of points. AOI (Area of         Interest), which is core concept of BEIRA, is an arbitrary shaped area boundary with text summary information. With AOI, users         can instantly grasp area characteristics without examining each point. AOI is deduced by performing geo-semantic co-clustering         of location-based contents. Geo-semantic co-clustering takes both geographic and semantic features of contents into account.         We confirm that the ratio of the geo-semantic blend is the key to deducing an appropriate boundary. We further propose and         evaluate location-aware term weighting to obtain an informative summary.      </content></document><document><year>2008</year><authors>Hongzhi Wang1 | Jianzhong Li1 | Wei Wang2  | Xuemin Lin2 </authors><title>Coding-based Join Algorithms for Structural Queries on Graph-Structured XML Document      </title><content>In many applications, XML documents need to be modelled as graphs. The query processing of graph-structured XML documents         brings new challenges. In this paper, we design a method based on labelling scheme for structural queries processing on graph-structured         XML documents. We give each node some labels, the reachability labelling scheme. By extending an interval-based reachability         labelling scheme for DAG by Rakesh et al., we design labelling schemes to support the judgements of reachability relationships         for general graphs. Based on the labelling schemes, we design graph structural join algorithms to answer the structural queries         with only ancestor-descendant relationship efficiently. For the processing of subgraph query, we design a subgraph join algorithm.         With efficient data structure, the subgraph join algorithm can process subgraph queries with various structures efficiently.         Experimental results show that our algorithms have good performance and scalability.      </content></document><document><year>2008</year><authors>Klaus-Dieter Schewe1 | Bernhard Thalheim2  | Qing Wang1 </authors><title>Customising Web Information Systems According to User Preferences      </title><content>Web Information Systems have to serve a variety of users with very diverse preferences regarding content, functionality and         presentation. We first investigate the customisation of functionality at a high-level of abstraction, where possible action         sequences are represented by an algebraic expression called plot, and user preferences give rise to equations. We show that         the problem can be solved by applying conditional term rewriting on the basis of Kleene algebras with tests. By exploiting         the idea of weakest preconditions such expressions can be represented by formal power series with coefficients in a Boolean         algebra. This gives rise to a sufficient condition for termination based on well-founded orders on such power series. As confluence         cannot be guaranteed, we propose critical pair completion to be used in order to enforce the desirable Church-Rosser property.         In a second step we parametrise the actions and replace the Boolean conditions by first-order formulae. We show that still         term rewriting can be applied, but termination and Church Rosser property become problems that will require manual interaction,         in particular, as preference rules will make use of the parameters. On the other hand the presence of first-order conditions         can be used to extend the customisation to the content.      </content></document><document><year>2008</year><authors>Devis Bianchini1 | Valeria De Antonellis1  | Michele Melchiori1 </authors><title>Flexible Semantic-Based Service Matchmaking and Discovery      </title><content>Automated techniques and tools are required to effectively locate services that fulfill a given user request in a mobility         context. To this purpose, the use of semantic descriptions of services has been widely motivated and recommended for automated         service discovery under highly dynamic and context-dependent requirements. Our aim in this work is to propose an ontology-based         hybrid approach where different kinds of matchmaking strategies are combined together to provide an adaptive, flexible and         efficient service discovery environment. The approach, in particular, exploits the semantic knowledge about the business domain         provided by a domain ontology underlying service descriptions, and the semantic organization of services in a service ontology,         at different levels of abstraction.      </content></document><document><year>2008</year><authors>Guoren Wang1 | Bo Ning1 | Ge Yu1</authors><title>Holistically Stream-based Processing Xtwig Queries      </title><content>Unlike a twig query, an Xtwig query contains some selection predicates with reverse axes which are either ancestor or parent. To evaluate such queries in the stream-based context, some rewriting rules have been proposed to transform the paths with         reverse axes into equivalent reverse-axis-free ones. However, the transformation method is expensive due to multiple scanning         input streams and the generation of unnecessary intermediate results. To solve these problems, a holistic stream-based algorithm         XtwigStack is proposed for Xtwig queries. Experiments show that XtwigStack is much more efficient than the transformation method.      </content></document><document><year>2008</year><authors>Jonathan Beaver1 | Kirk Pruhs1 | Panos K. Chrysanthis1  | Vincenzo Liberatore2 </authors><title>Improving the Hybrid Data Dissemination Model of Web Documents      </title><content>One of the major problems in the Internet today is the scalable delivery of data. With more and more people joining the Internet         community, web servers and services are being forced to deal with workloads beyond their original data dissemination design         capacity. One solution that has arisen to address scalability is to use multicasting, or push-based data dissemination, to         send out data to many clients at once. More recently, the idea of using multicasting as part of a hybrid system with unicasting         has shown positive results in increasing server scalability. In this paper we focus on solving problems associated with the         hybrid dissemination model. In particular, we address the issues of document popularity and document division while arguing for the use of a third channel, called the multicast pull channel, in the hybrid system model. This channel improves performance in terms of response time while improving the robustness of         the hybrid system. We show through extensive simulation using our working hybrid server the usefulness of this additional         channel and its improving effects in creating a more scalable and more efficient web server.      </content></document><document><year>2008</year><authors>Junhu Wang1 | Jeffrey Xu Yu2  | Chengfei Liu3 </authors><title>Independence of Containing Patterns Property and Its Application in Tree Pattern Query Rewriting Using Views      </title><content>We show that several classes of tree patterns observe the independence of containing patterns property, that is, if a pattern is contained in the union of several patterns, then it is contained in one of them. We apply         this property to two related problems on tree pattern rewriting using views. First, given view V and query Q, is it possible for Q to have an equivalent rewriting using V which is the union of two or more tree patterns, but not an equivalent rewriting which is a single pattern? This problem         is of both theoretical and practical importance because, if the answer is no, then, to find an equivalent rewriting of a tree         pattern using a view, we should use more efficient methods, such as the polynomial time algorithm of Xu and Г–zsoyoglu (2005), rather than try to find the union of all contained rewritings (which takes exponential time in the worst case) and test         its equivalence to Q. Second, given a set S of views, we want to know under what conditions a subset S&amp;#8242; of S is redundant in the sense that for any query         Q, the contained rewritings of Q using the views in S&amp;#8242; are contained in those using the views in S&amp;#8201;&amp;#8722;&amp;#8201;S&amp;#8242;. Solving this problem can help us to, for example, choose the minimum number of views to be cached, or better design the         virtual schema in a mediated data integration system, or avoid repeated calculation in query optimization. For the first problem,         we identify several classes of tree patterns for which the equivalent rewriting can be expressed as a single tree pattern.         For the second problem, we present necessary and sufficient conditions for S&amp;#8242; to be redundant with respect to some classes of tree patterns. For both problems we consider extension to cases where there         are rewritings using the intersection of multiple views and/or where a schema graph is present.      </content></document><document><year>2008</year><authors>JosГ© A. MacГ­as1 </authors><title>Intelligent Assistance in Authoring Dynamically Generated Web Interfaces      </title><content>Since its emergence in the early 1990s, the WWW has become not only an information system of unprecedented size, but a universal         platform for the development of services and applications. However, most of the advances in web technologies are intended         for professional developers, paying poor attention to end-users with no programming abilities but with explicit needs of creating         and customizing web-based presentations. This provides a strong motivation for end-users to act as designers at some point,         leading to an emerging role of new computing-related professionals to be considered. This paper is an effort to leverage such         difficulties by providing intelligent mechanism to assist end-users in web-based authoring tasks. To carry out such a challenge,         intelligent user-monitoring techniques are exploited to obtain high-level information that will be used to infer the user&amp;#8217;s         preferences and assist him throughout the interaction. Furthermore, we report on how iteration patterns can be applied to avoid repetitive tasks that are automatically carried out on behalf of the user. In order to bring off         a feasible trade-off between expressivity and ease of use, a user experiment to obtain the user&amp;#8217;s perception and evaluate         the hit-rate of our system is also presented.      </content></document><document><year>2008</year><authors>Qing Li1 | Jianmin Zhao1  | Xinzhong Zhu1 </authors><title>Multimedia Data Modeling Through a Semantic View Mechanism      </title><content>The semantics of multimedia data, which features context-dependency and media-independency, is of vital importance to multimedia         applications but inadequately supported by the state-of-the-art database technology. In this paper, we address this problem         by proposing MediaView as an extended object-oriented view mechanism to bridge the &amp;#8220;semantic gap&amp;#8221; between conventional databases and semantics-intensive         multimedia applications. This mechanism captures the dynamic semantics of multimedia using a modelling construct named media view, which formulates a customized context where heterogeneous media objects with similar/related semantics are characterized by additional properties and user-defined semantic relationships. Due to the complex ingredients and dynamic application requirements of multimedia databases, it is however difficult for         users to define by themselves individual media views in a top&amp;#8211;down fashion. To this end, a unique approach of constructing media views logically is devised. In addition, a set         of user level operators is defined and implemented to accommodate the specialization and generalization relationships among         the views. The usefulness and elegancy of MediaView are demonstrated by its application in a multi-modal information retrieval system.      </content></document><document><year>2008</year><authors>Bo Hu1  | Bin Hu2 </authors><title>On Capturing Semantics in Ontology Mapping      </title><content>Semantic interoperability between disparate systems in open, distributed environments has become the quest of many practitioners         in a variety of fields. One way to achieve such a goal is through ontology mapping. The perspective users of such technology,         however, are faced with a number of challenges including ambiguity of the meaning of mappings, difficulties of capturing semantics,         choice of the right ontology mapping tools, verification and validation of results and operationalisation in the beneficiary         semantic web application. In this paper we present a formalisation of ontologies and a triangle model for the ontology mapping         problems. This formalisation of ontology mapping reflects the engineering steps needed to materialise a versatile mapping         system in order to faithfully re-capture the semantics embodied in ontologies which is the fundamental requirements posed         by the semantic web environment. We further accommodate this formalisation with a series of specialist algorithms targeting         at particular aspects of semantic capturing. Finally, we evaluated the proposed algorithms by way of ontology mapping benchmark         tests.      </content></document><document><year>2008</year><authors>Guoren Wang1 | Huan Huo1| Donghong Han1 | Xiaoyun Hui1</authors><title>Query Processing and Optimization Techniques over Streamed Fragmented XML      </title><content>With the extensive use of XML in applications over the Web, efficient query processing over streaming XML has become a core         challenge due to one-pass processing and limited resources. Taking advantage of Hole-Filler model for XML fragments, this         paper proposes a hybrid structure (FQ-Index) for both the queries and fragments, and proposes an XML fragment processing algorithm to evaluate forward XPath queries         over streamed XML fragments. Two optimization rules, dependence pruning and prefix pruning are also developed. Dependence pruning scheme prunes off the dependent operations caused by fragmentation and transforms the queries for XML tag into queries for XML fragments,         while prefix pruning scheme prunes off the &amp;#8220;redundant&amp;#8221; prefix along the path according to the tag structure. The effectiveness of the techniques developed         is illustrated with a detailed set of experiments.      </content></document><document><year>2008</year><authors>Zaki Malik1  | Athman Bouguettaya2 </authors><title>Rater Credibility Assessment in Web Services Interactions      </title><content>We investigate the problem of establishing trust in service-oriented environments. The focus is on providing an infrastructure         for evaluating the credibility of raters in a reputation-based framework that would enable trust-based Web services interactions.         The techniques we develop aid a service consumer in assigning an appropriate weight to the testimonies of different raters         regarding a prospective service provider. The experimental analysis show that the proposed techniques successfully dilute         the effects of malicious ratings, thereby facilitating accurate and fair assessment of the providers&amp;#8217; reputations.      </content></document><document><year>2008</year><authors>Saeid Asadi1 | Xiaofang Zhou1  | Guowei Yang2 </authors><title>Using Local Popularity of Web Resources for Geo-Ranking of Search Engine Results      </title><content>Search engines retrieve and rank Web pages which are not only relevant to a query but also important or popular for the users.         This popularity has been studied by analysis of the links between Web resources. Link-based page ranking models such as PageRank         and HITS assign a global weight to each page regardless of its location. This popularity measurement has shown successful         on general search engines. However unlike general search engines, location-based search engines should retrieve and rank higher         the pages which are more popular locally. The best results for a location-based query are those which are not only relevant         to the topic but also popular with or cited by local users. Current ranking models are often less effective for these queries         since they are unable to estimate the local popularity. We offer a model for calculating the local popularity of Web resources         using back link locations. Our model automatically assigns correct locations to the links and content and uses them to calculate         new geo-rank scores for each page. The experiments show more accurate geo-ranking of search engine results when this model         is used for processing location-based queries.      </content></document><document><year>2006</year><authors>L. Ardissono1 | A. Goy1 | R. Meo1| G. Petrone1| L. Console1| L. Lesmo1| C. Simone1 | P. Torasso1</authors><title>A configurable system for the construction of adaptive virtual stores      </title><content>With the recent expansion of the Internet, the interest towards electronic sales has quickly grown and many tools have been         built to help vendors to set up their Web stores. These tools offer all the facilities for building the store databases and         managing the order processing and secure payment transactions, but they typically do not focus on issues like the personalization         of the interaction with the customers. However, Web surfers are generally heterogeneous and have different needs and preferences;         moreover, the trend of marketing strategies is to pay more and more attention to the specific buyers. So, the importance of         personalizing the interaction with the user and the product presentation is increasing. In this paper, we describe the architecture         of a configurable virtual Web store supporting personalized hypertextual interactions with users. Our system builds a user         profile by applying user modeling techniques and stereotypical information about the characteristics of customer groups; this         profile is used during the interaction in order to tailor the product descriptions and the selection of items to recommend         to the user's needs, varying the layout of the hypertextual pages and the detail of the descriptions accordingly. Tailoring         the system's behavior requires the parallel execution of several complex tasks during the interaction (e.g., identifying the         user's preferences, selecting the products most suited to her, dynamically generating the hypertextual pages). Therefore,         we have defined a multiagent architecture where these tasks are executed by different agents, which cooperate offering specific         services to each other. In our system, the domain&amp;#8208;dependent knowledge, concerning information about products and customer         features, is declaratively represented and clearly separated from the domain&amp;#8208;independent components, which represent the core         of the virtual store. This separation has the advantage that our architecture can be easily instantiated on several sales         domains, therefore obtaining different Web stores out of a single shell. Our system is developed in a Java&amp;#8208;based environment         and the overall architecture includes the prototype of a virtual store and the configuration tools which can be used to set         up a new store on a specific sales domain.      </content></document><document><year>2006</year><authors>John Zakos1  | Brijesh Verma2 </authors><title>A Novel Context-based Technique for Web Information Retrieval      </title><content>In this paper we present context matching, a novel context-based technique for the ad-hoc retrieval of web documents. The aim of the technique is to dynamically generate         a measure of document term significance during retrieval that can be used as a substitute or co-contributor of the term frequency         measure. Unlike term frequency, which relies on a term occurring multiple times in a document to be considered significant,         context matching is based on the notion that if a term in a given document occurs in that document in the context of the query,         then that term is deemed to be significant. Context matching has the ability to potentially determine a term to be significant         even if it occurs only once in a document. Vice versa, it also has the ability to determine a term to be insignificant, even         if occurs frequently within a document. We show how expanded terms generated by a typical query expansion technique can be         used effectively as query context for context matching. The technique is ideally suited to the nature of web information retrieval         and we show how context matching significantly improves retrieval accuracy through experimental results on TREC web benchmark         data.      </content></document><document><year>2006</year><authors>Alex Rousskov1  | Valery Soloviev2 </authors><title>A performance study of the Squid proxy on HTTP/1.0      </title><content>This paper presents a performance study of the state&amp;#8208;of&amp;#8208;the&amp;#8208;art caching proxy called Squid. We instrumented Squid to measure         per request network and disk activities and conducted a series of experiments on large Web caches. We have discovered many         interesting and consistent patterns across a wide variety of environments. Our data and analysis are essential for understanding,         modeling, benchmarking, and tuning performance of a proxy server.      </content></document><document><year>2006</year><authors>Raffaella Grieco1 | Delfina Mal|rino1  | Vittorio Scarano1 </authors><title>A Scalable Cluster-based Infrastructure for Edge-computing Services      </title><content>In this paper we present a scalable and dynamic intermediary infrastructure, SEcS (acronym of &amp;#8220;Scalable Edge computing Services&amp;#8221;),         for developing and deploying advanced Edge computing services, by using a cluster of heterogeneous machines. Our goal is to address the challenges of the next-generation         Internet services: scalability, high availability, fault-tolerance and robustness, as well as programmability and quick prototyping.         The system is written in Java and is based on IBM's Web Based Intermediaries (WBI) [71] developed at IBM Almaden Research Center.      </content></document><document><year>2006</year><authors>GГјnter Preuner1  | Michael Schrefl2</authors><title>A three-level schema architecture for the conceptual design of web-based information systems: from web-data management to         integrated web-data and web-process management      </title><content>It has been recognized only recently that, like databases, web sites need models and schemes. Data-intensive web sites are         best developed using a multi-level design approach proceeding from data design via navigation design to web-page design. Modern         web-based information systems are no longer static in nature. Rather they are dynamic. Besides querying, they support workflow         tasks and e-commerce transactions. The design of such systems needs to consider the underlying business process next to the         data. Their integrated design has been mainly treated in an ad-hoc way so far. In this paper, we present a three-level schema         architecture for the conceptual design of dynamic web-based information systems. We employ an object-oriented approach that         integrates data and process management and complements previous approaches for the design of data-intensive web sites.      </content></document><document><year>2006</year><authors>Kia Ming Phua| Siu Cheung Hui | Chai Kiat Yeo</authors><title>A web-based Internet Java Phone for real-time voice communication      </title><content>Although a wide range of Internet telephony applications such as VocalTec's Internet Phone and Microsoft's NetMeeting has         been developed to support real-time voice communication over the Internet, they are predominantly written for the Windows         95/NT platform and are not compatible with other operating systems. Moreover, most of the Internet telephony software is operated         as stand-alone applications and must be downloaded and installed prior to operation. This has caused great inconvenience in         using Internet telephony software. To resolve this, a web-based Internet Java Phone (or IJPhone) which can be downloaded from         the Internet and run from standard Java-compliant web browser, is proposed in this paper. The IJPhone system consists of two         main components: the Web-based Telephone Exchange for user connection and Internet Java Phone Applet for establishing real-time         Internet voice communication. In addition, as Java applets have certain security restrictions placed on them, the paper also         discusses the method proposed to overcome them.      </content></document><document><year>2006</year><authors>Aoying Zhou1 | Linhao Xu1  | Chenyun Dai1 </authors><title>Adaptive Probabilistic Search Over Unstructured Peer-to-Peer Computing Systems      </title><content>A challenging problem that confronts unstructured peer-to-peer (P2P) computing systems is how to provide efficient support         to locate desired files. This paper addresses this problem by using some quantitative information in the form of probabilistic         knowledge. Two types of probabilistic knowledge are considered in this paper: overlap between topics shared in the network         and coverage of topics at each individual peer. Based on the probabilistic knowledge, this paper proposes an adaptive probabilistic         search algorithm that can efficiently support file locating operation in the unstructured P2P network. Then, an update algorithm         is devised to keep the freshness of the probabilistic knowledge of individual peers by taking advantage of feedback from the         previous user queries. Finally, some extensive experiments are conducted to evaluate the efficiency and effectiveness of the         proposed method.      </content></document><document><year>2006</year><authors>Ana G. Maguitman1| 2 | Filippo Menczer1| 2 | Fulya Erdinc1 | Heather Roinestad1  | Aless|ro Vespignani2 </authors><title>Algorithmic Computation and Approximation of Semantic Similarity      </title><content>Automatic extraction of semantic information from text and links in Web pages is key to improving the quality of search results.         However, the assessment of automatic semantic measures is limited by the coverage of user studies, which do not scale with         the size, heterogeneity, and growth of the Web. Here we propose to leverage human-generated metadata&amp;#8212;namely topical directories&amp;#8212;to         measure semantic relationships among massive numbers of pairs of Web pages or topics. The Open Directory Project classifies         millions of URLs in a topical ontology, providing a rich source from which semantic relationships between Web pages can be         derived. While semantic similarity measures based on taxonomies (trees) are well studied, the design of well-founded similarity         measures for objects stored in the nodes of arbitrary ontologies (graphs) is an open problem. This paper defines an information-theoretic         measure of semantic similarity that exploits both the hierarchical and non-hierarchical structure of an ontology. An experimental         study shows that this measure improves significantly on the traditional taxonomy-based approach. This novel measure allows         us to address the general question of how text and link analyses can be combined to derive measures of relevance that are         in good agreement with semantic similarity. Surprisingly, the traditional use of text similarity turns out to be ineffective         for relevance ranking.      </content></document><document><year>2006</year><authors>M. Hossein Sheikh Attar1  | M. Tamer Г–zsu1 </authors><title>Alternative Architectures and Protocols for Providing Strong Consistency in Dynamic Web Applications      </title><content>Dynamic Web applications have gained a great deal of popularity. Improving the performance of these applications has recently         attracted the attention of many researchers. One of the most important techniques proposed for this purpose is caching, which         can be done at different locations and within different stages of the process of generating a dynamic Web page. Most of the         caching schemes proposed in literature are lenient about the issue of consistency; they assume that users can tolerate receiving         stale data. However, an important class of dynamic Web applications are those in which users always expect to get the freshest         data available. Any caching scheme has to incur a significant overhead to be able to provide this level of consistency (i.e.,         strong consistency); the overhead may be so much that it neutralizes the benefits of caching. In this paper, three alternative         architectures are investigated for dynamic Web applications that require strong consistency. A proxy caching scheme is designed         and implemented, which performs caching at the level of database queries. This caching system is used in one of the alternative         architectures. The performance experiments show that, despite the high overhead of providing strong consistency in database         caching, this technique can improve the performance of dynamic Web applications, especially when there is a long network latency         between clients and the (origin) server.      </content></document><document><year>2006</year><authors>Sougata Mukherjea1 | Kyoji Hirata1 | Yoshinori Hara1</authors><title>AMORE: A World Wide Web image retrieval engine      </title><content>Search engines are useful because they allow the user to find information of interest from the World Wide Web. However, most         of the popular search engines today are textual; they do not allow the user to find images from the Web. This paper describes         AMORE, a Web search engine that allows the user to retrieve images from the Web by specifying relevant keywords or a similar         image. Text and image search can also be combined. Moreover, we have developed a Query Result Visualization Environment that         allows the organization of the results if many images are retrieved. In this paper we present AMORE's user interface and explain         the technique for retrieving images visually similar to a user specified image. The method of automatically assigning relevant         keywords to the images is then explained. Finally, the architecture of the system as well as some interesting observations         of our experiences with AMORE are discussed.      </content></document><document><year>2006</year><authors>Arun K. Iyengar1 | Mark S. Squillante1 | Li Zhang1</authors><title>Analysis and characterization of large&amp;#8208;scale Web server access patterns and performance      </title><content>In this paper we develop a general methodology for characterizing the access patterns of Web server requests based on a time&amp;#8208;series         analysis of finite collections of observed data from real systems. Our approach is used together with the access logs from         the IBM Web site for the Olympic Games to demonstrate some of its advantages over previous methods and to construct a particular         class of benchmarks for large&amp;#8208;scale heavily&amp;#8208;accessed Web server environments. We then apply an instance of this class of benchmarks         to analyze aspects of large&amp;#8208;scale Web server performance, demonstrating some additional problems with methods commonly used         to evaluate Web server performance at different request traffic intensities.      </content></document><document><year>2006</year><authors>Kam-Fai Wong1 | Jeffrey Xu Yu1  | Nan Tang1 </authors><title>Answering XML Queries Using Path-Based Indexes: A Survey      </title><content>The problem of answering XML queries using path-based indexes is to find efficient methods for accelerating the XML query         with pre-designed index structures over the XML database. This problem received increasing interests and have been lucubrated         in recent years. Regular path expression is the core of the XML query languages e.g., XPath and XQuery. Most of the state-of-the-art         path-based XML indexes, therefore, hammer at how to efficiently answer the path-based XML queries. This paper surveys various         approaches to indexing XML data proposed in the literature. We give a step by step analysis to show the evolution of index         structures for XML path information, based on tree structures or more commonly, directed labeled graphs. For each approach,         we first present the specific issue it aims to tackle, and then the proposed solution presented. Furthermore, construction,         physical data storage and maintenance costs, are analyzed.      </content></document><document><year>2006</year><authors>Lars Eggert1  | John Heidemann1</authors><title>Application&amp;#8208;level differentiated services for Web servers      </title><content>The current World Wide Web service model treats all requests equivalently, both while being processed by servers and while         being transmitted over the network. For some uses, such as Web prefetching or multiple priority schemes, different levels         of service are desirable. This paper presents three simple, server&amp;#8208;side, application&amp;#8208;level mechanisms (limiting process pool         size, lowering process priorities, limiting transmission rate) to provide two different levels of Web service (regular and         low priority). We evaluated the performance of these mechanisms under combinations of two foreground workloads (light and         heavy) and two levels of available network bandwidth (10 Mb/s and 100 Mb/s). Our experiments show that even with background         traffic sufficient to saturate the network, foreground performance is reduced by at most 4&amp;#8211;17%. Thus, our user&amp;#8208;level mechanisms         can effectively provide different service classes even in the absence of operating system and network support.      </content></document><document><year>2006</year><authors>Paul Barford1| Azer Bestavros1| Adam Bradley1 | Mark Crovella1</authors><title>Changes in Web client access patterns: Characteristics and caching implications      </title><content>Understanding the nature of the workloads and system demands created by users of the World Wide Web is crucial to properly         designing and provisioning Web services. Previous measurements of Web client workloads have been shown to exhibit a number         of characteristic features; however, it is not clear how those features may be changing with time. In this study we compare         two measurements of Web client workloads separated in time by three years, both captured from the same computing facility         at Boston University. The older dataset, obtained in 1995, is well known in the research literature and has been the basis         for a wide variety of studies. The newer dataset was captured in 1998 and is comparable in size to the older dataset. The         new dataset has the drawback that the collection of users measured may no longer be representative of general Web users; however,         using it has the advantage that many comparisons can be drawn more clearly than would be possible using a new, different source         of measurement. Our results fall into two categories. First we compare the statistical and distributional properties of Web         requests across the two datasets. This serves to reinforce and deepen our understanding of the characteristic statistical         properties of Web client requests. We find that the kinds of distributions that best describe document sizes have not changed         between 1995 and 1998, although specific values of the distributional parameters are different. Second, we explore the question         of how the observed differences in the properties of Web client requests, particularly the popularity and temporal locality         properties, affect the potential for Web file caching in the network. We find that for the computing facility represented         by our traces between 1995 and 1998, (1) the benefits of using size&amp;#8208;based caching policies have diminished; and (2) the potential         for caching requested files in the network has declined.      </content></document><document><year>2006</year><authors>Leeann Bent1 | Michael Rabinovich2 | Geoffrey M. Voelker1  | Zhen Xiao3 </authors><title>Characterization of a Large Web Site Population with Implications for Content Delivery      </title><content>This paper presents a systematic study of the properties of a large number of Web sites hosted by a major ISP. To our knowledge,         ours is the first comprehensive study of a large server farm that contains thousands of commercial Web sites. We also perform         a simulation analysis to estimate potential performance benefits of content delivery networks (CDNs) for these Web sites,         and validate our analysis for several sites by replaying our trace through a real cache. We make several interesting observations         about the current usage of Web technologies and Web site performance characteristics. First, compared with previous client         workload studies, the Web server farm workload contains a much higher degree of uncacheable responses and responses that require         mandatory cache validations. A significant reason for this is that cookie use is prevalent among our population, especially         among more popular sites. We found an indication of widespread indiscriminate usage of cookies, which unnecessarily impedes         the use of many content delivery optimizations. We also found that most Web sites do not utilize the cache-control features         of the HTTP 1.1 protocol, resulting in suboptimal performance. Moreover, the implicit expiration time in client caches for         responses is strongly constrained by the maximum values allowed in the Squid proxy. Thus, supplying explicit expiration information         would significantly improve Web sites&amp;#8217; cacheability. Finally, our simulation results indicate that while most Web sites benefit         from the use of a CDN, the amount of the benefit varies widely among the sites, which underscores the need for workload analysis         tools.      </content></document><document><year>2006</year><authors>Olga De Troyer1  | Tom Decruyenaere2 </authors><title>Conceptual modelling of web sites for end-users      </title><content>Internet and the WWW more and more play an important role in our information society. It is now one of the major sources of         information in every rank of our society. The overwhelming accessibility to data, on a global scale, does not necessarily         translate to widespread utility of data. We often find that we are drowning in data, with few tools to help managing relevant         data for our various activities. In this paper, we argue that the WWW and its end-users could benefit from the existence of         a conceptual web site schema. We propose such a conceptual web site schema that describes what information is available in         a web site and how this information is structured into pages and links. To allow to communicate this information through the         web, we developed an XML Document Type Definition (DTD) for this conceptual web site schema. We also illustrate the feasibility         of the approach by a simple application program developed using the XML Document Object Model (DOM).      </content></document><document><year>2006</year><authors>Chengfei Liu1 | Millist W. Vincent2  | Jixue Liu2 </authors><title>Constraint Preserving Transformation from Relational Schema to XML Schema      </title><content>XML has become the standard for publishing and exchanging data on the Web. However, most business data is managed and will         remain to be managed by relational database management systems. As such, there is an increasing need to efficiently and accurately         publish relational data as XML documents for Internet-based applications. One way to publish relational data is to provide         virtual XML documents for relational data via an XML schema which is transformed from the underlying relational database schema         such that users can access the relational database through the XML schema. In this paper, we discuss issues in transforming         a relational database schema into the corresponding XML schema. We aim to preserve all integrity constraints defined in a         relational database schema, to achieve high level of nesting and to avoid introducing data redundancy in the transformed XML         schema. In the paper, we first propose a basic transformation algorithm which introduces no data redundancy, then we improve         the algorithm by exploring further nesting of the transformed XML schema.      </content></document><document><year>2006</year><authors>Sourav S. Bhowmick1 | Sanjay Madria2 | Wee-Keong Ng1  | Ee-Peng Lim1 </authors><title>Cost-benefit analysis of web bag in a web warehouse: An analytical approach      </title><content>Sets and bags are closely related structures and have been studied in relational databases. A bag is different from a set         in that it is sensitive to the number of times an element occurs while a set is not. In this paper, we introduce the concept         of web bag in the context of a web warehouse called Whoweda (Warehouse         Of Weda Data) which we are currently building. Informally, a web bag is a web table which allows multiple occurrences of identical web tuples. Web bag helps to discover useful knowledge from a web table such as visible documents (or web sites), luminous documents and luminous paths. In this paper, we perform a cost-benefit analysis with respect to storage, transmission and operational cost of web bags         and discussed issues and implication of materializing web bags as opposed to web tables containing distinct web tuples. We         have computed analytically the upper and lower bounds for the parameters which affect the cost of materializing web bags.      </content></document><document><year>2006</year><authors>Jesse S. Jin | Ruth Kurniawati</authors><title>Cyberbroker: A lightweight web object collector      </title><content>Centralized heavyweight collectors have been used to collect text-based web objects for building search engines. However,         these collectors do not meet the need of collecting big web objects such as images and videos. We propose a lightweight mobile         on-call object collector for the world wide web. Using this approach, we can deploy a collector easily on the site being indexed.         We present a prototype system consisting of mobile Java collectors and search engines for collecting HTML/image objects. We         provide analysis to find a lower bound in terms of total data transfer time required. This idealized scenario is complemented         with another model based on implementation of the HTTP over TCP. The lower bound and the upper bound of the gains accrued         are then validated using experiments.      </content></document><document><year>2006</year><authors>Maya Ramanath1  | Jayant R. Haritsa1| 2</authors><title>DIASPORA: A highly distributed web-query processing system      </title><content>Current proposals for web querying systems have assumed a centralized processing architecture wherein data is shipped from         the remote sites to the user's site. We present here the design and implementation of DIASPORA, a highly distributed query         processing system for the web. It is based on the premise that several web applications are more naturally processed in a         distributed manner, opening up possibilities of significant reductions in network traffic and user response times. DIASPORA         is built over an expressive graph-based data model that utilizes simple heuristics and lends itself to automatic generation.         The model captures both the content of web documents and the hyperlink structural framework of a web site. Distributed queries         on the model are expressed through a declarative language that permits users to explicitly specify navigation. DIASPORA implements         a query-shipping model wherein queries are autonomously forwarded from one web-site to another, without requiring much coordination         from the query originating site. Its design addresses a variety of interesting issues that arise in the distributed web context         including determining query completion, handling query rewriting, supporting query termination and preventing multiple computations         of a query at a site due to the same query arriving through different paths in the hyperlink framework. The DIASPORA system         is currently operational and is undergoing testing on our campus network. In this paper we describe the design of the system         and report initial performance results that indicate significant performance improvements over comparable centralized approaches.      </content></document><document><year>2006</year><authors>Octavian Ureche1  | RГ©jean Plamondon1</authors><title>Digital payment systems for Internet commerce: The state of the art      </title><content>This paper presents a comprehensive survey of digital payment systems for Internet-based electronic commerce. A new taxonomy         and classification covering most, if not all, Internet payment methods, schemes and protocols is introduced. A set of evaluation         and analysis criteria, both internal and external to a particular system, is used to analyze system representatives of each         class. Principal characteristics are synthesized in a comparative table. Different approaches for future developments are         proposed, including a new digital envelope transport protocol named TRANZIX, used in the context of generic &amp;#8220;transport and         transfer of value&amp;#8221;. A comprehensive electronic resource list is also included.      </content></document><document><year>2006</year><authors>James Caverlee1 | Ling Liu1 | Daniel Rocco1</authors><title>Discovering Interesting Relationships among Deep Web Databases: A Source-Biased Approach      </title><content>The escalation of deep web databases has been phenomenal over the last decade, spawning a growing interest in automated discovery         of interesting relationships among available deep web databases. Unlike the &amp;#8220;surface&amp;#8221; web of static pages, these deep web         databases provide data through a web-based query interface and account for a huge portion of all web content. This paper presents         a novel source-biased approach to efficiently discover interesting relationships among web-enabled databases on the deep web.         Our approach supports a relationship-centric view over a collection of deep web databases through source-biased database analysis         and exploration. Our source-biased approach has three unique features: First, we develop source-biased probing techniques,         which allow us to determine in very few interactions whether a target database is relevant to the source database by probing         the target with very precise probes. Second, we introduce source-biased relevance metrics to evaluate the relevance of deep         web databases discovered, to identify interesting types of source-biased relationships for a collection of deep web databases,         and to rank them accordingly. The source-biased relationships discovered not only present value-added metadata for each deep         web database but can also provide direct support for personalized relationship-centric queries. Third, but not least, we also         develop a performance optimization using source-biased probing with focal terms to further improve the effectiveness of the         basic source-biased model. A prototype system is designed for crawling, probing, and supporting relationship-centric queries         over deep web databases using the source-biased approach. Our experiments evaluate the effectiveness of the proposed source-biased         analysis and discovery model, showing that the source-biased approach outperforms query-biased probing and unbiased probing.      </content></document><document><year>2006</year><authors>Peter L.T. Pirolli1  | James E. Pitkow1</authors><title>Distributions of surfers' paths through the World Wide Web: Empirical characterizations      </title><content>Surfing the World Wide Web (WWW) involves traversing hyperlink connections among documents. The ability to predict surfing         patterns could solve many problems facing producers and consumers of WWW content. We analyzed WWW server logs for a WWW site,         collected over ten days, to compare different path reconstruction methods and to investigate how past surfing behavior predicts         future surfing choices. Since log files do not explicitly contain user paths, various methods have evolved to reconstruct         user paths. Session times, number of clicks per visit, and Levenshtein Distance analyses were performed to show the impact         of various reconstruction methods. Different methods for measuring surfing patterns were also compared. Markov model approximations         were used to model the probability of users choosing links conditional on past surfing paths. Information&amp;#8208;theoretic (entropy)         measurements suggest that information is gained by using longer paths to estimate the conditional probability of link choice         given surf path. The improvements diminish, however, as one increases the length of path beyond one. Information&amp;#8208;theoretic         (total divergence to the average entropy) measurements suggest that the conditional probabilities of link choice given surf         path are more stable over time for shorter paths than longer paths. Direct examination of the accuracy of the conditional         probability models in predicting test data also suggests that shorter paths yield more stable models and can be estimated         reliably with less data than longer paths.      </content></document><document><year>2006</year><authors>Valeria Cardellini1 | Michele Colajanni2  | Philip S. Yu1</authors><title>DNS dispatching algorithms with state estimators for scalable Web&amp;#8208;server clusters      </title><content>Replication of information across a server cluster provides a promising way to support popular Web sites. However, a Web&amp;#8208;server         cluster requires some mechanism for the scheduling of requests to the most available server. One common approach is to use         the cluster Domain Name System (DNS) as a centralized dispatcher. The main problem is that WWW address caching mechanisms         (although reducing network traffic) only let this DNS dispatcher control a very small fraction of the requests reaching the         Web&amp;#8208;server cluster. The non&amp;#8208;uniformity of the load from different client domains, and the high variability of real Web workload         introduce additional degrees of complexity to the load balancing issue. These characteristics make existing scheduling algorithms         for traditional distributed systems not applicable to control the load of Web&amp;#8208;server clusters and motivate the research on         entirely new DNS policies that require some system state information. We analyze various DNS dispatching policies under realistic         situations where state information needs to be estimated with low computation and communication overhead so as to be applicable         to a Web cluster architecture. In a model of realistic scenarios for the Web cluster, a large set of simulation experiments         shows that, by incorporating the proposed state estimators into the dispatching policies, the effectiveness of the DNS scheduling         algorithms can improve substantially, in particular if compared to the results of DNS algorithms not using adequate state         information.      </content></document><document><year>2006</year><authors>Sanjay Kumar Madria1 | Mukesh Mohania2  | Bharat Bhargava3 </authors><title>Editorial      </title><content>Without Abstract</content></document><document><year>2006</year><authors>Wei Hao1 | Jicheng Fu1 | Jiang He1 | I-Ling Yen1 | Farokh Bastani1  | Ing-Ray Chen2 </authors><title>Extending Proxy Caching Capability: Issues and Performance      </title><content>Proxy caching is an effective approach to reduce the response latency to client requests, web server load, and network traffic.         Recently there has been a major shift in the usage of the Web. Emerging web applications require increasing amount of server-side         processing. Current proxy protocols do not support caching and execution of web processing units. In this paper, we present         a weblet environment, in which, processing units on web servers are implemented as weblets. These weblets can migrate from         web servers to proxy servers to perform required computation and provide faster responses. Weblet engine is developed to provide         the execution environment on proxy servers as well as web servers to facilitate uniform weblet execution. We have conducted         thorough experimental studies to investigate the performance of the weblet approach. We modify the industrial standard e-commerce         benchmark TPC-W to fit the weblet model and use its workload model for performance comparisons. The experimental results show         that the weblet environment significantly improves system performance in terms of client response latency, web server throughput,         and workload. Our prototype weblet system also demonstrates the feasibility of integrating weblet environment with current         web/proxy infrastructure.      </content></document><document><year>2006</year><authors>Fred Douglis1  | Prabhakar Raghavan2 </authors><title>Guest Editors&amp;#8217; Introduction      </title><content>Without Abstract</content></document><document><year>2006</year><authors>Gustaf Neumann1  | Uwe Zdun2 </authors><title>High-level design and architecture of an HTTP-based infrastructure for web applications      </title><content>xoComm is a communication infrastructure for web applications based on the HTTP protocol. It provides an HTTP server and client         access. Furthermore it is the basic communication service for the ActiWeb web object and mobile code system. The HTTP server         component of xoComm is used to implement ActiWeb places. The places use the HTTP client access to provide the communication         means for their agents. We present the design and architecture of xoComm on several crucial excerpts of the design. These         are closely related to their implementation in the object-oriented scripting language XOTcl. We discuss how a dynamic and         reflective environment, high-level language constructs, and concepts like design patterns influence the design and architecture.      </content></document><document><year>2006</year><authors>Heng Tao Shen1 | Xiaofang Zhou1  | Bin Cui1</authors><title>Indexing and Integrating Multiple Features for WWW Images      </title><content>In this paper, we present a novel indexing technique called Multi-scale Similarity Indexing (MSI) to index image's multi-features into a single one-dimensional structure. Both for text and visual feature spaces, the         similarity between a point and a local partition's center in individual space is used as the indexing key, where similarity         values in different features are distinguished by different scale. Then a single indexing tree can be built on these keys.         Based on the property that relevant images have similar similarity values from the center of the same local partition in any         feature space, certain number of irrelevant images can be fast pruned based on the triangle inequity on indexing keys. To         remove the &amp;#8220;dimensionality curse&amp;#8221; existing in high dimensional structure, we propose a new technique called Local Bit Stream (LBS). LBS transforms image's text and visual feature representations into simple, uniform and effective bit stream (BS)         representations based on local partition's center. Such BS representations are small in size and fast for comparison since         only bit operation are involved. By comparing common bits existing in two BSs, most of irrelevant images can be immediately         filtered. To effectively integrate multi-features, we also investigated the following evidence combination techniques&amp;#8212;Certainty Factor, Dempster Shafer Theory, Compound Probability, and Linear Combination. Our extensive experiment showed that single one-dimensional index on multi-features improves multi-indices on multi-features         greatly. Our LBS method outperforms sequential scan on high dimensional space by an order of magnitude. And Certainty Factor         and Dempster Shafer Theory perform best in combining multiple similarities from corresponding multiple features.      </content></document><document><year>2006</year><authors>Xuehong Gan1  | Byrav Ramamurthy2 </authors><title>LSMAC: An improved load sharing network service dispatcher      </title><content>The rapid growth of the Internet is changing the way we do business. Electronic Commerce (or E-Commerce) is already a reality         and will expand rapidly in the near future. However, the success of E-Commerce depends heavily on the scalability and availability         of the servers. Cluster-based servers using commodity hardware have been accepted as a good alternative to expensive specialized         hardware for building scalable services. In this paper, we summarize the two clustering architectures: IP-based clustering         and MAC-based clustering. A new efficient implementation of the MAC-based clustering architecture is presented and its performance         in clustering Web servers was measured using the WebStone benchmark and was found to be superior to that of existing MAC-based         clustering implementations.      </content></document><document><year>2006</year><authors>Gaurav Banga1  | Peter Druschel1</authors><title>Measuring the capacity of a Web server under realistic loads      </title><content>The World Wide Web and its related applications place substantial performance demands on network servers. The ability to measure         the effect of these demands is important for tuning and optimizing the various software components that make up a Web server.         To measure these effects, it is necessary to generate realistic HTTP client requests in a test&amp;#8208;bed environment. Unfortunately,         the state&amp;#8208;of&amp;#8208;the&amp;#8208;art approach for benchmarking Web servers is unable to generate client request rates that exceed the capacity         of the server being tested, even for short periods of time. Moreover, it fails to model important characteristics of the wide         area networks on which most servers are deployed (e.g., delay and packet loss). This paper examines pitfalls that one encounters         when measuring Web server capacity using a synthetic workload. We propose and evaluate a new method for Web traffic generation         that can generate bursty traffic, with peak loads that exceed the capacity of the server. Our method also models the delay         and loss characteristics of WANs. We use the proposed method to measure the performance of widely used Web servers. The results         show that actual server performance can be significantly lower than indicated by standard benchmarks under conditions of overload         and in the presence of wide area network delays and packet losses.      </content></document><document><year>2006</year><authors>James C. French | Allison L. Powell</authors><title>Metrics for evaluating database selection techniques      </title><content>The increasing availability of online databases and other information resources in digital libraries and on the World Wide         Web has created the need for efficient and effective algorithms for selecting databases to search. A number of techniques         have been proposed for query routing or database selection. We have developed a methodology and metrics that can be used to         directly compare competing techniques. They can also be used to isolate factors that influence the performance of these techniques         so that we can better understand performance issues. In this paper we describe the methodology we have used to examine the         performance of database selection algorithms such as gGlOSS and CORI. In addition we develop the theory behind a &amp;#8220;random&amp;#8221;         database selection algorithm and show how it can be used to help analyze the behavior of realistic database selection algorithms.      </content></document><document><year>2006</year><authors>Chung&amp;#8208 Ming Huang1 | Ming&amp;#8208 Yuhe Jang1 | Chih&amp;#8208 Wei Tung1</authors><title>Phone&amp;#8208;Web: Accessing WWW using a telephone set      </title><content>In order to provide a ubiquitous, comprehensive and versatile service on the WWW the development of a WWW telephone browsing         system named Phone&amp;#8208;Web is proposed. This Phone&amp;#8208;Web browser system would act as an intermediary between the telephone user         and Web sites, thereby facilitating access to the WWW from any phone. The Phone&amp;#8208;Web system would filter Web page information         and then convert it into speech format. Users of the Phone&amp;#8208;Web system could retrieve and hear information stored on WWW servers         by using telephone handsets. For this system to work it requires a new hypertext language &amp;#8220;Hyper Phone Markup Language&amp;#8221; (HPML)         and a dedicated Phone&amp;#8208;Web browser. By using the proposed HPML language, Web page designers can easily specify service information         in a set of HPML pages, which would be included in the site they are designing. The Phone&amp;#8208;Web browser would be capable of         retrieving and then converting the HPML pages into speech patterns. By connecting to the Phone&amp;#8208;Web browser, telephone users         can access any information on any site using the HPML language from any telephone anywhere in the world. However, HPML&amp;#8208;specified         pages can also be accessed using existing browsers (e.g., Netscape Navigator, Microsoft Internet Explorer, etc.) This means         that both telephone and computer users can now access the same set of Web pages to retrieve the same information. Therefore,         instead of maintaining the existing two systems (access via the telephone or computer) service providers can now maintain         one system, which would provide a versatile, and comprehensive service for users at all levels of Web&amp;#8208;literacy.      </content></document><document><year>2006</year><authors>Fred Douglis</authors><title>Preface      </title><content>Without Abstract</content></document><document><year>2006</year><authors>JosГ© Borges1  | Mark Levene2 </authors><title>Ranking Pages by Topology and Popularity within Web Sites      </title><content>We compare two link analysis ranking methods of web pages in a site. The first, called Site Rank, is an adaptation of PageRank to the granularity of a web site and the second, called Popularity Rank, is based on the frequencies of user clicks on the outlinks in a page that are captured by navigation sessions of users through         the web site. We ran experiments on artificially created web sites of different sizes and on two real data sets, employing         the relative entropy to compare the distributions of the two ranking methods. For the real data sets we also employ a nonparametric         measure, called Spearman's footrule, which we use to compare the top-ten web pages ranked by the two methods. Our main result         is that the distributions of the Popularity Rank and Site Rank are surprisingly close to each other, implying that the topology         of a web site is very instrumental in guiding users through the site. Thus, in practice, the Site Rank provides a reasonable         first order approximation of the aggregate behaviour of users within a web site given by the Popularity Rank.      </content></document><document><year>2006</year><authors>Aris Anagnostopoulos1 | Andrei Z. Broder1  | David Carmel2 </authors><title>Sampling Search-Engine Results      </title><content>We consider the problem of efficiently sampling Web search engine query results. In turn, using a small random sample instead         of the full set of results leads to efficient approximate algorithms for several applications, such as:                                                                &amp;#8226;;                                                         Determining the set of categories in a given taxonomy spanned by the search results;                                                                                       &amp;#8226;;                                                         Finding the range of metadata values associated with the result set in order to enable &amp;#8220;multi-faceted search&amp;#8221;;                                                                                       &amp;#8226;;                                                         Estimating the size of the result set;                                                                                       &amp;#8226;;                                                         Data mining associations to the query terms.                                                                           We present and analyze efficient algorithms for obtaining uniform random samples applicable to any search engine that is based         on posting lists and document-at-a-time evaluation. (To our knowledge, all popular Web search engines, for example, Google,         Yahoo Search, MSN Search, Ask, belong to this class.) Furthermore, our algorithm can be modified to follow the modern object-oriented         approach whereby posting lists are viewed as streams equipped with a next method, and the next method for Boolean and other complex queries is built from the next method for primitive terms. In our case we show how to construct a basic sample-next(p) method that samples term posting lists with probability p, and show how to construct sample-next(p) methods for Boolean operators (AND, OR, WAND) from primitive methods. Finally, we test the efficiency and quality of our approach on both synthetic and real-world data.      </content></document><document><year>2006</year><authors>Li Chen| Kajal T. Claypool | Elke A. Rundensteiner</authors><title>SERFing the web: The Re-Web approach for web re-structuring      </title><content>In our emerging digital paper-less society, massive amount of information is being maintained in on-line repositories and         diverse web site representations of this information must be served over the Internet to different user groups. E-commerce         and digital libraries are two representative sample applications with such needs. In this paper we present a database-centric         approach called Re-Web that addresses this need for flexible web site generation, re-structuring, and maintenance. Re-Web         is based on two key ideas. First, we exploit the web site structure by associating web semantics (XML equivalents) with the         modeling constructs of the ODMG object model to aid the web site generation process. By capturing the logical structure of         web views within the OODB system, we can efficiently maintain the web views using standard database techniques. Secondly,         to ease the process of specification and construction of multiple customized web view sites, we also propose the notion of         generic web view transformations that are encapsulated into re-usable templates. Thus desired new web view sites can be generated         simply by applying the corresponding transformations on the underlying database to produce web view classes and then by applying         the web semantics on the newly built view classes. The Re-Web system has been implemented using PSE by Object Design Inc.         as object repository, ODMG as object model, OQL as transformation language, SERF as OODB evolution facility and IBM XML parser         and LotusXSL processor to aid the web site generation. A case study using Re-Web is also presented to illustrate the working         of the system. To the best of our knowledge, Re-Web is the first web site management system focusing on the issue of re-usable         view generation templates at the content and not at the presentation style level of abstraction.      </content></document><document><year>2006</year><authors>Elisa Bertino1 | Silvana Castano1 | Elena Ferrari1  | Marco Mesiti2 </authors><title>Specifying and enforcing access control policies for XML document sources      </title><content>The Web is becoming the main information dissemination means in private and public organizations. As a consequence, several         applications at both internet and intranet level need mechanisms to support a selective access to data available over the         Web. In this context, developing an access control model, and related mechanisms, in terms of XML (eXtensible Markup Language)         is an important step, because XML is increasingly used as the language for representing information exchanged over the Web.         In this paper, we propose access control policies and an associated model for XML documents, addressing peculiar protection         requirements posed by XML. A first requirement is that varying protection granularity levels should be supported to guarantee         a differentiated protection of document contents. A second requirement arises from the fact that XML documents do not always         conform to a predefined document type. To cope with these requirements, the proposed model supports varying protection granularity         levels, ranging from a set of documents, to a single document or specific document portion(s). Moreover, it allows the Security         Administrator to choose different policies for documents not covered or only partially covered by the existing access control         policies for document types. An access control mechanism for the enforcement of the proposed model is finally described.      </content></document><document><year>2006</year><authors>Denilson Barbosa| Laurent Mignet | Pierangelo Veltri</authors><title>Studying the XML Web: Gathering Statistics from an XML Sample      </title><content>Without Abstract</content></document><document><year>2006</year><authors>James E. Pitkow1 </authors><title>Summary of WWW characterizations      </title><content>To date there have been a number of efforts that attempt to characterize various aspects of the World Wide Web. This paper         presents a summary of these efforts, highlighting regularities and insights that have been discovered across the variety of         access points available for instrumentation. Characterizations that are derived from client, proxy, and server instrumentation         are reviewed as well as efforts to characterize the entire structure of the WWW. Given the dynamic nature of the Web, it may         be surprising for some readers to find that many properties of the Web follow regular and predictable patterns that have not         changed in form over the Web's lifetime. Understanding these aspects as well as those that vary is critical to designing a         better Web, and as a direct consequence, creating a more enjoyable user experience.      </content></document><document><year>2006</year><authors>Chen Li1 | Liang Jin1 | Sharad Mehrotra1</authors><title>Supporting Efficient Record Linkage for Large Data Sets Using Mapping Techniques      </title><content>This paper describes an efficient approach to record linkage. Given two lists of records, the record-linkage problem consists         of determining all pairs that are similar to each other, where the overall similarity between two records is defined based         on domain-specific similarities over individual attributes. The record-linkage problem arises naturally in the context of         data cleansing that usually precedes data analysis and mining. Since the scalability issue of record linkage was addressed         in [21], the repertoire of database techniques dealing with multidimensional data sets has significantly increased. Specifically,         many effective and efficient approaches for distance-preserving transforms and similarity joins have been developed. Based         on these advances, we explore a novel approach to record linkage. For each attribute of records, we first map values to a         multidimensional Euclidean space that preserves domain-specific similarity. Many mapping algorithms can be applied, and we         use the Fastmap approach [16] as an example. Given the merging rule that defines when two records are similar based on their attribute-level similarities,         a set of attributes are chosen along which the merge will proceed. A multidimensional similarity join over the chosen attributes         is used to find similar pairs of records. Our extensive experiments using real data sets show that our solution has very good         efficiency and recall.      </content></document><document><year>2006</year><authors>Xiaohui Long1  | Torsten Suel1 </authors><title>Three-Level Caching for Efficient Query Processing in Large Web Search Engines      </title><content>Large web search engines have to answer thousands of queries per second with interactive response times. Due to the sizes         of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds         of megabytes or more of index data. To keep up with this immense workload, large search engines employ clusters of hundreds         or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used         to improve scalability. In particular, two-level caching techniques cache results of repeated identical queries at the frontend,         while index data for frequently used query terms are cached in each node at a lower level. We propose and evaluate a three-level         caching scheme that adds an intermediate level of caching for additional performance gains. This intermediate level attempts         to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists.         We propose and study several offline and online algorithms for the resulting weighted caching problem, which turns out to         be surprisingly rich in structure. Our experimental evaluation based on a large web crawl and real search engine query log         shows significant performance gains for the best schemes, both in isolation and in combination with the other caching levels.         We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance.      </content></document><document><year>2006</year><authors>Pedro S. Ripper1 | Marcus F. Fontoura1| Ayrton Maia Neto1 | Carlos JosГ© P. de Lucena1</authors><title>V-Market: A framework for agent e-commerce systems      </title><content>Software agent technology is still an emerging technology, and as such, agent based software design is still in its infancy.         Software agents have just started to be used in the e-commerce domain, and they are already beginning to create a series of         new possibilities for this arena. Agents can be used to automate, as well as to enhance many stages of the traditional consumer-buying         behavior process. This paper proposes a software engineering approach to the design of agent mediated e-commerce systems,         through the definition of an object-oriented framework. The paper presents the underlying concepts, and the architecture of         the environment, showing how it allows developers to customize virtual marketplaces, and to define transaction categories         on demand, incorporating many possible products and services that can be traded online.      </content></document><document><year>2006</year><authors>Schahram Dustdar1  | Martin Treiber1 </authors><title>View Based Integration of Heterogeneous Web Service Registries&amp;#8212;the Case of VISR      </title><content>Despite all standardization efforts in the Web service area, several different incompatible Web service registry implementations         exist. The initial focus of these implementations was geared towards working with a centralized Universal Business Registry         (UBR). However, these centralized approaches tend to be bottlenecks regarding performance and fault tolerance. A proposed         solution is the replication of registry information among multiple distributed Web service registries. In addition, the creation         of specialized Web service registries leads to a large number of different Web service registries. This leads to a situation         where the search for a particular Web service becomes a very complex task. Besides, Web service provisioning includes a considerable         administrative overhead when dealing with transient Web services. Transient Web services exist only for a limited lifetime         and in a certain context. In this paper, we propose the VISR (View based Integration of Web Service Registries) peer to peer architecture for the transparent integration of multiple Web service registries and transient Web         service providers. This work focuses on the integration concept of multiple Web service registries and transient Web service         providers. The integration concept relies on so-called views. Views provide the needed abstractions for the seamless integration         on the different registries. Views use common lightweight Web service profiles that serve as unified global data model. VISR         Web service profiles allow the flexible extension of registry entries with value added information without changing the original         Web service registry entries. To illustrate the view concept, we introduce a simple grammar (View Description Language) for         view descriptions that is used in the working example throughout the paper. We present Web service communities as a possible         application of the view concept and show how different types of Web services providers, respectively, their registries are         integrated into a unified global data model.      </content></document><document><year>2006</year><authors>Radek Vingralek1 | Mehmet Sayal2 | Yuri Breitbart3  | Peter Scheuermann2</authors><title>Web++ architecture, design and performance      </title><content>We describe the design of a system for fast and reliable HTTP service which we call Web++. Web++ achieves high reliability         by dynamically replicating web data among multiple web servers. Web++ selects the available server that is expected to provide         the fastest response time. Furthermore, Web++ guarantees data delivery given that at least one server containing the requested         data is available. After detecting a server failure, Web++ client requests are satisfied transparently to the user by another         server. Furthermore, the Web++ architecture is flexible enough for implementing additional performance optimizations. We describe         implementation of one such optimization, batch resource transmission, whereby all resources embedded in an HTML page that         are not cached by the client are sent to the client in a single response. Web++ is built on top of the standard HTTP protocol         and does not require any changes either in existing web browsers or the installation of any software on the client side. In         particular, Web++ clients are dynamically downloaded to web browsers as signed Java applets. We implemented a Web++ prototype;         performance experiments indicate that the Web++ system with 3 servers improves the response time perceived by clients on average         by 36.6%, and in many cases by as much as 59%, when compared with the current web performance. In addition, we show that batch         resource transmission can improve the response time on average by 39% for clients with fast network connections and 21% for         the clients with 56 Kb modem connections.      </content></document><document><year>2006</year><authors>Nicholas Kushmerick</authors><title>Wrapper verification      </title><content>Many Internet information-management applications (e.g., information integration systems) require a library of wrappers, specialized         information extraction procedures that translate a source's native format into a structured representation suitable for further         application-specific processing. Maintaining wrappers is tedious and error-prone, because the formatting regularities on which         wrappers rely change frequently on the decentralized and dynamic Internet. The wrapper verification problem is to determine         whether a wrapper is operating correctly. Standard regression testing approaches are inappropriate, because both the formatting         regularities on which wrappers rely and the source's underlying content may change. We introduce RAPTURE, a fully-implemented,         domain-independent wrapper verification algorithm. RAPTURE computes a probabilistic similarity measure between a wrapper's         expected and observed output, where similarity is defined in terms of simple numeric features (e.g., the length, or the fraction         of punctuation characters) of the extracted strings. Experiments with numerous actual Internet sources demostrate that RAPTURE         performs substantially better than standard regression testing.      </content></document><document><year>2007</year><authors>Sophoin Khy1 | Yoshiharu Ishikawa2  | Hiroyuki Kitagawa1| 3 </authors><title>A Novelty-based Clustering Method for On-line Documents      </title><content>In this paper, we describe a document clustering method called novelty-based document clustering. This method clusters documents based on similarity and novelty. The method assigns higher weights to recent documents than old ones and generates clusters with the focus on recent topics.         The similarity function is derived probabilistically, extending the conventional cosine measure of the vector space model         by incorporating a document forgetting model to produce novelty-based clusters. The clustering procedure is a variation of the K-means method. An additional feature of our clustering method is an incremental update facility, which is applied when new         documents are incorporated into a document repository. Performance of the clustering method is examined through experiments.         Experimental results show the efficiency and effectiveness of our method.      </content></document><document><year>2007</year><authors>Nikos Manouselis1  | Constantina Costopoulou1 </authors><title>Analysis and Classification of Multi-Criteria Recommender Systems      </title><content>Recent studies have indicated that the application of Multi-Criteria Decision Making (MCDM) methods in recommender systems         has yet to be systematically explored. This observation partially contradicts with the fact that in related literature, there         exist several contributions describing recommender systems that engage some MCDM method. Such systems, which we refer to as         multi-criteria recommender systems, have early demonstrated the potential of applying MCDM methods to facilitate recommendation,         in numerous application domains. On the other hand, a comprehensive analysis of existing systems would facilitate their understanding         and development. Towards this direction, this paper identifies a set of dimensions that distinguish, describe and categorize         multi-criteria recommender systems, based on existing taxonomies and categorizations. These dimensions are integrated into         an overall framework that is used for the analysis and classification of a sample of existing multi-criteria recommender systems.         The results provide a comprehensive overview of the ways current multi-criteria recommender systems support the decision of         online users.      </content></document><document><year>2007</year><authors>Anis Charfi1  | Mira Mezini1</authors><title>AO4BPEL: An Aspect-oriented Extension to BPEL      </title><content>Process-oriented composition languages such as BPEL allow Web Services to be composed into more sophisticated services using         a workflow process. However, such languages exhibit some limitations with respect to modularity and flexibility. They do not         provide means for a well-modularized specification of crosscutting concerns such as logging, persistence, auditing, and security.         They also do not support the dynamic adaptation of composition at runtime. In this paper, we advocate an aspect-oriented approach         to Web Service composition and present the design and implementation of AO4BPEL, an aspect-oriented extension to BPEL. We         illustrate through examples how AO4BPEL makes the composition specification more modular and the composition itself more flexible         and adaptable.      </content></document><document><year>2007</year><authors>MarГ­a Agustina CibrГЎn1 | Bart Verheecke1| Wim V|erperren1| Davy SuvГ©e1 | Viviane Jonckers1</authors><title>Aspect-oriented Programming for Dynamic Web Service Selection, Integration and Management      </title><content>In service-oriented computing, applications are often created by integrating third-party Web Services. Current integration         approaches, however, require client applications to hardcode references to specific Web Services, thereby affecting adaptability         and robustness. Moreover, support for client-side management is rarely provided. To enable the development of more flexible         and robust applications, we propose to insert a new layer between the client applications and the Web Services: the Web Services         Management Layer (WSML). This layer decouples Web Services from client applications and enables hot-swapping between semantically         equivalent Web Services based on availability. This mechanism allows for dynamic switching between Web Services based on selection         policies that encapsulate changing business requirements. In addition, with WSML, client-side management concerns (e.g., caching,         billing and logging) can be decoupled from the applications. In this paper, we identify a list of requirements for WSML to         realize dynamic integration and client-side service management, and provide support for service criteria to govern the selection,         integration and composition of Web Services. We also show that dynamic Aspect-Oriented Programming (AOP) is well suited to         implement the core functionality of WSML.      </content></document><document><year>2007</year><authors>David Martin1 | Mark Burstein2| Drew McDermott3| Sheila McIlraith4| Massimo Paolucci5| Katia Sycara6| Deborah L. McGuinness7| Evren Sirin8 | Naveen Srinivasan9</authors><title>Bringing Semantics to Web Services with OWL-S      </title><content>Current industry standards for describing Web Services focus on ensuring interoperability across diverse platforms, but do         not provide a good foundation for automating the use of Web Services. Representational techniques being developed for the         Semantic Web can be used to augment these standards. The resulting Web Service specifications enable the development of software         programs that can interpret descriptions of unfamiliar Web Services and then employ those services to satisfy user goals.         OWL-S (&amp;#8220;OWL for Services&amp;#8221;) is a set of notations for expressing such specifications, based on the Semantic Web ontology language         OWL. It consists of three interrelated parts: a profile ontology, used to describe what the service does; a process ontology         and corresponding presentation syntax, used to describe how the service is used; and a grounding ontology, used to describe         how to interact with the service. OWL-S can be used to automate a variety of service-related activities involving service         discovery, interoperation, and composition. A large body of research on OWL-S has led to the creation of many open-source         tools for developing, reasoning about, and dynamically utilizing Web Services.      </content></document><document><year>2007</year><authors>Hongqiang Wang1 | Jianzhong Li1  | Hongzhi Wang1 </authors><title>Clustered Chain Path Index for XML Document: Efficiently Processing Branch Queries      </title><content>Branch query processing is a core operation of XML query processing. In recent years, a number of stack based twig join algorithms         have been proposed to process twig queries based on tag stream index. However, in tag stream index, each element is labeled         separately without considering the similarity among elements. Besides, algorithms based on tag stream index perform inefficiently         on large document. This paper proposes a novel index, named Clustered Chain Path Index, based on a novel labeling scheme.         This index provides efficient support for processing branch queries. It also has the same cardinality as 1-index against tree         structured XML document. Based on CCPI, efficient algorithms, KMP-Match-Path and Related-Path-Segment-Join, are proposed to         process queries efficiently. Analysis and experimental results show that proposed query processing algorithms based on CCPI         outperform other algorithms and have good scalability.      </content></document><document><year>2007</year><authors>Lei Cao1 | Minglu Li1 | Jian Cao1  | Joshua Huang2 </authors><title>CNP-based Implementation of Service-oriented Workflow Mapping in SHGWMS      </title><content>ShanghaiGrid has a complex service-oriented infrastructure. Workflow management is emerging as one of the most important part         of it. Because the grid environment is very dynamic and the services are shared among many users, it is impossible to optimize         the workflow from the point of view of execution ahead of time. In fact, one may want to make decisions about the execution         locations and the access to a particular data set as late as possible. In this paper, we propose a method of using Contract         Net Protocol(CNP) to implement service-oriented workflow mapping in ShanghaiGrid workflow management system(SHGWMS). Three         types of workflow in SHGWMS are denoted as Abstract workflow(AW), Concrete workflow(CW) and Executable workflow(EW). Belief-Desire-Intention(BDI)         agent technology in SHGWMS helps the system meet challenges from the grid context. CNP provides a very proper negotiation         model for agents. The problem of workflow mapping has been transferred to the problem of multi-agent negotiation with the         help of CNP model in SHGWMS. We also propose AW2CW mapping algorithm and CW2EW mapping algorithm to accomplish service-oriented         workflow mapping.      </content></document><document><year>2007</year><authors>M. Kihl1 | A. Robertsson2 | M. Andersson1  | B. Wittenmark2 </authors><title>Control-theoretic Analysis of Admission Control Mechanisms for Web Server Systems      </title><content>Web sites are exposed to high rates of incoming requests. The servers may become overloaded during temporary traffic peaks         when more requests arrive than the server is designed for. An admission control mechanism rejects some requests whenever the         arriving traffic is too high and thereby maintains an acceptable load in the system. This paper presents how admission control         mechanisms can be designed with a combination of queueing theory and control theory. In this paper we model an Apache web         server as a GI/G/1-system and then design a PI-controller, commonly used in automatic control, for the server. The controller         has been implemented as a module inside the Apache source code. Measurements from the laboratory setup show how robust the         implemented controller is, and how it corresponds to the results from the theoretical analysis.      </content></document><document><year>2007</year><authors>Claudio Bettini1 | Dario Maggiorini1  | Daniele Riboni1 </authors><title>Distributed Context Monitoring for the Adaptation of Continuous Services      </title><content>This paper describes a middleware designed for distributed context acquisition and reconciliation intended to support the         adaptation of continuous Internet services, like e.g., multimedia streaming. These services persist in time, and are characterized by multiple transmissions         of data by the service provider, as a result of a single request from the user. Adapting these services to the current context         requires the continuous monitoring of context data, and a real-time adjustment of the adaptation parameters upon the detection         of a relevant context change. The proposed solution is based on asynchronous context change notifications, and specific techniques         have been designed to minimize the number of unnecessary updates and the re-evaluation of policies. The paper also provides         experimental results obtained by developing an adaptive video streaming system and running it on top of the proposed middleware.      </content></document><document><year>2007</year><authors>Wilfred Ng1 | Ho-Lam Lau1  | Aoying Zhou2 </authors><title>Divide, Compress and Conquer: Querying XML via Partitioned Path-Based Compressed Data Blocks      </title><content>We propose a novel partition path-based (PPB) grouping strategy to store compressed XML data in a stream of blocks. In addition,         we employ a minimal indexing scheme called block statistic signature (BSS) on the compressed data, which is a simple but effective         technique to support evaluation of selection and aggregate XPath queries of the compressed data. We present a formal analysis         and empirical study of these techniques. The BSS indexing is first extended into effective cluster statistic signature (CSS)         and multiple-cluster statistic signature (MSS) indexing by establishing more layers of indexes. We analyze how the response         time is affected by various parameters involved in our compression strategy such as the data stream block size, the number         of cluster layers, and the query selectivity. We also gain further insight about the compression and querying performance         by studying the optimal block size in a stream, which leads to the minimum processing cost for queries. The cost model analysis         provides a solid foundation for predicting the querying performance. Finally, we demonstrate that our PPB grouping and indexing         strategies are not only efficient enough to support path-based selection and aggregate queries of the compressed XML data,         but they also require relatively low computation time and storage space when compared with other state-of-the-art compression         strategies.      </content></document><document><year>2007</year><authors>Anne H. H. Ngu3 | Masaru Kitsuregawa2  | Erich J. Neuhold1 </authors><title>Editorial      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Lipyeow Lim1 | Min Wang1 | Sriram Padmanabhan2 | Jeffrey Scott Vitter3  | Ramesh Agarwal4 </authors><title>Efficient Update of Indexes for Dynamically Changing Web Documents      </title><content>Recent work on incremental crawling has enabled the indexed document collection of a search engine to be more synchronized         with the changing World Wide Web. However, this synchronized collection is not immediately searchable, because the keyword         index is rebuilt from scratch less frequently than the collection can be refreshed. An inverted index is usually used to index         documents crawled from the web. Complete index rebuild at high frequency is expensive. Previous work on incremental inverted         index updates have been restricted to adding and removing documents. Updating the inverted index for previously indexed documents         that have changed has not been addressed. In this paper, we propose an efficient method to update the inverted index for previously         indexed documents whose contents have changed. Our method uses the idea of landmarks together with the diff algorithm to significantly reduce the number of postings in the inverted index that need to be updated. Our experiments verify         that our landmark-diff method results in significant savings in the number of update operations on the inverted index.      </content></document><document><year>2007</year><authors>Dimitrios Georgakopoulos1 | Donald Baker1 | Marian Nodine1  | Andrzej Cichoki1 </authors><title>Event-driven Video Awareness Providing Physical Security      </title><content>The Video Event Awareness Workbench (VEAW) analyzes surveillance video from thousands of video cameras and automatically detects         complex events in near-real-time-at pace with their input video streams. For events of interest to security personnel, VEAW         generates and routes alerts and related video evidence to subscribing security personnel. Complex event processing in VEAW         is driven by user-authored awareness specifications comprised of inter-connected spatio-temporal stream and statistical operators         that consume and produce events described in VEAW&amp;#8217;s surveillance ontology. In this paper we introduce VEAW&amp;#8217;s event driven         architecture and describe its solutions for automating video surveillance, including the orchestration of continuous and tasked         video analysis algorithms (e.g., for entity tracking and identification), fusion of events from multiple sources in an installation-specific         &amp;#8220;world&amp;#8221; model, and proactive information gathering to deal with missing or incomplete information (this is done by tasking         video analysis algorithms and security personnel to provide it). We also discuss how VEAW deals with late arriving information         (due to out-of-band video analysis tasks and overhead), as well as a related resource optimization aimed at minimizing computation         costs. We illustrate the benefits of VEAW by illustrating its application on the automation of real-world security policies.      </content></document><document><year>2007</year><authors>Yanhong Zhai1  | Bing Liu1 </authors><title>Extracting Web Data Using Instance-Based Learning      </title><content>This paper studies structured data extraction from Web pages. Existing approaches to data extraction include wrapper induction         and automated methods. In this paper, we propose an instance-based learning method, which performs extraction by comparing         each new instance to be extracted with labeled instances. The key advantage of our method is that it does not require an initial         set of labeled pages to learn extraction rules as in wrapper induction. Instead, the algorithm is able to start extraction         from a single labeled instance. Only when a new instance cannot be extracted does it need labeling. This avoids unnecessary         page labeling, which solves a major problem with inductive learning (or wrapper induction), i.e., the set of labeled instances         may not be representative of all other instances. The instance-based approach is very natural because structured data on the         Web usually follow some fixed templates. Pages of the same template usually can be extracted based on a single page instance         of the template. A novel technique is proposed to match a new instance with a manually labeled instance and in the process         to extract the required data items from the new instance. The technique is also very efficient. Experimental results based         on 1,200 pages from 24 diverse Web sites demonstrate the effectiveness of the method. It also outperforms the state-of-the-art         existing systems significantly.      </content></document><document><year>2007</year><authors>Peixiang Zhao1  | Jeffrey Xu Yu1 </authors><title>Fast Frequent Free Tree Mining in Graph Databases      </title><content>Free tree, as a special undirected, acyclic and connected graph, is extensively used in computational biology, pattern recognition,         computer networks, XML databases, etc. In this paper, we present a computationally efficient algorithm F3TM (Fast Frequent Free Tree Mining) to find all frequently-occurred free trees in a graph database, . Two key steps of F3TM are candidate generation and frequency counting. The frequency counting step is to compute how many graphs in  containing a candidate frequent free tree, which is proved to be the subgraph isomorphism problem in nature and is NP-complete.         Therefore, the key issue becomes how to reduce the number of false positives in the candidate generation step. Based on our         observations, the cost of false positive reduction can be prohibitive itself. In this paper, we focus ourselves on how to         reduce the candidate generation cost and minimize the number of infrequent candidates being generated. We prove a theorem         that the complete set of frequent free trees can be discovered from a graph database by growing vertices on a limited range         of positions of a free tree. We propose two pruning algorithms, namely, automorphism-based pruning and canonical mapping-based         pruning, which significantly reduce the candidate generation cost. We conducted extensive experimental studies using a real         application dataset and a synthetic dataset. The experiment results show that our algorithm F3TM outperforms the up-to-date algorithms by an order of magnitude in mining frequent free trees in large graph databases.      </content></document><document><year>2007</year><authors>Daniel A. MenascГ©1  | Vasudeva Akula2 </authors><title>Improving the Performance of Online Auctions Through Server-side Activity-based Caching      </title><content>Online auction sites have very specific workloads and user behavior characteristics. Previous studies on workload characterization         conducted by the authors showed that (1) bidding activity on auctions increases considerably after 90% of an auction&amp;#8217;s life         time has elapsed, (2) a very large percentage of auctions have a relatively low number of bids and bidders and a very small         percentage of auctions have a high number of bids and bidders, (3) prices rise very fast after an auction has lasted more         than 90% of its life time. Thus, if bidders are not able to successfully bid at the very last moments of an auction because         of site overload, the final price may not be as high as it could be and sellers, and consequently the auction site, may lose         revenue. In this paper, we propose server-side caching strategies in which cache placement and replacement policies are based         on auction-related parameters such as number of bids placed or percent remaining time till closing time. A main-memory auction         cache at the application server can be used to reduce accesses to the back-end database server. Trace-based simulations were         used to evaluate these caching strategies in terms of cache hit ratio and cache efficiency. The performance characteristics         of the best policies were then evaluated through experiments conducted on a benchmark online auction system.      </content></document><document><year>2007</year><authors>Srinivas Vadrevu1 | Fatih Gelgi1  | Hasan Davulcu1 </authors><title>Information Extraction from Web Pages Using Presentation Regularities and Domain Knowledge      </title><content>World Wide Web is transforming itself into the largest information resource making the process of information extraction (IE)         from Web an important and challenging problem. In this paper, we present an automated IE system that is domain independent         and that can automatically transform a given Web page into a semi-structured hierarchical document using presentation regularities.         The resulting documents are weakly annotated in the sense that they might contain many incorrect annotations and missing labels.         We also describe how to improve the quality of weakly annotated data by using domain knowledge in terms of a statistical domain model. We demonstrate that such system can recover from ambiguities         in the presentation and boost the overall accuracy of a base information extractor by up to 20%. Our experimental evaluations         with TAP data, computer science department Web sites, and RoadRunner document sets indicate that our algorithms can scale         up to very large data sets.      </content></document><document><year>2007</year><authors>Beat Signer1 | Michael Grossniklaus1  | Moira C. Norrie1 </authors><title>Interactive Paper as a Mobile Client for a Multi-channel Web Information System      </title><content>We describe how interactive paper can be used together with a multi-channel web information system to build a platform for         experimenting with multi-modal context-aware mobile information services. As an application, we present a tourist guide for         visitors to an international festival that was developed to investigate alternative modes of information delivery and interaction         in mobile environments. The guide is based around a set of interactive paper documents&amp;#8212;an event brochure, map and bookmark.         The brochure and map are augmented with digital services by using a digital pen to activate links and a text-to-speech engine         for information delivery. The digital pen is also used for data capture of event ratings and reviews. The bookmark provides         access to advanced searches and ticket reservations. We describe the architecture and operation of the system, highlighting         the challenges of extending a web information system to support both the generation of the paper documents and the interaction         from these documents, alongside more traditional access channels. Finally, we discuss the range of context-aware interactions         that is supported by our platform.      </content></document><document><year>2007</year><authors>Stefano Ceri1 | Florian Daniel1 | Federico M. Facca1  | Maristella Matera1 </authors><title>Model-driven Engineering of Active Context-awareness      </title><content>More and more Web users ask for contents and services highly tailored to their particular contexts of use. Especially due         to the increasing affordability of new and powerful mobile communication devices, they also appreciate the availability of         ubiquitous access, independent from the device actually in use. Due to such premises, traditional software design methods         need to be extended, and new issues and requirements need to be addressed for supporting context-aware access to services         and applications. In this paper we propose a model-driven approach towards adaptive, context-aware Web applications, accompanied         by a general-purpose execution framework enabling active context-awareness. Whereas conventional adaptive hypermedia systems address the problem of adapting HTML pages in response to user-generated         requests, in this work we especially stress the importance of user-independent, context-triggered adaptivity actions. This         finally leads us to interpret the context as an active actor, operating independently from users during their navigations.      </content></document><document><year>2007</year><authors>Paolo Atzeni1 | Tiziana Catarci2 | Barbara Pernici3</authors><title>Multi-channel Adaptive Information Systems      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Andrei Arion1 | Angela Bonifati2 | Ioana Manolescu1| 3  | Andrea Pugliese4 </authors><title>Path Summaries and Path Partitioning in Modern XML Databases      </title><content>XML path summaries are compact structures representing all the simple parent-child paths of an XML document. Such paths have         also been used in many works as a basis for partitioning the document&amp;#8217;s content in a persistent store, under the form of path         indices or path tables. We revisit the notions of path summaries and path-driven storage model in the context of current-day         XML databases. This context is characterized by complex queries, typically expressed in an XQuery subset, and by the presence         of efficient encoding techniques such as structural node identifiers. We review a path summary&amp;#8217;s many uses for query optimization,         and given them a common basis, namely relevant paths. We discuss summary-based tree pattern minimization and present some efficient summary-based minimization heuristics. We         consider relevant path computation and provide a time- and memory-efficient computation algorithm. We combine the principle         of path partitioning with the presence of structural identifiers in a simple path-partitioned storage model, which allows         for selective data access and efficient query plans. This model improves the efficiency of twig query processing up to two         orders of magnitude over the similar tag-partitioned indexing model. We have implemented the path-partitioned storage model         and path summaries in the XQueC compressed database prototype;[8]. We present an experimental evaluation of a path summary&amp;#8217;s practical feasibility and of tree pattern matching in a path-partitioned         store.      </content></document><document><year>2007</year><authors>Kotagiri Ramamohanarao1  | Hongjian Fan1 </authors><title>Patterns Based Classifiers      </title><content>Data mining is one of the most important areas in the 21 century for its applications are wide ranging. This includes medicine,         finance, commerce and engineering, to name a few. Pattern mining is amongst the most important and challenging techniques         employed in data mining. Patterns are collections of items which satisfy certain properties. Emerging Patterns are those whose         frequencies change significantly from one dataset to another. They represent strong contrast knowledge and have been shown         very successful for constructing accurate and robust classifiers. In this paper, we examine various kinds of patterns. We         also investigate efficient pattern mining techniques and discuss how to exploit patterns to construct effective classifiers.      </content></document><document><year>2007</year><authors>Antonis Sidiropoulos1 | George Pallis1 | Dimitrios Katsaros1| 2 | Konstantinos Stamos1 | Athena Vakali1  | Yannis Manolopoulos1 </authors><title>Prefetching in Content Distribution Networks via Web Communities Identification and Outsourcing      </title><content>Content distribution networks (CDNs) improve scalability and reliability, by replicating content to the &amp;#8220;edge&amp;#8221; of the Internet.         Apart from the pure networking issues of the CDNs relevant to the establishment of the infrastructure, some very crucial data         management issues must be resolved to exploit the full potential of CDNs to reduce the &amp;#8220;last mile&amp;#8221; latencies. A very important         issue is the selection of the content to be prefetched to the CDN servers. All the approaches developed so far, assume the         existence of adequate content popularity statistics to drive the prefetch decisions. Such information though, is not always         available, or it is extremely volatile, turning such methods problematic. To address this issue, we develop self-adaptive         techniques to select the outsourced content in a CDN infrastructure, which requires no apriori knowledge of request statistics.         We identify clusters of &amp;#8220;correlated&amp;#8221; Web pages in a site, called Web site communities, and make these communities the basic outsourcing unit. Through a detailed simulation environment, using both real and synthetic         data, we show that the proposed techniques are very robust and effective in reducing the user-perceived latency, performing         very close to an unfeasible, off-line policy, which has full knowledge of the content popularity.      </content></document><document><year>2007</year><authors>Roberto De Virgilio1 | Riccardo Torlone1  | Geert-Jan Houben2 </authors><title>Rule-based Adaptation of Web Information Systems      </title><content>Mobile devices provide a variety of ways to access information resources available on the Web and a high level of adaptability         to different aspects of the context (such as the device capabilities, the network QoS, the user preferences, and the location)         is strongly required in this scenario. In this paper, we present a rule-based approach supporting the automatic adaptation         of content delivery in Web Information Systems. The approach relies on the general notions of profile and configuration. The         former is used to model a variety of context characteristics in a uniform way. The latter describes, in abstract terms, how         to build the various levels of a suitable Web interface (content, navigation and presentation). We propose an original notion         of adaptation rule that can be used to specify, in a declarative way, how to build a configuration that satisfies the requirements         of adaptation for a profile. The evaluation process defined for these rules supports: (1) the handling of many separately         specified adaptation requirements according to different aspects of the context, possibly not fixed in advance, and (2) their         integration into one coherent recipe for adaptation. We also describe the architecture and functionality of a prototype implementing         the proposed approach and illustrate experimental results supporting its flexibility and efficiency.      </content></document><document><year>2007</year><authors>J. P. Martin-Flatin1  | Welf LГ¶we2 </authors><title>Special Issue on Recent Advances in Web Services         Guest Editorial</title><content>Without Abstract</content></document><document><year>2007</year><authors>Morgan Ericsson1 </authors><title>The Effects of XML Compression on SOAP Performance      </title><content>XML is the foundation of the SOAP protocol, and in turn, Web Service communication. This self-descriptive textual format for         structured data is renowned to be verbose. This verbosity can cause problems due to communication and processing overhead         in resource-constrained environments (e.g., small wireless devices). In this paper, we compare different binary representations         of XML documents. To this end, we propose a multifaceted and reusable test suite based on real-world scenarios. Our main result         is that only simple XML compression methods are suitable for a wide range of scenarios. While these simple methods do not         match the compression ratios of more specialized ones, they are still competitive in most scenarios. We also show that there         are scenarios that none of the evaluated methods can deal with efficiently.      </content></document><document><year>2007</year><authors>C. Batini1 | D. Bolchini3 | S. Ceri2 | M. Matera2 | A. Maurino1  | P. Paolini2 </authors><title>The UM-MAIS Methodology for Multi-channel Adaptive Web Information Systems      </title><content>Multichannel Adaptive Web Information Systems (WISs) are emerging as a new class of information systems, characterized by         their powerful use of mobility and context-awareness. Different methodologies have been proposed so far for the analysis and         design of Multichannel Adaptive WISs, specifically focused on the front-end layer or the back-end layer, but no methodology         has aimed to cover all the lifecycle and to design all the components that characterize Multichannel Adaptive WIS. This paper         fills such a gap, by presenting UM-MAIS (Unified Methodology for Multichannel Adaptive Information Systems), a new methodology         that capitalizes on well-established existing methods. It supports the analysis and design of the various components of Multichannel         Adaptive WISs (including the user&amp;#8217;s experience) in a comprehensive and unified manner with special emphasis on context modeling,         personalization, and adaptation.      </content></document><document><year>2007</year><authors>Hai He1 | Weiyi Meng1 | Yiyao Lu1 | Clement Yu2  | Zonghuan Wu3 </authors><title>Towards Deeper Understanding of the Search Interfaces of the Deep Web      </title><content>Many databases have become Web-accessible through form-based search interfaces (i.e., HTML forms) that allow users to specify         complex and precise queries to access the underlying databases. In general, such a Web search interface can be considered         as containing an interface schema with multiple attributes and rich semantic/meta-information; however, the schema is not         formally defined in HTML. Many Web applications, such as Web database integration and deep Web crawling, require the construction         of the schemas. In this paper, we first propose a schema model for representing complex search interfaces, and then present         a layout-expression based approach to automatically extract the logical attributes from search interfaces. We also rephrase         the identification of different types of semantic information as a classification problem, and design several Bayesian classifiers         to help derive semantic information from extracted attributes. A system, WISE-iExtractor, has been implemented to automatically         construct the schema from any Web search interfaces. Our experimental results on real search interfaces indicate that this         system is highly effective.      </content></document><document><year>2007</year><authors>Istvan Beszteri1  | Petri Vuorimaa1</authors><title>Vertical Navigation of Layout Adapted Web Documents      </title><content>With the development of mobile devices, digital televisions and game consoles, a vast amount of different client terminals         become capable to browse the Internet. Content providers have to find adaptation solutions to serve all devices at the same         time. This paper introduces a dynamic layout adaptation algorithm for web documents and an eXtensible Markup Language (XML)         that supports layout adaptation. The presented layout adaptation language is based on existing World Wide Web Consortium (W3C)         specifications, i.e., XFrames and CSS. Adaptation might results in several pages within one document. There is a need for         a navigation method for browsing such documents. Therefore, we introduce a navigation method that supports paging. An implementation         of the adaptation algorithm and the navigation method is also discussed in this paper.      </content></document><document><year>2007</year><authors>Stefania B|ini1 | Marcello Sarini2 | Carla Simone1  | Giuseppe Vizzari1 </authors><title>WWW in the Small         Towards Sustainable Adaptivity</title><content>The web technology is increasingly used by communities of various kinds to support their memory and the interactions among         their members. In these cases the WWW can be viewed as a web of local spaces (Web in the small), each characterized by local structure and semantics. The local space can be used to provide the community members with         adaptive functionalities to enhance their navigations and to promote new forms of interaction among them. The realization         and management of these functionalities are sustainable since they do not require more knowledge and effort than the ones         the community generates and spends to survive. The paper presents a technology, the LAW system, which is a step towards a         view of the Web in the small and complements more traditional technologies supporting the current acceptation of the World         Wide Web (Web in the large). The LAW system is informed by an agent based model where space and its topology are first class concepts.      </content></document><document><year>2005</year><authors>Huamin Chen1  | Prasant Mohapatra1 </authors><title>A Context-Aware HTML/XML Document Transmission Process for Mobile Wireless Clients      </title><content>Delivery and rendering of HTML/XML documents has been a core task in many contemporary networking applications. In mobile         wireless networks, efficient handling of these types of documents is necessary due to the frequent disconnections, packet         loss, and high bit error rate. One approach to address the challenge is the ability to process and reuse the partial data.         Current application protocols like HTTP cannot support this approach due to the following constraints: TCP's in-order data         uploading to the applications and tag matching. We propose a context-aware transmission process (CATP) to run on top of UDP.         This protocol does not transmit HTML/XML files in-order. Instead, it reorganizes the files and transmit tags first before         transporting their enclosed data. Conforming browsers receive the file structures and fill in with subsequent data packets         in whatever sequence they arrive. As a result, lost and delayed packets do not hinder rendering of those that are logically         behind but have already arrived at the client sides. Thus the retransmission of the lost frames can be concealed and overall         user perceived performance improved. The user-perceivable performance is quantified in terms of silent time during which no         activity is observed at the browser display. The protocol also facilitates partial content caching, amortizing network transmission         overhead, and non-interactive applications of Web services. We validated this protocol through prototype implementation and         compared the performance with TCP and in-order delivery UDP schemes. Our protocol provides better user-perceivable performance         under various loss rates and document sizes.      </content></document><document><year>2005</year><authors>Siu-Nam Chuang1  | Alvin T. S. Chan1 </authors><title>Active Service for Mobile Middleware</title><content>This paper describes a dynamic service reconfiguration model where the proxy is composed of a chain of service objects called mobilets (pronounced as mo-be-lets), which can be actively deployed onto a network. This model offers flexibility because the chain of mobilets can be dynamically reconfigured to adapt to dynamic changes in the wireless environment, without interrupting service provision for other mobile nodes. We have realized the dynamic service reconfiguration model by crafting its design into a programmable infrastructure that forms the baseline architecture of the WebPADS (short for Web Proxy for Actively Deployable Services) system.</content></document><document><year>2005</year><authors>S. Bergamaschi1 | G. Gelati1| F. Guerra1 | M. Vincini1</authors><title>An Intelligent Data Integration Approach for Collaborative Project Management in Virtual Enterprises      </title><content>The increasing globalization and flexibility required by companies has generated new issues in the last decade related to         the managing of large scale projects and to the cooperation of enterprises within geographically distributed networks. ICT         support systems are required to help enterprises share information, guarantee data-consistency and establish synchronized         and collaborative processes.                     In this paper we present a collaborative project management system that integrates data coming from aerospace industries with               a main goal: to facilitate the activity of assembling, integration and the verification of a multi-enterprise project. The               main achievement of the system from a data management perspective is to avoid inconsistencies generated by updates at the               sources&amp;#8217; level and minimizes data replications. The developed system is composed of a collaborative project management component               supported by a web interface, a multi-agent data integration system, which supports information sharing and querying, and               web-services that ensure the interoperability of the software components.            </content></document><document><year>2005</year><authors>An XML-Based Approach to Publishing and Querying the History of Databases      </authors><title>There is much current interest in publishing and viewing databases as XML documents. The general benefits of this approach         follow from the popularity of XML and the tool set available for visualizing and processing information encoded in this universal         standard. In this paper, we explore the additional and unique benefits achieved by this approach on temporal database applications.         We show that XML with XQuery can provide surprisingly effective solutions to the problem of supporting historical queries         on past content of database relations and their evolution. Indeed, using XML, the histories of database relations can be naturally         represented by temporally grouped data models. Thus, we identify mappings from relations to XML that are most conducive to         modeling and querying database histories, and show that temporal queries that would be difficult to express in SQL can be         easily expressed in standard XQuery. This approach is very general, insofar as it can be used to store the version history         of arbitrary documents and, for relational databases, it also supports queries on the evolution of their schema.                     Then, we turn to the problem of supporting efficiently the storage and the querying of relational table histories. We present               an experimental study of the pros and cons of using native XML databases, versus using traditional databases, where the XML-represented               histories are supported as views on the historical tables.            </title><content/></document></documents>