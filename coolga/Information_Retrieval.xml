<?xml version="1.0" encoding="UTF-8" standalone="no"?><documents><document><year>2004</year><authors>Daniel P. Lopresti1</authors><title>A Comparison of Text-Based Methods for Detecting Duplication in Scanned Document Databases</title><content>This paper presents an experimental evaluation of several text-based methods for detecting duplication in scanned document databases using uncorrected OCR output. This task is made challenging both by the wide range of degradations printed documents can suffer, and by conflicting interpretations of what it means to be a duplicate. We report results for four sets of experiments exploring various aspects of the problem space. While the techniques studied are generally robust in the face of most types of OCR errors, there are nonetheless important differences which we identify and discuss in detail.</content></document><document><year>2004</year><authors>Jee-Hyub Kim1| Byung-Kwan Kwak2| Seungwoo Lee2| Geunbae Lee2 | Jong-Hyeok Lee2</authors><title>A Corpus-Based Learning Method of Compound Noun Indexing Rules for Korean</title><content>In Korean information retrieval, compound nouns play an important role in improving precision in search experiments. There are two major approaches to compound noun indexing in Korean: statistical and linguistic. Each method, however, has its own shortcomings, such as limitations when indexing diverse types of compound nouns, over-generation of compound nouns, and data sparseness in training. In this paper, we propose a corpus-based learning method, which can index diverse types of compound nouns using rules automatically extracted from a large corpus. The automatic learning method is more portable and requires less human effort, although it exhibits a performance level similar to the manual-linguistic approach. We also present a new filtering method to solve the problems of compound noun over-generation and data sparseness.</content></document><document><year>2004</year><authors>Ronald R. Yager1</authors><title>A Hierarchical Document Retrieval Language</title><content>The focus of this work is on the development of a document retrieval language which attempts to enable users to better represent their requirements with respect to retrieved documents. We describe a framework for evaluating documents which allows, in the spirit of computing with words, a linguistic specification of the interrelationship between the desired attributes. This framework, which makes considerable use of the Ordered Weighted Averaging (OWA) operator, also supports a hierarchical structure which allows for an increased expressiveness of queries.</content></document><document><year>2004</year><authors>Georgios Sakkis1 | Ion Androutsopoulos2 | Georgios Paliouras1 | Vangelis Karkaletsis1 | Constantine D. Spyropoulos1  | Panagiotis Stamatopoulos2 </authors><title>A Memory-Based Approach to Anti-Spam Filtering for Mailing Lists</title><content>This paper presents an extensive empirical evaluation of memory-based learning in the context of anti-spam filtering, a novel cost-sensitive application of text categorization that attempts to identify automatically unsolicited commercial messages that flood mailboxes. Focusing on anti-spam filtering for mailing lists, a thorough investigation of the effectiveness of a memory-based anti-spam filter is performed using a publicly available corpus. The investigation includes different attribute and distance-weighting schemes, and studies on the effect of the neighborhood size, the size of the attribute set, and the size of the training corpus. Three different cost scenarios are identified, and suitable cost-sensitive evaluation functions are employed. We conclude that memory-based anti-spam filtering for mailing lists is practically feasible, especially when combined with additional safety nets. Compared to a previously tested Naive Bayes filter, the memory-based filter performs on average better, particularly when the misclassification cost for non-spam messages is high.</content></document><document><year>2004</year><authors>I.A.R. Moghrabi1  | R.A. Makholian2</authors><title>A New Approach to Clustering Records in Information Retrieval Systems</title><content>This work introduces a new approach to record clustering where a hybrid algorithm is presented to cluster records based upon threshold values and the query patterns made to a particular database. The Hamming Distance of a file is used as a measure of space density. The objective of the algorithm is to minimize the Hamming Distance of the file while attaching significance to the most frequent queries being asked. Simulation experiments conducted proved that a great reduction in response time is yielded after the restructuring of a file. We study the space density properties of a file and how it affects retrieval time before and after clustering, as a means of predicting file performance and making appropriate choices of parameters. Criteria, such as, block size, threshold value, percentage of records satisfying a given set of queries, etc., which affect clustering and response time are also studied.</content></document><document><year>2004</year><authors>Jane Reid1 </authors><title>A Task-Oriented Non-Interactive Evaluation Methodology for Information Retrieval Systems</title><content>Past research has identified many different types of relevance in information retrieval (IR). So far, however, most evaluation of IR systems has been through batch experiments conducted with test collections containing only expert, topical relevance judgements. Recently, there has been some movement away from this traditional approach towards interactive, more user-centred methods of evaluation. However, these are expensive for evaluators in terms both of time and of resources. This paper describes a new evaluation methodology, using a task-oriented test collection, which combines the advantages of traditional non-interactive testing with a more user-centred emphasis. The main features of a task-oriented test collection are the adoption of the task, rather than the query, as the primary unit of evaluation and the naturalistic character of the relevance judgements.</content></document><document><year>2004</year><authors>Marjo Markkula1| Marius Tico2| Bemmu Sepponen3| Katja Nirkkonen3 | Eero Sormunen1</authors><title>A Test Collection for the Evaluation of Content-Based Image Retrieval Algorithms&amp;#x2014;A User and Task-Based Approach</title><content>Content-based image retrieval (CBIR) algorithms have been seen as a promising access method for digital photograph collections. Unfortunately, we have very little evidence of the usefulness of these algorithms in real user needs and contexts. In this paper, we introduce a test collection for the evaluation of CBIR algorithms. In the test collection, the performance testing is based on photograph similarity perceived by end-users in the context of realistic illustration tasks and environment. The building process and the characteristics of the resulting test collection are outlined, including a typology of similarity criteria expressed by the subjects judging the similarity of photographs. A small-scale study on the consistency of similarity assessments is presented. A case evaluation of two CBIR algorithms is reported. The results show clear correlation between the subjects' similarity assessments and the functioning of feature parameters of the tested algorithms.</content></document><document><year>2004</year><authors>David Eichmann1  | Padmini Srinivasan2 </authors><title>Adaptive Filtering of Newswire Stories using Two-Level Clustering</title><content>Adaptive filtering of news is an area of information retrieval gaining substantial interest as services become more available on the Internet. This paper reports on a number of experiments involving a two-level clustering approach using a variety of techniques including threshold adaptation, topic vocabulary adaptation and both noun phrase and named entity adaptation. Our goal in this exploratory research is to empirically compare alternative configurations of our filtering approach that will allow us to better understand the relative value of the component subsystems.</content></document><document><year>2004</year><authors>Gonzalo Navarro1 | Edleno Silva de Moura2 | Marden Neubert3 | Nivio Ziviani4  | Ricardo Baeza-Yates5 </authors><title>Adding Compression to Block Addressing Inverted Indexes</title><content>Inverted index compression, block addressing and sequential search on compressed text are three techniques that have been separately developed for efficient, low-overhead text retrieval. Modern text compression techniques can reduce the text to less than 30% of its size and allow searching it directly and faster than the uncompressed text. Inverted index compression obtains significant reduction of its original size at the same processing speed. Block addressing makes the inverted lists point to text blocks instead of exact positions and pay the reduction in space with some sequential text scanning.In this work we combine the three ideas in a single scheme. We present a compressed inverted file that indexes compressed text and uses block addressing. We consider different techniques to compress the index and study their performance with respect to the block size. We compare the index against three separate techniques for varying block sizes, showing that our index is superior to each isolated approach. For instance, with just 4% of extra space overhead the index has to scan less than 12% of the text for exact searches and about 20% allowing one error in the matches.</content></document><document><year>2004</year><authors>Elizabeth Liddy1</authors><title>Advances in Automatic Text Summarization</title><content>Without Abstract</content></document><document><year>2004</year><authors>Jon Herlocker1 | Joseph A. Konstan2  | John Riedl2 </authors><title>An Empirical Analysis of Design Choices in Neighborhood-Based Collaborative Filtering Algorithms</title><content>Collaborative filtering systems predict a user's interest in new items based on the recommendations of other people with similar interests. Instead of performing content indexing or content analysis, collaborative filtering systems rely entirely on interest ratings from members of a participating community. Since predictions are based on human ratings, collaborative filtering systems have the potential to provide filtering based on complex attributes, such as quality, taste, or aesthetics. Many implementations of collaborative filtering apply some variation of the neighborhood-based prediction algorithm. Many variations of similarity metrics, weighting approaches, combination measures, and rating normalization have appeared in each implementation. For these parameters and others, there is no consensus as to which choice of technique is most appropriate for what situations, nor how significant an effect on accuracy each parameter has. Consequently, every person implementing a collaborative filtering system must make hard design choices with little guidance. This article provides a set of recommendations to guide design of neighborhood-based prediction systems, based on the results of an empirical study. We apply an analysis framework that divides the neighborhood-based prediction approach into three components and then examines variants of the key parameters in each component. The three components identified are similarity computation, neighbor selection, and rating combination.</content></document><document><year>2004</year><authors>Massimo Melucci</authors><title>An Evaluation of Automatically Constructed Hypertexts for Information Retrieval</title><content>This paper assesses the retrieval effectiveness of automatically constructed inter-document hypertext links in Information Retrieval (IR). The objectives of the experiments described are to obtain evidence concerning the usefulness of querying and browsing automatically constructed IR hypertexts. Links are built by using IR techniques, as these enable rapid, automatic production of hypertexts from a document collection for accessing the collection itself. These tests are carried out in a laboratory environment and through simulation of link browsing. Results of experiments show that browsing has little impact on the retrieval of relevant documents if used in place of querying or relevance feedback methods, though may be practical if used in combination with them.</content></document><document><year>2004</year><authors>Yiming Yang</authors><title>An Evaluation of Statistical Approaches to Text Categorization</title><content>This paper focuses on a comparative evaluation of a wide-range of text categorization methods, including previously published results on the Reuters corpus and new results of additional experiments. A controlled study using three classifiers, kNN, LLSF and WORD, was conducted to examine the impact of configuration variations in five versions of Reuters on the observed performance of classifiers. Analysis and empirical evidence suggest that the evaluation results on some versions of Reuters were significantly affected by the inclusion of a large portion of unlabelled documents, mading those results difficult to interpret and leading to considerable confusions in the literature. Using the results evaluated on the other versions of Reuters which exclude the unlabelled documents, the performance of twelve methods are compared directly or indirectly. For indirect compararions, kNN, LLSF and WORD were used as baselines, since they were evaluated on all versions of Reuters that exclude the unlabelled documents. As a global observation, kNN, LLSF and a neural network method had the best performance; except for a Naive Bayes approach, the other learning algorithms also performed relatively well.</content></document><document><year>2004</year><authors>Quan Wang1  | Yiu-Kai Ng1 </authors><title>An Ontology-Based Binary-Categorization Approach for Recognizing Multiple-Record Web Documents Using a Probabilistic Retrieval Model</title><content>The Web contains a tremendous amount of information. It is challenging to determine which Web documents are relevant to a user query, and even more challenging to rank them according to their degrees of relevance. In this paper, we propose a probabilistic retrieval model using logistic regression for recognizing multiple-record Web documents against an application ontology, a simple conceptual modeling approach. We notice that many Web documents contain a sequence of chunks of textual information, each of which constitutes a record. This type of documents is referred to as multiple-record documents. In our categorization approach, a document is represented by a set of term frequencies of index terms, a density heuristic value, and a grouping heuristic value. We first apply the logistic regression analysis on relevant probabilities using the (i) index terms, (ii) density value, and (iii) grouping value of each training document. Hereafter, the relevant probability of each test document is interpolated from the fitting curves. Contrary to other probabilistic retrieval models, our model makes only a weak independent assumption and is capable of handling any important dependent relationships among index terms. In addition, we use logistic regression, instead of linear regression analysis, because the relevance probabilities of training documents are discrete. Using a test set of car-ads and another one for obituary Web documents, our probabilistic model achieves the averaged recall ratio of 100%, precision ratio of 83.3%, and accuracy ratio of 92.5%.</content></document><document><year>2004</year><authors>CJ Van Rijsbergen1</authors><title>Another Look at the Logical Uncertainty Principle</title><content>The Logical Uncertainty Principle is re-examined from the point of classical logic. Two interpretations are given, an objective one in terms of an axiomatic theory of information, and a subjective one based on Ramsey's theory of probability.</content></document><document><year>2004</year><authors>Xiangji Huang1 | Fuchun Peng1 | Dale Schuurmans1 | Nick Cercone1  | Stephen E. Robertson1| 2 </authors><title>Applying Machine Learning to Text Segmentation for Information Retrieval</title><content>We propose a self-supervised word segmentation technique for text segmentation in Chinese information retrieval. This method combines the advantages of traditional dictionary based, character based and mutual information based approaches, while overcoming many of their shortcomings. Experiments on TREC data show this method is promising. Our method is completely language independent and unsupervised, which provides a promising avenue for constructing accurate multi-lingual or cross-lingual information retrieval systems that are flexible and adaptive. We find that although the segmentation accuracy of self-supervised segmentation is not as high as some other segmentation methods, it is enough to give good retrieval performance. It is commonly believed that word segmentation accuracy is monotonically related to retrieval performance in Chinese information retrieval. However, for Chinese, we find that the relationship between segmentation and retrieval performance is in fact nonmonotonic; that is, at around 70% word segmentation accuracy an over-segmentation phenomenon begins to occur which leads to a reduction in information retrieval performance. We demonstrate this effect by presenting an empirical investigation of information retrieval on Chinese TREC data, using a wide variety of word segmentation algorithms with word segmentation accuracies ranging from 44% to 95%, including 70% word segmentation accuracy from our self-supervised word-segmentation approach. It appears that the main reason for the drop in retrieval performance is that correct compounds and collocations are preserved by accurate segmenters, while they are broken up by less accurate (but reasonable) segmenters, to a surprising advantage. This suggests that words themselves might be too broad a notion to conveniently capture the general semantic meaning of Chinese text. Our research suggests machine learning techniques can play an important role in building adaptable information retrieval systems and different evaluation standards for word segmentation should be given to different applications.</content></document><document><year>2004</year><authors>Aleks|er Kocz1  | Joshua Alspector2 </authors><title>Asymmetric Missing-data Problems: Overcoming the Lack of Negative Data in Preference Ranking</title><content>In certain classification problems there is a strong a asymmetry between the number of labeled examples available for each of the classes involved. In an extreme case, there may be a complete lack of labeled data for one of the classes while, at the same time, there are adequate labeled examples for the others, accompanied by a large body of unlabeled data. Since most classification algorithms require some information about all classes involved, label estimation for the un-represented class is desired. An important representative of this group of problems is that of user interest/preference modeling where there may be a large number of examples of what the user likes with essentially no counterexamples.Recently, there has been much interest in applying the EM algorithm to incomplete data problems in the area of text retrieval and categorization. We adapt this approach to the asymmetric case of modeling user interests in news articles, where only labeled positive training data are available, with access to a large corpus of unlabeled documents. User modeling is here equivalent to that of user-specific document ranking. EM is used in conjunction with the Naive Bayes model while its output is also utilized by a Support Vector Machine and Rocchio's technique.</content></document><document><year>2004</year><authors>Fuchun Peng1 | Dale Schuurmans2  | Shaojun Wang3 </authors><title>Augmenting Naive Bayes Classifiers with Statistical Language Models</title><content>We augment naive Bayes models with statistical n-gram language models to address short-comings of the standard naive Bayes text classifier. The result is a generalized naive Bayes classifier which allows for a local Markov dependence among observations; a model we refer to as the ChainAugmentedNaiveBayes (CAN) Bayes classifier. CAN models have two advantages over standard naive Bayes classifiers. First, they relax some of the independence assumptions of naive Bayes&amp;#x2014;allowing a local Markov chain dependence in the observed variables&amp;#x2014;while still permitting efficient inference and learning. Second, they permit straightforward application of sophisticated smoothing techniques from statistical language modeling, which allows one to obtain better parameter estimates than the standard Laplace smoothing used in naive Bayes classification. In this paper, we introduce CAN models and apply them to various text classification problems. To demonstrate the language independent and task independent nature of these classifiers, we present experimental results on several text classification problems&amp;#x2014;authorship attribution, text genre classification, and topic detection&amp;#x2014;in several languages&amp;#x2014;Greek, English, Japanese and Chinese. We then systematically study the key factors in the CAN model that can influence the classification performance, and analyze the strengths and weaknesses of the model.</content></document><document><year>2004</year><authors>Maayan Geffet1 | Yair Wiseman1  | Dror Feitelson1 </authors><title>Automatic Alphabet Recognition</title><content>The last step of the Information Retrieval process is to display the found documents to the user. However, some difficulties might occur at that point. English texts are usually written in the ASCII standard. Unlike the English language, many languages have different character sets, and do not have one standard. This plurality of standards causes problems, especially in a web environment, where one may download a document with an unknown standard. This paper suggests a purely automatic way of finding the standard which was used by the document writer based on the statistical letters distribution in the language. We developed a vector-space-based method that creates frequencies vectors for each letter of the language and then matches a new document''s vectors to the pre-computed templates. The algorithm was applied on various types of corpora in Hebrew, Russian and English, and provides an efficient solution to the stated problem in most cases.</content></document><document><year>2004</year><authors>Andrew Kachites McCallum1 | Kamal Nigam2 | Jason Rennie3  | Kristie Seymore4 </authors><title>Automating the Construction of Internet Portals with Machine Learning</title><content>Domain-specific internet portals are growing in popularity because they gather content from the Web and organize it for easy access, retrieval and search. For example, www.campsearch.com allows complex queries by age, location, cost and specialty over summer camps. This functionality is not possible with general, Web-wide search engines. Unfortunately these portals are difficult and time-consuming to maintain. This paper advocates the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific Internet portals. We describe new research in reinforcement learning, information extraction and text classification that enables efficient spidering, the identification of informative text segments, and the population of topic hierarchies. Using these techniques, we have built a demonstration system: a portal for computer science research papers. It already contains over 50,000 papers and is publicly available at www.cora.justresearch.com. These techniques are widely applicable to portal creation in other domains.</content></document><document><year>2004</year><authors>Alistair Moffat1  | Lang Stuiver2</authors><title>Binary Interpolative Coding for Effective Index Compression</title><content>Information retrieval systems contain large volumes of text, and currently have typical sizes into the gigabyte range. Inverted indexes are one important method for providing search facilities into these collections, but unless compressed require a great deal of space. In this paper we introduce a new method for compressing inverted indexes that yields excellent compression, fast decoding, and exploits clustering&amp;#x2014;the tendency for words to appear relatively frequently in some parts of the collection and infrequently in others. We also describe two other quite separate applications for the same compression method: representing the MTF list positions generated by the Burrows-Wheeler Block Sorting transformation; and transmitting the codebook for semi-static block-based minimum-redundancy coding.</content></document><document><year>2004</year><authors>David Banks| Paul Over | Nien-Fan Zhang</authors><title>Blind Men and Elephants: Six Approaches to TREC data</title><content>The paper reviews six recent efforts to better understand performance measurements on information retrieval (IR) systems within the framework of the Text REtrieval Conferences (TREC): analysis of variance, cluster analyses, rank correlations, beadplots, multidimensional scaling, and item response analysis. None of this work has yielded any substantial new insights. Prospects that additional work along these lines will yield more interesting results vary but are in general not promising. Some suggestions are made for paying greater attention to richer descriptions of IR system behavior but within smaller, better controlled settings.</content></document><document><year>2004</year><authors>Book Reviews</authors><title/></document><document><year>2009</year><authors>Enrique Amig&amp;#972 1 | Julio Gonzalo1 | Javier Artiles1  | Felisa Verdejo1 </authors><title>A comparison of extrinsic clustering evaluation metrics based on formal constraints      </title><content>Without Abstract</content></document><document><year>2009</year><authors>E. Herrera-Viedma1 | A. G. LГіpez-Herrera1 | S. Alonso2 | J. M. Moreno3 | F. J. Cabrerizo4  | C. Porcel5 </authors><title>A computer-supported learning system to help teachers to teach Fuzzy Information Retrieval Systems      </title><content>This paper describes a computer-supported learning system to teach students the principles and concepts of Fuzzy Information         Retrieval Systems based on weighted queries. This tool is used to support the teacher&amp;#8217;s activity in the degree course Information Retrieval Systems Based on Artificial Intelligence at the Faculty of Library and Information Sciences at the University of Granada. Learning of languages of weighted queries         in Fuzzy Information Retrieval Systems is complex because it is very difficult to understand the different semantics that         could be associated to the weights of queries together with their respective strategies of query evaluation. We have developed         and implemented this computer-supported education system because it allows to support the teacher&amp;#8217;s activity in the classroom         to teach the use of weighted queries in FIRSs and it helps students to develop self-learning processes on the use of such         queries. We have evaluated the performance of its use in the learning process according to the students&amp;#8217; perceptions and their         results obtained in the course&amp;#8217;s exams. We have observed that using this software tool the students learn better the management         of the weighted query languages and then their performance in the exams is improved.      </content></document><document><year>2009</year><authors>Yuting Liu1 | Tie-Yan Liu2 | Bin Gao2 | Zhiming Ma3  | Hang Li2 </authors><title>A framework to compute page importance based on user behaviors      </title><content>This paper is concerned with a framework to compute the importance of webpages by using real browsing behaviors of Web users.         In contrast, many previous approaches like PageRank compute page importance through the use of the hyperlink graph of the         Web. Recently, people have realized that the hyperlink graph is incomplete and inaccurate as a data source for determining         page importance, and proposed using the real behaviors of Web users instead. In this paper, we propose a formal framework         to compute page importance from user behavior data (which covers some previous works as special cases). First, we use a stochastic         process to model the browsing behaviors of Web users. According to the analysis on hundreds of millions of real records of         user behaviors, we justify that the process is actually a continuous-time time-homogeneous Markov process, and its stationary         probability distribution can be used as the measure of page importance. Second, we propose a number of ways to estimate parameters         of the stochastic process from real data, which result in a group of algorithms for page importance computation (all referred         to as BrowseRank). Our experimental results have shown that the proposed algorithms can outperform the baseline methods such         as PageRank and TrustRank in several tasks, demonstrating the advantage of using our proposed framework.      </content></document><document><year>2009</year><authors>Ian Soboroff1 </authors><title>A guide to the RIA workshop data archive      </title><content>During the course of the Reliable Information Access (RIA) workshop, a data archive was created to hold the outputs of the         many experiments being done. This archive was designed to serve both as an organizational structure to support the researchers         at the workshop itself and as a public archive of experimental retrieval results. This article describes the structure of         the data in the archive and the ways in which the data may be accessed.      </content></document><document><year>2009</year><authors>Bettina Berendt1  | Anett Kralisch2</authors><title>A user-centric approach to identifying best deployment strategies for language tools: the impact of content and access language         on Web user behaviour and attitudes      </title><content>The number of Web users whose first language is not English continues to grow, as does the amount of content provided in languages         other than English. This poses new challenges for actors on the Web, such as in which language(s) content should be offered,         how search tools should deal with mono- and multilingual content, and how users can make the best use of navigation and search         options, suited to their individual linguistic skills. How should these challenges be dealt with? Technological approaches         to non-English (or in general, cross-language) Web search have made large progress; however, translation remains a hard problem.         This precludes a low-cost but high-quality blanket all-language coverage of the whole Web. In this paper, we propose a user-centric         approach to answering questions of where to best concentrate efforts and investments. Drawing on linguistic research, we describe         data on the availability of content and access to it in first and second languages across the Web. We then present three studies         that investigated the impact of the availability (or not) of first-language content and access forms on user behaviour and         attitudes. The results indicate that non-English languages are under-represented on the Web and that this is partly due to         content-creation, link-setting and link-following behaviour. They also show that user satisfaction is influenced both by the         cognitive effort of searching and the availability of alternative information in that language. These findings suggest that         more cross-language tools are desirable. However, they also indicate that context (such as user groups&amp;#8217; domain expertise or         site type) should be considered when tradeoffs between information quality and multilinguality need to be taken into account.      </content></document><document><year>2009</year><authors>Gareth J. F. Jones1 </authors><title>An inquiry-based learning approach to teaching information retrieval      </title><content>The study of information retrieval (IR) has increased in interest and importance with the explosive growth of online information         in recent years. Learning about IR within formal courses of study enables users of search engines to use them more knowledgeably         and effectively, while providing the starting point for the explorations of new researchers into novel search technologies.         Although IR can be taught in a traditional manner of formal classroom instruction with students being led through the details         of the subject and expected to reproduce this in assessment, the nature of IR as a topic makes it an ideal subject for inquiry-based         learning approaches to teaching. In an inquiry-based learning approach students are introduced to the principles of a subject         and then encouraged to develop their understanding by solving structured or open problems. Working through solutions in subsequent         class discussions enables students to appreciate the availability of alternative solutions as proposed by their classmates.         Following this approach students not only learn the details of IR techniques, but significantly, naturally learn to apply         them in solution of problems. In doing this they not only gain an appreciation of alternative solutions to a problem, but         also how to assess their relative strengths and weaknesses. Developing confidence and skills in problem solving enables student         assessment to be structured around solution of problems. Thus students can be assessed on the basis of their understanding         and ability to apply techniques, rather simply their skill at reciting facts. This has the additional benefit of encouraging         general problem solving skills which can be of benefit in other subjects. This approach to teaching IR was successfully implemented         in an undergraduate module where students were assessed in a written examination exploring their knowledge and understanding         of the principles of IR and their ability to apply them to solving problems, and a written assignment based on developing         an individual research proposal.      </content></document><document><year>2009</year><authors>Lars Asker1 | Atelach Alemu Argaw1 | BjГ¶rn GambГ¤ck2| 3 | Samuel Eyassu Asfeha4  | Lemma Nigussie Habte4 </authors><title>Classifying Amharic webnews      </title><content>We present work aimed at compiling an Amharic corpus from the Web and automatically categorizing the texts. Amharic is the         second most spoken Semitic language in the World (after Arabic) and used for countrywide communication in Ethiopia. It is         highly inflectional and quite dialectally diversified. We discuss the issues of compiling and annotating a corpus of Amharic         news articles from the Web. This corpus was then used in three sets of text classification experiments. Working with a less-researched         language highlights a number of practical issues that might otherwise receive less attention or go unnoticed. The purpose         of the experiments has not primarily been to develop a cutting-edge text classification system for Amharic, but rather to         put the spotlight on some of these issues. The first two sets of experiments investigated the use of Self-Organizing Maps         (SOMs) for document classification. Testing on small datasets, we first looked at classifying unseen data into 10 predefined         categories of news items, and then at clustering it around query content, when taking 16 queries as class labels. The second         set of experiments investigated the effect of operations such as stemming and part-of-speech tagging on text classification         performance. We compared three representations while constructing classification models based on bagging of decision trees         for the 10 predefined news categories. The best accuracy was achieved using the full text as representation. A representation         using only the nouns performed almost equally well, confirming the assumption that most of the information required for distinguishing         between various categories actually is contained in the nouns, while stemming did not have much effect on the performance         of the classifier.      </content></document><document><year>2009</year><authors>Yuye Zhang1 | Laurence A. F. Park1  | Alistair Moffat1 </authors><title>Click-based evidence for decaying weight distributions in search effectiveness metrics      </title><content>Search effectiveness metrics are used to evaluate the quality of the answer lists returned by search services, usually based         on a set of relevance judgments. One plausible way of calculating an effectiveness score for a system run is to compute the         inner-product of the run&amp;#8217;s relevance vector and a &amp;#8220;utility&amp;#8221; vector, where the ith element in the utility vector represents the relative benefit obtained by the user of the system if they encounter a relevant         document at depth i in the ranking. This paper uses such a framework to examine the user behavior patterns&amp;#8212;and hence utility weightings&amp;#8212;that         can be inferred from a web query log. We describe a process for extrapolating user observations from query log clickthroughs,         and employ this user model to measure the quality of effectiveness weighting distributions. Our results show that for measures         with static distributions (that is, utility weighting schemes for which the weight vector is independent of the relevance         vector), the geometric weighting model employed in the rank-biased precision effectiveness metric offers the closest fit to         the user observation model. In addition, using past TREC data as to indicate likelihood of relevance, we also show that the         distributions employed in the BPref and MRR metrics are the best fit out of the measures for which static distributions do         not exist.      </content></document><document><year>2009</year><authors>Fotis Lazarinis1 | JesГєs Vilares2 | John Tait3  | Efthimis N. Efthimiadis4 </authors><title>Current research issues and trends in non-English Web searching      </title><content>With increasingly higher numbers of non-English language web searchers the problems of efficient handling of non-English Web         documents and user queries are becoming major issues for search engines. The main aim of this review paper is to make researchers         aware of the existing problems in monolingual non-English Web retrieval by providing an overview of open issues. A significant         number of papers are reviewed and the research issues investigated in these studies are categorized in order to identify the         research questions and solutions proposed in these papers. Further research is proposed at the end of each section.      </content></document><document><year>2009</year><authors>Heather L. O&amp;#8217 Brien1 </authors><title>D. Nahl, D. Bilal (eds.): Information and emotion: the emergent affective paradigm in information behaviour research and theory         Information Today, Inc.; Medford, New Jersey, 2007, 392 pp, Price $59.50, ISBN 978-1-5787-310-9</title><content>Without Abstract</content></document><document><year>2009</year><authors>Kjell LemstrГ¶m1 | Niko MikkilГ¤1 | Veli MГ¤kinen1</authors><title>Filtering methods for content-based retrieval on indexed symbolic music databases      </title><content>We introduce fast filtering methods for content-based music retrieval problems, where the music is modeled as sets of points         in the Euclidean plane, formed by the (on-set time, pitch) pairs. The filters exploit a precomputed index for the database,         and run in time dependent on the query length and intermediate output sizes of the filters, being almost independent of the         database size. With a quadratic size index, the filters are provably lossless for general point sets of this kind. In the         context of music, the search space can be narrowed down, which enables the use of a linear sized index for effective and efficient         lossless filtering. For the checking phase, which dominates the overall running time, we exploit previously designed algorithms         suitable for local checking. In our experiments on a music database, our best filter-based methods performed several orders         of a magnitude faster than the previously designed solutions.      </content></document><document><year>2009</year><authors>Fotis Lazarinis1 | Jesus Vilares2 | John Tait3  | Efthimis N. Efthimiadis4 </authors><title>Introduction to the special issue on non-english web retrieval      </title><content>Without Abstract</content></document><document><year>2009</year><authors>Juan M. FernГЎndez-Luna1 | Juan F. Huete1  | Andrew MacFarlane2 </authors><title>Introduction to the special issue on teaching and learning in information retrieval      </title><content>We present an overview of the special issue in this paper. The main objective is to provide information for lecturers on how         to improve the student experience, using current knowledge in the field. To this end we present an overview of six papers         covering areas as diverse as tools and methods used to support teaching and learning, pedagogical challenges in teaching mathematics         for search, etc.      </content></document><document><year>2009</year><authors>Iraklis A. Klampanos1 </authors><title>Manning Christopher, Prabhakar Raghavan, Hinrich SchГјtze: Introduction to information retrieval         Cambridge University Press, Cambridge, 2008, 478 pp, Price 60, ISBN 97805218657515</title><content>Without Abstract</content></document><document><year>2009</year><authors>Efthimis N. Efthimiadis1 | Nicos Malevris2 | Apostolos Kousaridas2| 3 | Alex|ra Lepeniotou2| 4  | Nikos Loutas2| 5 </authors><title>Non-english web search: an evaluation of indexing and searching the Greek web      </title><content>The study reports on a longitudinal and comparative evaluation of Greek language searching on the web. Ten engines, five global         (A9, AltaVista, Google, MSN Search, and Yahoo!) and five Greek (Anazitisi, Ano-Kato, Phantis. Trinity, and Visto), were evaluated         using (a) navigational queries in 2004 and 2006; and (b) by measuring the freshness of the search engine indices in 2005 and         2006. Homepage finding queries for known Greek organizations were created and searched. Queries included the name of the organization         in its Greek and non-Greek, English or transliterated equivalent forms. The organizations represented ten categories: government         departments, universities, colleges, travel agencies, museums, media (TV, radio, newspapers), transportation, and banks. The         freshness of the indices was evaluated by examining the status of the returned URLs (live versus dead) from the navigational         queries, and by identifying if the engines have indexed 32480 active (live) Greek domain URLs. Effectiveness measures included         (a) qualitative assessment of how engines handle the Greek language; (b) precision at 10 documents (P@10); (c) mean reciprocal         rank (MRR); (d) Navigational Query Discounted Cumulative Gain (NQ-DCG), a new heuristic evaluation measure; (e) response time;         (f) the ratio of the dead URL links returned, (g) the presence or absence of URLs and the decay observed over the period of         the study. The results report on which of the global and Greek search engines perform best; and if the performance achieved         is good enough from a user&amp;#8217;s perspective.      </content></document><document><year>2009</year><authors>Jakub Piskorski1 | Karol Wieloch2  | Marcin Sydow3 </authors><title>On knowledge-poor methods for person name matching and lemmatization for highly inflectional languages      </title><content>Web person search is one of the most common activities of Internet users. Recently, a vast amount of work on applying various         NLP techniques for person name disambiguation in large web document collections has been reported, where the main focus was         on English and few other major languages. This article reports on knowledge-poor methods for tackling person name matching         and lemmatization in Polish, a highly inflectional language with complex person name declension paradigm. These methods apply         mainly well-established string distance metrics, some new variants thereof, automatically acquired simple suffix-based lemmatization         patterns and some combinations of the aforementioned techniques. Furthermore, we also carried out some initial experiments         on deploying techniques that utilize the context, in which person names appear. Results of numerous experiments are presented.         The evaluation carried out on a data set extracted from a corpus of on-line news articles revealed that achieving lemmatization         accuracy figures greater than 90% seems to be difficult, whereas combining string distance metrics with suffix-based patterns         results in 97.6&amp;#8211;99% accuracy for the name matching task. Interestingly, no significant additional gain could be achieved through         integrating some basic techniques, which try to exploit the local context the names appear in. Although our explorations were         focused on Polish, we believe that the work presented in this article constitutes practical guidelines for tackling the same         problem for other highly inflectional languages with similar phenomena.      </content></document><document><year>2009</year><authors>Paul Ogilvie1| 2 | Ellen Voorhees3  | Jamie Callan1 </authors><title>On the number of terms used in automatic query expansion      </title><content>This paper investigates the number of expansion terms to use in automatic query expansion by examining the behavior of eight         retrieval systems participating in the NRRC Reliable Information Access Workshop. The results demonstrate that current systems         are able to obtain nearly all of the benefit of using a fixed number of expansion terms per topic, but significant additional         improvement is possible if systems were able to accurately select the best number of expansion terms on a per topic basis.         When optimizing average effectiveness as measured by mean average precision, using a fixed number of terms increases the score         a large amount for a small number of topics but has little effect for most topics. The analysis further suggests that when         a topic is helped by automatic feedback, the increase is from a set of terms that reinforce each other rather than from the         system finding a single excellent term.      </content></document><document><year>2009</year><authors>Donna Harman1  | Chris Buckley2 </authors><title>Overview of the Reliable Information Access Workshop      </title><content>The Reliable Information Access (RIA) Workshop was held in the summer of 2003, with a goal of improved understanding of information         retrieval systems, in particular with regard to the variability of retrieval performance across topics. The workshop ran massive         cross-system failure analysis on 45 of the TREC topics and also performed cross-system experiments on pseudo-relevance feedback.         This paper presents an overview of that workshop, along with some preliminary conclusions from these experiments. Even if         this workshop was held 6;years ago, the issues of improving system performance across all topics is still critical to the         field and this paper, along with the others in this issue, are the first widely published full papers for the workshop.      </content></document><document><year>2009</year><authors>Koji Eguchi1  | W. Bruce Croft2</authors><title>Query structuring and expansion with two-stage term dependence for Japanese web retrieval      </title><content>In this paper, we propose a new term dependence model for information retrieval, which is based on a theoretical framework         using Markov random fields. We assume two types of dependencies of terms given in a query: (i) long-range dependencies that         may appear for instance within a passage or a sentence in a target document, and (ii) short-range dependencies that may appear         for instance within a compound word in a target document. Based on this assumption, our two-stage term dependence model captures         both long-range and short-range term dependencies differently, when more than one compound word appear in a query. We also         investigate how query structuring with term dependence can improve the performance of query expansion using a relevance model.         The relevance model is constructed using the retrieval results of the structured query with term dependence to expand the         query. We show that our term dependence model works well, particularly when using query structuring with compound words, through         experiments using a 100-gigabyte test collection of web documents mostly written in Japanese. We also show that the performance         of the relevance model can be significantly improved by using the structured query with our term dependence model.      </content></document><document><year>2009</year><authors>Paul Thomas1  | David Hawking2 </authors><title>Server selection methods in personal metasearch: a comparative empirical study      </title><content>Server selection is an important subproblem in distributed information retrieval (DIR) but has commonly been studied with         collections of more or less uniform size and with more or less homogeneous content. In contrast, realistic DIR applications         may feature much more varied collections. In particular, personal metasearch&amp;#8212;a novel application of DIR which includes all         of a user&amp;#8217;s online resources&amp;#8212;may involve collections which vary in size by several orders of magnitude, and which have highly         varied data. We describe a number of algorithms for server selection, and consider their effectiveness when collections vary         widely in size and are represented by imperfect samples. We compare the algorithms on a personal metasearch testbed comprising         calendar, email, mailing list and web collections, where collection sizes differ by three orders of magnitude. We then explore         the effect of collection size variations using four partitionings of the TREC ad hoc data used in many other DIR experiments.         Kullback-Leibler divergence, previously considered poorly effective, performs better than expected in this application; other         techniques thought to be effective perform poorly and are not appropriate for this problem. A strong correlation with size-based         rankings for many techniques may be responsible.      </content></document><document><year>2009</year><authors>Charles L. A. Clarke1 | Gordon V. Cormack1 | Thomas R. Lynam1 | Chris Buckley2  | Donna Harman3 </authors><title>Swapping documents and terms      </title><content>Experiments were conducted to explore the impact of combining various components of eight leading information retrieval systems.         Each system demonstrated improved effectiveness through the use of blind feedback, also known as pseudo-relevance feedback, a form of query expansion. Blind feedback uses the results of a preliminary retrieval step to augment the efficacy of a         secondary retrieval step. The hybrid combination of primary and secondary retrieval steps from different systems in a number         of cases yielded better effectiveness than either of the constituent systems alone. This positive combining effect was observed         when entire documents were passed between the two retrieval steps, but not when only the expansion terms were passed. Several         combinations of primary and secondary retrieval steps were fused using the CombMNZ algorithm; all yielded significant effectiveness         improvement over the individual systems, with the best yielding an improvement of 13% (p;=;10&amp;#8722;6) over the best individual system and an improvement of 4% (p;=;10&amp;#8722;5) over a simple fusion of the eight systems.      </content></document><document><year>2009</year><authors>Juan M. FernГЎndez-Luna1 | Juan F. Huete1 | Andrew MacFarlane2  | Efthimis N. Efthimiadis3 </authors><title>Teaching and learning in information retrieval      </title><content>A literature review of pedagogical methods for teaching and learning information retrieval is presented. From the analysis         of the literature a taxonomy was built and it is used to structure the paper. Information Retrieval (IR) is presented from         different points of view: technical levels, educational goals, teaching and learning methods, assessment and curricula. The         review is organized around two levels of abstraction which form a taxonomy that deals with the different aspects of pedagogy         as applied to information retrieval. The first level looks at the technical level of delivering information retrieval concepts,         and at the educational goals as articulated by the two main subject domains where IR is delivered: computer science (CS) and         library and information science (LIS). The second level focuses on pedagogical issues, such as teaching and learning methods,         delivery modes (classroom, online or e-learning), use of IR systems for teaching, assessment and feedback, and curricula design.         The survey, and its bibliography, provides an overview of the pedagogical research carried out in the field of IR. It also         provides a guide for educators on approaches that can be applied to improving the student learning experiences.      </content></document><document><year>2009</year><authors>Chris Buckley1 </authors><title>Why current IR engines fail      </title><content>Observations from a unique investigation of failure analysis of Information Retrieval research engines held in 2003 are presented.         The Reliable Information Access Workshop invited seven leading IR research groups to supply both their systems and their experts         to an effort to analyze why their systems fail on some topics and whether the failures are due to system flaws, approach flaws,         or the topic itself. There were surprising results from this cross-system failure analysis. One is that despite systems retrieving         very different documents, the major cause of failure for any particular topic was almost always the same across all systems.         Another is that relationships between aspects of a topic are not especially important for state-of-the-art systems; the systems         are failing at a much more basic level where the top-retrieved documents are not reflecting some aspect at all. The investigatory         framework and the lessons learned can serve as a model for needed future research in this area.      </content></document><document><year>2009</year><authors>Nicola Stokes1 </authors><title>William Hersh: Information retrieval: a health and biomedical perspective, 3rd ed         Health and Informatics Series, Springer, 2009, 504;pp, $79.95, ISBN: 978-0-387-78702-2)</title><content>Without Abstract</content></document><document><year>2008</year><authors>Enrique AmigГі1 | Julio Gonzalo1 | Javier Artiles1  | Felisa Verdejo1 </authors><title>A comparison of extrinsic clustering evaluation metrics based on formal constraints      </title><content>There is a wide set of evaluation metrics available to compare the quality of text clustering algorithms. In this article,         we define a few intuitive formal constraints on such metrics which shed light on which aspects of the quality of a clustering         are captured by different metric families. These formal constraints are validated in an experiment involving human assessments,         and compared with other constraints proposed in the literature. Our analysis of a wide range of metrics shows that only BCubed satisfies all formal constraints. We also extend the analysis to the problem of overlapping clustering, where items can simultaneously         belong to more than one cluster. As Bcubed cannot be directly applied to this task, we propose a modified version of Bcubed         that avoids the problems found with other metrics.      </content></document><document><year>2008</year><authors>LuГ­s M. S. Russo1  | Arlindo L. Oliveira1 </authors><title>A compressed self-index using a Ziv&amp;#8211;Lempel dictionary      </title><content>A compressed full-text self-index for a text T, of size u, is a data structure used to search for patterns P, of size m, in T, that requires reduced space, i.e. space that depends on the empirical entropy (H                     k             or H         0) of T, and is, furthermore, able to reproduce any substring of T. In this paper we present a new compressed self-index able to locate the occurrences of P in O((m;+;occ)log;u) time, where occ is the number of occurrences. The fundamental improvement over previous LZ78 based indexes is the reduction of the search         time dependency on m from O(m         2) to O(m). To achieve this result we point out the main obstacle to linear time algorithms based on LZ78 data compression and expose         and explore the nature of a recurrent structure in LZ-indexes, the  suffix tree. We show that our method is very competitive in practice by comparing it against other state of the art compressed         indexes.      </content></document><document><year>2008</year><authors>Erik Boiy1  | Marie-Francine Moens1 </authors><title>A machine learning approach to sentiment analysis in multilingual Web texts      </title><content>Sentiment analysis, also called opinion mining, is a form of information extraction from text of growing research and commercial         interest. In this paper we present our machine learning experiments with regard to sentiment analysis in blog, review and         forum texts found on the World Wide Web and written in English, Dutch and French. We train from a set of example sentences         or statements that are manually annotated as positive, negative or neutral with regard to a certain entity. We are interested         in the feelings that people express with regard to certain consumption products. We learn and evaluate several classification         models that can be configured in a cascaded pipeline. We have to deal with several problems, being the noisy character of         the input texts, the attribution of the sentiment to a particular entity and the small size of the training set. We succeed         to identify positive, negative and neutral feelings to the entity under consideration with ca. 83% accuracy for English texts         based on unigram features augmented with linguistic features. The accuracy results of processing the Dutch and French texts         are ca. 70 and 68% respectively due to the larger variety of the linguistic expressions that more often diverge from standard         language, thus demanding more training patterns. In addition, our experiments give us insights into the portability of the         learned models across domains and languages. A substantial part of the article investigates the role of active learning techniques         for reducing the number of examples to be manually annotated.      </content></document><document><year>2008</year><authors>Norbert Fuhr1 </authors><title>A probability ranking principle for interactive information retrieval      </title><content>The classical Probability Ranking Principle (PRP) forms the theoretical basis for probabilistic Information Retrieval (IR)         models, which are dominating IR theory since about 20;years. However, the assumptions underlying the PRP often do not hold,         and its view is too narrow for interactive information retrieval (IIR). In this article, a new theoretical framework for interactive         retrieval is proposed: The basic idea is that during IIR, a user moves between situations. In each situation, the system presents         to the user a list of choices, about which s/he has to decide, and the first positive decision moves the user to a new situation.         Each choice is associated with a number of cost and probability parameters. Based on these parameters, an optimum ordering         of the choices can the derived&amp;#8212;the PRP for IIR. The relationship of this rule to the classical PRP is described, and issues         of further research are pointed out.      </content></document><document><year>2008</year><authors>Bebo White1 </authors><title>Amy Langville and Carl Meyer, Google&amp;#8217;s Page Rank and Beyond: The Science of Search Engine Rankings         Princeton University Press, Princeton, 2006, 234 pp, $35.00, ISBN 978069112201</title><content>Without Abstract</content></document><document><year>2008</year><authors>Yue Lu1 | Hui Fang2  | Chengxiang Zhai1 </authors><title>An empirical study of gene synonym query expansion in biomedical information retrieval      </title><content>Due to the heavy use of gene synonyms in biomedical text, people have tried many query expansion techniques using synonyms         in order to improve performance in biomedical information retrieval. However, mixed results have been reported. The main challenge         is that it is not trivial to assign appropriate weights to the added gene synonyms in the expanded query; under-weighting         of synonyms would not bring much benefit, while overweighting some unreliable synonyms can hurt performance significantly.         So far, there has been no systematic evaluation of various synonym query expansion strategies for biomedical text. In this         work, we propose two different strategies to extend a standard language modeling approach for gene synonym query expansion         and conduct a systematic evaluation of these methods on all the available TREC biomedical text collections for ad hoc document         retrieval. Our experiment results show that synonym expansion can significantly improve the retrieval accuracy. However, different         query types require different synonym expansion methods, and appropriate weighting of gene names and synonym terms is critical         for improving performance.      </content></document><document><year>2008</year><authors>Mohamed Farah1| 2  | Daniel V|erpooten1 </authors><title>An outranking approach for information retrieval      </title><content>Over the last three decades, research in Information Retrieval (IR) shows performance improvement when many sources of evidence         are combined to produce a ranking of documents. Most current approaches assess document relevance by computing a single score         which aggregates values of some attributes or criteria. They use analytic aggregation operators which either lead to a loss         of valuable information, e.g., the min or lexicographic operators, or allow very bad scores on some criteria to be compensated         with good ones, e.g., the weighted sum operator. Moreover, all these approaches do not handle imprecision of criterion scores.         In this paper, we propose a multiple criteria framework using a new aggregation mechanism based on decision rules identifying         positive and negative reasons for judging whether a document should get a better ranking than another. The resulting procedure         also handles imprecision in criteria design. Experimental results are reported showing that the suggested method performs         better than standard aggregation operators.      </content></document><document><year>2008</year><authors>Andreas Henrich1  | Stefanie Sieber1 </authors><title>Blended learning and pure e-learning concepts for information retrieval: experiences and future directions      </title><content>Today, teaching and learning are mostly supported by digital material and electronic communication ranging from the provision         of slides or scripts in digital form to elaborate, interactive learning environments. This article describes the prospects         and risks of blended learning and e-learning for information retrieval courses. It deals with adequate content presentation         and representation, as well as with interaction concepts and didactic considerations concerning the cost-benefit ratio of         animations, applets, and multimedia elements. We present lessons learnt from 6;years of teaching information retrieval in         blended learning and pure e-learning scenarios, and derive graded concepts for basic and advanced topics based on a book-like         content representation on the one side, and lecture-recordings on the other side. Each concept is complemented by a pragmatic         and focussed use of auxiliary elements such as forums and self-tests. Examples for beneficial and misguided applets and animations         are given, along with criteria for their differentiation. Finally, critical success factors for technology enhanced learning         approaches in the information retrieval field are derived concerning the creation, utilisation, and maintenance of courses.         In short, we will argue that taking into account the nature and stability of the presented content, as well as a thorough         consideration of the affordable creation and maintenance effort, are crucial for the success of such concepts. In addition,         the closer the concept is to pure e-learning, the more important a high digital presence of the lecturer becomes.      </content></document><document><year>2008</year><authors>Andrea Esuli1| Tiziano Fagni1 | Fabrizio Sebastiani1 </authors><title>Boosting multi-label hierarchical text categorization      </title><content>         Hierarchical Text Categorization (HTC) is the task of generating (usually by means of supervised learning algorithms) text classifiers that operate on hierarchically         structured classification schemes. Notwithstanding the fact that most large-sized classification schemes for text have a hierarchical         structure, so far the attention of text classification researchers has mostly focused on algorithms for &amp;#8220;flat&amp;#8221; classification,         i.e. algorithms that operate on non-hierarchical classification schemes. These algorithms, once applied to a hierarchical         classification problem, are not capable of taking advantage of the information inherent in the class hierarchy, and may thus         be suboptimal, in terms of efficiency and/or effectiveness. In this paper we propose TreeBoost.MH, a multi-label HTC algorithm consisting of a hierarchical variant of AdaBoost.MH, a very well-known member of the family of &amp;#8220;boosting&amp;#8221; learning algorithms. TreeBoost.MH embodies several intuitions that had arisen before within HTC: e.g. the intuitions that both feature selection and the selection         of negative training examples should be performed &amp;#8220;locally&amp;#8221;, i.e. by paying attention to the topology of the classification         scheme. It also embodies the novel intuition that the weight distribution that boosting algorithms update at every boosting         round should likewise be updated &amp;#8220;locally&amp;#8221;. All these intuitions are embodied within TreeBoost.MH in an elegant and simple way, i.e. by defining TreeBoost.MH as a recursive algorithm that uses AdaBoost.MH as its base step, and that recurs over the tree structure. We present the results of experimenting TreeBoost.MH on three HTC benchmarks, and discuss analytically its computational cost.      </content></document><document><year>2008</year><authors>Luo Si1 | Danni Yu2 | Daisuke Kihara3  | Yi Fang4 </authors><title>Combining gene sequence similarity and textual information for gene function annotation in the literature      </title><content>Annotation of the functions of genes and proteins is an essential step in genome analysis. Information extraction techniques         have been proposed to obtain the function information of genes and proteins in the biomedical literature. However, the performance         of most information extraction techniques of function annotation in the biomedical literature is not satisfactory due to the         large variability in the expression of concepts in the biomedical literature. This paper proposes a framework to improve the         gene function annotation in the literature by considering both the textual information in the literature and the functions         of genes with sequences similar to a target gene. The new framework collects multiple types of evidence as: (i) textual information         about gene functions by matching keywords of the gene functions; (ii) gene function information from the known functions of         genes with sequences similar to a target gene; and (iii) the prior probabilities of gene functions to be associated with an         arbitrary gene by mining the known gene functions from curated databases. A supervised learning method is utilized to obtain         the weights for combining the three types of evidence to assign appropriate Gene Ontology terms for target genes. Empirical         studies on two testbeds demonstrate that the combination of sequence similarity scores, function prior probabilities and textual         information improves the accuracy of gene function annotation in the literature. The F-measure scores obtained with the proposed framework are substantially higher than the scores of the solutions in prior research.      </content></document><document><year>2008</year><authors>Isak Taksa1 </authors><title>David Taniar: Research and Trends in Data Mining Technologies and Applications         Hershey, PA, IGI Global, 2007, 340 pp, $85.46, ISBN: 1-59904-272-X</title><content>Without Abstract</content></document><document><year>2008</year><authors>Yi-fang Brook Wu1  | Quanzhi Li2 </authors><title>Document keyphrases as subject metadata: incorporating document key concepts in search results      </title><content>Most search engines display some document metadata, such as title, snippet and URL, in conjunction with the returned hits         to aid users in determining documents. However, metadata is usually fragmented pieces of information that, even when combined,         does not provide an overview of a returned document. In this paper, we propose a mechanism of enriching metadata of the returned         results by incorporating automatically extracted document keyphrases with each returned hit. We hypothesize that keyphrases         of a document can better represent the major theme in that document. Therefore, by examining the keyphrases in each returned         hit, users can better predict the content of documents and the time spent on downloading and examining the irrelevant documents         will be reduced substantially.      </content></document><document><year>2008</year><authors>Walid Magdy1  | Kareem Darwish1 </authors><title>Effect of OCR error correction on Arabic retrieval      </title><content>Arabic documents that are available only in print continue to be ubiquitous and they can be scanned and subsequently OCR&amp;#8217;ed         to ease their retrieval. This paper explores the effect of context-based OCR correction on the effectiveness of retrieving         Arabic OCR documents using different index terms. Different OCR correction techniques based on language modeling with different         correction abilities were tested on real OCR and synthetic OCR degradation. Results show that the reduction of word error         rates needs to pass a certain limit to get a noticeable effect on retrieval. If only moderate error reduction is available,         then using short character n-gram for retrieval without error correction is not a bad strategy. Word-based correction in conjunction         with language modeling had a statistically significant impact on retrieval even for character 3-grams, which are known to         be among the best index terms for OCR degraded Arabic text. Further, using a sufficiently large language model for correction         can minimize the need for morphologically sensitive error correction.      </content></document><document><year>2008</year><authors>Kimmo Fredriksson1  | Szymon Grabowski2 </authors><title>Efficient algorithms for pattern matching with general gaps, character classes, and transposition invariance      </title><content>We develop efficient dynamic programming algorithms for pattern matching with general gaps and character classes. We consider         patterns of the form p         0         g(a         0,b         0)p         1         g(a         1,b         1)&amp;#8230;p                     m&amp;#8722;1, where p                     i             &amp;#8834; &amp;#931;,;&amp;#931; is some finite alphabet, and g(a                     i            ,b                     i            ) denotes a gap of length a                     i            &amp;#8230;b                     i             between symbols p                     i             and p                     i+1. The text symbol t                     j             matches p                     i             iff t                     j            ;&amp;#8712;;p                     i            . Moreover, we require that if p                     i             matches t                     j            , then p                     i+1 should match one of the text symbols  Either or both of a                     i             and b                     i             can be negative. We also consider transposition invariant matching, i.e., the matching condition becomes t                     j            ;&amp;#8712;;p                     i             +;&amp;#964;, for some constant &amp;#964; determined by the algorithms. We give algorithms that have efficient average and worst case running times. The algorithms         have important applications in music information retrieval and computational biology. We give experimental results showing         that the algorithms work well in practice.      </content></document><document><year>2008</year><authors>Vanessa Murdock1 </authors><title>Ellen Voorhees and Donna Harman (eds): TREC Experiment and Evaluation in Information Retrieval         MIT Press, Cambridge, 2005, 462 pp, Price: $45.00, ISBN: 0262220733</title><content>Without Abstract</content></document><document><year>2008</year><authors>Heikki Keskustalo1 | Kalervo JГ¤rvelin1  | Ari Pirkola1 </authors><title>Evaluating the effectiveness of relevance feedback based on a user simulation model: effects of a user scenario on cumulated         gain value      </title><content>We propose a method for performing evaluation of relevance feedback based on simulating real users. The user simulation applies         a model defining the user&amp;#8217;s relevance threshold to accept individual documents as feedback in a graded relevance environment;         user&amp;#8217;s patience to browse the initial list of retrieved documents; and his/her effort in providing the feedback. We evaluate         the result by using cumulated gain-based evaluation together with freezing all documents seen by the user in order to simulate         the point of view of a user who is browsing the documents during the retrieval process. We demonstrate the method by performing         a simulation in the laboratory setting and present the &amp;#8220;branching&amp;#8221; curve sets characteristic for the presented evaluation         method. Both the average and topic-by-topic results indicate that if the freezing approach is adopted, giving feedback of         mixed quality makes sense for various usage scenarios even though the modeled users prefer finding especially the most relevant         documents.      </content></document><document><year>2008</year><authors>Zhiyong Lu1 | Won Kim1 | W. John Wilbur1</authors><title>Evaluation of query expansion using MeSH in PubMed      </title><content>This paper investigates the effectiveness of using MeSHВ® in PubMed through its automatic query expansion process: Automatic Term Mapping (ATM). We run Boolean searches based on a         collection of 55 topics and about 160,000 MEDLINEВ® citations used in the 2006 and 2007 TREC Genomics Tracks. For each topic, we first automatically construct a query by selecting         keywords from the question. Next, each query is expanded by ATM, which assigns different search tags to terms in the query.         Three search tags: [MeSH Terms], [Text Words], and [All Fields] are chosen to be studied after expansion because they all         make use of the MeSH field of indexed MEDLINE citations. Furthermore, we characterize the two different mechanisms by which         the MeSH field is used. Retrieval results using MeSH after expansion are compared to those solely based on the words in MEDLINE         title and abstracts. The aggregate retrieval performance is assessed using both F-measure and mean rank precision. Experimental         results suggest that query expansion using MeSH in PubMed can generally improve retrieval performance, but the improvement         may not affect end PubMed users in realistic situations.      </content></document><document><year>2008</year><authors>Nicola Stokes1 | Yi Li1| Lawrence Cavedon1 | Justin Zobel1</authors><title>Exploring criteria for successful query expansion in the genomic domain      </title><content>Query Expansion is commonly used in Information Retrieval to overcome vocabulary mismatch issues, such as synonymy between         the original query terms and a relevant document. In general, query expansion experiments exhibit mixed results. Overall TREC         Genomics Track results are also mixed; however, results from the top performing systems provide strong evidence supporting         the need for expansion. In this paper, we examine the conditions necessary for optimal query expansion performance with respect         to two system design issues: IR framework and knowledge source used for expansion. We present a query expansion framework         that improves Okapi baseline passage MAP performance by 185%. Using this framework, we compare and contrast the effectiveness         of a variety of biomedical knowledge sources used by TREC 2006 Genomics Track participants for expansion. Based on the outcome         of these experiments, we discuss the success factors required for effective query expansion with respect to various sources         of term expansion, such as corpus-based cooccurrence statistics, pseudo-relevance feedback methods, and domain-specific and         domain-independent ontologies and databases. Our results show that choice of document ranking algorithm is the most important         factor affecting retrieval performance on this dataset. In addition, when an appropriate ranking algorithm is used, we find         that query expansion with domain-specific knowledge sources provides an equally substantive gain in performance over a baseline         system.      </content></document><document><year>2008</year><authors>Tuomas Talvensaari1 | Ari Pirkola2| Kalervo JГ¤rvelin2| Martti Juhola1 | Jorma Laurikkala1</authors><title>Focused web crawling in the acquisition of comparable corpora      </title><content>Cross-Language Information Retrieval (CLIR) resources, such as dictionaries and parallel corpora, are scarce for special domains.         Obtaining comparable corpora automatically for such domains could be an answer to this problem. The Web, with its vast volumes         of data, offers a natural source for this. We experimented with focused crawling as a means to acquire comparable corpora         in the genomics domain. The acquired corpora were used to statistically translate domain-specific words. The same words were         also translated using a high-quality, but non-genomics-related parallel corpus, which fared considerably worse. We also evaluated         our system with standard information retrieval (IR) experiments, combining statistical translation using the Web corpora with         dictionary-based translation. The results showed improvement over pure dictionary-based translation. Therefore, mining the         Web for comparable corpora seems promising.      </content></document><document><year>2008</year><authors>Stefan BГјttcher1  | Charles L. A. Clarke2</authors><title>Hybrid index maintenance for contiguous inverted lists      </title><content>Index maintenance strategies employed by dynamic text retrieval systems based on inverted files can be divided into two categories:         merge-based and in-place update strategies. Within each category, individual update policies can be distinguished based on         whether they store their on-disk posting lists in a contiguous or in a discontiguous fashion. Contiguous inverted lists, in         general, lead to higher query performance, by minimizing the disk seek overhead at query time, while discontiguous inverted         lists lead to higher update performance, requiring less effort during index maintenance operations. In this paper, we focus         on retrieval systems with high query load, where the on-disk posting lists have to be stored in a contiguous fashion at all         times. We discuss a combination of re-merge and in-place index update, called Hybrid Immediate Merge. The method performs strictly better than the re-merge baseline policy used in our experiments, as it leads to the same query         performance, but substantially better update performance. The actual time savings achievable depend on the size of the text         collection being indexed; a larger collection results in greater savings. In our experiments, variations of Hybrid Immediate Merge were able to reduce the total index update overhead by up to 73% compared to the re-merge baseline.      </content></document><document><year>2008</year><authors>Roi Blanco1  | Christina Lioma2 </authors><title>Mixed monolingual homepage finding in 34 languages: the role of language script and search domain      </title><content>The information that is available or sought on the World Wide Web (Web) is increasingly multilingual. Information Retrieval         systems, such as the freely available search engines on the Web, need to provide fair and equal access to this information,         regardless of the language in which a query is written or where the query is posted from. In this work, we ask two questions:         How do existing state of the art search engines deal with languages written in different alphabets (scripts)? Do local language-based         search domains actually facilitate access to information? We conduct a thorough study on the effect of multilingual queries         for homepage finding, where the aim of the retrieval system is to return only one document, namely the homepage described         in the query. We evaluate the effect of multilingual queries in retrieval performance with regard to (i) the alphabet in which         the queries are written (e.g., Latin, Russian, Arabic), and (ii) the language domain where the queries are posted (e.g., google.com,         google.fr). We query four major freely available search engines with 764 queries in 34 different languages, and look for the         correct homepage in the top retrieved results. In order to have fair multilingual experimental settings, we use an ontology         that is comparable across languages and also representative of realistic Web searches: football premier leagues in different         countries; the official team name represents our query, and the official team homepage represents the document to be retrieved.         A series of thorough experiments involving over 10,000 runs, with queries both in their correct and in Latin characters, and         also using both global-domain and local-domain searches, reveal that queries issued in the correct script of a language are         more likely to be found and ranked in the top 3, while queries in non-Latin script languages which are however issued in Latin         script are less likely to be found; also, queries issued to the correct local domain of a search engine, e.g., French queries         to yahoo.fr, are likely to have better retrieval performance than queries issued to the global domain of a search engine.         To our knowledge, this is the first Web retrieval study that uses such a wide range of languages.      </content></document><document><year>2008</year><authors>Jimmy Lin1| 2  | W. John Wilbur2 </authors><title>Modeling actions of PubMed users with n-gram language models      </title><content>Transaction logs from online search engines are valuable for two reasons: First, they provide insight into human information-seeking         behavior. Second, log data can be used to train user models, which can then be applied to improve retrieval systems. This         article presents a study of logs from PubMedВ®, the public gateway to the MEDLINEВ®  database of bibliographic records from the medical and biomedical primary literature. Unlike most previous studies on general         Web search, our work examines user activities with a highly-specialized search engine. We encode user actions as string sequences         and model these sequences using n-gram language models. The models are evaluated in terms of perplexity and in a sequence prediction task. They help us better         understand how PubMed users search for information and provide an enabler for improving users&amp;#8217; search experience.      </content></document><document><year>2008</year><authors>Lior Rokach1 | Roni Romano2 | Oded Maimon2</authors><title>Negation recognition in medical narrative reports      </title><content>Substantial medical data, such as discharge summaries and operative reports are stored in electronic textual form. Databases         containing free-text clinical narratives reports often need to be retrieved to find relevant information for clinical and         research purposes. The context of negation, a negative finding, is of special importance, since many of the most frequently         described findings are such. When searching free-text narratives for patients with a certain medical condition, if negation         is not taken into account, many of the documents retrieved will be irrelevant. Hence, negation is a major source of poor precision         in medical information retrieval systems. Previous research has shown that negated findings may be difficult to identify if         the words implying negations (negation signals) are more than a few words away from them. We present a new pattern learning         method for automatic identification of negative context in clinical narratives reports. We compare the new algorithm to previous         methods proposed for the same task, and show its advantages: accuracy improvement compared to other machine learning methods,         and much faster than manual knowledge engineering techniques with matching accuracy. The new algorithm can be applied also         to further context identification and information extraction tasks.      </content></document><document><year>2008</year><authors>Robert W. P. Luk1 </authors><title>On event space and rank equivalence between probabilistic retrieval models      </title><content>This paper discusses various issues about the rank equivalence of Lafferty and Zhai between the log-odds ratio and the query         likelihood of probabilistic retrieval models. It highlights that Robertson&amp;#8217;s concerns about this equivalence may arise when         multiple probability distributions are assumed to be uniformly distributed, after assuming that the marginal probability logically         follows from Kolmogorov&amp;#8217;s probability axioms. It also clarifies that there are two types of rank equivalence relations between         probabilistic models, namely strict and weak rank equivalence. This paper focuses on the strict rank equivalence which requires         the event spaces of the participating probabilistic models to be identical. It is possible that two probabilistic models are         strict rank equivalent when they use different probability estimation methods. This paper shows that the query likelihood,         p(q|d, r), is strict rank equivalent to p(q|d) of the language model of Ponte and Croft by applying assumptions 1 and 2 of Lafferty and Zhai. In addition, some statistical         component language model may be strict rank equivalent to the log-odds ratio, and that some statistical component model using         the log-odds ratio may be strict rank equivalent to the query likelihood. Finally, we suggest adding a random variable for         the user information need to the probabilistic retrieval models for clarification when these models deal with multiple requests.      </content></document><document><year>2008</year><authors>Tetsuya Sakai1  | Noriko K|o2 </authors><title>On information retrieval metrics designed for evaluation with incomplete relevance assessments      </title><content>Modern information retrieval (IR) test collections have grown in size, but the available manpower for relevance assessments         has more or less remained constant. Hence, how to reliably evaluate and compare IR systems using incomplete relevance data, where many documents exist that were never examined by the relevance assessors, is receiving a lot of attention.         This article compares the robustness of IR metrics to incomplete relevance assessments, using four different sets of graded-relevance         test collections with submitted runs&amp;#8212;the TREC 2003 and 2004 robust track data and the NTCIR-6 Japanese and Chinese IR data         from the crosslingual task. Following previous work, we artificially reduce the original relevance data to simulate IR evaluation         environments with extremely incomplete relevance data. We then investigate the effect of this reduction on discriminative power, which we define as the proportion of system pairs with a statistically significant difference for a given probability of         Type I Error, and on Kendall&amp;#8217;s rank correlation, which reflects the overall resemblance of two system rankings according to two different metrics or two different relevance         data sets. According to these experiments, Q&amp;#8242;, nDCG&amp;#8242; and AP&amp;#8242; proposed by Sakai are superior to bpref proposed by Buckley and         Voorhees and to Rank-Biased Precision proposed by Moffat and Zobel. We also point out some weaknesses of bpref and Rank-Biased         Precision by examining their formal definitions.      </content></document><document><year>2008</year><authors>Holger Bast1| Christian W. Mortensen1 | Ingmar Weber1 </authors><title>Output-sensitive autocompletion search      </title><content>We consider the following autocompletion search scenario: imagine a user of a search engine typing a query; then with every         keystroke display those completions of the last query word that would lead to the best hits, and also display the best such         hits. The following problem is at the core of this feature: for a fixed document collection, given a set D of documents, and an alphabetical range W of words, compute the set of all word-in-document pairs (w,;d) from the collection such that w &amp;#8712;;W and d;&amp;#8712;;D. We present a new data structure with the help of which such autocompletion queries can be processed, on the average, in         time linear in the input plus output size, independent of the size of the underlying document collection. At the same time,         our data structure uses no more space than an inverted index. Actual query processing times on a large test collection correlate         almost perfectly with our theoretical bound.      </content></document><document><year>2008</year><authors>Renato Cordeiro de Amorim1 </authors><title>Pascal Poncelet, Florent Masseglia, Maguelonne Teisseire: Successes and New Directions in Data Mining         IGI Global, Hershey, PA, 2007; 369 pp, Price: $180.00, ISBN: 978-1-59904-645-7</title><content>Without Abstract</content></document><document><year>2008</year><authors>Fabio Crestani1| Paolo Ferragina2 | Mark S|erson3 </authors><title>Preface      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Fabio Aiolli1 | Riccardo Cardin1 | Fabrizio Sebastiani2  | Aless|ro Sperduti1 </authors><title>Preferential text classification: learning algorithms and evaluation measures      </title><content>In many applicative contexts in which textual documents are labelled with thematic categories, a distinction is made between         the primary categories of a document, which represent the topics that are central to it, and its secondary categories, which         represent topics that the document only touches upon. We contend that this distinction, so far neglected in text categorization         research, is important and deserves to be explicitly tackled. The contribution of this paper is threefold. First, we propose         an evaluation measure for this preferential text categorization task, whereby different kinds of misclassifications involving either primary or secondary categories have a different impact         on effectiveness. Second, we establish several baseline results for this task on a well-known benchmark for patent classification         in which the distinction between primary and secondary categories is present; these results are obtained by reformulating         the preferential text categorization task in terms of well established classification problems, such as single and/or multi-label         multiclass classification; state-of-the-art learning technology such as SVMs and kernel-based methods are used. Third, we         improve on these results by using a recently proposed class of algorithms explicitly devised for learning from training data         expressed in preferential form, i.e., in the form &amp;#8220;for document d                     i            , category c&amp;#8242; is preferred to category c&amp;#8242;&amp;#8242;&amp;#8221;; this allows us to distinguish between primary and secondary categories not only in the classification phase but also         in the learning phase, thus differentiating their impact on the classifiers to be generated.      </content></document><document><year>2008</year><authors>Jun Wang1 | Stephen Robertson2 | Arjen P. de Vries3  | Marcel J. T. Reinders4 </authors><title>Probabilistic relevance ranking for collaborative filtering      </title><content>Collaborative filtering is concerned with making recommendations about items to users. Most formulations of the problem are         specifically designed for predicting user ratings, assuming past data of explicit user ratings is available. However, in practice         we may only have implicit evidence of user preference; and furthermore, a better view of the task is of generating a top-N         list of items that the user is most likely to like. In this regard, we argue that collaborative filtering can be directly         cast as a relevance ranking problem. We begin with the classic Probability Ranking Principle of information retrieval, proposing a probabilistic         item ranking framework. In the framework, we derive two different ranking models, showing that despite their common origin,         different factorizations reflect two distinctive ways to approach item ranking. For the model estimations, we limit our discussions         to implicit user preference data, and adopt an approximation method introduced in the classic text retrieval model (i.e. the         Okapi BM25 formula) to effectively decouple frequency counts and presence/absence counts in the preference data. Furthermore,         we extend the basic formula by proposing the Bayesian inference to estimate the probability of relevance (and non-relevance),         which largely alleviates the data sparsity problem. Apart from a theoretical contribution, our experiments on real data sets         demonstrate that the proposed methods perform significantly better than other strong baselines.      </content></document><document><year>2008</year><authors>Oren Kurl|1 </authors><title>Re-ranking search results using language models of query-specific clusters      </title><content>To obtain high precision at top ranks by a search performed in response to a query, researchers have proposed a cluster-based         re-ranking paradigm: clustering an initial list of documents that are the most highly ranked by some initial search, and using         information induced from these (often called) query-specific clusters for re-ranking the list. However, results concerning the effectiveness of various automatic cluster-based re-ranking methods have been inconclusive. We show that using query-specific clusters for automatic re-ranking         of top-retrieved documents is effective with several methods in which clusters play different roles, among which is the smoothing of document language models. We do so by adapting previously-proposed cluster-based retrieval approaches, which are based on (static) query-independent         clusters for ranking all documents in a corpus, to the re-ranking setting wherein clusters are query-specific. The best performing         method that we develop outperforms both the initial document-based ranking and some previously proposed cluster-based re-ranking         approaches; furthermore, this algorithm consistently outperforms a state-of-the-art pseudo-feedback-based approach. In further         exploration we study the performance of cluster-based smoothing methods for re-ranking with various (soft and hard) clustering         algorithms, and demonstrate the importance of clusters in providing context from the initial list through a comparison to         using single documents to this end.      </content></document><document><year>2008</year><authors>Azadeh Shakery1  | ChengXiang Zhai1 </authors><title>Smoothing document language models with probabilistic term count propagation      </title><content>Smoothing of document language models is critical in language modeling approaches to information retrieval. In this paper,         we present a novel way of smoothing document language models based on propagating term counts probabilistically in a graph         of documents. A key difference between our approach and previous approaches is that our smoothing algorithm can iteratively         propagate counts and achieve smoothing with remotely related documents. Evaluation results on several TREC data sets show that the proposed method significantly outperforms the         simple collection-based smoothing method. Compared with those other smoothing methods that also exploit local corpus structures,         our method is especially effective in improving precision in top-ranked documents through &amp;#8220;filling in&amp;#8221; missing query terms         in relevant documents, which is attractive since most users only pay attention to the top-ranked documents in search engine         applications.      </content></document><document><year>2008</year><authors>Phoebe M. Roberts1 | Aaron M. Cohen2 | William R. Hersh2</authors><title>Tasks, topics and relevance judging for the TREC Genomics Track: five years of experience evaluating biomedical text information         retrieval systems      </title><content>With the help of a team of expert biologist judges, the TREC Genomics track has generated four large sets of &amp;#8220;gold standard&amp;#8221;         test collections, comprised of over a hundred unique topics, two kinds of ad hoc retrieval tasks, and their corresponding         relevance judgments. Over the years of the track, increasingly complex tasks necessitated the creation of judging tools and         training guidelines to accommodate teams of part-time short-term workers from a variety of specialized biological scientific         backgrounds, and to address consistency and reproducibility of the assessment process. Important lessons were learned about         factors that influenced the utility of the test collections including topic design, annotations provided by judges, methods         used for identifying and training judges, and providing a central moderator &amp;#8220;meta-judge&amp;#8221;.      </content></document><document><year>2008</year><authors>Andrew MacFarlane1 </authors><title>Teaching mathematics for search using a tutorial style of delivery      </title><content>Understanding of mathematics is needed to underpin the process of search, either explicitly with Exact Match (Boolean logic,         adjacency) or implicitly with Best match natural language search. In this paper we outline some pedagogical challenges in         teaching mathematics for information retrieval (IR) to postgraduate information science students. The aim is to take these         challenges either found by experience or in the literature, to identify both theoretical and practical ideas in order to improve         the delivery of the material and positively affect the learning of the target audience by using a tutorial style of teaching.         Results show that there is evidence to support the notion that a more pro-active style of teaching using tutorials yield benefits         both in terms of assessment results and student satisfaction.      </content></document><document><year>2008</year><authors>Polona Vilar1  | Maja &amp;#381 umer1 </authors><title>The Bologna reform at the department of library and information science and book studies, university of Ljubljana      </title><content>Relevance of the new Bologna study programme at the Department of Library and Information Science and Book Studies at University         of Ljubljana is demonstrated from different aspects. The example of IS&amp;amp;R themes and topics which are recognized as one of         LIS core areas is used to show the differences from the previous study programme in terms of content, together with coverage         of Web 2.0 related themes. In regard to teaching and learning methods it is shown how e-learning is used to support the educational         process. At the end a few insights into employability of future graduates are added.      </content></document><document><year>2008</year><authors>W. John Wilbur1  | Won Kim1 </authors><title>The ineffectiveness of within-document term frequency in text classification      </title><content>For the purposes of classification it is common to represent a document as a bag of words. Such a representation consists         of the individual terms making up the document together with the number of times each term appears in the document. All classification         methods make use of the terms. It is common to also make use of the local term frequencies at the price of some added complication         in the model. Examples are the naГЇve Bayes multinomial model (MM), the Dirichlet compound multinomial model (DCM) and the         exponential-family approximation of the DCM (EDCM), as well as support vector machines (SVM). Although it is usually claimed         that incorporating local word frequency in a document improves text classification performance, we here test whether such         claims are true or not. In this paper we show experimentally that simplified forms of the MM, EDCM, and SVM models which ignore         the frequency of each word in a document perform about at the same level as MM, DCM, EDCM and SVM models which incorporate         local term frequency. We also present a new form of the naГЇve Bayes multivariate Bernoulli model (MBM) which is able to make         use of local term frequency and show again that it offers no significant advantage over the plain MBM. We conclude that word         burstiness is so strong that additional occurrences of a word essentially add no useful information to a classifier.      </content></document><document><year>2008</year><authors>Bassam H. Hammo1 </authors><title>Towards enhancing retrieval effectiveness of search engines for diacritisized Arabic documents      </title><content>The majority of Arabic text available on the web is written without short vowels (diacritics). Diacritics are commonly used         in religious scripts such as the holy Quran (the book of Islam), Al-Hadith (the teachings of Prophet Mohammad (PBUH)), children&amp;#8217;s         literature, and in some words where ambiguity of articulation might arise. Internet Arabic users might lose credible sources         of Arabic text to be retrieved if they could not match the correct diacritical marks attached to the words in the collection.         However, typing the diacritical marks is very annoying and time consuming. The other way around, is to ignore these marks         and fall into the problem of ambiguity. Previous work suggested pre-processing of Arabic text to remove these diacritical         marks before indexing. Consequently, there are noticeable discrepancies when searching the web for Arabic text using international         search engines such as Google and yahoo. In this article, we propose a framework to enhance the retrieval effectiveness of         search engines to search for diacritic and diacritic-less Arabic text through query expansion techniques. We used a rule-based         stemmer and a semantic relational database compiled in an experimental thesaurus to do the expansion. We tested our approach         on the scripts of the Quran. We found that query expansion for searching Arabic text is promising and it is likely that the         efficiency can be further improved by advanced natural language processing tools.      </content></document><document><year>2008</year><authors>William Hersh1  | Ellen Voorhees2</authors><title>TREC genomics special issue overview      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Rafael GuzmГЎn-Cabrera1| 2 | Manuel Montes-y-GГіmez3 | Paolo Rosso2  | Luis VillaseГ±or-Pineda3 </authors><title>Using the Web as corpus for self-training text categorization      </title><content>Most current methods for automatic text categorization are based on supervised learning techniques and, therefore, they face         the problem of requiring a great number of training instances to construct an accurate classifier. In order to tackle this         problem, this paper proposes a new semi-supervised method for text categorization, which considers the automatic extraction         of unlabeled examples from the Web and the application of an enriched self-training approach for the construction of the classifier.         This method, even though language independent, is more pertinent for scenarios where large sets of labeled resources do not         exist. That, for instance, could be the case of several application domains in different non-English languages such as Spanish.         The experimental evaluation of the method was carried out in three different tasks and in two different languages. The achieved         results demonstrate the applicability and usefulness of the proposed method.      </content></document><document><year>2008</year><authors>Max Chevalier1 </authors><title>Zdravko Markov and Daniel T. Larose, Data Mining the Web: Uncovering Patterns in Web Content, Structure, and Usage         Wiley, New Britain, CT, 2007, 218 pp, $69.95. ISBN: 978-0-471-66655-4</title><content>Without Abstract</content></document><document><year>2006</year><authors>Vassilis Plachouras1 | Fidel Cacheda2 | Iadh Ounis1</authors><title>A decision mechanism for the selective combination of evidence in topic distillation      </title><content>The combination of evidence can increase retrieval effectiveness. In this paper, we investigate the effectiveness of a decision         mechanism for the selective combination of evidence for Web Information Retrieval and particularly for topic distillation.         We introduce two measures of a query&amp;#8217;s broadness and use them to select an appropriate combination of evidence for each query.         The results from our experiments show that there is a statistically significant association between the output of the decision         mechanism and the relative effectiveness of the different combinations of evidence. Moreover, we show that the proposed methodology         can be applied in an operational setting, where relevance information is not available, by setting the decision mechanism&amp;#8217;s         thresholds automatically.      </content></document><document><year>2006</year><authors>Kostas Fragos1  | Yannis Maistros1 </authors><title>A goodness of fit test approach in information retrieval      </title><content>In many probabilistic modeling approaches to Information Retrieval we are interested in estimating how well a document model         &amp;#8220;fits&amp;#8221; the user&amp;#8217;s information need (query model). On the other hand in statistics, goodness of fit tests are well established         techniques for assessing the assumptions about the underlying distribution of a data set. Supposing that the query terms are         randomly distributed in the various documents of the collection, we actually want to know whether the occurrences of the query         terms are more frequently distributed by chance in a particular document. This can be quantified by the so-called goodness         of fit tests. In this paper, we present a new document ranking technique based on Chi-square goodness of fit tests. Given         the null hypothesis that there is no association between the query terms q and the document d irrespective of any chance occurrences, we perform a Chi-square goodness of fit test for assessing this hypothesis and calculate         the corresponding Chi-square values. Our retrieval formula is based on ranking the documents in the collection according to         these calculated Chi-square values. The method was evaluated over the entire test collection of TREC data, on disks 4 and         5, using the topics of TREC-7 and TREC-8 (50 topics each) conferences. It performs well, outperforming steadily the classical         OKAPI term frequency weighting formula but below that of KL-Divergence from language modeling approach. Despite this, we believe         that the technique is an important non-parametric way of thinking of retrieval, offering the possibility to try simple alternative         retrieval formulas within goodness-of-fit statistical tests&amp;#8217; framework, modeling the data in various ways estimating or assigning any arbitrary theoretical distribution         in terms.      </content></document><document><year>2006</year><authors>Fern|o MartГ­nez-Santiago1 | L. Alfonso UreГ±a-LГіpez1  | Maite MartГ­n-Valdivia1 </authors><title>A merging strategy proposal: The 2-step retrieval status value method      </title><content>A usual strategy to implement CLIR (Cross-Language Information Retrieval) systems is the so-called query translation approach.         The user query is translated for each language present in the multilingual collection in order to compute an independent monolingual         information retrieval process per language. Thus, this approach divides documents according to language. In this way, we obtain         as many different collections as languages. After searching in these corpora and obtaining a result list per language, we         must merge them in order to provide a single list of retrieved articles.                     In this paper, we propose an approach to obtain a single list of relevant documents for CLIR systems driven by query translation.               This approach, which we call 2-step RSV (RSV: Retrieval Status Value), is based on the re-indexing of the retrieval documents               according to the query vocabulary, and it performs noticeably better than traditional methods.            </content></document><document><year>2006</year><authors>Alistair Moffat1 | William Webber1| 2 | Justin Zobel2  | Ricardo Baeza-Yates3| 4 </authors><title>A pipelined architecture for distributed text query evaluation      </title><content>Two principal query-evaluation methodologies have been described for cluster-based implementation of distributed information         retrieval systems: document partitioning and term partitioning. In a document-partitioned system, each of the processors hosts         a subset of the documents in the collection, and executes every query against its local sub-collection. In a term-partitioned         system, each of the processors hosts a subset of the inverted lists that make up the index of the collection, and serves them         to a central machine as they are required for query evaluation.                     In this paper we introduce a pipelined query-evaluation methodology, based on a term-partitioned index, in which partially               evaluated queries are passed amongst the set of processors that host the query terms. This arrangement retains the disk read               benefits of term partitioning, but more effectively shares the computational load. We compare the three methodologies experimentally,               and show that term distribution is inefficient and scales poorly. The new pipelined approach offers efficient memory utilization               and efficient use of disk accesses, but suffers from problems with load balancing between nodes. Until these problems are               resolved, document partitioning remains the preferred method.            </content></document><document><year>2006</year><authors>Gloria T. Lau1 | Kincho H. Law1  | Gio Wiederhold2 </authors><title>A relatedness analysis of government regulations using domain knowledge and structural organization</title><content>The complexity and diversity of government regulations make understanding and retrieval of regulations a non-trivial task. One of the issues is the existence of multiple sources of regulations and interpretive guides with differences in format, terminology and context. This paper describes a comparative analysis scheme developed to help retrieval of related provisions from different regulatory documents. Specifically, the goal is to identify the most strongly related provisions between regulations. The relatedness analysis makes use of not only traditional term match but also a combination of feature matches, and not only content comparison but also structural analysis.Regulations are first compared based on conceptual information as well as domain knowledge through feature matching. Regulations also possess specific organizational structures, such as a tree hierarchy of provisions and heavy referencing between provisions. These structures represent useful information in locating related provisions, and are therefore exploited in the comparison of regulations for completeness. System performance is evaluated by comparing a similarity ranking produced by users with the machine-predicted ranking. Ranking produced by the relatedness analysis system shows a reduction in error compared to that of Latent Semantic Indexing. Various pairs of regulations are compared and the results are analyzed along with observations based on different feature usages. An example of an e-rulemaking scenario is shown to demonstrate capabilities and limitations of the prototype relatedness analysis system.</content></document><document><year>2006</year><authors>Rong Jin1 | Luo Si2  | Chengxiang Zhai3 </authors><title>A study of mixture models for collaborative filtering      </title><content>Collaborative filtering is a general technique for exploiting the preference patterns of a group of users to predict the utility         of items for a particular user. Three different components need to be modeled in a collaborative filtering problem: users,         items, and ratings. Previous research on applying probabilistic models to collaborative filtering has shown promising results.         However, there is a lack of systematic studies of different ways to model each of the three components and their interactions.         In this paper, we conduct a broad and systematic study on different mixture models for collaborative filtering. We discuss         general issues related to using a mixture model for collaborative filtering, and propose three properties that a graphical         model is expected to satisfy. Using these properties, we thoroughly examine five different mixture models, including Bayesian         Clustering (BC), Aspect Model (AM), Flexible Mixture Model (FMM), Joint Mixture Model (JMM), and the Decoupled Model (DM).         We compare these models both analytically and experimentally. Experiments over two datasets of movie ratings under different         configurations show that in general, whether a model satisfies the proposed properties tends to be correlated with its performance.         In particular, the Decoupled Model, which satisfies all the three desired properties, outperforms the other mixture models         as well as many other existing approaches for collaborative filtering. Our study shows that graphical models are powerful         tools for modeling collaborative filtering, but careful design is necessary to achieve good performance.      </content></document><document><year>2006</year><authors>D. R. Campbell1 | S. J. Culley1 | C. A. McMahon1  | F. Sellini2 </authors><title>An approach for the capture of context-dependent document relationships extracted from Bayesian analysis of users' interactions         with information      </title><content>A number of technologies exist which enable the unobtrusive capture of computer interface interactions in the background of         a user's working environment. The resulting data can be used in a variety of ways to model aspects of search activity and         the general use of electronic documents in normal working routines. In this paper we present an approach for using captured         data to identify relationships between documents used by an individual or group, representing their value in a given context&amp;#8212;that         may relate to specific information need or activity. The approach employs the use of a naГЇve Bayesian classifier to evaluate         possible relationships that are derived implicitly from the data. It is intended that the relationships established be stored         within an information retrieval (IR) system to aid in the retrieval of related documents where future users arrive at a similar         context. In the evaluation of the approach over 70 hours of data from computer users in industrial and academic settings are         collected to assess its overall feasibility. The results indicate that the approach provides a useful method for the establishment         of identifiable relationships between documents based on the context of their usage, rather than their content.      </content></document><document><year>2006</year><authors>Barry Smyth1 | Evelyn Balfe2 </authors><title>Anonymous personalization in collaborative web search      </title><content>We present an innovative approach to Web search, called collaborative search, that seeks to cope with the type of vague queries that are commonplace in Web search. We do this by leveraging the search         behaviour of previous searchers to personalize future result-lists according to the implied preferences of a community of         like-minded individuals. This technique is implemented in the I-SPY meta-search engine and we present the results of a live-user         trial which indicates that I-SPY can offer improved search performance when compared to a benchmark search engine, across         a variety of performance metrics. In addition, I-SPY achieves its level of personalization while preserving the anonymity         of individual users, and we argue that this offers unique privacy benefits compared to alternative approaches to personalization.      </content></document><document><year>2006</year><authors>Radu Soricut1  | Eric Brill2 </authors><title>Automatic question answering using the web: Beyond the Factoid      </title><content>In this paper we describe and evaluate a Question Answering (QA) system that goes beyond answering factoid questions. Our         approach to QA assumes no restrictions on the type of questions that are handled, and no assumption that the answers to be         provided are factoids. We present an unsupervised approach for collecting question and answer pairs from FAQ pages, which         we use to collect a corpus of 1 million question/answer pairs from FAQ pages available on the Web. This corpus is used to         train various statistical models employed by our QA system: a statistical chunker used to transform a natural language-posed         question into a phrase-based query to be submitted for exact match to an off-the-shelf search engine; an answer/question translation         model, used to assess the likelihood that a proposed answer is indeed an answer to the posed question; and an answer language         model, used to assess the likelihood that a proposed answer is a well-formed answer. We evaluate our QA system in a modular         fashion, by comparing the performance of baseline algorithms against our proposed algorithms for various modules in our QA         system. The evaluation shows that our system achieves reasonable performance in terms of answer accuracy for a large variety         of complex, non-factoid questions.      </content></document><document><year>2006</year><authors>Anni R. Coden1  | Eric W. Brown1 </authors><title>Automatic search from streaming data      </title><content>Streaming data poses a variety of new and interesting challenges for information retrieval and text analysis. Unlike static         document collections, which are typically analyzed and indexed off-line to support ad-hoc queries, streaming data often must         be analyzed on the fly and acted on as the data passes through the analysis system. Speech is one example of streaming data         that is a challenge to exploit, yet has significant potential to provide value in a knowledge management system. We are specifically         interested in techniques that analyze streaming data and automatically find collateral information, or information that clarifies, expands, and generally enhances the value of the streaming data. We present a system that         analyzes a data stream and automatically finds documents related to the current topic of discussion in the data stream. Experimental         results show that the system generates result lists with an average precision at 10 hits of better than 60%. We also present         a hit-list re-ranking technique based on named entity analysis and automatic text categorization that can improve the search         results by 6%&amp;#8211;12%.      </content></document><document><year>2006</year><authors>Vishwa Vinay1 | Ingemar J. Cox1 | Natasa Milic-Frayling2  | Ken Wood2 </authors><title>Can constrained relevance feedback and display strategies help users retrieve items on mobile devices?      </title><content>Searching online information resources using mobile devices is affected by small screens which can display only a fraction         of ranked search results. In this paper we investigate whether the search effort can be reduced by means of a simple user         feedback: for a screenful of search results the user is encouraged to indicate a single most relevant document. In our approach         we exploit the fact that, for small display sizes and limited user actions, we can construct a user decision tree representing         all possible outcomes of the user interaction with the system. Examining the trees we can compute an upper limit on relevance         feedback performance. In this study we consider three standard feedback algorithms: Rocchio, Robertson/Sparck-Jones (RSJ)         and a Bayesian algorithm. We evaluate them in conjunction with two strategies for presenting search results: a document ranking         that attempts to maximize information gain from the user&amp;#8217;s choices and the top-D ranked documents. Experimental results indicate         that for RSJ feedback which involves an explicit feature selection policy, the greedy top-D display is more appropriate. For         the other two algorithms, the exploratory display that maximizes information gain produces better results. We conducted a         user study to compare the performance of the relevance feedback methods with real users and compare the results with the findings         from the tree analysis. This comparison between the simulations and real user behaviour indicates that the Bayesian algorithm,         coupled with the sampled display, is the most effective.      </content></document><document><year>2006</year><authors>In-Su Kang1 | Seung-Hoon Na1  | Jong-Hyeok Lee1 </authors><title>Collection-based compound noun segmentation for Korean information retrieval</title><content>Compound noun segmentation is a key first step in language processing for Korean. Thus far, most approaches require some form of human supervision, such as pre-existing dictionaries, segmented compound nouns, or heuristic rules. As a result, they suffer from the unknown word problem, which can be overcome by unsupervised approaches. However, previous unsupervised methods normally do not consider all possible segmentation candidates, and/or rely on character-based segmentation clues such as bi-grams or all-length n-grams. So, they are prone to falling into a local solution. To overcome the problem, this paper proposes an unsupervised segmentation algorithm that searches the most likely segmentation result from all possible segmentation candidates using a word-based segmentation context. As word-based segmentation clues, a dictionary is automatically generated from a corpus. Experiments using three test collections show that our segmentation algorithm is successfully applied to Korean information retrieval, improving a dictionary-based longest-matching algorithm.</content></document><document><year>2006</year><authors>Youjin Chang1 | Minkoo Kim2  | Vijay V. Raghavan3 </authors><title>Construction of query concepts based on feature clustering of documents      </title><content>In Information Retrieval, since it is hard to identify users&amp;#8217; information needs, many approaches have been tried to solve         this problem by expanding initial queries and reweighting the terms in the expanded queries using users&amp;#8217; relevance judgments.         Although relevance feedback is most effective when relevance information about retrieved documents is provided by users, it         is not always available. Another solution is to use correlated terms for query expansion. The main problem with this approach         is how to construct the term-term correlations that can be used effectively to improve retrieval performance. In this study,         we try to construct query concepts that denote users&amp;#8217; information needs from a document space, rather than to reformulate initial queries using the term correlations         and/or users&amp;#8217; relevance feedback. To form query concepts, we extract features from each document, and then cluster the features into primitive concepts that are then used to form         query concepts. Experiments are performed on the Associated Press (AP) dataset taken from the TREC collection. The experimental evaluation         shows that our proposed framework called QCM (Query Concept Method) outperforms baseline probabilistic retrieval model on         TREC retrieval.      </content></document><document><year>2006</year><authors>A. Z. Broder1 | R. Lempel2 | F. Maghoul3  | J. Pedersen3 </authors><title>Efficient PageRank approximation via graph aggregation      </title><content>We present a framework for approximating random-walk based probability distributions over Web pages using graph aggregation.         The basic idea is to partition the graph into classes of quasi-equivalent vertices, to project the page-based random walk         to be approximated onto those classes, and to compute the stationary probability distribution of the resulting class-based         random walk. From this distribution we can quickly reconstruct a distribution on pages. In particular, our framework can approximate         the well-known PageRank distribution by setting the classes according to the set of pages on each Web host.                     We experimented on a Web-graph containing over 1.4 billion pages and over 6.6 billion links from a crawl of the Web conducted               by AltaVista in September 2003. We were able to produce a ranking that has Spearman rank-order correlation of 0.95 with respect               to PageRank. The clock time required by a simplistic implementation of our method was less than half the time required by               a highly optimized implementation of PageRank, implying that larger speedup factors are probably possible.            </content></document><document><year>2006</year><authors>Norbert GГ¶vert1 | Norbert Fuhr2 | Mounia Lalmas3  | Gabriella Kazai3 </authors><title>Evaluating the effectiveness of content-oriented XML retrieval methods</title><content>Content-oriented XML retrieval approaches aim at a more focused retrieval strategy: Instead of retrieving whole documents, document components that are exhaustive to the information need while at the same time being as specific as possible should be retrieved. In this article, we show that the evaluation methods developed for standard retrieval must be modified in order to deal with the structure of XML documents. More precisely, the size and overlap of document components must be taken into account. For this purpose, we propose a new effectiveness metric based on the definition of a concept space defined upon the notions of exhaustiveness and specificity of a search result. We compare the results of this new metric by the results obtained with the official metric used in INEX, the evaluation initiative for content-oriented XML retrieval.</content></document><document><year>2006</year><authors>J. I. Serrano1  | M. D. del Castillo1 </authors><title>Evolutionary learning of document categories      </title><content>This paper deals with a supervised learning method devoted to producing categorization models of text documents. The goal         of the method is to use a suitable numerical measurement of example similarity to find centroids describing different categories         of examples. The centroids are not abstract or statistical models, but rather consist of bits of examples. The centroid-learning         method is based on a Genetic Algorithm for Texts (GAT). The categorization system using this genetic algorithm infers a model         by applying the genetic algorithm to each set of preclassified documents belonging to a category. The models thus obtained         are the category centroids that are used to predict the category of a test document. The experimental results validate the         utility of this approach for classifying incoming documents.      </content></document><document><year>2006</year><authors>Ronan Cummins1  | Colm O&amp;#8217 Riordan1 </authors><title>Evolving local and global weighting schemes in information retrieval      </title><content>This paper describes a method, using Genetic Programming, to automatically determine term weighting schemes for the vector         space model. Based on a set of queries and their human determined relevant documents, weighting schemes are evolved which         achieve a high average precision. In Information Retrieval (IR) systems, useful information for term weighting schemes is         available from the query, individual documents and the collection as a whole.                     We evolve term weighting schemes in both local (within-document) and global (collection-wide) domains which interact with               each other correctly to achieve a high average precision. These weighting schemes are tested on well-known test collections               and are compared to the traditional tf-idf weighting scheme and to the BM25 weighting scheme using standard IR performance metrics.            </content></document><document><year>2006</year><authors>Raija Lehtokangas1 | Heikki Keskustalo1  | Kalervo JГ¤rvelin1 </authors><title>Experiments with dictionary-based CLIR using graded relevance assessments: Improving effectiveness by pseudo-relevance feedback      </title><content>Research on cross-language information retrieval (CLIR) has typically been restricted to settings using binary relevance assessments.         In this paper, we present evaluation results for dictionary-based CLIR using graded relevance assessments in a best match         retrieval environment. A text database containing newspaper articles and a related set of 35 search topics were used in the         tests. First, monolingual baseline queries were automatically formed from the topics. Secondly, source language topics (in         English, German, and Swedish) were automatically translated into the target language (Finnish), using structured target queries.         The effectiveness of the translated queries was compared to that of the monolingual queries. Thirdly, pseudo-relevance feedback         was used to expand the original target queries. CLIR performance was evaluated using three relevance thresholds: stringent,         regular, and liberal. When regular or liberal threshold was used, a reasonable performance was achieved. Using stringent threshold,         equally high performance could not be achieved. On all the relevance thresholds the performance of the translated queries         was successfully raised by pseudo-relevance feedback based query expansion. However, the performance of the stringent threshold         in relation to the other thresholds could not be raised by this method.      </content></document><document><year>2006</year><authors>Sarah Zelikovitz1 | William W. Cohen2  | Haym Hirsh3 </authors><title>Extending WHIRL with background knowledge for improved text classification      </title><content>Intelligent use of the many diverse forms of data available on the Internet requires new tools for managing and manipulating         heterogeneous forms of information. This paper uses WHIRL, an extension of relational databases that can manipulate textual         data using statistical similarity measures developed by the information retrieval community. We show that although WHIRL is         designed for more general similarity-based reasoning tasks, it is competitive with mature systems designed explicitly for         inductive classification. In particular, WHIRL is well suited for combining different sources of knowledge in the classification         process. We show on a diverse set of tasks that the use of appropriate sets of unlabeled background knowledge often decreases         error rates, particularly if the number of examples or the size of the strings in the training set is small. This is especially         useful when labeling text is a labor-intensive job and when there is a large amount of information available about a particular         problem on the World Wide Web.      </content></document><document><year>2006</year><authors>David Madigan1 | Yehuda Vardi2 | Ishay Weissman3</authors><title>Extreme value theory applied to document retrieval from large collections      </title><content>We consider text retrieval applications that assign query-specific relevance scores to documents drawn from particular collections.         Such applications represent a primary focus of the annual Text Retrieval Conference (TREC), where the participants compare         the empirical performance of different approaches. P(K), the proportion of the top K documents that are relevant, is a popular measure of retrieval effectiveness.                     Participants in the TREC Very Large Corpus track have observed that when the target is a random sample from a collection,               P(K) is substantially smaller than when the target is the entire collection. Hawking and Robertson (2003) confirmed this finding               in a number of experimental settings. Hawking et al. (1999) posed as an open research question the cause of this phenomenon               and proposed five possible explanatory hypotheses. In this paper, we present a mathematical analysis that sheds some light               on these hypotheses and complements the experimental work of Hawking and Robertson (2003). We will also introduce C(L), contamination at L, the number of irrelevant documents amongst the top L relevant documents, and describe its properties.            </content></document><document><year>2006</year><authors>Jie Lu1  | Jamie Callan1 </authors><title>Full-text federated search of text-based digital libraries in peer-to-peer networks      </title><content>Peer-to-peer (P2P) networks integrate autonomous computing resources without requiring a central coordinating authority, which         makes them a potentially robust and scalable model for providing federated search capability to large-scale networks of text-based         digital libraries. However, peer-to-peer networks have so far provided very limited support for full-text federated search         with relevance-based document ranking. This paper provides solutions to full-text federated search of text-based digital libraries         in hierarchical peer-to-peer networks. Existing approaches to full-text search are adapted and new methods are developed for         the problems of resource representation, resource selection, and result merging according to the unique characteristics of         hierarchical peer-to-peer networks. Experimental results demonstrate that the proposed approaches offer a better combination         of accuracy and efficiency than more common alternatives for federated search of text-based digital libraries in peer-to-peer         networks.      </content></document><document><year>2006</year><authors>Tuomo Korenius1 | Jorma Laurikkala1 | Martti Juhola1  | Kalervo JГ¤rvelin2 </authors><title>Hierarchical clustering of a Finnish newspaper article collection with graded relevance assessments      </title><content>Search facilitated with agglomerative hierarchical clustering methods was studied in a collection of Finnish newspaper articles         (N = 53,893). To allow quick experiments, clustering was applied to a sample (N = 5,000) that was reduced with principal components analysis. The dendrograms were heuristically cut to find an optimal partition,         whose clusters were compared with each of the 30 queries to retrieve the best-matching cluster. The four-level relevance assessment         was collapsed into a binary one by (A) considering all the relevant and (B) only the highly relevant documents relevant, respectively.         Single linkage (SL) was the worst method. It created many tiny clusters, and, consequently, searches enabled with it had high         precision and low recall. The complete linkage (CL), average linkage (AL), and Ward's methods (WM) returned reasonably-sized         clusters typically of 18&amp;#8211;32 documents. Their recall (A: 27&amp;#8211;52%, B: 50&amp;#8211;82%) and precision (A: 83&amp;#8211;90%, B: 18&amp;#8211;21%) was higher         than and comparable to those of the SL clusters, respectively. The AL and WM clustering had 1&amp;#8211;8% better effectiveness than         nearest neighbor searching (NN), and SL and CL were 1&amp;#8211;9% less efficient that NN. However, the differences were statistically         insignificant. When evaluated with the liberal assessment A, the results suggest that the AL and WM clustering offer better         retrieval ability than NN. Assessment B renders the AL and WM clustering better than NN, when recall is considered more important         than precision. The results imply that collections in the highly inflectional and agglutinative languages, such as Finnish,         may be clustered as the collections in English, provided that documents are appropriately preprocessed.      </content></document><document><year>2006</year><authors>Dagobert Soergel1 </authors><title>Information Representation and Retrieval in the Digital Age (ASIST Monograph Series) by Heting Chu, Medford, NJ: Information         Today; 2003. 248 p. ISBN 1-57387-172-9      </title><content>Without Abstract</content></document><document><year>2006</year><authors>Massimo Melucci1  | David Hawking2 </authors><title>Introduction: A perspective on Web Information Retrieval      </title><content>Without Abstract</content></document><document><year>2006</year><authors>David E. Losada1  | Juan M. FernГЎndez-Luna2 </authors><title>Introduction to the special issue on the 27th European Conference on Information Retrieval Research      </title><content>Without Abstract</content></document><document><year>2006</year><authors>Massih R. Amini1 | Anastasios Tombros2 | Nicolas Usunier1  | Mounia Lalmas2 </authors><title>Learning-based summarisation of XML documents      </title><content>Documents formatted in eXtensible Markup Language (XML) are available in collections of various document types. In this paper,         we present an approach for the summarisation of XML documents. The novelty of this approach lies in that it is based on features         not only from the content of documents, but also from their logical structure. We follow a machine learning, sentence extraction-based         summarisation technique. To find which features are more effective for producing summaries, this approach views sentence extraction         as an ordering task. We evaluated our summarisation model using the INEX and SUMMAC datasets. The results demonstrate that         the inclusion of features from the logical structure of documents increases the effectiveness of the summariser, and that         the learnable system is also effective and well-suited to the task of summarisation in the context of XML documents. Our approach         is generic, and is therefore applicable, apart from entire documents, to elements of varying granularity within the XML tree.         We view these results as a step towards the intelligent summarisation of XML documents.      </content></document><document><year>2006</year><authors>Nieves R. Brisaboa1 | Antonio FariГ±a1 | Gonzalo Navarro2  | JosГ© R. ParamГЎ1 </authors><title>Lightweight natural language text compression      </title><content>Variants of Huffman codes where words are taken as the source symbols are currently the most attractive choices to compress         natural language text databases. In particular, Tagged Huffman Code by Moura et al. offers fast direct searching on the compressed         text and random access capabilities, in exchange for producing around 11% larger compressed files. This work describes End-Tagged         Dense Code and (s, c)-Dense Code, two new semistatic statistical methods for compressing natural language texts. These techniques permit simpler         and faster encoding and obtain better compression ratios than Tagged Huffman Code, while maintaining its fast direct search         and random access capabilities. We show that Dense Codes improve Tagged Huffman Code compression ratio by about 10%, reaching         only 0.6% overhead over the optimal Huffman compression ratio. Being simpler, Dense Codes are generated 45% to 60% faster         than Huffman codes. This makes Dense Codes a very attractive alternative to Huffman code variants for various reasons: they         are simpler to program, faster to build, of almost optimal size, and as fast and easy to search as the best Huffman variants,         which are not so close to the optimal size.      </content></document><document><year>2006</year><authors>Jimmy Lin1| 2| 3  | Dina Demner-Fushman2| 3 </authors><title>Methods for automatically evaluating answers to complex questions</title><content>Evaluation is a major driving force in advancing the state of the art in language technologies. In particular, methods for automatically assessing the quality of machine output is the preferred method for measuring progress, provided that these metrics have been validated against human judgments. Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called POURPRE, an automatic technique for evaluating answers to complex questions based on n-gram co-occurrences between machine output and a human-generated answer key. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information &amp;#8220;nugget&amp;#8221; appears in a system's response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address with this work. Experiments with the TREC 2003, TREC 2004, and TREC 2005 QA tracks indicate that rankings produced by our metric correlate highly with official rankings, and that POURPRE outperforms direct application of existing metrics.</content></document><document><year>2006</year><authors>Krister LindГ©n1 </authors><title>Multilingual modeling of cross-lingual spelling variants      </title><content>Technical term translations are important for cross-lingual information retrieval. In many languages, new technical terms         have a common origin rendered with different spelling of the underlying sounds, also known as cross-lingual spelling variants         (CLSV).                     To find the best CLSV in a text database index, we contribute a formulation of the problem in a probabilistic framework, and               implement this with an instance of the general edit distance using weighted finite-state transducers. Some training data is               required when estimating the costs for the general edit distance. We demonstrate that after some basic training our new multilingual               model is robust and requires little or no adaptation for covering additional languages, as the model takes advantage of language               independent transliteration patterns.            </content></document><document><year>2006</year><authors>Saliha Azzam1  | Kevin Humphreys1 </authors><title>New Directions in Question Answering      </title><content>Without Abstract</content></document><document><year>2006</year><authors>Olga Vechtomova1 </authors><title>Noun phrases in interactive query expansion and document ranking      </title><content>The paper presents several techniques for selecting noun phrases for interactive query expansion following pseudo-relevance         feedback and a new phrase-based document ranking method. A combined syntactico-statistical method was used for the selection         of phrases for query expansion. Several statistical measures of phrase selection were evaluated. Experiments were also conducted         studying the effectiveness of noun phrases in document ranking. One of the major problems in phrase-based document retrieval         is weighting of overlapping and non-contiguous word sequences in documents. The paper presents a new method of phrase weighting,         which addressed this problem, and its evaluation on the TREC dataset.      </content></document><document><year>2006</year><authors>Jason J. Jung1 </authors><title>Ontological framework based on contextual mediation for collaborative information retrieval      </title><content>On the heterogeneous web information spaces, users have been suffering from efficiently searching for relevant information.         This paper proposes a mediator agent system to estimate the semantics of unknown web spaces by learning the fragments gathered         during the users' focused crawling. This process is organized as the following three tasks; (i) gathering semantic information         about web spaces from personal agents while focused crawling in unknown spaces, (ii) reorganizing the information by using         ontology alignment algorithm, and (iii) providing relevant semantic information to personal agents right before focused crawling.         It makes the personal agent possible to recognize the corresponding user's behaviors in semantically heterogeneous spaces         and predict his searching contexts. For the experiments, we implemented comparison-shopping system with heterogeneous web         spaces. As a result, our proposed method efficiently supported the users, and then, network traffic was also reduced.      </content></document><document><year>2006</year><authors>Steve Cronen-Townsend1| 2 | Yun Zhou1  | W. Bruce Croft1 </authors><title>Precision prediction based on ranked list coherence</title><content>We introduce a statistical measure of the coherence of a list of documents called the clarity score. Starting with a document list ranked by the query-likelihood retrieval model, we demonstrate the score's relationship to query ambiguity with respect to the collection. We also show that the clarity score is correlated with the average precision of a query and lay the groundwork for useful predictions by discussing a method of setting decision thresholds automatically. We then show that passage-based clarity scores correlate with average-precision measures of ranked lists of passages, where a passage is judged relevant if it contains correct answer text, which extends the basic method to passage-based systems. Next, we introduce variants of document-based clarity scores to improve the robustness, applicability, and predictive ability of clarity scores. In particular, we introduce the ranked list clarity score that can be computed with only a ranked list of documents, and the weighted clarity score where query terms contribute more than other terms. Finally, we show an approach to predicting queries that perform poorly on query expansion that uses techniques expanding on the ideas presented earlier.</content></document><document><year>2006</year><authors>Thanh Tin Tang1 | Nick Craswell2 | David Hawking2 | Kathy Griffiths3  | Helen Christensen3 </authors><title>Quality and relevance of domain-specific search: A case study in mental health      </title><content>When searching for health information, results quality can be judged against available scientific evidence: Do search engines         return advice consistent with evidence based medicine? We compared the performance of domain-specific health and depression         search engines against a general-purpose engine (Google) on both relevance of results and quality of advice. Over 101 queries,         to which the term &amp;#8216;depression&amp;#8217; was added if not already present, Google returned more relevant results than those of the domain-specific         engines. However, over the 50 treatment-related queries, Google returned 70 pages recommending for or against a well studied         treatment, of which 19 strongly disagreed with the scientific evidence. A domain-specific index of 4 sites selected by domain         experts was only wrong in 5 of 50 recommendations. Analysis suggests a tension between relevance and quality. Indexing more         pages can give a greater number of relevant results, but selective inclusion can give better quality.      </content></document><document><year>2006</year><authors>Norbert Fuhr1  | Norbert GГ¶vert2 </authors><title>Retrieval quality vs. effectiveness of specificity-oriented search in XML collections      </title><content>Content-only queries in hierarchically structured documents should retrieve the most specific document nodes which are exhaustive         to the information need. For this problem, we investigate two methods of augmentation, which both yield high retrieval quality.         As retrieval effectiveness, we consider the ratio of retrieval quality and response time; thus, fast approximations to the         'correct' retrieval result may yield higher effectiveness. We present a classification scheme for algorithms addressing this         issue, and adopt known algorithms from standard document retrieval for XML retrieval. As a new strategy, we propose incremental-interruptible retrieval, which allows for instant presentation of the top ranking documents. We develop a new algorithm implementing this strategy         and evaluate the different methods with the INEX collection.      </content></document><document><year>2006</year><authors>Kevin C. Desouza1 </authors><title>Review of Profiling Machines: Mapping the Personal Information Economy      </title><content>Without Abstract</content></document><document><year>2006</year><authors>W. John Wilbur1 | Won Kim1 | Natalie Xie1</authors><title>Spelling correction in the PubMed search engine</title><content>It is known that users of internet search engines often enter queries with misspellings in one or more search terms. Several web search engines make suggestions for correcting misspelled words, but the methods used are proprietary and unpublished to our knowledge. Here we describe the methodology we have developed to perform spelling correction for the PubMed search engine. Our approach is based on the noisy channel model for spelling correction and makes use of statistics harvested from user logs to estimate the probabilities of different types of edits that lead to misspellings. The unique problems encountered in correcting search engine queries are discussed and our solutions are outlined.</content></document><document><year>2006</year><authors>Per Ahlgren1  | Jaana KekГ¤lГ¤inen2 </authors><title>Swedish full text retrieval: Effectiveness of different combinations of indexing strategies with query terms</title><content>In this paper, which treats Swedish full text retrieval, the problem of morphological variation of query terms in the document database is studied. The Swedish CLEF 2003 test collection was used, and the effects of combination of indexing strategies with query terms on retrieval effectiveness were studied. Four of the seven tested combinations involved indexing strategies that used normalization, a form of conflation. All of these four combinations employed compound splitting, both during indexing and at query phase. SWETWOL, a morphological analyzer for the Swedish language, was used for normalization and compound splitting. A fifth combination used stemming, while a sixth attempted to group related terms by right hand truncation of query terms. The truncation was performed by a search expert. These six combinations were compared to each other and to a baseline combination, where no attempt was made to counteract the problem of morphological variation of query terms in the document database. Both the truncation combination, the four combinations based on normalization and the stemming combination outperformed the baseline. Truncation had the best performance. The main conclusion of the paper is that truncation, normalization and stemming enhanced retrieval effectiveness in comparison to the baseline. Further, normalization and stemming were not far below truncation.</content></document><document><year>2006</year><authors>Xing Wei1 | Bruce Croft1  | Andrew McCallum1 </authors><title>Table extraction for answer retrieval</title><content>The ability to find tables and extract information from them is a necessary component of many information retrieval tasks. Documents often contain tables in order to communicate densely packed, multi-dimensional information. Tables do this by employing layout patterns to efficiently indicate fields and records in two-dimensional form. Their rich combination of formatting and content presents difficulties for traditional retrieval techniques. This paper describes techniques for extracting tables from text and retrieving answers from the extracted information. We compare machine learning (especially, Conditional Random Fields) and heuristic methods for table extraction. To retrieve answers, our approach creates a cell document, which contains the cell and its metadata (headers, titles) for each table cell, and the retrieval model ranks the cells of the extracted tables using a language-modeling approach. Performance is tested using government statistical Web sites and news articles, and errors are analyzed in order to improve the system.</content></document><document><year>2006</year><authors>Le Zhao1 | Min Zhang1  | Shaoping Ma1 </authors><title>The nature of novelty detection</title><content>Sentence level novelty detection aims at spotting sentences with novel information from an ordered sentence list. In the task, sentences appearing later in the list with no new meanings are eliminated. For the task of novelty detection, the contributions of this paper are three-fold. First, conceptually, this paper reveals the computational nature of the task currently overlooked by the Novelty community&amp;#8212;Novelty as a combination of partial overlap (PO) and complete overlap (CO) relations between sentences. We define partial overlap between two sentences as a sharing of common facts, while complete overlap is when one sentence covers all of the meanings of the other sentence. Second, technically, a novel approach, the selected pool method is provided which follows naturally from the PO-CO computational structure. We provide formal error analysis for selected pool and methods based on this PO-CO framework. We address the question how accurate must the PO judgments be to outperform the baseline pool method. Third, experimentally, results were presented for all the three novelty datasets currently available. Results show that the selected pool is significantly better or no worse than the current methods, an indication that the term overlap criterion for the PO judgments could be adequately accurate.</content></document><document><year>2006</year><authors>Roi Blanco1  | ГЃlvaro Barreiro1 </authors><title>TSP and cluster-based solutions to the reassignment of document identifiers      </title><content>Recent studies demonstrated that it is possible to reduce Inverted Files (IF) sizes by reassigning the document identifiers of the original collection, as this lowers the distance between the positions         of documents related to a single term. Variable-bit encoding schemes can exploit the average gap reduction and decrease the         total amount of bits per document pointer. This paper presents an efficient solution to the reassignment problem, which consists         in reducing the input data dimensionality using a SVD transformation, as well as considering it a Travelling Salesman Problem (TSP). We also present some efficient solutions based on clustering. Finally, we combine both the TSP and the clustering         strategies for reordering the document identifiers. We present experimental tests and performance results in two text TREC         collections, obtaining good compression ratios with low running times, and advance the possibility of obtaining scalable solutions         for web collections based on the techniques presented here.      </content></document><document><year>2006</year><authors>Hui Yang1  | Minjie Zhang1 </authors><title>Two-stage statistical language models for text database selection      </title><content>As the number and diversity of distributed Web databases on the Internet exponentially increase, it is difficult for user         to know which databases are appropriate to search. Given database language models that describe the content of each database,         database selection services can provide assistance in locating databases relevant to the information needs of users. In this         paper, we propose a database selection approach based on statistical language modeling. The basic idea behind the approach         is that, for databases that are categorized into a topic hierarchy, individual language models are estimated at different         search stages, and then the databases are ranked by the similarity to the query according to the estimated language model.         Two-stage smoothed language models are presented to circumvent inaccuracy due to word sparseness. Experimental results demonstrate         that such a language modeling approach is competitive with current state-of-the-art database selection approaches.      </content></document><document><year>2006</year><authors>Sari Suomela1  | Jaana KekГ¤lГ¤inen1 </authors><title>User evaluation of ontology as query construction tool      </title><content>This study examines the use of an ontology as a search tool. Sixteen subjects created queries using Concept-based Information         Retrieval Interface (CIRI) and a regular baseline IR interface. The simulated work task method was used to make the searching         situations realistic. Subjects&amp;#8217; search experiences, queries and search results were examined. The numbers of search concepts         and keys, as well as their overlap in the queries were investigated. The effectiveness of the CIRI and baseline queries was         compared. An Ontology Index (OI) was calculated for all search tasks and the correlation between the OI and the overlap of         search concepts and keys in queries was investigated. The number of search keys and concepts was higher in CIRI queries than         in baseline interface queries. Also the overlap of search keys was higher among CIRI users than among baseline users. These         both findings are due to CIRI&amp;#8217;s expansion feature. There was no clear correlation between OI and overlap of search concepts         and keys. The search results were evaluated with generalised precision and recall, and relevance scores based on individual         relevance assessments. The baseline interface queries performed better in all comparisons, but the difference was statistically         significant only in relevance scores based on individual relevance assessments.      </content></document><document><year>2006</year><authors>H. O. Nyongesa1  | S. Maleki-dizaji2</authors><title>User modelling using evolutionary interactive reinforcement learning      </title><content>As the volume and variety of information sources continues to grow, there is increasing difficulty with respect to obtaining         information that accurately matches user information needs. A number of factors affect information retrieval effectiveness         (the accuracy of matching user information needs against the retrieved information). First, users often do not present search         queries in the form that optimally represents their information need. Second, the measure of a document&amp;#8217;s relevance is often         highly subjective between different users. Third, information sources might contain heterogeneous documents, in multiple formats         and the representation of documents is not unified. This paper discusses an approach for improvement of information retrieval         effectiveness from document databases. It is proposed that retrieval effectiveness can be improved by applying computational         intelligence techniques for modelling information needs, through interactive reinforcement learning. The method combines qualitative         (subjective) user relevance feedback with quantitative (algorithmic) measures of the relevance of retrieved documents. An         information retrieval is developed whose retrieval effectiveness is evaluated using traditional precision and recall.      </content></document><document><year>2006</year><authors>Eija Airio1 </authors><title>Word normalization and decompounding in mono- and bilingual IR      </title><content>The present research studies the impact of decompounding and two different word normalization methods, stemming and lemmatization,         on monolingual and bilingual retrieval. The languages in the monolingual runs are English, Finnish, German and Swedish. The         source language of the bilingual runs is English, and the target languages are Finnish, German and Swedish. In the monolingual         runs, retrieval in a lemmatized compound index gives almost as good results as retrieval in a decompounded index, but in the         bilingual runs differences are found: retrieval in a lemmatized decompounded index performs better than retrieval in a lemmatized         compound index. The reason for the poorer performance of indexes without decompounding in bilingual retrieval is the difference         between the source language and target languages: phrases are used in English, while compounds are used instead of phrases         in Finnish, German and Swedish. No remarkable performance differences could be found between stemming and lemmatization.      </content></document><document><year>2006</year><authors>Marta Rukoz1 | Maude Manouvrier2  | GeneviГЁve Jomier2 </authors><title>&amp;#916;-distance: A family of dissimilarity metrics between images represented by multi-level feature vectors</title><content>This article presents the &amp;#916;-distance, a family of distances between images recursively decomposed into segments and represented by multi-level feature vectors. Such a structure is a quad, a quin or a nona-tree resulting from a fixed and arbitrary image partition or from an image segmentation process. It handles positional information of image features (e.g. color, texture or shape). &amp;#916;-distance is the generalized form of dissimilarity measures between multi-level feature vectors. Using different weights on tree nodes and different distances between nodes, distances between trees or visual similarity between images can be computed based on the general definition of &amp;#916;. In this article, we present three &amp;#916;-based distance families: two families of distances between tree structures, called                 -distance( for Tree) and                 -distance ( for Segment), and a family of visual distances between images, called ( for Visual). The -distance visually compares two images using their tree representation and the other two distances compare the tree structures resulting from image segmentation. Moreover, we show how existing distances between multi-level feature vectors appear to be particular cases of the &amp;#916;-distance</content></document><document><year>2007</year><authors>Diego Reforgiato Recupero1 </authors><title>A new unsupervised method for document clustering by using WordNet lexical and conceptual relations      </title><content>Text document clustering provides an effective and intuitive navigation mechanism to organize a large amount of retrieval         results by grouping documents in a small number of meaningful classes. Many well-known methods of text clustering make use         of a long list of words as vector space which is often unsatisfactory for a couple of reasons: first, it keeps the dimensionality         of the data very high, and second, it ignores important relationships between terms like synonyms or antonyms. Our unsupervised         method solves both problems by using ANNIE and WordNet lexical categories and WordNet ontology in order to create a well structured         document vector space whose low dimensionality allows common clustering algorithms to perform well. For the clustering step         we have chosen the bisecting k-means and the Multipole tree, a modified version of the Antipole tree data structure for, respectively, their accuracy and         speed.      </content></document><document><year>2007</year><authors>Rong Yan1  | Alex|er G. Hauptmann2 </authors><title>A review of text and image retrieval approaches for broadcast news video      </title><content>The effectiveness of a video retrieval system largely depends on the choice of underlying text and image retrieval components.         The unique properties of video collections (e.g., multiple sources, noisy features and temporal relations) suggest we examine         the performance of these retrieval methods in such a multimodal environment, and identify the relative importance of the underlying         retrieval components. In this paper, we review a variety of text/image retrieval approaches as well as their individual components         in the context of broadcast news video. Numerous components of text/image retrieval have been discussed in detail, including         retrieval models, text sources, temporal expansion methods, query expansion methods, image features, and similarity measures.         For each component, we conduct a series of retrieval experiments on TRECVID video collections to identify their advantages         and disadvantages. To provide a more complete coverage of video retrieval, we briefly discuss an emerging approach called         concept-based video retrieval, and review strategies for combining multiple retrieval outputs.      </content></document><document><year>2007</year><authors>David E. Losada1  | Leif Azzopardi2 </authors><title>An analysis on document length retrieval trends in language modeling smoothing      </title><content>Document length is widely recognized as an important factor for adjusting retrieval systems. Many models tend to favor the         retrieval of either short or long documents and, thus, a length-based correction needs to be applied for avoiding any length         bias. In Language Modeling for Information Retrieval, smoothing methods are applied to move probability mass from document         terms to unseen words, which is often dependant upon document length. In this article, we perform an in-depth study of this         behavior, characterized by the document length retrieval trends, of three popular smoothing methods across a number of factors,         and its impact on the length of documents retrieved and retrieval performance. First, we theoretically analyze the Jelinek&amp;#8211;Mercer,         Dirichlet prior and two-stage smoothing strategies and, then, conduct an empirical analysis. In our analysis we show how Dirichlet         prior smoothing caters for document length more appropriately than Jelinek&amp;#8211;Mercer smoothing which leads to its superior retrieval         performance. In a follow up analysis, we posit that length-based priors can be used to offset any bias in the length retrieval         trends stemming from the retrieval formula derived by the smoothing technique. We show that the performance of Jelinek&amp;#8211;Mercer         smoothing can be significantly improved by using such a prior, which provides a natural and simple alternative to decouple         the query and document modeling roles of smoothing. With the analysis of retrieval behavior conducted in this article, it         is possible to understand why the Dirichlet Prior smoothing performs better than the Jelinek&amp;#8211;Mercer, and why the performance         of the Jelinek&amp;#8211;Mercer method is improved by including a length-based prior.      </content></document><document><year>2007</year><authors>Luo Si1 | Jamie Callan2 | Suleyman Cetintas1  | Hao Yuan1 </authors><title>An effective and efficient results merging strategy for multilingual information retrieval in federated search environments      </title><content>Multilingual information retrieval is generally understood to mean the retrieval of relevant information in multiple target         languages in response to a user query in a single source language. In a multilingual federated search environment, different         information sources contain documents in different languages. A general search strategy in multilingual federated search environments         is to translate the user query to each language of the information sources and run a monolingual search in each information         source. It is then necessary to obtain a single ranked document list by merging the individual ranked lists from the information         sources that are in different languages. This is known as the results merging problem for multilingual information retrieval.         Previous research has shown that the simple approach of normalizing source-specific document scores is not effective. On the         other side, a more effective merging method was proposed to download and translate all retrieved documents into the source         language and generate the final ranked list by running a monolingual search in the search client. The latter method is more         effective but is associated with a large amount of online communication and computation costs. This paper proposes an effective         and efficient approach for the results merging task of multilingual ranked lists. Particularly, it downloads only a small         number of documents from the individual ranked lists of each user query to calculate comparable document scores by utilizing         both the query-based translation method and the document-based translation method. Then, query-specific and source-specific         transformation models can be trained for individual ranked lists by using the information of these downloaded documents. These         transformation models are used to estimate comparable document scores for all retrieved documents and thus the documents can         be sorted into a final ranked list. This merging approach is efficient as only a subset of the retrieved documents are downloaded         and translated online. Furthermore, an extensive set of experiments on the Cross-Language Evaluation Forum (CLEF) (http://www.clef-campaign.org/) data has demonstrated the effectiveness of the query-specific and source-specific results merging algorithm against other         alternatives. The new research in this paper proposes different variants of the query-specific and source-specific results         merging algorithm with different transformation models. This paper also provides thorough experimental results as well as         detailed analysis. All of the work substantially extends the preliminary research in (Si and Callan, in: Peters (ed.) Results         of the cross-language evaluation forum-CLEF 2005, 2005).      </content></document><document><year>2007</year><authors>Jing Jiang1  | ChengXiang Zhai1 </authors><title>An empirical study of tokenization strategies for biomedical information retrieval      </title><content>Due to the great variation of biological names in biomedical text, appropriate tokenization is an important preprocessing         step for biomedical information retrieval. Despite its importance, there has been little study on the evaluation of various         tokenization strategies for biomedical text. In this work, we conducted a careful, systematic evaluation of a set of tokenization         heuristics on all the available TREC biomedical text collections for ad hoc document retrieval, using two representative retrieval         methods and a pseudo-relevance feedback method. We also studied the effect of stemming and stop word removal on the retrieval         performance. As expected, our experiment results show that tokenization can significantly affect the retrieval accuracy; appropriate         tokenization can improve the performance by up to 96%, measured by mean average precision (MAP). In particular, it is shown         that different query types require different tokenization heuristics, stemming is effective only for certain queries, and         stop word removal in general does not improve the retrieval performance on biomedical text.      </content></document><document><year>2007</year><authors>Li Zhang1 | Tao Li2 | ShiXia Liu1  | Yue Pan1 </authors><title>An integrated system for building enterprise taxonomies      </title><content>Although considerable research has been conducted in the field of hierarchical text categorization, little has been done on         automatically collecting labeled corpus for building hierarchical taxonomies. In this paper, we propose an automatic method         of collecting training samples to build hierarchical taxonomies. In our method, the category node is initially defined by         some keywords, the web search engine is then used to construct a small set of labeled documents, and a topic tracking algorithm         with keyword-based content normalization is applied to enlarge the training corpus on the basis of the seed documents. We         also design a method to check the consistency of the collected corpus. The above steps produce a flat category structure which         contains all the categories for building the hierarchical taxonomy. Next, linear discriminant projection approach is utilized         to construct more meaningful intermediate levels of hierarchies in the generated flat set of categories. Experimental results         show that the training corpus is good enough for statistical classification methods.      </content></document><document><year>2007</year><authors>Chris Buckley1 | Darrin Dimmick2 | Ian Soboroff2  | Ellen Voorhees2 </authors><title>Bias and the limits of pooling for large collections      </title><content>Modern retrieval test collections are built through a process called pooling in which only a sample of the entire document         set is judged for each topic. The idea behind pooling is to find enough relevant documents such that when unjudged documents         are assumed to be nonrelevant the resulting judgment set is sufficiently complete and unbiased. Yet a constant-size pool represents         an increasingly small percentage of the document set as document sets grow larger, and at some point the assumption of approximately         complete judgments must become invalid. This paper shows that the judgment sets produced by traditional pooling when the pools         are too small relative to the total document set size can be biased in that they favor relevant documents that contain topic         title words. This phenomenon is wholly dependent on the collection size and does not depend on the number of relevant documents         for a given topic. We show that the AQUAINT test collection constructed in the recent TREC 2005 workshop exhibits this biased         relevance set; it is likely that the test collections based on the much larger GOV2 document set;also exhibit the bias. The         paper concludes with suggested modifications to traditional pooling and evaluation methodology that may allow very large reusable         test collections to be built.      </content></document><document><year>2007</year><authors>Thomas Deselaers1 | Daniel Keysers2  | Hermann Ney1 </authors><title>Features for image retrieval: an experimental comparison      </title><content>An experimental comparison of a large number of different image descriptors for content-based image retrieval is presented.         Many of the papers describing new techniques and descriptors for content-based image retrieval describe their newly proposed         methods as most appropriate without giving an in-depth comparison with all methods that were proposed earlier. In this paper,         we first give an overview of a large variety of features for content-based image retrieval and compare them quantitatively         on four different tasks: stock photo retrieval, personal photo collection retrieval, building retrieval, and medical image         retrieval. For the experiments, five different, publicly available image databases are used and the retrieval performance         of the features is analyzed in detail. This allows for a direct comparison of all features considered in this work and furthermore         will allow a comparison of newly proposed features to these in the future. Additionally, the correlation of the features is         analyzed, which opens the way for a simple and intuitive method to find an initial set of suitable features for a new task.         The article concludes with recommendations which features perform well for what type of data. Interestingly, the often used,         but very simple, color histogram performs well in the comparison and thus can be recommended as a simple baseline for many         applications.      </content></document><document><year>2007</year><authors>Fabio Crestani1 | Ian Ruthven1 </authors><title>Introduction to special issue on contextual information retrieval systems      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Paul B. Kantor1 </authors><title>Keith van Rijsbergen, The Geometry of Information Retrieval         Cambridge. 2005, 119pp + Bibiliography</title><content>Without Abstract</content></document><document><year>2007</year><authors>Zhenyu Liu1  | Wesley W. Chu1 </authors><title>Knowledge-based query expansion to support scenario-specific retrieval of medical free text      </title><content>In retrieving medical free text, users are often interested in answers pertinent to certain scenarios that correspond to common         tasks performed in medical practice, e.g., treatment or diagnosis of a disease. A major challenge in handling such queries is that scenario terms in the query (e.g., treatment) are often too general to match specialized terms in relevant documents (e.g., chemotherapy). In this paper, we propose a knowledge-based query expansion method that exploits the UMLS knowledge source to append the         original query with additional terms that are specifically relevant to the query's scenario(s). We compared the proposed method         with traditional statistical expansion that expands terms which are statistically correlated but not necessarily scenario         specific. Our study on two standard testbeds shows that the knowledge-based method, by providing scenario-specific expansion,         yields notable improvements over the statistical method in terms of average precision-recall. On the OHSUMED testbed, for         example, the improvement is more than 5% averaging over all scenario-specific queries studied and about 10% for queries that         mention certain scenarios, such as treatment of a disease and differential diagnosis of a symptom/disease.      </content></document><document><year>2007</year><authors>Donald Metzler1  | W. Bruce Croft1</authors><title>Linear feature-based models for information retrieval      </title><content>There have been a number of linear, feature-based models proposed by the information retrieval community recently. Although         each model is presented differently, they all share a common underlying framework. In this paper, we explore and discuss the         theoretical issues of this framework, including a novel look at the parameter space. We then detail supervised training algorithms         that directly maximize the evaluation metric under consideration, such as mean average precision. We present results that         show training models in this way can lead to significantly better test set performance compared to other training methods         that do not directly maximize the metric. Finally, we show that linear feature-based models can consistently and significantly         outperform current state of the art retrieval models with the correct choice of features.      </content></document><document><year>2007</year><authors>Nathalie Hern|ez1 | Josiane Mothe2 | Claude Chrisment1  | Daniel Egret3 </authors><title>Modeling context through domain ontologies      </title><content>Traditional information retrieval systems aim at satisfying most users for most of their searches, leaving aside the context         in which the search takes place. We propose to model two main aspects of context: The themes of the user's information need         and the specific data the user is looking for to achieve the task that has motivated his search. Both aspects are modeled         by means of ontologies. Documents are semantically indexed according to the context representation and the user accesses information         by browsing the ontologies. The model has been applied to a case study that has shown the added value of such a semantic representation         of context.      </content></document><document><year>2007</year><authors>Kumiko Tanaka-Ishii1  | Yuichiro Ishii2 </authors><title>Multilingual phrase-based concordance generation in real-time      </title><content>We present software that generates phrase-based concordances in real-time based on Internet searching. When a user enters         a string of words for which he wants to find concordances, the system sends this string as a query to a search engine and         obtains search results for the string. The concordances are extracted by performing statistical analysis on search results         and then fed back to the user. Unlike existing tools, this concordance consultation tool is language-independent, so concordances         can be obtained even in a language for which there are no well-established analytical methods. Our evaluation has revealed         that concordances can be obtained more effectively than by only using a search engine directly.      </content></document><document><year>2007</year><authors>Panagiotis Symeonidis1 | Alex|ros Nanopoulos1 | Apostolos N. Papadopoulos1  | Yannis Manolopoulos1 </authors><title>Nearest-biclusters collaborative filtering based on constant and coherent values      </title><content>Collaborative Filtering (CF) Systems have been studied extensively for more than a decade to confront the &amp;#8220;information overload&amp;#8221;         problem. Nearest-neighbor CF is based either on similarities between users or between items, to form a neighborhood of users         or items, respectively. Recent research has tried to combine the two aforementioned approaches to improve effectiveness. Traditional         clustering approaches (k-means or hierarchical clustering) has been also used to speed up the recommendation process. In this paper, we use biclustering         to disclose this duality between users and items, by grouping them in both dimensions simultaneously. We propose a novel nearest-biclusters         algorithm, which uses a new similarity measure that achieves partial matching of users&amp;#8217; preferences. We apply nearest-biclusters         in combination with two different types of biclustering algorithms&amp;#8212;Bimax and xMotif&amp;#8212;for constant and coherent biclustering,         respectively. Extensive performance evaluation results in three real-life data sets are provided, which show that the proposed         method improves substantially the performance of the CF process.      </content></document><document><year>2007</year><authors>Stephen Robertson1  | Hugo Zaragoza2 </authors><title>On rank-based effectiveness measures and optimization      </title><content>Many current retrieval models and scoring functions contain free parameters which need to be set&amp;#8212;ideally, optimized. The process         of optimization normally involves some training corpus of the usual document-query-relevance judgement type, and some choice         of measure that is to be optimized. The paper proposes a way to think about the process of exploring the space of parameter         values, and how moving around in this space might be expected to affect different measures. One result, concerning local optima,         is demonstrated for a range of rank-based evaluation measures.      </content></document><document><year>2007</year><authors>Jason J. Jung1 </authors><title>Ontological framework based on contextual mediation for collaborative information retrieval      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Fern|o Diaz1 </authors><title>Regularizing query-based retrieval scores      </title><content>We adapt the cluster hypothesis for score-based information retrieval by claiming that closely related documents should have         similar scores. Given a retrieval from an arbitrary system, we describe an algorithm which directly optimizes this objective         by adjusting retrieval scores so that topically related documents receive similar scores. We refer to this process as score         regularization. Because score regularization operates on retrieval scores, regardless of their origin, we can apply the technique         to arbitrary initial retrieval rankings. Document rankings derived from regularized scores, when compared to rankings derived         from un-regularized scores, consistently and significantly result in improved performance given a variety of baseline retrieval         algorithms. We also present several proofs demonstrating that regularization generalizes methods such as pseudo-relevance         feedback, document expansion, and cluster-based retrieval. Because of these strong empirical and theoretical results, we argue         for the adoption of score regularization as general design principle or post-processing step for information retrieval systems.      </content></document><document><year>2007</year><authors>Kimmo Kettunen1 | Eija Airio1  | Kalervo JГ¤rvelin1 </authors><title>Restricted inflectional form generation in management of morphological keyword variation      </title><content>Word form normalization through lemmatization or stemming is a standard procedure in information retrieval because morphological         variation needs to be accounted for and several languages are morphologically non-trivial. Lemmatization is effective but         often requires expensive resources. Stemming is also effective in most contexts, generally almost as good as lemmatization         and typically much less expensive; besides it also has a query expansion effect. However, in both approaches the idea is to         turn many inflectional word forms to a single lemma or stem both in the database index and in queries. This means extra effort         in creating database indexes. In this paper we take an opposite approach: we leave the database index un-normalized and enrich         the queries to cover for surface form variation of keywords. A potential penalty of the approach would be long queries and         slow processing. However, we show that it only matters to cover a negligible number of possible surface forms even in morphologically         complex languages to arrive at a performance that is almost as good as that delivered by stemming or lemmatization. Moreover,         we show that, at least for typical test collections, it only matters to cover nouns and adjectives in queries. Furthermore,         we show that our findings are particularly good for short queries that resemble normal searches of web users. Our approach         is called FCG (for Frequent Case (form) Generation). It can be relatively easily implemented for Latin/Greek/Cyrillic alphabet         languages by examining their (typically very skewed) nominal form statistics in a small text sample and by creating surface         form generators for the 3&amp;#8211;9 most frequent forms. We demonstrate the potential of our FCG approach for several languages of         varying morphological complexity: Swedish, German, Russian, and Finnish in well-known test collections. Applications include         in particular Web IR in languages poor in morphological resources.      </content></document><document><year>2007</year><authors>Shengli Wu1  | Sally McClean1 </authors><title>Result merging methods in distributed information retrieval with overlapping databases      </title><content>In distributed information retrieval systems, document overlaps occur frequently among different component databases. This         paper presents an experimental investigation and evaluation of a group of result merging methods including the shadow document         method and the multi-evidence method in the environment of overlapping databases. We assume, with the exception of resultant         document lists (either with rankings or scores), no extra information about retrieval servers and text databases is available,         which is the usual case for many applications on the Internet and the Web.                     The experimental results show that the shadow document method and the multi-evidence method are the two best methods when               overlap is high, while Round-robin is the best for low overlap. The experiments also show that [0,1] linear normalization               is a better option than linear regression normalization for result merging in a heterogeneous environment.            </content></document><document><year>2007</year><authors>Jacques Savoy1 </authors><title>Searching strategies for the Bulgarian language      </title><content>This paper reports on the underlying IR problems encountered when indexing and searching with the Bulgarian language. For         this language we propose a general light stemmer and demonstrate that it can be quite effective, producing significantly better         MAP (around + 34%) than an approach not applying stemming. We implement the GL2 model derived from the Divergence from Randomness paradigm and find its retrieval effectiveness better than other probabilistic, vector-space and language models. The resulting         MAP is found to be about 50% better than the classical tf idf approach. Moreover, increasing the query size enhances the MAP by around 10% (from T to TD). In order to compare the retrieval         effectiveness of our suggested stopword list and the light stemmer developed for the Bulgarian language, we conduct a set         of experiments on another stopword list and also a more complex and aggressive stemmer. Results tend to indicate that there         is no statistically significant difference between these variants and our suggested approach. This paper evaluates other indexing         strategies such as 4-gram indexing and indexing based on the automatic decompounding of compound words. Finally, we analyze         certain queries to discover why we obtained poor results, when indexing Bulgarian documents using the suggested word-based         approach.      </content></document><document><year>2007</year><authors>Jimmy Lin1  | W. John Wilbur2 </authors><title>Syntactic sentence compression in the biomedical domain: facilitating access to related articles      </title><content>We explore a syntactic approach to sentence compression in the biomedical domain, grounded in the context of result presentation         for related article search in the PubMed search engine. By automatically trimming inessential fragments of article titles,         a system can effectively display more results in the same amount of space. Our implemented prototype operates by applying         a sequence of syntactic trimming rules over the parse trees of article titles. Two separate studies were conducted using a         corpus of manually compressed examples from MEDLINE: an automatic evaluation using Bleu and a summative evaluation involving human assessors. Experiments show that a syntactic approach to sentence compression         is effective in the biomedical domain and that the presentation of compressed article titles supports accurate &amp;#8220;interest judgments&amp;#8221;,         decisions by users as to whether an article is worth examining in more detail.      </content></document><document><year>2007</year><authors>Xiaojun Wan1 </authors><title>Using only cross-document relationships for both generic and topic-focused multi-document summarizations      </title><content>In recent years graph-ranking based algorithms have been proposed for single document summarization and generic multi-document         summarization. The algorithms make use of the &amp;#8220;votings&amp;#8221; or &amp;#8220;recommendations&amp;#8221; between sentences to evaluate the importance         of the sentences in the documents. This study aims to differentiate the cross-document and within-document relationships between         sentences for generic multi-document summarization and adapt the graph-ranking based algorithm for topic-focused summarization.         The contributions of this study are two-fold: (1) For generic multi-document summarization, we apply the graph-based ranking         algorithm based on each kind of sentence relationship and explore their relative importance for summarization performance.         (2) For topic-focused multi-document summarization, we propose to integrate the relevance of the sentences to the specified         topic into the graph-ranking based method. Each individual kind of sentence relationship is also differentiated and investigated         in the algorithm. Experimental results on DUC 2002&amp;#8211;DUC 2005 data demonstrate the great importance of the cross-document relationships         between sentences for both generic and topic-focused multi-document summarizations. Even the approach based only on the cross-document         relationships can perform better than or at least as well as the approaches based on both kinds of relationships between sentences.      </content></document><document><year>2005</year><authors>Benjamin Piwowarski1  | Patrick Gallinari2 </authors><title>A Bayesian Framework for XML Information Retrieval: Searching and Learning with the INEX Collection</title><content>Most recent document standards like XML rely on structured representations. On the other hand, current information retrieval systems have been developed for flat document representations and cannot be easily extended to cope with more complex document types. The design of such systems is still an open problem. We present a new model for structured document retrieval which allows computing scores of document parts. This model is based on Bayesian networks whose conditional probabilities are learnt from a labelled collection of structured documents&amp;#x2014;which is composed of documents, queries and their associated assessments. Training these models is a complex machine learning task and is not standard. This is the focus of the paper: we propose here to train the structured Bayesian Network model using a cross-entropy training criterion. Results are presented on the INEX corpus of XML documents.</content></document><document><year>2005</year><authors>Ray R. Larson1 </authors><title>A Fusion Approach to XML Structured Document Retrieval</title><content>In this paper we evaluate the application of data fusion or meta-search methods, combining different algorithms and XML elements, to content-oriented retrieval of XML structured data. The primary approach is the combination of a probabilistic methods using Logistic regression and the Okapi BM-25 algorithm for estimation of document relevance or XML element relevance, in conjunction with Boolean approaches for some query elements. In the evaluation we use the INEX XML test collection to examine the relative performance of individual algorithms and elements and compare these to the performance of the data fusion approaches.</content></document><document><year>2005</year><authors>P. Srinivasan1 | F. Menczer2  | G. Pant3 </authors><title>A General Evaluation Framework for Topical Crawlers</title><content>Topical crawlers are becoming important tools to support applications such as specialized Web portals, online searching, and competitive intelligence. As the Web mining field matures, the disparate crawling strategies proposed in the literature will have to be evaluated and compared on common tasks through well-defined performance measures. This paper presents a general framework to evaluate topical crawlers. We identify a class of tasks that model crawling applications of different nature and difficulty. We then introduce a set of performance measures for fair comparative evaluations of crawlers along several dimensions including generalized notions of precision, recall, and efficiency that are appropriate and practical for the Web. The framework relies on independent relevance judgements compiled by human editors and available from public directories. Two sources of evidence are proposed to assess crawled pages, capturing different relevance criteria. Finally we introduce a set of topic characterizations to analyze the variability in crawling effectiveness across topics. The proposed evaluation framework synthesizes a number of methodologies in the topical crawlers literature and many lessons learned from several studies conducted by our group. The general framework is described in detail and then illustrated in practice by a case study that evaluates four public crawling algorithms. We found that the proposed framework is effective at evaluating, comparing, differentiating and interpreting the performance of the four crawlers. For example, we found the IS crawler to be most sensitive to the popularity of topics.</content></document><document><year>2005</year><authors>Maristella Agosti1  | Luca Pretto1 </authors><title>A Theoretical Study of a Generalized Version of Kleinbergs HITS Algorithm</title><content>Kleinbergs HITS algorithm (Kleinberg 1999), which was originally developed in a Web context, tries to infer the authoritativeness of a Web page in relation to a specific query using the structure of a subgraph of the Web graph, which is obtained considering this specific query. Recent applications of this algorithm in contexts far removed from that of Web searching (Bacchin, Ferro and Melucci 2002, Ng et al. 2001) inspired us to study the algorithm in the abstract, independently of its particular applications, trying to mathematically illuminate its behaviour. In the present paper we detail this theoretical analysis. The original work starts from the definition of a revised and more general version of the algorithm, which includes the classic one as a particular case. We perform an analysis of the structure of two particular matrices, essential to studying the behaviour of the algorithm, and we prove the convergence of the algorithm in the most general case, finding the analytic expression of the vectors to which it converges. Then we study the symmetry of the algorithm and prove the equivalence between the existence of symmetry and the independence from the order of execution of some basic operations on initial vectors. Finally, we expound some interesting consequences of our theoretical results.</content></document><document><year>2005</year><authors>Analysis of Statistical Question Classification for Fact-Based Questions</authors><title>Question classification systems play an important role in question answering systems and can be used in a wide range of other domains. The goal of question classification is to accurately assign labels to questions based on expected answer type. Most approaches in the past have relied on matching questions against hand-crafted rules. However, rules require laborious effort to create and often suffer from being too specific. Statistical question classification methods overcome these issues by employing machine learning techniques. We empirically show that a statistical approach is robust and achieves good performance on three diverse data sets with little or no hand tuning. Furthermore, we examine the role different syntactic and semantic features have on performance. We find that semantic features tend to increase performance more than purely syntactic features. Finally, we analyze common causes of misclassification error and provide insight into ways they may be overcome.</title><content/></document></documents>