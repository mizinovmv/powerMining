<?xml version="1.0" encoding="UTF-8" standalone="no"?><documents><document><year>2004</year><authors>Haim Schweitzer1 </authors><title>A Distributed Algorithm for Content Based Indexing of Images by Projections on Ritz Primary Images      </title><content>Large collections of images can be indexed by their projections on a few &amp;#8220;primary&amp;#8221; images. The optimal primary images are         the eigenvectors of a large covariance matrix. We address the problem of computing primary images when access to the images         is expensive. This is the case when the images cannot be kept locally, but must be accessed through slow communication such         as the Internet, or stored in a compressed form. A distributed algorithm that computes optimal approximations to the eigenvectors         (known as Ritz vectors) in one pass through the image set is proposed. When iterated, the algorithm can recover the exact         eigenvectors. The widely used SVD technique for computing the primary images of a small image set is a special case of the         proposed algorithm. In applications to image libraries and learning, it is necessary to compute different primary images for         several sub-categories of the image set. The proposed algorithm can compute these additional primary images &amp;#8220;offline&amp;#8221;, without         the image data. Similar computation by other algorithms is impractical even when access to the images is inexpensive.      </content></document><document><year>2004</year><authors>Xiaowei Xu1 | Jochen JГ¤ger2  | Hans-Peter Kriegel2 </authors><title>A Fast Parallel Clustering Algorithm for Large Spatial Databases      </title><content>The clustering algorithm DBSCAN relies on a density-based notion of clusters and is designed to discover clusters of arbitrary         shape as well as to distinguish noise. In this paper, we present PDBSCAN, a parallel version of this algorithm. We use the         &amp;#8216;shared-nothing&amp;#8217; architecture with multiple computers interconnected through a network. A fundamental component of a shared-nothing         system is its distributed data structure. We introduce the dR*-tree, a distributed spatial index structure in which the data         is spread among multiple computers and the indexes of the data are replicated on every computer. We implemented our method         using a number of workstations connected via Ethernet (10 Mbit). A performance evaluation shows that PDBSCAN offers nearly         linear speedup and has excellent scaleup and sizeup behavior.      </content></document><document><year>2004</year><authors>Jon Kleinberg1 | Christos Papadimitriou2  | Prabhakar Raghavan3 </authors><title>A Microeconomic View of Data Mining</title><content>We present a rigorous framework, based on optimization, for evaluating data mining operations such as associations and clustering, in terms of their utility in decision-making. This framework leads quickly to some interesting computational problems related to sensitivity analysis, segmentation and the theory of games.</content></document><document><year>2004</year><authors>R.D. Lawrence1 | G.S. Almasi1  | H.E. Rushmeier1 </authors><title>A Scalable Parallel Algorithm for Self-Organizing Maps with Applications to Sparse Data Mining Problems</title><content>We describe a scalable parallel implementation of the self organizing map (SOM) suitable for data-mining applications involving clustering or segmentation against large data sets such as those encountered in the analysis of customer spending patterns. The parallel algorithm is based on the batch SOM formulation in which the neural weights are updated at the end of each pass over the training data. The underlying serial algorithm is enhanced to take advantage of the sparseness often encountered in these data sets. Analysis of a realistic test problem shows that the batch SOM algorithm captures key features observed using the conventional on-line algorithm, with comparable convergence rates.Performance measurements on an SP2 parallel computer are given for two retail data sets and a publicly available set of census data.These results demonstrate essentially linear speedup for the parallel batch SOM algorithm, using both a memory-contained sparse formulation as well as a separate implementation in which the mining data is accessed directly from a parallel file system. We also present visualizations of the census data to illustrate the value of the clustering information obtained via the parallel SOM method.</content></document><document><year>2004</year><authors>Greg Ridgeway1  | David Madigan2 </authors><title>A Sequential Monte Carlo Method for Bayesian Analysis of Massive Datasets</title><content>Markov chain Monte Carlo (MCMC) techniques revolutionized statistical practice in the 1990s by providing an essential toolkit for making the rigor and flexibility of Bayesian analysis computationally practical. At the same time the increasing prevalence of massive datasets and the expansion of the field of data mining has created the need for statistically sound methods that scale to these large problems. Except for the most trivial examples, current MCMC methods require a complete scan of the dataset for each iteration eliminating their candidacy as feasible data mining techniques.In this article we present a method for making Bayesian analysis of massive datasets computationally feasible. The algorithm simulates from a posterior distribution that conditions on a smaller, more manageable portion of the dataset. The remainder of the dataset may be incorporated by reweighting the initial draws using importance sampling. Computation of the importance weights requires a single scan of the remaining observations. While importance sampling increases efficiency in data access, it comes at the expense of estimation efficiency. A simple modification, based on the rejuvenation step used in particle filters for dynamic systems models, sidesteps the loss of efficiency with only a slight increase in the number of data accesses.</content></document><document><year>2004</year><authors>Gregory F. Cooper1</authors><title>A Simple Constraint-Based Algorithm for Efficiently Mining Observational Databases for Causal Relationships</title><content>This paper presents a simple, efficient computer-based method for discovering causal relationships from databases that contain observational data. Observational data is passively observed, as contrasted with experimental data. Most of the databases available for data mining are observational. There is great potential for mining such databases to discover causal relationships. We illustrate how observational data can constrain the causal relationships among measured variables, sometimes to the point that we can conclude that one variable is causing another variable. The presentation here is based on a constraint-based approach to causal discovery. A primary purpose of this paper is to present the constraint-based causal discovery method in the simplest possible fashion in order to (1) readily convey the basic ideas that underlie more complex constraint-based causal discovery techniques, and (2) permit interested readers to rapidly program and apply the method to their own databases, as a start toward using more elaborate causal discovery algorithms.</content></document><document><year>2004</year><authors>Ashwin Srinivasan1 </authors><title>A Study of Two Sampling Methods for Analyzing Large Datasets with ILP</title><content>This paper is concerned with problems that arise when submitting large quantities of data to analysis by an Inductive Logic Programming (ILP) system. Complexity arguments usually make it prohibitive to analyse such datasets in their entirety. We examine two schemes that allow an ILP system to construct theories by sampling from this large pool of data. The first, subsampling, is a single-sample design in which the utility of a potential rule is evaluated on a randomly selected sub-sample of the data. The second, logical windowing, is multiple-sample design that tests and sequentially includes errors made by a partially correct theory. Both schemes are derived from techniques developed to enable propositional learning methods (like decision trees) to cope with large datasets. The ILP system CProgol, equipped with each of these methods, is used to construct theories for two datasets&amp;#x2014;one artificial (a chess endgame) and the other naturally occurring (a language tagging problem). In each case, we ask the following questions of CProgol equipped with sampling: (1) Is its theory comparable in predictive accuracy to that obtained if all the data were used (that is, no sampling was employed)?; and (2) Is its theory constructed in less time than the one obtained with all the data? For the problems considered, the answers to these questions is yes. This suggests that an ILP program equipped with an appropriate sampling method could begin to address problems satisfactorily that have hitherto been inaccessible simply due to data extent.</content></document><document><year>2004</year><authors>Woong-Kee Loh1 | Sang-Wook Kim2  | Kyu-Young Whang1 </authors><title>A Subsequence Matching Algorithm that Supports Normalization Transform in Time-Series Databases</title><content>In this paper, an algorithm is proposed for subsequence matching that supports normalization transform in time-series databases. Normalization transform enables finding sequences with similar fluctuation patterns even though they are not close to each other before the normalization transform. Simple application of existing subsequence matching algorithms to support normalization transform is not feasible since the algorithms do not have information for normalization transform of subsequences of arbitrary lengths. Application of the existing whole matching algorithm supporting normalization transform to the subsequence matching is feasible, but requires an index for every possible length of the query sequence causing serious overhead on both storage space and update time. The proposed algorithm generates indexes only for a small number of different lengths of query sequences. For subsequence matching it selects the most appropriate index among them. Better search performance can be obtained by using more indexes. In this paper, the approach is called index interpolation. It is formally proved that the proposed algorithm does not cause false dismissal. The search performance can be traded off with storage space by adjusting the number of indexes. For performance evaluation, a series of experiments is conducted using the indexes for only five different lengths out of lengths 256512 of the query sequence. The results show that the proposed algorithm outperforms the sequential scan by up to 2.4 times on the average when the selectivity of the query is 10&amp;#x2013;2 and up to 14.6 times when it is 10&amp;#x2013;5. Since the proposed algorithm performs better with smaller selectivities, it is suitable for practical situations, where the queries with smaller selectivities are much more frequent.</content></document><document><year>2004</year><authors>Foster Provost1  | Venkateswarlu Kolluri2| 3 </authors><title>A Survey of Methods for Scaling Up Inductive Algorithms</title><content>One of the defining challenges for the KDD research community is to enable inductive learning algorithms to mine very large databases. This paper summarizes, categorizes, and compares existing work on scaling up inductive algorithms. We concentrate on algorithms that build decision trees and rule sets, in order to provide focus and specific details; the issues and techniques generalize to other types of data mining. We begin with a discussion of important issues related to scaling up. We highlight similarities among scaling techniques by categorizing them into three main approaches. For each approach, we then describe, compare, and contrast the different constituent techniques, drawing on specific examples from published papers. Finally, we use the preceding analysis to suggest how to proceed when dealing with a large problem, and where to focus future research.</content></document><document><year>2004</year><authors>Won Kim1| Byoung-Ju Choi2| Eui-Kyeong Hong3| Soo-Kyung Kim4 | Doheon Lee5</authors><title>A Taxonomy of Dirty Data</title><content>Today large corporations are constructing enterprise data warehouses from disparate data sources in order to run enterprise-wide data analysis applications, including decision support systems, multidimensional online analytical applications, data mining, and customer relationship management systems. A major problem that is only beginning to be recognized is that the data in data sources are often dirty. Broadly, dirty data include missing data, wrong data, and non-standard representations of the same data. The results of analyzing a database/data warehouse of dirty data can be damaging and at best be unreliable. In this paper, a comprehensive classification of dirty data is developed for use as a framework for understanding how dirty data arise, manifest themselves, and may be cleansed to ensure proper construction of data warehouses and accurate data analysis. The impact of dirty data on data mining is also explored.</content></document><document><year>2004</year><authors>Christopher J.C. Burges1 </authors><title>A Tutorial on Support Vector Machines for Pattern Recognition</title><content>The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.</content></document><document><year>2004</year><authors>Thomas Reinartz1</authors><title>A Unifying View on Instance Selection</title><content>In this paper, we consider instance selection as an important focusing task in the data preparation phase of knowledge discovery and data mining. Focusing generally covers all issues related to data reduction. First of all, we define a broader perspective on focusing tasks, choose instance selection as one particular focusing task, and outline the specification of concrete evaluation criteria to measure success of instance selection approaches. Thereafter, we present a unifying framework that covers existing approaches towards solutions for instance selection as instantiations. We describe specific examples of instantiations of this framework and discuss their strengths and weaknesses. Then, we outline an enhanced framework for instance selection, generic sampling, and summarize example evaluation results for several different instantiations of its implementation. Finally, we conclude with open issues and research challenges for instance selection as well as focusing in general.</content></document><document><year>2004</year><authors>Tom Fawcett1| 1  | Foster Provost1| 1 </authors><title>Adaptive Fraud Detection</title><content>One method for detecting fraud is to check for suspicious changes in user behavior. This paper describes the automatic design of user profiling methods for the purpose of fraud detection, using a series of data mining techniques. Specifically, we use a rule-learning program to uncover indicators of fraudulent behavior from a large database of customer transactions. Then the indicators are used to create a set of monitors, which profile legitimate customer behavior and indicate anomalies. Finally, the outputs of the monitors are used as features in a system that learns to combine evidence to generate high-confidence alarms. The system has been applied to the problem of detecting cellular cloning fraud based on a database of call records. Experiments indicate that this automatic approach performs better than hand-crafted methods for detecting fraud. Furthermore, this approach can adapt to the changing conditions typical of fraud detection environments.</content></document><document><year>2004</year><authors>Carlos Domingo1| Ricard Gavald&amp;agrave 2 | Osamu Watanabe1</authors><title>Adaptive Sampling Methods for Scaling Up Knowledge Discovery Algorithms</title><content>Scalability is a key requirement for any KDD and data mining algorithm, and one of the biggest research challenges is to develop methods that allow to use large amounts of data. One possible approach for dealing with huge amounts of data is to take a random sample and do data mining on it, since for many data mining applications approximate answers are acceptable. However, as argued by several researchers, random sampling is difficult to use due to the difficulty of determining an appropriate sample size. In this paper, we take a sequential sampling approach for solving this difficulty, and propose an adaptive sampling method that solves a general problem covering many actual problems arising in applications of discovery science. An algorithm following this method obtains examples sequentially in an on-line fashion, and it determines from the obtained examples whether it has already seen a large enough number of examples. Thus, sample size is not fixed a priori; instead, it adaptively depends on the situation. Due to this adaptiveness, if we are not in a worst case situation as fortunately happens in many practical applications, then we can solve the problem with a number of examples much smaller than required in the worst case. We prove the correctness of our method and estimates its efficiency theoretically. For illustrating its usefulness, we consider one concrete task requiring sampling, provide an algorithm based on our method, and show its efficiency experimentally.</content></document><document><year>2004</year><authors>Inderpal Bh|ari1| 1 | Edward Colet1| 1 | Jennifer Parker1| 1 | Zachary Pines1| 1 | Rajiv Pratap1| 1  | Krishnakumar Ramanujam1| 1 </authors><title>Advanced Scout: Data Mining and Knowledge Discovery in NBA Data</title><content>Advanced Scout is a PC-based data mining application used by National Basketball Association (NBA)coaching staffs to discover interesting patterns in basketball game data. We describe Advanced Scout software from the perspective of data mining and knowledge discovery. This paper highlights the pre-processing of raw data that the program performs, describes the data mining aspects of the software and how the interpretation of patterns supports the processof knowledge discovery. The underlying technique of attribute focusing asthe basis of the algorithm is also described. The process of pattern interpretation is facilitated by allowing the user to relate patterns to video tape.</content></document><document><year>2004</year><authors>Henry Brighton1 | Chris Mellish2</authors><title>Advances in Instance Selection for Instance-Based Learning Algorithms</title><content>The basic nearest neighbour classifier suffers from the indiscriminate storage of all presented training instances. With a large database of instances classification response time can be slow. When noisy instances are present classification accuracy can suffer. Drawing on the large body of relevant work carried out in the past 30 years, we review the principle approaches to solving these problems. By deleting instances, both problems can be alleviated, but the criterion used is typically assumed to be all encompassing and effective over many domains. We argue against this position and introduce an algorithm that rivals the most successful existing algorithm. When evaluated on 30 different problems, neither algorithm consistently outperforms the other: consistency is very hard. To achieve the best results, we need to develop mechanisms that provide insights into the structure of class definitions. We discuss the possibility of these mechanisms and propose some initial measures that could be useful for the data miner.</content></document><document><year>2004</year><authors>Rosa Meo1 | Giuseppe Psaila1  | Stefano Ceri1 </authors><title>An Extension to SQL for Mining Association Rules</title><content>Data mining evolved as a collection of applicative problems and efficient solution algorithms relative to rather peculiar problems, all focused on the discovery of relevant information hidden in databases of huge dimensions. In particular, one of the most investigated topics is the discovery of association rules.This work proposes a unifying model that enables a uniform description of the problem of discovering association rules. The model provides a SQL-like operator, named XY, which is capable of expressing all the problems presented so far in the literature concerning the mining of association rules. We demonstrate the expressive power of the new operator by means of several examples, some of which are classical, while some others are fully original and correspond to novel and unusual applications. We also present the operational semantics of the operator by means of an extended relational algebra.</content></document><document><year>2004</year><authors>Darya Chudova1  | Padhraic Smyth1 </authors><title>Analysis of Pattern Discovery in Sequences Using a Bayes Error Framework</title><content>In this paper we investigate the general problem of discovering recurrent patterns that are embedded in categorical sequences. An important real-world problem of this nature is motif discovery in DNA sequences. There are a number of fundamental aspects of this data mining problem that can make discovery easy or hard&amp;#x2014;we characterize the difficulty of this problem using an analysis based on the Bayes error rate under a Markov assumption. The Bayes error framework demonstrates why certain patterns are much harder to discover than others. It also explains the role of different parameters such as pattern length and pattern frequency in sequential discovery. We demonstrate how the Bayes error can be used to calibrate existing discovery algorithms, providing a lower bound on achievable performance. We discuss a number of fundamental issues that characterize sequential pattern discovery in this context, present a variety of empirical results to complement and verify the theoretical analysis, and apply our methodology to real-world motif-discovery problems in computational biology.</content></document><document><year>2004</year><authors>Ron Kohavi1  | Foster Provost2 </authors><title>Applications of Data Mining to Electronic Commerce</title><content>Without Abstract</content></document><document><year>2004</year><authors>Paolo Giudici1  | Robert Castelo2 </authors><title>Association Models for Web Mining</title><content>We describe how statistical association models and, specifically, graphical models, can be usefully employed to model web mining data. We describe some methodological problems related to the implementation of discrete graphical models for web mining data. In particular, we discuss model selection procedures.</content></document><document><year>2004</year><authors>Joseph Ramsey1| Paul Gazis2| Ted Roush2| Peter Spirtes3| 4 | Clark Glymour5| 4</authors><title>Automated Remote Sensing with Near Infrared Reflectance Spectra: Carbonate Recognition</title><content>Reflectance spectroscopy is a standard tool for studying the mineral composition of rock and soil samples and for remote sensing of terrestrial and extraterrestrial surfaces. We describe research on automated methods of mineral identification from reflectance spectra and give evidence that a simple algorithm, adapted from a well-known search procedure for Bayes nets, identifies the most frequently occurring classes of carbonates with reliability equal to or greater than that of human experts. We compare the reliability of the procedure to the reliability of several other automated methods adapted to the same purpose. Evidence is given that the procedure can be applied to some other mineral classes as well. Since the procedure is fast with low memory requirements, it is suitable for on-board scientific analysis by orbiters or surface rovers.</content></document><document><year>2004</year><authors>Sreerama K. Murthy1 </authors><title>Automatic Construction of Decision Trees from Data: A Multi-Disciplinary Survey</title><content>Decision trees have proved to be valuable tools for the description, classification and generalization of data. Work on constructing decision trees from data exists in multiple disciplines such as statistics, pattern recognition, decision theory, signal processing, machine learning and artificial neural networks. Researchers in these disciplines, sometimes working on quite different problems, identified similar issues and heuristics for decision tree construction. This paper surveys existing work on decision tree construction, attempting to identify the important issues involved, directions the work has taken and the current state of the art.</content></document><document><year>2004</year><authors>David Heckerman1 </authors><title>Bayesian Networks for Data Mining</title><content>A Bayesian network is a graphical model that encodesprobabilistic relationships among variables of interest. When used inconjunction with statistical techniques, the graphical model hasseveral advantages for data modeling. One, because the model encodesdependencies among all variables, it readily handles situations wheresome data entries are missing. Two, a Bayesian network can be used tolearn causal relationships, and hence can be used to gain understanding about a problem domain and to predict the consequencesof intervention. Three, because the model has both a causal andprobabilistic semantics, it is an ideal representation for combiningprior knowledge (which often comes in causal form) and data. Four,Bayesian statistical methods in conjunction with Bayesian networksoffer an efficient and principled approach for avoiding theoverfitting of data. In this paper, we discuss methods for constructing Bayesian networks from prior knowledge and summarizeBayesian statistical methods for using data to improve these models.With regard to the latter task, we describe methods for learning boththe parameters and structure of a Bayesian network, includingtechniques for learning with incomplete data. In addition, we relateBayesian-network methods for learning to techniques for supervised andunsupervised learning. We illustrate the graphical-modeling approachusing a real-world case study.</content></document><document><year>2004</year><authors>Craig Silverstein1 | Sergey Brin1  | Rajeev Motwani1 </authors><title>Beyond Market Baskets: Generalizing Association Rules to Dependence Rules</title><content>One of the more well-studied problems in data mining is the search for association rules in market basket data. Association rules are intended to identify patterns of the type: A customer purchasing item A often also purchases item B. Motivated partly by the goal of generalizing beyond market basket data and partly by the goal of ironing out some problems in the definition of association rules, we develop the notion of dependence rules that identify statistical dependence in both the presence and absence of items in itemsets. We propose measuring significance of dependence via the chi-squared test for independence from classical statistics. This leads to a measure that is upward-closed in the itemset lattice, enabling us to reduce the mining problem to the search for a border between dependent and independent itemsets in the lattice. We develop pruning strategies based on the closure property and thereby devise an efficient algorithm for discovering dependence rules. We demonstrate our algorithm's effectiveness by testing it on census data, text data (wherein we seek term dependence), and synthetic data.</content></document><document><year>2004</year><authors>Tian Zhang1 | Raghu Ramakrishnan1  | Miron Livny1 </authors><title>BIRCH: A New Data Clustering Algorithm and Its Applications</title><content>Data clustering is an important technique for exploratory data analysis, and has been studied for several years. It has been shown to be useful in many practical domains such as data classification and image processing. Recently, there has been a growing emphasis on exploratory analysis of very large datasets to discover useful patterns and/or correlations among attributes. This is called data mining, and data clustering is regarded as a particular branch. However existing data clustering methods do not adequately address the problem of processing large datasets with a limited amount of resources (e.g., memory and cpu cycles). So as the dataset size increases, they do not scale up well in terms of memory requirement, running time, and result quality.In this paper, an efficient and scalable data clustering method is proposed, based on a new in-memory data structure called CF-tree, which serves as an in-memory summary of the data distribution. We have implemented it in a system called BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and studied its performance extensively in terms of memory requirements, running time, clustering quality, stability and scalability; we also compare it with other available methods. Finally, BIRCH is applied to solve two real-life problems: one is building an iterative and interactive pixel classification tool, and the other is generating the initial codebook for image compression.</content></document><document><year>2004</year><authors>J. Sunil Rao1 </authors><title>Bootstrapping to Assess and Improve Atmospheric Prediction Models      </title><content>Bootstrapping is a simple technique typically used to assess accuracy of estimates of model parameters by using simple plug-in         principles and replacing sometimes unwieldy theory by computer simulation. Common uses include variance estimation and confidence         interval construction of model parameters. It also provides a way to estimate prediction accuracy of continuous and class-valued         outcomes regression models. In this paper we will overview some of these applications of the bootstrap focusing on bootstrap         estimates of prediction error, and also explore how the bootstrap can be used to improve prediction accuracy of unstable models         like tree-structured classifiers through aggregation. The improvements can typically be attributed to variance reduction in         the classical regression setting and more generally a smoothing of decision boundaries for the classification setting. These         advancements have important implications in the way that atmospheric prediction models can be improved, and illustrations         of this will be shown. For class-valued outcomes, an interesting graphic known as the CAT scan can be constructed to help         understand the aggregated decision boundary. This will be illustrated using simulated data.      </content></document><document><year>2004</year><authors>Kanti Bansal1 | Sanjeev Vadhavkar2| 3  | Amar Gupta4 </authors><title>Brief Application Description. Neural Networks Based Forecasting Techniques for Inventory Control Applications</title><content>An increasing number of organizations are involved in the development of information systems for effective linkages with their suppliers, customers, and other channel partners involved in transportation, distribution, warehousing and maintenance activities. We use neural network based data mining and knowledge discovery techniques to solve the problems of inventory in a large medical distribution company. The paper describes the use of traditional statistical techniques to evaluate the best neural network type. Based on the neural network model described in this paper, a prototype was conceived with data from a large decentralized organization. The prototype was successful in reducing the total level of inventory by 50% in the organization, while maintaining the same level of probability that a particular customer's demand will be satisfied.</content></document><document><year>2004</year><authors>Kenneth C. Cox1 | Stephen G. Eick1 | Graham J. Wills1  | Ronald J. Brachman2</authors><title>Brief Application Description; Visual Data Mining: Recognizing Telephone Calling Fraud</title><content>Human pattern recognition skills are remarkable and in many situations far exceed the ability of automated mining algorithms. By building domain-specific interfaces that present information visually, we can combine human detection with machines' far greater computational capacity. We illustrate our ideas by describing a suite of visual interfaces we built for telephone fraud detection.</content></document><document><year>2004</year><authors>Tom Brijs1 | Gilbert Swinnen1 | Koen Vanhoof1  | Geert Wets1 </authors><title>Building an Association Rules Framework to Improve Product Assortment Decisions</title><content>It has been claimed that the discovery of association rules is well suited for applications of market basket analysis to reveal regularities in the purchase behaviour of customers. However today, one disadvantage of associations discovery is that there is no provision for taking into account the business value of an association. Therefore, recent work indicates that the discovery of interesting rules can in fact best be addressed within a microeconomic framework. This study integrates the discovery of frequent itemsets with a (microeconomic) model for product selection (PROFSET). The model enables the integration of both quantitative and qualitative (domain knowledge) criteria. Sales transaction data from a fully automated convenience store are used to demonstrate the effectiveness of the model against a heuristic for product selection based on product-specific profitability. We show that with the use of frequent itemsets we are able to identify the cross-sales potential of product items and use this information for better product selection. Furthermore, we demonstrate that the impact of product assortment decisions on overall assortment profitability can easily be evaluated by means of sensitivity analysis.</content></document><document><year>2004</year><authors>Qiang Yang1 | Tianyi Li1  | Ke Wang1 </authors><title>Building Association-Rule Based Sequential Classifiers for Web-Document Prediction</title><content>Web servers keep track of web users' browsing behavior in web logs. From these logs, one can build statistical models that predict the users' next requests based on their current behavior. These data are complex due to their large size and sequential nature. In the past, researchers have proposed different methods for building association-rule based prediction models using the web logs, but there has been no systematic study on the relative merits of these methods. In this paper, we provide a comparative study on different kinds of sequential association rules for web document prediction. We show that the existing approaches can be cast under two important dimensions, namely the type of antecedents of rules and the criterion for selecting prediction rules. From this comparison we propose a best overall method and empirically test the proposed model on real web logs.</content></document><document><year>2004</year><authors>Minos Garofalakis1 | Dongjoon Hyun2 | Rajeev Rastogi1  | Kyuseok Shim3 </authors><title>Building Decision Trees with Constraints</title><content>Classification is an important problem in data mining. Given a database of records, each with a class label, a classifier generates a concise and meaningful description for each class that can be used to classify subsequent records. A number of popular classifiers construct decision trees to generate class models. Frequently, however, the constructed trees are complex with hundreds of nodes and thus difficult to comprehend, a fact that calls into question an often-cited benefit that decision trees are easy to interpret. In this paper, we address the problem of constructing simple decision trees with few nodes that are easy for humans to interpret. By permitting users to specify constraints on tree size or accuracy, and then building the best tree that satisfies the constraints, we ensure that the final tree is both easy to understand and has good accuracy. We develop novel branch-and-bound algorithms for pushing the constraints into the building phase of classifiers, and pruning early tree nodes that cannot possibly satisfy the constraints. Our experimental results with real-life and synthetic data sets demonstrate that significant performance speedups and reductions in the number of nodes expanded can be achieved as a result of incorporating knowledge of the constraints into the building step as opposed to applying the constraints after the entire tree is built.</content></document><document><year>2004</year><authors>Jon Kleinberg1 </authors><title>Bursty and Hierarchical Structure in Streams</title><content>A fundamental problem in text data mining is to extract meaningful structure from document streams that arrive continuously over time. E-mail and news articles are two natural examples of such streams, each characterized by topics that appear, grow in intensity for a period of time, and then fade away. The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale. Underlying much of the text mining work in this area is the following intuitive premise&amp;#x2014;that the appearance of a topic in a document stream is signaled by a burst of activity, with certain features rising sharply in frequency as the topic emerges.The goal of the present work is to develop a formal approach for modeling such bursts, in such a way that they can be robustly and efficiently identified, and can provide an organizational framework for analyzing the underlying content. The approach is based on modeling the stream using an infinite-state automaton, in which bursts appear naturally as state transitions; it can be viewed as drawing an analogy with models from queueing theory for bursty network traffic. The resulting algorithms are highly efficient, and yield a nested representation of the set of bursts that imposes a hierarchical structure on the overall stream. Experiments with e-mail and research paper archives suggest that the resulting structures have a natural meaning in terms of the content that gave rise to them.</content></document><document><year>2004</year><authors>Brij Mas|1 | Piew Datta1 | D.R. Mani1  | Bin Li1 </authors><title>CHAMP: A Prototype for Automated Cellular Churn Prediction</title><content>We describe CHAMP (CHurn Analysis, Modeling, and Prediction), an automated system for modeling cellular customer behavior on a large scale. Using historical data from GTE's data warehouse for cellular phone customers, every month CHAMP identifies churn factors for several geographic regions and updates models to generate churn scores predicting who is likely to churn within the near future. CHAMP is capable of developing customized monthly models and churn scores for over one hundred GTE cellular phone markets totaling over 5 million customers.</content></document><document><year>2004</year><authors>Jeremy Kepner1  | Rita Kim1 </authors><title>Cluster Detection in Databases: The Adaptive Matched Filter Algorithm and Implementation</title><content>Matched filter techniques are a staple of modern signal and image processing. They provide a firm foundation (both theoretical and empirical) for detecting and classifying patterns in statistically described backgrounds. Application of these methods to databases has become increasingly common in certain fields (e.g. astronomy). This paper describes an algorithm (based on statistical signal processing methods), a software architecture (based on a hybrid layered approach) and a parallelization scheme (based on a client/server model) for finding clusters in large astronomical databases. The method has proved successful in identifying clusters in real and simulated data. The implementation is flexible and readily executed in parallel on a network of workstations.</content></document><document><year>2004</year><authors>Assaf Schuster1  | Ran Wolff1 </authors><title>Communication-Efficient Distributed Mining of Association Rules</title><content>Mining for associations between items in large transactional databases is a central problem in the field of knowledge discovery. When the database is partitioned among several share-nothing machines, the problem can be addressed using distributed data mining algorithms. One such algorithm, called CD, was proposed by Agrawal and Shafer and was later enhanced by the FDM algorithm of Cheung, Han et al. The main problem with these algorithms is that they do not scale well with the number of partitions. They are thus impractical for use in modern distributed environments such as peer-to-peer systems, in which hundreds or thousands of computers may interact.In this paper we present a set of new algorithms that solve the Distributed Association Rule Mining problem using far less communication. In addition to being very efficient, the new algorithms are also extremely robust. Unlike existing algorithms, they continue to be efficient even when the data is skewed or the partition sizes are imbalanced. We present both experimental and theoretical results concerning the behavior of these algorithms and explain how they can be implemented in different settings.</content></document><document><year>2004</year><authors>Roberto J. Bayardo Jr1 | Rakesh Agrawal2  | Dimitrios Gunopulos3 </authors><title>Constraint-Based Rule Mining in Large, Dense Databases      </title><content>Constraint-based rule miners find all rules in a given data-set meeting user-specified constraints such as minimum support         and confidence. We describe a new algorithm that directly exploits all user-specified constraints including minimum support,         minimum confidence, and a new constraint that ensures every mined rule offers a predictive advantage over any of its simplifications.         Our algorithm maintains efficiency even at low supports on data that is dense (e.g. relational tables). Previous approaches         such as Apriori and its variants exploit only the minimum support constraint, and as a result are ineffective on dense data         due to a combinatorial explosion of &amp;#8220;frequent itemsets&amp;#8221;.      </content></document><document><year>2004</year><authors>Tomasz Imieliski1| Leonid Khachiyan1 | Amin Abdulghani1</authors><title>Cubegrades: Generalizing Association Rules</title><content>Cubegrades are a generalization of association rules which represent how a set of measures (aggregates) is affected by modifying a cube through specialization (rolldown), generalization (rollup) and mutation (which is a change in one of the cube's dimensions). Cubegrades are significantly more expressive than association rules in capturing trends and patterns in data because they can use other standard aggregate measures, in addition to COUNT. Cubegrades are atoms which can support sophisticated what if analysis tasks dealing with behavior of arbitrary aggregates over different database segments. As such, cubegrades can be useful in marketing, sales analysis, and other typical data mining applications in business.In this paper we introduce the concept of cubegrades. We define them and give examples of their usage. We then describe in detail an important task for computing cubegrades: generation of significant cubes whichis analogous to generating frequent sets. A novel Grid Based Pruning (GBP) method is employed for this purpose. We experimentally demonstrate the practicality of the method. We conclude with a number of open questions and possible extensions of the work.</content></document><document><year>2004</year><authors>Saharon Rosset1 | Einat Neumann1 | Uri Eick1  | Nurit Vatnik1 </authors><title>Customer Lifetime Value Models for Decision Support</title><content>We present and discuss the important business problem of estimating the effect of marketing activities on the Lifetime Value of a customer in the Telecommunications industry. We discuss the components of this problem, in particular customer value and length of service (or tenure) modeling, and present a novel segment-based approach, motivated by the segment-level view marketing analysts usually employ. We describe in detail how we build on this approach to estimate the effects of retention campaigns on Lifetime Value, and also discuss its application in other situations. Our solution has been successfully implemented by the Business Insight (BI) Professional Services.</content></document><document><year>2004</year><authors>Jim Gray1| 1 | Surajit Chaudhuri1| 1 | Adam Bosworth1| 1 | Andrew Layman1| 1 | Don Reichart1| 1 | Murali Venkatrao1| 1 | Frank Pellow2| 2  | Hamid Pirahesh2| 2 </authors><title>Data Cube: A Relational Aggregation Operator Generalizing Group-By, Cross-Tab, and Sub-Totals</title><content>Data analysis applications typically aggregate data across manydimensions looking for anomalies or unusual patterns. The SQL aggregatefunctions and the GROUP BY operator produce zero-dimensional orone-dimensional aggregates. Applications need the N-dimensionalgeneralization of these operators. This paper defines that operator, calledthe data cube or simply cube. The cube operator generalizes the histogram,cross-tabulation, roll-up,drill-down, and sub-total constructs found in most report writers.The novelty is that cubes are relations. Consequently, the cubeoperator can be imbedded in more complex non-procedural dataanalysis programs. The cube operator treats each of the Naggregation attributes as a dimension of N-space. The aggregate ofa particular set of attribute values is a point in this space. Theset of points forms an N-dimensional cube. Super-aggregates arecomputed by aggregating the N-cube to lower dimensional spaces.This paper (1) explains the cube and roll-up operators, (2) showshow they fit in SQL, (3) explains how users can define new aggregatefunctions for cubes, and (4) discusses efficient techniques tocompute the cube. Many of these features are being added to the SQLStandard.</content></document><document><year>2004</year><authors>Myra Spiliopoulou1  | Carsten Pohle2 </authors><title>Data Mining for Measuring and Improving the Success of Web Sites</title><content>For many companies, competitiveness in e-commerce requires a successful presence on the web. Web sites are used to establish the company's image, to promote and sell goods and to provide customer support. The success of a web site affects and reflects directly the success of the company in the electronic market. In this study, we propose a methodology to improve the success of web sites, based on the exploitation of navigation pattern discovery. In particular, we present a theory, in which success is modelled on the basis of the navigation behaviour of the site's users. We then exploit WUM, a navigation pattern discovery miner, to study how the success of a site is reflected in the users' behaviour. With WUM we measure the success of a site's components and obtain concrete indications of how the site should be improved. We report on our first experiments with an online catalog, the success of which we have studied. Our mining analysis has shown very promising results, on the basis of which the site is currently undergoing concrete improvements.</content></document><document><year>2004</year><authors>Art Owen1 </authors><title>Data Squashing by Empirical Likelihood</title><content>Data squashing was introduced by W. DuMouchel, C. Volinsky, T. Johnson, C. Cortes, and D. Pregibon, in Proceedings of the 5th International Conference on KDD (1999). The idea is to scale data sets down to smaller representative samples instead of scaling up algorithms to very large data sets. They report success in learning model coefficients on squashed data. This paper presents a form of data squashing based on empirical likelihood. This method reweights a random sample of data to match certain expected values to the population. The computation required is a relatively easy convex optimization. There is also a theoretical basis to predict when it will and won't produce large gains. In a credit scoring example, empirical likelihood weighting also accelerates the rate at which coefficients are learned. We also investigate the extent to which these benefits translate into improved accuracy, and consider reweighting in conjunction with boosted decision trees.</content></document><document><year>2004</year><authors>J&amp;ouml rg S|er1 | Martin Ester1 | Hans-Peter Kriegel1  | Xiaowei Xu1 </authors><title>Density-Based Clustering in Spatial Databases: The Algorithm GDBSCAN and Its Applications</title><content>The clustering algorithm DBSCAN relies on a density-based notion of clusters and is designed to discover clusters of arbitrary shape as well as to distinguish noise. In this paper, we generalize this algorithm in two important directions. The generalized algorithm&amp;#x2014;called GDBSCAN&amp;#x2014;can cluster point objects as well as spatially extended objects according to both, their spatial and their nonspatial attributes. In addition, four applications using 2D points (astronomy), 3D points (biology), 5D points (earth science) and 2D polygons (geography) are presented, demonstrating the applicability of GDBSCAN to real-world problems.</content></document><document><year>2004</year><authors>Stephen D. Bay1  | Michael J. Pazzani2 </authors><title>Detecting Group Differences: Mining Contrast Sets</title><content>A fundamental task in data analysis is understanding the differences between several contrasting groups. These groups can represent different classes of objects, such as male or female students, or the same group over time, e.g. freshman students in 1993 through 1998. We present the problem of mining contrast sets: conjunctions of attributes and values that differ meaningfully in their distribution across groups. We provide a search algorithm for mining contrast sets with pruning rules that drastically reduce the computational complexity. Once the contrast sets are found, we post-process the results to present a subset that are surprising to the user given what we have already shown. We explicitly control the probability of Type I error (false positives) and guarantee a maximum error rate for the entire analysis by using Bonferroni corrections.</content></document><document><year>2004</year><authors>Machiel Westerdijk1 | David Barber2  | Wim Wiegerinck3 </authors><title>Deterministic Generative Models for Fast Feature Discovery</title><content>We propose a vector quantisation method which does not only provide a compact description of data vectors in terms codebook vectors, but also gives an explanation of codebook vectors as binary combinations of elementary features. This corresponds to the intuitive notion that, in the real world, patterns can be usefully thought of as being constructed by compositions from simpler features. The model can be understood as a generative model, in which the codebook vector is generated by a hidden binary state vector. The model is non-probabilistic in the sense that it assigns each data vector to a single codebook vector. We describe exact and approximate learning algorithms for learning deterministic feature representations. In contrast to probabilistic models, the deterministic approach allows the use of message propagation algorithms within the learning scheme. These are compared with standard mean-field/Gibbs sampling learning. We show that Generative Vector Quantisation gives a good performance in large scale real world tasks like image compression and handwritten digit analysis with up to 400 data dimensions.</content></document><document><year>2004</year><authors>Vasant Dhar1 | Dashin Chou2 | Foster Provost2</authors><title>Discovering Interesting Patterns for Investment Decision Making with GLOWER &amp;#9785;&amp;#8212;A Genetic Learner Overlaid with Entropy Reduction      </title><content>Prediction in financial domains is notoriously difficult for a number of reasons. First, theories tend to be weak or non-existent,         which makes problem formulation open ended by forcing us to consider a large number of independent variables and thereby increasing         the dimensionality of the search space. Second, the weak relationships among variables tend to be nonlinear, and may hold         only in limited areas of the search space. Third, in financial practice, where analysts conduct extensive manual analysis         of historically well performing indicators, a key is to find the hidden interactions among variables that perform well in         combination. Unfortunately, these are exactly the patterns that the greedy search biases incorporated by many standard rule         learning algorithms will miss. In this paper, we describe and evaluate several variations of a new genetic learning algorithm         (GLOWER) on a variety of data sets. The design of GLOWER has been motivated by financial prediction problems, but incorporates         successful ideas from tree induction and rule learning. We examine the performance of several GLOWER variants on two UCI data         sets as well as on a standard financial prediction problem (S&amp;amp;P500 stock returns), using the results to identify one of the         better variants for further comparisons. We introduce a new (to KDD) financial prediction problem (predicting positive and         negative earnings surprises), and experiment with GLOWER, contrasting it with tree- and rule-induction approaches. Our results         are encouraging, showing that GLOWER has the ability to uncover effective patterns for difficult problems that have weak structure         and significant nonlinearities.      </content></document><document><year>2004</year><authors>Chun-Nan Hsu1  | Craig A. Knoblock2 </authors><title>Discovering Robust Knowledge from Databases that Change</title><content>Many applications of knowledge discovery and data mining such as rule discovery for semantic query optimization, database integration and decision support, require the knowledge to be consistent with the data. However, databases usually change over time and make machine-discovered knowledge inconsistent. Useful knowledge should be robust against database changes so that it is unlikely to become inconsistent after database updates. This paper defines this notion of robustness in the context of relational databases and describes how robustness of first-order Horn-clause rules can be estimated. Experimental results show that our estimation approach can accurately identify robust rules. We also present a rule antecedent pruning algorithm that improves the robustness and applicability of machine discovered rules to demonstrate the usefulness of robustness estimation.</content></document><document><year>2004</year><authors>Bamshad Mobasher1| Honghua Dai1| Tao Luo1 | Miki Nakagawa1</authors><title>Discovery and Evaluation of Aggregate Usage Profiles for Web Personalization</title><content>Web usage mining, possibly used in conjunction with standard approaches to personalization such as collaborative filtering, can help address some of the shortcomings of these techniques, including reliance on subjective user ratings, lack of scalability, and poor performance in the face of high-dimensional and sparse data. However, the discovery of patterns from usage data by itself is not sufficient for performing the personalization tasks. The critical step is the effective derivation of good quality and useful (i.e., actionable) aggregate usage profiles from these patterns. In this paper we present and experimentally evaluate two techniques, based on clustering of user transactions and clustering of pageviews, in order to discover overlapping aggregate profiles that can be effectively used by recommender systems for real-time Web personalization. We evaluate these techniques both in terms of the quality of the individual profiles generated, as well as in the context of providing recommendations as an integrated part of a personalization engine. In particular, our results indicate that using the generated aggregate profiles, we can achieve effective personalization at early stages of users' visits to a site, based only on anonymous clickstream data and without the benefit of explicit input by these users or deeper knowledge about them.</content></document><document><year>2004</year><authors>Luc Dehaspe1  | Hannu Toivonen2 </authors><title>Discovery of frequent DATALOG patterns</title><content>Discovery of frequent patterns has been studied in a variety of data mining settings. In its simplest form, known from association rule mining, the task is to discover all frequent itemsets, i.e., all combinations of items that are found in a sufficient number of examples. The fundamental task of association rule and frequent set discovery has been extended in various directions, allowing more useful patterns to be discovered with special purpose algorithms. We present WARMR, a general purpose inductive logic programming algorithm that addresses frequent query discovery: a very general DATALOG formulation of the frequent pattern discovery problem.The motivation for this novel approach is twofold. First, exploratory data mining is well supported: WARMR offers the flexibility required to experiment with standard and in particular novel settings not supported by special purpose algorithms. Also, application prototypes based on WARMR can be used as benchmarks in the comparison and evaluation of new special purpose algorithms. Second, the unified representation gives insight to the blurred picture of the frequent pattern discovery domain. Within the DATALOG formulation a number of dimensions appear that relink diverged settings.</content></document><document><year>2004</year><authors>Heikki Mannila1 | Hannu Toivonen1  | A. Inkeri Verkamo1 </authors><title>Discovery of Frequent Episodes in Event Sequences</title><content>Sequences of events describing the behavior and actions of users or systems can be collected in several domains. An episode is a collection of events that occur relatively close to each other in a given partial order. We consider the problem of discovering frequently occurring episodes in a sequence. Once such episodes are known, one can produce rules for describing or predicting the behavior of the sequence. We give efficient algorithms for the discovery of all frequent episodes from a given class of episodes, and present detailed experimental results. The methods are in use in telecommunication alarm management.</content></document><document><year>2004</year><authors>Pang-Ning Tan1 | Vipin Kumar1</authors><title>Discovery of Web Robot Sessions Based on their Navigational Patterns</title><content>Web robots are software programs that automatically traverse the hyperlink structure of the World Wide Web in order to locate and retrieve information. There are many reasons why it is important to identify visits by Web robots and distinguish them from other users. First of all, e-commerce retailers are particularly concerned about the unauthorized deployment of robots for gathering business intelligence at their Web sites. In addition, Web robots tend to consume considerable network bandwidth at the expense of other users. Sessions due to Web robots also make it more difficult to perform clickstream analysis effectively on the Web data. Conventional techniques for detecting Web robots are often based on identifying the IP address and user agent of the Web clients. While these techniques are applicable to many well-known robots, they may not be sufficient to detect camouflaged and previously unknown robots. In this paper, we propose an alternative approach that uses the navigational patterns in the click-stream data to determine if it is due to a robot. Experimental results on our Computer Science department Web server logs show that highly accurate classification models can be built using this approach. We also show that these models are able to discover many camouflaged and previously unidentified robots.</content></document><document><year>2004</year><authors>Huan Liu1| Farhad Hussain1| Chew Lim Tan1 | Manoranjan Dash1</authors><title>Discretization: An Enabling Technique</title><content>Discrete values have important roles in data mining and knowledge discovery. They are about intervals of numbers which are more concise to represent and specify, easier to use and comprehend as they are closer to a knowledge-level representation than continuous values. Many studies show induction tasks can benefit from discretization: rules with discrete values are normally shorter and more understandable and discretization can lead to improved predictive accuracy. Furthermore, many induction algorithms found in the literature require discrete features. All these prompt researchers and practitioners to discretize continuous features before or during a machine learning or data mining task. There are numerous discretization methods available in the literature. It is time for us to examine these seemingly different methods for discretization and find out how different they really are, what are the key components of a discretization process, how we can improve the current level of research for new development as well as the use of existing methods. This paper aims at a systematic study of discretization methods with their history of development, effect on classification, and trade-off between speed and accuracy. Contributions of this paper are an abstract description summarizing existing discretization methods, a hierarchical framework to categorize the existing methods and pave the way for further development, concise discussions of representative discretization methods, extensive experiments and their analysis, and some guidelines as to how to choose a discretization method under various circumstances. We also identify some issues yet to solve and future research for discretization.</content></document><document><year>2004</year><authors>Tomasz Imieliski1 | Aashu Virmani1  | Amin Abdulghani1 </authors><title>DMajor&amp;#x2014;Application Programming Interface for Database Mining</title><content>In the process of rule generation from databases, the volume of generated rules often greatly exceeds the size of the underlying database. Typically only a small fraction of that large volume of rules is of any interest to the user. We believe that the main challenge facing database mining is what to do with the rules after having generated them. Rule post-processing involves selecting rules which are relevant or interesting, building applications which use the rules and finally, combining rules together to form a larger and more meaningful statements. In this paper we propose an application programming interface which enables faster development of applications which rely on rules. We also provide a rule query language which allows both selective rule generation as well as retrieval of selected categories of rules from the pre-generated rule collections.</content></document><document><year>2004</year><authors>Cristian Bucil1 | Johannes Gehrke1 | Daniel Kifer1  | Walker White2 </authors><title>DualMiner: A Dual-Pruning Algorithm for Itemsets with Constraints</title><content>Recently, constraint-based mining of itemsets for questions like find all frequent itemsets whose total price is at least $50 has attracted much attention. Two classes of constraints, monotone and antimonotone, have been very useful in this area. There exist algorithms that efficiently take advantage of either one of these two classes, but no previous algorithms can efficiently handle both types of constraints simultaneously. In this paper, we present DualMiner, the first algorithm that efficiently prunes its search space using both monotone and antimonotone constraints. We complement a theoretical analysis and proof of correctness of DualMiner with an experimental study that shows the efficacy of DualMiner compared to previous work.</content></document><document><year>2004</year><authors>J. Ben Schafer1 | Joseph A. Konstan2  | John Riedl3 </authors><title>E-Commerce Recommendation Applications</title><content>Recommender systems are being used by an ever-increasing number of E-commerce sites to help consumers find products to purchase. What started as a novelty has turned into a serious business tool. Recommender systems use product knowledge&amp;#x2014;either hand-coded knowledge provided by experts or mined knowledge learned from the behavior of consumers&amp;#x2014;to guide consumers through the often-overwhelming task of locating products they will like. In this article we present an explanation of how recommender systems are related to some traditional database analysis techniques. We examine how recommender systems help E-commerce sites increase sales and analyze the recommender systems at six market-leading sites. Based on these examples, we create a taxonomy of recommender systems, including the inputs required from the consumers, the additional knowledge required from the database, the ways the recommendations are presented to consumers, the technologies used to create the recommendations, and the level of personalization of the recommendations. We identify five commonly used E-commerce recommender application models, describe several open research problems in the field of recommender systems, and examine privacy implications of recommender systems technology.</content></document><document><year>2004</year><authors>Usama M. Fayyad1 </authors><title>Editorial</title><content>Without Abstract</content></document><document><year>2004</year><authors>Usama M. Fayyad1 | Heikki Mannila2  | Raghu Ramakrishnan3 </authors><title>Editorial</title><content>Without Abstract</content></document><document><year>2004</year><authors>Yike Guo1  | Robert Grossman2 </authors><title>Editorial      </title><content>Without Abstract</content></document><document><year>2004</year><authors>Usama Fayyad1 </authors><title>Editorial</title><content>Without Abstract</content></document><document><year>2004</year><authors>Usama M. Fayyad1 | Heikki Mannila2  | Raghu Ramakrishnan3 </authors><title>Editorial</title><content>Without Abstract</content></document><document><year>2004</year><authors>Usama Fayyad1 </authors><title>Editorial</title><content>Without Abstract</content></document><document><year>2004</year><authors>Usama Fayyad1 </authors><title>Editorial</title><content>Without Abstract</content></document><document><year>2004</year><authors>Raymond Ng| Jiawei Han | Laks Lakshmanan</authors><title>Editorial      </title><content>Without Abstract</content></document><document><year>2004</year><authors>Sao Deroski1 </authors><title>Editorial</title><content>Without Abstract</content></document><document><year>2004</year><authors>Paul Stolorz1  | Ron Musick2 </authors><title>Editorial      </title><content>Without Abstract</content></document><document><year>2004</year><authors>Usama Fayyad1 </authors><title>Editorial</title><content>Without Abstract</content></document><document><year>2004</year><authors>David W. Cheung1  | Yongqiao Xiao1</authors><title>Effect of Data Distribution in Parallel Mining of Associations      </title><content>Association rule mining is an important new problem in data mining. It has crucial applications in decision support and marketing         strategy. We proposed an efficient parallel algorithm for mining association rules on a distributed share-nothing parallel         system. Its efficiency is attributed to the incorporation of two powerful candidate set pruning techniques. The two techniques,         distributed and global prunings, are sensitive to two data distribution characteristics: data skewness and workload balance.         The prunings are very effective when both the skewness and balance are high. We have implemented FPM on an IBM SP2 parallel         system. The performance studies show that FPM outperforms CD consistently, which is a parallel version of the representative         Apriori algorithm (Agrawal and Srikant, 1994). Also, the results have validated our observation on the effectiveness of the         two pruning techniques with respect to the data distribution characteristics. Furthermore, it shows that FPM has nice scalability         and parallelism, which can be tuned for different business applications.      </content></document><document><year>2004</year><authors>Weiyang Lin1| Sergio A. Alvarez2 | Carolina Ruiz3</authors><title>Efficient Adaptive-Support Association Rule Mining for Recommender Systems</title><content>Collaborative recommender systems allow personalization for e-commerce by exploiting similarities and dissimilarities among customers' preferences. We investigate the use of association rule mining as an underlying technology for collaborative recommender systems. Association rules have been used with success in other domains. However, most currently existing association rule mining algorithms were designed with market basket analysis in mind. Such algorithms are inefficient for collaborative recommendation because they mine many rules that are not relevant to a given user. Also, it is necessary to specify the minimum support of the mined rules in advance, often leading to either too many or too few rules; this negatively impacts the performance of the overall system. We describe a collaborative recommendation technique based on a new algorithm specifically designed to mine association rules for this purpose. Our algorithm does not require the minimum support to be specified in advance. Rather, a target range is given for the number of rules, and the algorithm adjusts the minimum support for each user in order to obtain a ruleset whose size is in the desired range. Rules are mined for a specific target user, reducing the time required for the mining process. We employ associations between users as well as associations between items in making recommendations. Experimental evaluation of a system based on our algorithm reveals performance that is significantly better than that of traditional correlation-based approaches.</content></document><document><year>2004</year><authors>Tapio Elomaa1  | Juho Rousu1 </authors><title>Efficient Multisplitting Revisited: Optima-Preserving Elimination of Partition Candidates</title><content>We consider multisplitting of numerical value ranges, a task that is encountered as a discretization step preceding induction and also embedded into learning algorithms. We are interested in finding the partition that optimizes the value of a given attribute evaluation function. For most commonly used evaluation functions this task takes quadratic time in the number of potential cut points in the numerical range. Hence, it is a potential bottleneck in data mining algorithms.We present two techniques that speed up the optimal multisplitting task. The first one aims at discarding cut point candidates in a quick linear-time preprocessing scan before embarking on the actual search. We generalize the definition of boundary points by Fayyad and Irani to allow us to merge adjacent example blocks that have the same relative class distribution. We prove for several commonly used evaluation functions that this processing removes only suboptimal cut points. Hence, the algorithm does not lose optimality.</content></document><document><year>2004</year><authors>Ayhan Demiriz1 </authors><title>Enhancing Product Recommender Systems on Sparse Binary Data</title><content>Commercial recommender systems use various data mining techniques to make appropriate recommendations to users during online, real-time sessions. Published algorithms focus more on the discrete user ratings instead of binary results, which hampers their predictive capabilities when usage data is sparse. The system proposed in this paper, e-VZpro, is an association mining-based recommender tool designed to overcome these problems through a two-phase approach. In the first phase, batches of customer historical data are analyzed through association mining in order to determine the association rules for the second phase. During the second phase, a scoring algorithm is used to rank the recommendations online for the customer. The second phase differs from the traditional approach and an empirical comparison between the methods used in e-VZpro and other collaborative filtering methods including dependency networks, item-based, and association mining is provided in this paper. This comparison evaluates the algorithms used in each of the above methods using two internal customer datasets and a benchmark dataset. The results of this comparison clearly show that e-VZpro performs well compared to dependency networks and association mining. In general, item-based algorithms with cosine similarity measures have the best performance.</content></document><document><year>2004</year><authors>Paul W. Mielke Jr.1  | Kenneth J. Berry2 </authors><title>Euclidean Distance Based Permutation Methods in Atmospheric Science      </title><content>The majority of existing statistical methods inherently involve complex nonmetric analysis spaces due to their least squares         regression origin; consequently, the analysis space of such statistical methods is not consistent with the simple metric Euclidean         geometry of the data space in question. The statistical methods presented in this paper are consistent with the data spaces         in question. These alternative methods depend on exact and approximate permutation procedures for univariate and multivariate         data involving cyclic phenomena, autoregressive patterns, covariate residual analyses including most linear model based experimental         designs, and linear and nonlinear prediction model evaluations. Specific atmospheric science applications include climate         change, Atlantic basin seasonal tropical cyclone predictions, analyses of weather modification experiments, and numerical         model evaluations for phenomena such as cumulus clouds, clear-sky surface energy budgets, and mesoscale atmospheric predictions.      </content></document><document><year>2004</year><authors>Gediminas Adomavicius1  | Alex|er Tuzhilin2 </authors><title>Expert-Driven Validation of Rule-Based User Models in Personalization Applications</title><content>In many e-commerce applications, ranging from dynamic Web content presentation, to personalized ad targeting, to individual recommendations to the customers, it is important to build personalized profiles of individual users from their transactional histories. These profiles constitute models of individual user behavior and can be specified with sets of rules learned from user transactional histories using various data mining techniques. Since many discovered rules can be spurious, irrelevant, or trivial, one of the main problems is how to perform post-analysis of the discovered rules, i.e., how to validate user profiles by separating good rules from the bad. This validation process should be done with an explicit participation of the human expert. However, complications may arise because there can be very large numbers of rules discovered in the applications that deal with many users, and the expert cannot perform the validation on a rule-by-rule basis in a reasonable period of time. This paper presents a framework for building behavioral profiles of individual users. It also introduces a new approach to expert-driven validation of a very large number of rules pertaining to these users. In particular, it presents several types of validation operators, including rule grouping, filtering, browsing, and redundant rule elimination operators, that allow a human expert validate many individual rules at a time. By iteratively applying such operators, the human expert can validate a significant part of all the initially discovered rules in an acceptable time period. These validation operators were implemented as a part of a one-to-one profiling system. The paper also presents a case study of using this system for validating individual user rules discovered in a marketing application.</content></document><document><year>2004</year><authors>Eddie C. Shek1 | Richard R. Muntz2  | Edmond Mesrobian3 </authors><title>Extensible Parallel Query Processing for Exploratory Geoscientific Data Mining</title><content>Exploratory data mining and analysis requires a computing environment which provides facilities for the user-friendly expression and rapid execution of scientific queries. In this paper, we address research issues in the parallelization of scientific queries containing complex user-defined operations. In a parallel query execution environment, parallelizing a query execution plan involves determining how input data streams to evaluators implementing logical operations can be divided to be processed by clones of the same evaluator in parallel. We introduced the concept of relevance window that characterizes data lineage and data partitioning opportunities available for an user-defined evaluator. In addition, we developed a query parallelization framework by extending relational parallel query optimization algorithms to allow the parallelization characteristics of user-defined evaluators to guide the process of query parallelization in an extensible query processing environment. We demonstrated the utility of our system by performing experiments mining cyclonic activity, blocking events, and the upward wave-energy propagation features from several observational and model simulation datasets.</content></document><document><year>2004</year><authors>Zhexue Huang1 </authors><title>Extensions to the k-Means Algorithm for Clustering Large Data Sets with Categorical Values      </title><content>The k-means algorithm is well known for its efficiency in clustering large data sets. However, working only on numeric values         prohibits it from being used to cluster real world data containing categorical values. In this paper we present two algorithms         which extend the k-means algorithm to categorical domains and domains with mixed numeric and categorical values. The k-modes         algorithm uses a simple matching dissimilarity measure to deal with categorical objects, replaces the means of clusters with         modes, and uses a frequency-based method to update modes in the clustering process to minimise the clustering cost function.         With these extensions the k-modes algorithm enables the clustering of categorical data in a fashion similar to k-means. The         k-prototypes algorithm, through the definition of a combined dissimilarity measure, further integrates the k-means and k-modes         algorithms to allow for clustering objects described by mixed numeric and categorical attributes. We use the well known soybean         disease and credit approval data sets to demonstrate the clustering performance of the two algorithms. Our experiments on         two real world data sets with half a million objects each show that the two algorithms are efficient when clustering large         data sets, which is critical to data mining applications.      </content></document><document><year>2004</year><authors>Brock Barber1 | Howard J. Hamilton1 </authors><title>Extracting Share Frequent Itemsets with Infrequent Subsets</title><content>Itemset share has been proposed as an additional measure of the importance of itemsets in association rule mining (Carter et al., 1997). We compare the share and support measures to illustrate that the share measure can provide useful information about numerical values that are typically associated with transaction items, which the support measure cannot. We define the problem of finding share frequent itemsets, and show that share frequency does not have the property of downward closure when it is defined in terms of the itemset as a whole. We present algorithms that do not rely on the property of downward closure, and thus are able to find share frequent itemsets that have infrequent subsets. The algorithms use heuristic methods to generate candidate itemsets. They supplement the information contained in the set of frequent itemsets from a previous pass, with other information that is available at no additional processing cost. They count only those generated itemsets that are predicted to be frequent. The algorithms are applied to a large commercial database and their effectiveness is examined using principles of classifier evaluation from machine learning.</content></document><document><year>2004</year><authors>V. Estivill-Castro1  | J. Yang2</authors><title>Fast and Robust General Purpose Clustering Algorithms</title><content>General purpose and highly applicable clustering methods are usually required during the early stages of knowledge discovery exercises. k-MEANS has been adopted as the prototype of iterative model-based clustering because of its speed, simplicity and capability to work within the format of very large databases. However, k-MEANS has several disadvantages derived from its statistical simplicity. We propose an algorithm that remains very efficient, generally applicable, multidimensional but is more robust to noise and outliers. We achieve this by using medians rather than means as estimators for the centers of clusters. Comparison with k-MEANS, EXPECTATION and MAXIMIZATION sampling demonstrates the advantages of our algorithm.</content></document><document><year>2004</year><authors>Ashwin Srinivasan1  | Ross D. King2 </authors><title>Feature construction with Inductive Logic Programming: A Study of Quantitative Predictions of Biological Activity Aided by Structural Attributes</title><content>Recently, computer programs developed within the field of Inductive Logic Programming (ILP) have received some attention for their ability to construct restricted first-order logic solutions using problem-specific background knowledge. Prominent applications of such programs have been concerned with determining structure-activity relationships in the areas of molecular biology and chemistry. Typically the task here is to predict the activity of a compound (for example, toxicity), from its chemical structure. A summary of the research in the area is: (a) ILP programs have largely been restricted to qualitative predictions of activity (high, low etc.); (b) When appropriate attributes are available, ILP programs have equivalent predictivity to standard quantitative analysis techniques like linear regression. However ILP programs usually perform better when such attributes are unavailable; and (c) By using structural information as background knowledge, an ILP program can provide comprehensible explanations for biological activity. This paper examines the use of ILP programs as a method of discovering new attributes. These attributes could then be used by methods like linear regression, thus allowing for quantitative predictions while retaining the ability to use structural information as background knowledge. Using structure-activity tasks as a test-bed, the utility of ILP programs in constructing new features was evaluated by examining the prediction of biological activity using linear regression, with and without the aid of ILP learnt logical attributes. In three out of the five data sets examined the addition of ILP attributes produced statistically better results. In addition six important structural features that have escaped the attention of the expert chemists were discovered. The method used here to construct new attributes is not specific to the problem of predicting biological activity, and the results obtained suggest a wider role for ILP programs in aiding the process of scientific discovery.</content></document><document><year>2004</year><authors>Jean-Fran&amp;ccedil ois Boulicaut1 | Artur Bykowski1  | Christophe Rigotti1 </authors><title>Free-Sets: A Condensed Representation of Boolean Data for the Approximation of Frequency Queries</title><content>Given a large collection of transactions containing items, a basic common data mining problem is to extract the so-called frequent itemsets (i.e., sets of items appearing in at least a given number of transactions). In this paper, we propose a structure called free-sets, from which we can approximate any itemset support (i.e., the number of transactions containing the itemset) and we formalize this notion in the framework of -adequate representations (H. Mannila and H. Toivonen, 1996. In Proc. of the Second International Conference on Knowledge Discovery and Data Mining (KDD'96), pp. 189&amp;#x2013;194). We show that frequent free-sets can be efficiently extracted using pruning strategies developed for frequent itemset discovery, and that they can be used to approximate the support of any frequent itemset. Experiments on real dense data sets show a significant reduction of the size of the output when compared with standard frequent itemset extraction. Furthermore, the experiments show that the extraction of frequent free-sets is still possible when the extraction of frequent itemsets becomes intractable, and that the supports of the frequent free-sets can be used to approximate very closely the supports of the frequent itemsets. Finally, we consider the effect of this approximation on association rules (a popular kind of patterns that can be derived from frequent itemsets) and show that the corresponding errors remain very low in practice.</content></document><document><year>2004</year><authors>David J. H|| Daniel A. Keim | Raymond Ng</authors><title>Guest Editorial</title><content>Without Abstract</content></document><document><year>2004</year><authors>Surajit Chaudhuri </authors><title>Guest Editorial      </title><content>Without Abstract</content></document><document><year>2004</year><authors>David J. H|| Daniel A. Keim | Raymond Ng</authors><title>Guest Editorial</title><content>Without Abstract</content></document><document><year>2004</year><authors>Timothy J. Brown1  | Paul W. Mielke Jr.2 </authors><title>Guest Editorial: Statistical Mining and Data Visualization in Atmospheric Sciences      </title><content>Without Abstract</content></document><document><year>2004</year><authors>David W. Pfitzner1 | John K. Salmon2  | Thomas Sterling2 </authors><title>Halo World: Tools for Parallel Cluster Finding in Astrophysical N-body Simulations      </title><content>Cosmological N-body simulations on parallel computers produce large datasets--gigabytes at each instant of simulated cosmological         time, and hundreds of gigabytes over the course of a simulation. These large datasets require further analysis before they         can be compared to astronomical observations. The &amp;#8220;Halo World&amp;#8221; tools include two methods for performing halo finding: identifying         all of the gravitationally stable clusters in a point-sampled density field. One of these methods is a parallel implementation         of the friends of friends (FOF) algorithm, widely used in the field of N-body cosmology. The new IsoDen method based on isodensity         surfaces has been developed to overcome some of the shortcomings of FOF. Parallel processing is the only viable way of obtaining         the necessary performance and storage capacity to carry out these analysis tasks. Ultimately, we must also plan to use disk         storage as the only economically viable alternative for storing and manipulating such large data sets. Both IsoDen and friends         of friends have been implemented on a variety of computer systems, with parallelism up to 512 processors, and successfully         used to extract halos from simulations with up to 16.8 million particles.      </content></document><document><year>2004</year><authors>Sanjay Goil1  | Alok Choudhary1 </authors><title>High Performance OLAP and Data Mining on Parallel Computers      </title><content>On-Line Analytical Processing (OLAP) techniques are increasingly being used in decision support systems to provide analysis         of data. Queries posed on such systems are quite complex and require different views of data. Analytical models need to capture         the multidimensionality of the underlying data, a task for which multidimensional databases are well suited. Multidimensional         OLAP systems store data in multidimensional arrays on which analytical operations are performed. Knowledge discovery and data         mining requires complex operations on the underlying data which can be very expensive in terms of computation time. High performance         parallel systems can reduce this analysis time.                     Precomputed aggregate calculations in a Data Cube can provide efficient query processing for OLAP applications. In this article,               we present algorithms for construction of data cubes on distributed-memory parallel computers. Data is loaded from a relational               database into a multidimensional array. We present two methods, sort-based and hash-based for loading the base cube and compare               their performances. Data cubes are used to perform consolidation queries used in roll-up operations using dimension hierarchies.               Finally, we show how data cubes are used for data mining using Attribute Focusing techniques. We present results for these               on the IBM-SP2 parallel machine. Results show that our algorithms and techniques for OLAP and data mining on parallel systems               are scalable to a large number of processors, providing a high performance platform for such applications.            </content></document><document><year>2004</year><authors>William H. Hsu1| 2| Michael Welge2| Tom Redman2 | David Clutter2</authors><title>High-Performance Commercial Data Mining: A Multistrategy Machine Learning Application</title><content>We present an application of inductive concept learning and interactive visualization techniques to a large-scale commercial data mining project. This paper focuses on design and configuration of high-level optimization systems (wrappers) for relevance determination and constructive induction, and on integrating these wrappers with elicited knowledge on attribute relevance and synthesis. In particular, we discuss decision support issues for the application (cost prediction for automobile insurance markets in several states) and report experiments using D2K, a Java-based visual programming system for data mining and information visualization, and several commercial and research tools. We describe exploratory clustering, descriptive statistics, and supervised decision tree learning in this application, focusing on a parallel genetic algorithm (GA) system, Jenesis, which is used to implement relevance determination (attribute subset selection). Deployed on several high-performance network-of-workstation systems (Beowulf clusters), Jenesis achieves a linear speedup, due to a high degree of task parallelism. Its test set accuracy is significantly higher than that of decision tree inducers alone and is comparable to that of the best extant search-space based wrappers.</content></document><document><year>2004</year><authors>Muhammet Mustafa Ozdal1  | Cevdet Aykanat2 </authors><title>Hypergraph Models and Algorithms for Data-Pattern-Based Clustering</title><content>In traditional approaches for clustering market basket type data, relations among transactions are modeled according to the items occurring in these transactions. However, an individual item might induce different relations in different contexts. Since such contexts might be captured by interesting patterns in the overall data, we represent each transaction as a set of patterns through modifying the conventional pattern semantics. By clustering the patterns in the dataset, we infer a clustering of the transactions represented this way. For this, we propose a novel hypergraph model to represent the relations among the patterns. Instead of a local measure that depends only on common items among patterns, we propose a global measure that is based on the cooccurences of these patterns in the overall data. The success of existing hypergraph partitioning based algorithms in other domains depends on sparsity of the hypergraph and explicit objective metrics. For this, we propose a two-phase clustering approach for the above hypergraph, which is expected to be dense. In the first phase, the vertices of the hypergraph are merged in a multilevel algorithm to obtain large number of high quality clusters. Here, we propose new quality metrics for merging decisions in hypergraph clustering specifically for this domain. In order to enable the use of existing metrics in the second phase, we introduce a vertex-to-cluster affinity concept to devise a method for constructing a sparse hypergraph based on the obtained clustering. The experiments we have performed show the effectiveness of the proposed framework.</content></document><document><year>2004</year><authors>Sunita Sarawagi1 </authors><title>iDiff: Informative Summarization of Differences in Multidimensional Aggregates</title><content>Multidimensional OLAP products provide an excellent opportunity for integrating mining functionality because of their widespread acceptance as a decision support tool and their existing heavy reliance on manual, user-driven analysis. Most OLAP products are rather simplistic and rely heavily on the user's intuition to manually drive the discovery process. Such ad hoc user-driven exploration gets tedious and error-prone as data dimensionality and size increases. Our goal is to automate these manual discovery processes. In this paper we present an example of such automation through a iDiff operator that in a single step returns summarized reasons for drops or increases observed at an aggregated level.We formulate this as a problem of summarizing the difference between two multidimensional arrays of real numbers. We develop a general framework for such summarization and propose a specific formulation for the case of OLAP aggregates. We develop an information theoretic formulation for expressing the reasons that is compact and easy to interpret. We design an efficient dynamic programming algorithm that requires only one pass of the data and uses a small amount of memory independent of the data size. This allows easy integration with existing OLAP products. Our prototype has been tested on the Microsoft OLAP server, DB2/UDB and Oracle 8i. Experiments using the OLAP benchmark demonstrate (1) scalability of our algorithm as the size and dimensionality of the cube increases and (2) feasibility of getting interactive answers with modest hardware resources.</content></document><document><year>2004</year><authors>Jinyan Li1 | Thomas Manoukian2 | Guozhu Dong3  | Kotagiri Ramamohanarao2 </authors><title>Incremental Maintenance on the Border of the Space of Emerging Patterns</title><content>Emerging patterns (EPs) are useful knowledge patterns with many applications. In recent studies on bio-medical profiling data, we have successfully used such patterns to solve difficult cancer diagnosis problems and produced higher classification accuracy when compared to alternative methods. However, the discovery of EPs is a challenging and computationally expensive problem.In this paper, we study how to incrementally modify and maintain the concise boundary descriptions of the space of all emerging patterns when small changes occur to the data. As EP spaces are convex, the maintenance on the bounds guarantees that no desired patterns are lost. We introduce algorithms to handle four types of changes: insertion of new data, deletion of old data, addition of new attributes, and deletion of old attributes. We compare these incremental algorithms, on six benchmark data sets, against an efficient algorithm that computes from scratch. The results show that the incremental algorithms are much faster than the From-Scratch method, often with tremendous speed-up rates.</content></document><document><year>2004</year><authors>Joseph M. Hellerstein1 | Ron Avnur1  | Vijayshankar Raman1 </authors><title>Informix under CONTROL: Online Query Processing      </title><content>The goal of the CONTROL project at Berkeley is to develop systems for interactive analysis of large data sets. We focus on         systems that provide users with iteratively refining answers to requests and online control of processing, thereby tightening         the loop in the data analysis process. This paper presents the database-centric subproject of CONTROL: a complete online query processing facility, implemented in a commercial Object-Relational DBMS from Informix. We describe the algorithms at         the core of the system, and detail the end-to-end issues required to bring the algorithms together and deliver a complete         system.      </content></document><document><year>2004</year><authors>Sunita Sarawagi1 | Shiby Thomas2  | Rakesh Agrawal3 </authors><title>Integrating Association Rule Mining with Relational Database Systems: Alternatives and Implications      </title><content>Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum         of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through         a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly         and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We         comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining         as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR).         Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache option is         superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache and the SQL-OR approaches         incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache.         The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis         of qualitative factors like automatic parallelization, development ease, portability and inter-operability. As a byproduct         of this study, we identify some primitives for native support in database systems for decision-support applications.      </content></document><document><year>2004</year><authors>S.D. Lee1 | David W. Cheung1  | Ben Kao1 </authors><title>Is Sampling Useful in Data Mining? A Case in the Maintenance of Discovered Association Rules      </title><content>By nature, sampling is an appealing technique for data mining, because approximate solutions in most cases may already be         of great satisfaction to the need of the users. We attempt to use sampling techniques to address the problem of maintaining         discovered association rules. Some studies have been done on the problem of maintaining the discovered association rules when         updates are made to the database. All proposed methods must examine not only the changed part but also the unchanged part         in the original database, which is very large, and hence take much time. Worse yet, if the updates on the rules are performed         frequently on the database but the underlying rule set has not changed much, then the effort could be mostly wasted. In this         paper, we devise an algorithm which employs sampling techniques to estimate the difference between the association rules in         a database before and after the database is updated. The estimated difference can be used to determine whether we should update         the mined association rules or not. If the estimated difference is small, then the rules in the original database is still         a good approximation to those in the updated database. Hence, we do not have to spend the resources to update the rules. We         can accumulate more updates before actually updating the rules, thereby avoiding the overheads of updating the rules too frequently.         Experimental results show that our algorithm is very efficient and highly accurate.      </content></document><document><year>2004</year><authors>Sean Wallis1  | Gerald Nelson2 </authors><title>Knowledge Discovery in Grammatically Analysed Corpora</title><content>Collections of grammatically annotated texts (corpora), and in particular, parsed corpora, present a challenge to current methods of analysis. Such corpora are large and highly structured heterogeneous data sources. In this paper we briefly describe the parsed one-million word ICE-GB corpus, and the ICECUP query system. We then consider the application of knowledge discovery in databases (KDD) to text corpora. Following Cupit and Shadbolt (Proceedings 9th European Knowledge Acquisition Workshop, EKAW '96; Berlin: Springer Verlag, pp. 245&amp;#x2013;261, 1996), we argue that effective linguistic knowledge discovery must be based on a process of redescription or, more precisely, abstraction, based on the research question to be investigated. Abstraction maps relevant elements from the corpus to an abstract model of the research topic. This mapping may be implemented using a grammatical query representation such as ICECUP's Fuzzy Tree Fragments (FTFs). Since this abstractive process must be both experimental and expert-guided, ultimately a workbench is necessary to maintain, evaluate and refine the abstract model. We conclude with a pilot study, employing our approach, into aspects of noun phrase postmodifying clause structure. The data is analysed using the UNIT machine learning algorithm to search for significant interactions between domain variables. We show that our results are commensurable with those published in the linguistics literature, and discuss how the methodology may be improved.</content></document><document><year>2004</year><authors>Heikki Mannila1| 1  | Hannu Toivonen1| 1 </authors><title>Levelwise Search and Borders of Theories in Knowledge Discovery</title><content>One of the basic problems in knowledge discovery in databases (KDD) is the following: given a data set r, a class L of sentences for defining subgroups of r, and a selection predicate, find all sentences of L deemed interesting by the selection predicate. We analyze the simple levelwise algorithm for finding all such descriptions. We give bounds for the number of database accesses that the algorithm makes. For this, we introduce the concept of the border of a theory, a notion that turns out to be surprisingly powerful in analyzing the algorithm. We also consider the verification problem of a KDD process: given r and a set of sentences S  L determine whether S is exactly the set of interesting statements about r. We show strong connections between the verification problem and the hypergraph transversal problem. The verification problem arises in a natural way when using sampling to speed up the pattern discovery step in KDD.</content></document><document><year>2004</year><authors>David Madigan1| N|ini Raghavan2| William Dumouchel2| Martha Nason3| Christian Posse3 | Greg Ridgeway4</authors><title>Likelihood-Based Data Squashing: A Modeling Approach to Instance Construction</title><content>Squashing is a lossy data compression technique that preserves statistical information. Specifically, squashing compresses a massive dataset to a much smaller one so that outputs from statistical analyses carried out on the smaller (squashed) dataset reproduce outputs from the same statistical analyses carried out on the original dataset. Likelihood-based data squashing (LDS) differs from a previously published squashing algorithm insofar as it uses a statistical model to squash the data. The results show that LDS provides excellent squashing performance even when the target statistical analysis departs from the model used to squash the data.</content></document><document><year>2004</year><authors>O.L. Mangasarian1 </authors><title>Mathematical Programming in Data Mining</title><content>Mathematical programming approaches to three fundamental problems will be described: feature selection, clustering and robust representation. The feature selection problem considered is that of discriminating between two sets while recognizing irrelevant and redundant features and suppressing them. This creates a lean model that often generalizes better to new unseen data. Computational results on real data confirm improved generalization of leaner models. Clustering is exemplified by the unsupervised learning of patterns and clusters that may exist in a given database and is a useful tool for knowledge discovery in databases (KDD). A mathematical programming formulation of this problem is proposed that is theoretically justifiable and computationally implementable in a finite number of steps. A resulting k-Median Algorithm is utilized to discover very useful survival curves for breast cancer patients from a medical database. Robust representation is concerned with minimizing trained model degradation when applied to new problems. A novel approach is proposed that purposely tolerates a small error in the training process in order to avoid overfitting data that may contain errors. Examples of applications of these concepts are given.</content></document><document><year>2004</year><authors>Emilio Corchado1 | Donald MacDonald1  | Colin Fyfe1 </authors><title>Maximum and Minimum Likelihood Hebbian Learning for Exploratory Projection Pursuit</title><content>In this paper, we review an extension of the learning rules in a Principal Component Analysis network which has been derived to be optimal for a specific probability density function. We note that this probability density function is one of a family of pdfs and investigate the learning rules formed in order to be optimal for several members of this family. We show that, whereas we have previously (Lai et al., 2000; Fyfe and MacDonald, 2002) viewed the single member of the family as an extension of PCA, it is more appropriate to view the whole family of learning rules as methods of performing Exploratory Projection Pursuit. We illustrate this on both artificial and real data sets.</content></document><document><year>2004</year><authors>Jiawei Han1 | Jian Pei2 | Yiwen Yin3  | Runying Mao4 </authors><title>Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach</title><content>Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist a large number of patterns and/or long patterns.In this study, we propose a novel frequent-pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a condensed, smaller data structure, FP-tree which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern-fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent-pattern mining methods.</content></document><document><year>2004</year><authors>Stefan Schroedl1 | Kiri Wagstaff1 | Seth Rogers1 | Pat Langley1  | Christopher Wilson1 </authors><title>Mining GPS Traces for Map Refinement</title><content>Despite the increasing popularity of route guidance systems, current digital maps are still inadequate for many advanced applications in automotive safety and convenience. Among the drawbacks are the insufficient accuracy of road geometry and the lack of fine-grained information, such as lane positions and intersection structure. In this paper, we present an approach to induce high-precision maps from traces of vehicles equipped with differential GPS receivers. Since the cost of these systems is rapidly decreasing and wireless technology is advancing to provide the communication infrastructure, we expect that in the next few years large amounts of car data will be available inexpensively. Our approach consists of successive processing steps: individual vehicle trajectories are divided into road segments and intersections; a road centerline is derived for each segment; lane positions are determined by clustering the perpendicular offsets from it; and the transitions of traces between segments are utilized in the generation of intersection models. This paper describes an approach to this complex data-mining task in a contiguous manner. Among the new contributions are a spatial clustering algorithm for inferring the connectivity structure, more powerful lane finding algorithms that are able to handle lane splits and merges, and an approach to inferring detailed intersection models.</content></document><document><year>2004</year><authors>Yukinobu Hamuro1 | Naoki Katoh2 | Yasuyuki Matsuda3  | Katsutoshi Yada1 </authors><title>Mining Pharmacy Data Helps to Make Profits</title><content>Pharma, a drugstore chain in Japan, has been remarkably successful in the effective use of data mining. From over one tera bytes of sales data accumulated in databases, it has derived much interesting and useful knowledge that in turn has been applied to produce profits. In this paper, we shall explain several interesting cases of knowledge discovery at Pharma. We then discuss the innovative features of the data mining system developed in Pharma that led to meaningful knowledge discovery.</content></document><document><year>2004</year><authors>Jiong Yang1 | Wei Wang2  | Philip S. Yu3 </authors><title>Mining Surprising Periodic Patterns</title><content>In this paper, we focus on mining surprising periodic patterns in a sequence of events. In many applications, e.g., computational biology, an infrequent pattern is still considered very significant if its actual occurrence frequency exceeds the prior expectation by a large margin. The traditional metric, such as support, is not necessarily the ideal model to measure this kind of surprising patterns because it treats all patterns equally in the sense that every occurrence carries the same weight towards the assessment of the significance of a pattern regardless of the probability of occurrence. A more suitable measurement, information, is introduced to naturally value the degree of surprise of each occurrence of a pattern as a continuous and monotonically decreasing function of its probability of occurrence. This would allow patterns with vastly different occurrence probabilities to be handled seamlessly. As the accumulated degree of surprise of all repetitions of a pattern, the concept of information gain is proposed to measure the overall degree of surprise of the pattern within a data sequence. The bounded information gain property is identified to tackle the predicament caused by the violation of the downward closure property by the information gain measure and in turn provides an efficient solution to this problem. Furthermore, the user has a choice between specifying a minimum information gain threshold and choosing the number of surprising patterns wanted. Empirical tests demonstrate the efficiency and the usefulness of the proposed model.</content></document><document><year>2004</year><authors>Igor Cadez1 | David Heckerman2 | Christopher Meek2 | Padhraic Smyth3  | Steven White2 </authors><title>Model-Based Clustering and Visualization of Navigation Patterns on a Web Site</title><content>We present a new methodology for exploring and analyzing navigation patterns on a web site. The patterns that can be analyzed consist of sequences of URL categories traversed by users. In our approach, we first partition site users into clusters such that users with similar navigation paths through the site are placed into the same cluster. Then, for each cluster, we display these paths for users within that cluster. The clustering approach we employ is model-based (as opposed to distance-based) and partitions users according to the order in which they request web pages. In particular, we cluster users by learning a mixture of first-order Markov models using the Expectation-Maximization algorithm. The runtime of our algorithm scales linearly with the number of clusters and with the size of the data; and our implementation easily handles hundreds of thousands of user sessions in memory. In the paper, we describe the details of our method and a visualization tool based on it called WebCANVAS. We illustrate the use of our approach on user-traffic data from msnbc.com.</content></document><document><year>2004</year><authors>Tomasz Imieliski1  | Aashu Virmani1 </authors><title>MSQL: A Query Language for Database Mining</title><content>The tremendous number of rules generated in the mining process makes it necessary for any good data mining system to provide for powerful query primitives to post-process the generated rulebase, as well as for performing selective, query based generation. In this paper, we present the design and compilation of MSQL, the rule query language developed as part of the Discovery Board system.</content></document><document><year>2004</year><authors>Jerome H. Friedman1</authors><title>On Bias, Variance, 0/1&amp;#x2014;Loss, and the Curse-of-Dimensionality</title><content>The classification problem is considered in which an outputvariable y assumes discrete values with respectiveprobabilities that depend upon the simultaneous values of a set of input variablesx = {x_1,....,x_n}. At issue is how error in the estimates of theseprobabilities affects classification error when the estimates are used ina classification rule. These effects are seen to be somewhat counterintuitive in both their strength and nature. In particular the bias andvariance components of the estimation error combine to influenceclassification in a very different way than with squared error on theprobabilities themselves. Certain types of (very high) bias can becanceled by low variance to produce accurate classification. This candramatically mitigate the effect of the bias associated with some simpleestimators like naive Bayes, and the bias induced by thecurse-of-dimensionality on nearest-neighbor procedures. This helps explainwhy such simple methods are often competitive with and sometimes superiorto more sophisticated ones for classification, and whybagging/aggregating classifiers can often improveaccuracy. These results also suggest simple modifications to theseprocedures that can (sometimes dramatically) further improve theirclassification performance.</content></document><document><year>2004</year><authors>Steven L. Salzberg1 </authors><title>On Comparing Classifiers: Pitfalls to Avoid and a Recommended Approach</title><content>An important component of many data mining projects is finding a good classification algorithm, a process that requires very careful thought about experimental design. If not done very carefully, comparative studies of classification and other types of algorithms can easily result in statistically invalid conclusions. This is especially true when one is using data mining techniques to analyze very large databases, which inevitably contain some statistically unlikely data. This paper describes several phenomena that can, if ignored, invalidate an experimental comparison. These phenomena and the conclusions that follow apply not only to classification, but to computational experiments in almost any aspect of data mining. The paper also discusses why comparative analysis is more important in evaluating some types of algorithms than for others, and provides some suggestions about how to avoid the pitfalls suffered by many experimental studies.</content></document><document><year>2004</year><authors>Huan Liu1 | Hiroshi Motoda2</authors><title>On Issues of Instance Selection</title><content>Without Abstract</content></document><document><year>2004</year><authors>Charu C. Aggarwal1 </authors><title>On Leveraging User Access Patterns for Topic Specific Crawling</title><content>In recent years, there has been considerable research on constructing crawlers which find resources satisfying specific conditions called predicates. Such a predicate could be a keyword query, a topical query, or some arbitrary contraint on the internal structure of the web page. Several techniques such as focussed crawling and intelligent crawling have recently been proposed for performing the topic specific resource discovery process. All these crawlers are linkage based, since they use the hyperlink behavior in order to perform resource discovery. Recent studies have shown that the topical correlations in hyperlinks are quite noisy and may not always show the consistency necessary for a reliable resource discovery process. In this paper, we will approach the problem of resource discovery from an entirely different perspective; we will mine the significant browsing patterns of world wide web users in order to model the likelihood of web pages belonging to a specified predicate. This user behavior can be mined from the freely available traces of large public domain proxies on the world wide web. For example, proxy caches such as Squid are hierarchical proxies which make their logs publically available. As we shall see in this paper, such traces are a rich source of information which can be mined in order to find the users that are most relevant to the topic of a given crawl. We refer to this technique as collaborative crawling because it mines the collective user experiences in order to find topical resources. Such a strategy turns out to be extremely effective because the topical consistency in world wide web browsing patterns turns out to very high compared to the noisy linkage information. In addition, the user-centered crawling system can be combined with linkage based systems to create an overall system which works more effectively than a system based purely on either user behavior or hyperlinks.</content></document><document><year>2004</year><authors>Jef Wijsen1  | Robert Meersman1 </authors><title>On the Complexity of Mining Quantitative Association Rules      </title><content>The discovery of quantitative association rules in large databases is considered an interesting and important research problem.         Recently, different aspects of the problem have been studied, and several algorithms have been presented in the literature,         among others in (Srikant and Agrawal, 1996; Fukuda et al., 1996a; Fukuda et al., 1996b; Yoda et al., 1997; Miller and Yang,         1997). An aspect of the problem that has so far been ignored, is its computational complexity. In this paper, we study the         computational complexity of mining quantitative association rules.      </content></document><document><year>2004</year><authors>Eamonn Keogh1  | Shruti Kasetty1 </authors><title>On the Need for Time Series Data Mining Benchmarks: A Survey and Empirical Demonstration</title><content>In the last decade there has been an explosion of interest in mining time series data. Literally hundreds of papers have introduced new algorithms to index, classify, cluster and segment time series. In this work we make the following claim. Much of this work has very little utility because the contribution made (speed in the case of indexing, accuracy in the case of classification and clustering, model accuracy in the case of segmentation) offer an amount of improvement that would have been completely dwarfed by the variance that would have been observed by testing on many real world datasets, or the variance that would have been observed by changing minor (unstated) implementation details.To illustrate our point, we have undertaken the most exhaustive set of time series experiments ever attempted, re-implementing the contribution of more than two dozen papers, and testing them on 50 real world, highly diverse datasets. Our empirical results strongly support our assertion, and suggest the need for a set of time series benchmarks and more careful empirical evaluation in the data mining community.</content></document><document><year>2004</year><authors>Kenji Yamanishi1 | Jun-ichi Takeuchi1 | Graham Williams2  | Peter Milne2 </authors><title>On-Line Unsupervised Outlier Detection Using Finite Mixtures with Discounting Learning Algorithms</title><content>Outlier detection is a fundamental issue in data mining, specifically in fraud detection, network intrusion detection, network monitoring, etc. SmartSifter is an outlier detection engine addressing this problem from the viewpoint of statistical learning theory. This paper provides a theoretical basis for SmartSifter and empirically demonstrates its effectiveness. SmartSifter detects outliers in an on-line process through the on-line unsupervised learning of a probabilistic model (using a finite mixture model) of the information source. Each time a datum is input SmartSifter employs an on-line discounting learning algorithm to learn the probabilistic model. A score is given to the datum based on the learned model with a high score indicating a high possibility of being a statistical outlier. The novel features of SmartSifter are: (1) it is adaptive to non-stationary sources of data; (2) a score has a clear statistical/information-theoretic meaning; (3) it is computationally inexpensive; and (4) it can handle both categorical and continuous variables. An experimental application to network intrusion detection shows that SmartSifter was able to identify data with high scores that corresponded to attacks, with low computational costs. Further experimental application has identified a number of meaningful rare cases in actual health insurance pathology data from Australia's Health Insurance Commission.</content></document><document><year>2004</year><authors>Manuel Castej&amp;oacute n Limas1| Joaqu&amp;iacute n B. Ordieres Mer&amp;eacute 1 | Francisco J. Mart&amp;iacute nez de Pis&amp;oacute n Ascacibar2 | Eliseo P. Vergara Gonz&amp;aacute lez2</authors><title>Outlier Detection and Data Cleaning in Multivariate Non-Normal Samples: The PAELLA Algorithm</title><content>A new method of outlier detection and data cleaning for both normal and non-normal multivariate data sets is proposed. It is based on an iterated local fit without a priori metric assumptions. We propose a new approach supported by finite mixture clustering which provides good results with large data sets. A multi-step structure, consisting of three phases, is developed. The importance of outlier detection in industrial modeling for open-loop control prediction is also described. The described algorithm gives good results both in simulations runs with artificial data sets and with experimental data sets recorded in a rubber factory. Finally, some discussion about this methodology is exposed.</content></document><document><year>2004</year><authors>Mohammed J. Zaki1 | Srinivasan Parthasarathy1 | Mitsunori Ogihara1  | Wei Li2 </authors><title>Parallel Algorithms for Discovery of Association Rules      </title><content>Discovery of association rules is an important data mining task. Several parallel and sequential algorithms have been proposed         in the literature to solve this problem. Almost all of these algorithms make repeated passes over the database to determine         the set of frequent itemsets (a subset of database items), thus incurring high I/O overhead. In the parallel case, most algorithms         perform a sum-reduction at the end of each pass to construct the global counts, also incurring high synchronization cost.                     In this paper we describe new parallel association mining algorithms. The algorithms use novel itemset clustering techniques               to approximate the set of potentially maximal frequent itemsets. Once this set has been identified, the algorithms make use               of efficient traversal techniques to generate the frequent itemsets contained in each cluster. We propose two clustering schemes               based on equivalence classes and maximal hypergraph cliques, and study two lattice traversal techniques based on bottom-up               and hybrid search. We use a vertical database layout to cluster related transactions together. The database is also selectively               replicated so that the portion of the database needed for the computation of associations is local to each processor. After               the initial set-up phase, the algorithms do not need any further communication or synchronization. The algorithms minimize               I/O overheads by scanning the local database portion only twice. Once in the set-up phase, and once when processing the itemset               clusters. Unlike previous parallel approaches, the algorithms use simple intersection operations to compute frequent itemsets               and do not have to maintain or search complex hash structures.            </content></document><document><year>2004</year><authors>Anurag Srivastava1 | Eui-Hong Han2 | Vipin Kumar2  | Vineet Singh3 </authors><title>Parallel Formulations of Decision-Tree Classification Algorithms      </title><content>Classification decision tree algorithms are used extensively for data mining in many domains such as retail target marketing,         fraud detection, etc. Highly parallel algorithms for constructing classification decision trees are desirable for dealing         with large data sets in reasonable amount of time. Algorithms for building classification decision trees have a natural concurrency,         but are difficult to parallelize due to the inherent dynamic nature of the computation. In this paper, we present parallel         formulations of classification decision tree learning algorithm based on induction. We describe two basic parallel formulations.         One is based on Synchronous Tree Construction Approach and the other is based on Partitioned Tree Construction Approach. We         discuss the advantages and disadvantages of using these methods and propose a hybrid method that employs the good features         of these methods. We also provide the analysis of the cost of computation and communication of the proposed hybrid method.         Moreover, experimental results on an IBM SP-2 demonstrate excellent speedups and scalability.      </content></document><document><year>2004</year><authors>Y. Xiang1  | T. Chu2</authors><title>Parallel Learning of Belief Networks in Large and Difficult Domains      </title><content>Learning belief networks from large domains can be expensive even with single-link lookahead search (SLLS). Since a SLLS cannot         learn correctly in a class of problem domains, multi-link lookahead search (MLLS) is needed which further increases the computational         complexity. In our experiment, learning in some difficult domains over more than a dozen variables took days. In this paper,         we study how to use parallelism to speed up SLLS for learning in large domains and to tackle the increased complexity of MLLS         for learning in difficult domains. We propose a natural decomposition of the learning task for parallel processing. We investigate         two strategies for job allocation among processors to further improve load balancing and efficiency of the parallel system.         For learning from very large datasets, we present a regrouping of the available processors such that slow data access through         the file system can be replaced by fast memory access. Experimental results in a distributed memory MIMD computer demonstrate         the effectiveness of the proposed algorithms.      </content></document><document><year>2004</year><authors>Don Coppersmith1 | Se June Hong1  | Jonathan R.M. Hosking1 </authors><title>Partitioning Nominal Attributes in Decision Trees</title><content>To find the optimal branching of a nominal attribute at a node in an L-ary decision tree, one is often forced to search over all possible L-ary partitions for the one that yields the minimum impurity measure. For binary trees (L = 2) when there are just two classes a short-cut search is possible that is linear in n, the number of distinct values of the attribute. For the general case in which the number of classes, k, may be greater than two, Burshtein et al. have shown that the optimal partition satisfies a condition that involves the existence of2L hyperplanes in the class probability space. We derive a property of the optimal partition for concave impurity measures (including in particular the Gini and entropy impurity measures) in terms of the existence ofL vectors in the dual of the class probability space, which implies the earlier condition.Unfortunately, these insights still do not offer a practical search method when n and k are large, even for binary trees. We therefore present a new heuristic search algorithm to find a good partition. It is based on ordering the attribute's values according to their principal component scores in the class probability space, and is linear in n. We demonstrate the effectiveness of the new method through Monte Carlo simulation experiments and compare its performance against other heuristic methods.</content></document><document><year>2004</year><authors>R.D. Lawrence1 | G.S. Almasi2 | V. Kotlyar3 | M.S. Viveros4  | S.S. Duri5 </authors><title>Personalization of Supermarket Product Recommendations</title><content>We describe a personalized recommender system designed to suggest new products to supermarket shoppers. The recommender functions in a pervasive computing environment, namely, a remote shopping system in which supermarket customers use Personal Digital Assistants (PDAs) to compose and transmit their orders to the store, which assembles them for subsequent pickup. The recommender is meant to provide an alternative source of new ideas for customers who now visit the store less frequently. Recommendations are generated by matching products to customers based on the expected appeal of the product and the previous spending of the customer. Associations mining in the product domain is used to determine relationships among product classes for use in characterizing the appeal of individual products. Clustering in the customer domain is used to identify groups of shoppers with similar spending histories. Cluster-specific lists of popular products are then used as input to the matching process.The recommender is currently being used in a pilot program with several hundred customers. Analysis of results to date have shown a 1.8% boost in program revenue as a result of purchases made directly from the list of recommended products. A substantial fraction of the accepted recommendations are from product classes new to the customer, indicating a degree of willingness to expand beyond present purchase patterns in response to reasonable suggestions.</content></document><document><year>2004</year><authors>Daniel Boley1 </authors><title>Principal Direction Divisive Partitioning</title><content>We propose a new algorithm capable of partitioning a set of documents or other samples based on an embedding in a high dimensional Euclidean space (i.e., in which every document is a vector of real numbers). The method is unusual in that it is divisive, as opposed to agglomerative, and operates by repeatedly splitting clusters into smaller clusters. The documents are assembled into a matrix which is very sparse. It is this sparsity that permits the algorithm to be very efficient. The performance of the method is illustrated with a set of text documents obtained from the World Wide Web. Some possible extensions are proposed for further investigation.</content></document><document><year>2004</year><authors>Rajeev Rastogi1  | Kyuseok Shim2 </authors><title>PUBLIC: A Decision Tree Classifier that Integrates Building and Pruning      </title><content>Classification is an important problem in data mining. Given a database of records, each with a class label, a classifier         generates a concise and meaningful description for each class that can be used to classify subsequent records. A number of         popular classifiers construct decision trees to generate class models. These classifiers first build a decision tree and then         prune subtrees from the decision tree in a subsequent pruning phase to improve accuracy and prevent &amp;#8220;overfitting&amp;#8221;.                     Generating the decision tree in two distinct phases could result in a substantial amount of wasted effort since an entire               subtree constructed in the first phase may later be pruned in the next phase. In this paper, we propose PUBLIC, an improved               decision tree classifier that integrates the second &amp;#8220;pruning&amp;#8221; phase with the initial &amp;#8220;building&amp;#8221; phase. In PUBLIC, a node is               not expanded during the building phase, if it is determined that it will be pruned during the subsequent pruning phase. In               order to make this determination for a node, before it is expanded, PUBLIC computes a lower bound on the minimum cost subtree               rooted at the node. This estimate is then used by PUBLIC to identify the nodes that are certain to be pruned, and for such               nodes, not expend effort on splitting them. Experimental results with real-life as well as synthetic data sets demonstrate               the effectiveness of PUBLIC's integrated approach which has the ability to deliver substantial performance improvements.            </content></document><document><year>2004</year><authors>Jian Pei1 | Jiawei Han2  | Laks V.S. Lakshmanan3 </authors><title>Pushing Convertible Constraints in Frequent Itemset Mining</title><content>Recent work has highlighted the importance of the constraint-based mining paradigm in the context of frequent itemsets, associations, correlations, sequential patterns, and many other interesting patterns in large databases. Constraint pushing techniques have been developed for mining frequent patterns and associations with antimonotonic, monotonic, and succinct constraints. In this paper, we study constraints which cannot be handled with existing theory and techniques in frequent pattern mining. For example, avg(S)v, median(S)v, sum(S)v (S can contain items of arbitrary values,   {, } and v is a real number.) are customarily regarded as tough constraints in that they cannot be pushed inside an algorithm such as Apriori. We develop a notion of convertible constraints and systematically analyze, classify, and characterize this class. We also develop techniques which enable them to be readily pushed deep inside the recently developed FP-growth algorithm for frequent itemset mining. Results from our detailed experiments show the effectiveness of the techniques developed.</content></document><document><year>2004</year><authors>Johannes Gehrke1| Raghu Ramakrishnan1 | Venkatesh Ganti1</authors><title>RainForest&amp;#8212;A Framework for Fast Decision Tree Construction of Large Datasets      </title><content>Classification of large datasets is an important data mining problem. Many classification algorithms have been proposed in         the literature, but studies have shown that so far no algorithm uniformly outperforms all other algorithms in terms of quality.         In this paper, we present a unifying framework called Rain Forest for classification tree construction that separates the         scalability aspects of algorithms for constructing a tree from the central features that determine the quality of the tree.         The generic algorithm is easy to instantiate with specific split selection methods from the literature (including C4.5, CART,         CHAID, FACT, ID3 and extensions, SLIQ, SPRINT and QUEST).                     In addition to its generality, in that it yields scalable versions of a wide range of classification algorithms, our approach               also offers performance improvements of over a factor of three over the SPRINT algorithm, the fastest scalable classification               algorithm proposed previously. In contrast to SPRINT, however, our generic algorithm requires a certain minimum amount of               main memory, proportional to the set of distinct values in a column of the input relation. Given current main memory costs,               this requirement is readily met in most if not all workloads.            </content></document><document><year>2004</year><authors>Roberto K.H. Galv&amp;atilde o1 | Victor M. Becerra2  | Magda Abou-Seada3 </authors><title>Ratio Selection for Classification Models</title><content>This paper is concerned with the selection of inputs for classification models based on ratios of measured quantities. For this purpose, all possible ratios are built from the quantities involved and variable selection techniques are used to choose a convenient subset of ratios. In this context, two selection techniques are proposed: one based on a pre-selection procedure and another based on a genetic algorithm. In an example involving the financial distress prediction of companies, the models obtained from ratios selected by the proposed techniques compare favorably to a model using ratios usually found in the financial distress literature.</content></document><document><year>2004</year><authors>Mauricio A. Hern&amp;aacute ndez1  | Salvatore J. Stolfo1 </authors><title>Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem</title><content>The problem of merging multiple databases of information about common entities is frequently encountered in KDD and decision support applications in large commercial and government organizations. The problem we study is often called the Merge/Purge problem and is difficult to solve both in scale and accuracy. Large repositories of data typically have numerous duplicate information entries about the same entities that are difficult to cull together without an intelligent equational theory that identifies equivalent items by a complex, domain-dependent matching process. We have developed a system for accomplishing this Data Cleansing task and demonstrate its use for cleansing lists of names of potential customers in a direct marketing-type application. Our results for statistically generated data are shown to be accurate and effective when processing the data multiple times using different keys for sorting on each successive pass. Combing results of individual passes using transitive closure over the independent results, produces far more accurate results at lower cost. The system provides a rule programming module that is easy to program and quite good at finding duplicates especially in an environment with massive amounts of data. This paper details improvements in our system, and reports on the successful implementation for a real-world database that conclusively validates our results previously achieved for statistically generated data.</content></document><document><year>2004</year><authors>David M. Rocke1 | Jian Dai1</authors><title>Sampling and Subsampling for Cluster Analysis in Data Mining: With Applications to Sky Survey Data</title><content>This paper describes a clustering method for unsupervised classification of objects in large data sets. The new methodology combines the mixture likelihood approach with a sampling and subsampling strategy in order to cluster large data sets efficiently. This sampling strategy can be applied to a large variety of data mining methods to allow them to be used on very large data sets. The method is applied to the problem of automated star/galaxy classification for digital sky data and is tested using a sample from the Digitized Palomar Sky Survey (DPOSS) data. The method is quick and reliable and produces classifications comparable to previous work on these data using supervised clustering.</content></document><document><year>2004</year><authors>Craig Silverstein1 | Sergey Brin2 | Rajeev Motwani3  | Jeff Ullman4 </authors><title>Scalable Techniques for Mining Causal Structures      </title><content>Mining for association rules in market basket data has proved a fruitful area of research. Measures such as conditional probability         (confidence) and correlation have been used to infer rules of the form &amp;#8220;the existence of item A implies the existence of item         B.&amp;#8221; However, such rules indicate only a statistical relationship between A and B. They do not specify the nature of the relationship:         whether the presence of A causes the presence of B, or the converse, or some other attribute or phenomenon causes both to         appear together. In applications, knowing such causal relationships is extremely useful for enhancing understanding and effecting         change. While distinguishing causality from correlation is a truly difficult problem, recent work in statistics and Bayesian         learning provide some avenues of attack. In these fields, the goal has generally been to learn complete causal models, which         are essentially impossible to learn in large-scale data mining applications with a large number of variables.                     In this paper, we consider the problem of determining casual relationships, instead of mere associations, when mining market               basket data. We identify some problems with the direct application of Bayesian learning ideas to mining large databases, concerning               both the scalability of algorithms and the appropriateness of the statistical techniques, and introduce some initial ideas               for dealing with these problems. We present experimental results from applying our algorithms on several large, real-world               data sets. The results indicate that the approach proposed here is both computationally feasible and successful in identifying               interesting causal structures. An interesting outcome is that it is perhaps easier to infer the lack of causality than to               infer causality, information that is useful in preventing erroneous decision making.            </content></document><document><year>2004</year><authors>Volker Tresp1 </authors><title>Scaling Kernel-Based Systems to Large Data Sets</title><content>In the form of the support vector machine and Gaussian processes, kernel-based systems are currently very popular approaches to supervised learning. Unfortunately, the computational load for training kernel-based systems increases drastically with the size of the training data set, such that these systems are not ideal candidates for applications with large data sets. Nevertheless, research in this direction is very active. In this paper, I review some of the current approaches toward scaling kernel-based systems to large data sets.</content></document><document><year>2004</year><authors>Hendrik Blockeel1 | Luc De Raedt1 | Nico Jacobs1  | Bart Demoen1 </authors><title>Scaling Up Inductive Logic Programming by Learning from Interpretations</title><content>When comparing inductive logic programming (ILP) and attribute-value learning techniques, there is a trade-off between expressive power and efficiency. Inductive logic programming techniques are typically more expressive but also less efficient. Therefore, the data sets handled by current inductive logic programming systems are small according to general standards within the data mining community. The main source of inefficiency lies in the assumption that several examples may be related to each other, so they cannot be handled independently.Within the learning from interpretations framework for inductive logic programming this assumption is unnecessary, which allows to scale up existing ILP algorithms. In this paper we explain this learning setting in the context of relational databases. We relate the setting to propositional data mining and to the classical ILP setting, and show that learning from interpretations corresponds to learning from multiple relations and thus extends the expressiveness of propositional learning, while maintaining its efficiency to a large extent (which is not the case in the classical ILP setting).</content></document><document><year>2004</year><authors>Corinna Cortes1 | Daryl Pregibon1</authors><title>Signature-Based Methods for Data Streams</title><content>We have been developing signature-based methods in the telecommunications industry for the past 5 years. In this paper, we describe our work as it evolved due to improvements in technology and our aggressive attitude toward scale. We discuss the types of features that our signatures contain, nuances of how these are updated through time, our treatment of outliers, and the trade-off between time-driven and event-driven processing. We provide a number of examples, all drawn from the application of signatures to toll fraud detection.</content></document><document><year>2004</year><authors>Martin Ester1 | Alex|er Frommelt2| Hans-Peter Kriegel3  | JГ¶org S|er4 </authors><title>Spatial Data Mining: Database Primitives, Algorithms and Efficient DBMS Support      </title><content>Spatial data mining algorithms heavily depend on the efficient processing of neighborhood relations since the neighbors of         many objects have to be investigated in a single run of a typical algorithm. Therefore, providing general concepts for neighborhood         relations as well as an efficient implementation of these concepts will allow a tight integration of spatial data mining algorithms         with a spatial database management system. This will speed up both, the development and the execution of spatial data mining         algorithms. In this paper, we define neighborhood graphs and paths and a small set of database primitives for their manipulation.         We show that typical spatial data mining algorithms are well supported by the proposed basic operations. For finding significant         spatial patterns, only certain classes of paths &amp;#8220;leading away&amp;#8221; from a starting object are relevant. We discuss filters allowing         only such neighborhood paths which will significantly reduce the search space for spatial data mining algorithms. Furthermore,         we introduce neighborhood indices to speed up the processing of our database primitives. We implemented the database primitives         on top of a commercial spatial database management system. The effectiveness and efficiency of the proposed approach was evaluated         by using an analytical cost model and an extensive experimental study on a geographic database.      </content></document><document><year>2004</year><authors>Paolo Giudici1 | David Heckerman2  | Joe Whittaker3 </authors><title>Statistical Models for Data Mining</title><content>We review the background to the papers presented in this special issue and give a short introduction to each. We also briefly describe the workshop on Statistical models for data mining, held in Pavia (Italy), in October 2000, where the papers were presented.</content></document><document><year>2004</year><authors>Clark Glymour1 | David Madigan2 | Daryl Pregibon3  | Padhraic Smyth4 </authors><title>Statistical Themes and Lessons for Data Mining</title><content>Data mining is on the interface of Computer Science andStatistics, utilizing advances in both disciplines to make progressin extracting information from large databases. It is an emergingfield that has attracted much attention in a very short period oftime. This article highlights some statistical themes and lessonsthat are directly relevant to data mining and attempts to identifyopportunities where close cooperation between the statistical andcomputational communities might reasonably provide synergy forfurther progress in data analysis.</content></document><document><year>2004</year><authors>Yi Lin1</authors><title>Support Vector Machines and the Bayes Rule in Classification</title><content>The Bayes rule is the optimal classification rule if the underlying distribution of the data is known. In practice we do not know the underlying distribution, and need to learn classification rules from the data. One way to derive classification rules in practice is to implement the Bayes rule approximately by estimating an appropriate classification function. Traditional statistical methods use estimated log odds ratio as the classification function. Support vector machines (SVMs) are one type of large margin classifier, and the relationship between SVMs and the Bayes rule was not clear. In this paper, it is shown that the asymptotic target of SVMs are some interesting classification functions that are directly related to the Bayes rule. The rate of convergence of the solutions of SVMs to their corresponding target functions is explicitly established in the case of SVMs with quadratic or higher order loss functions and spline kernels. Simulations are given to illustrate the relation between SVMs and the Bayes rule in other cases. This helps understand the success of SVMs in many classification studies, and makes it easier to compare SVMs and traditional statistical methods.</content></document><document><year>2004</year><authors>Johannes Grabmeier1 | Andreas Rudolph2</authors><title>Techniques of Cluster Algorithms in Data Mining</title><content>An overview of cluster analysis techniques from a data mining point of view is given. This is done by a strict separation of the questions of various similarity and distance measures and related optimization criteria for clusterings from the methods to create and modify clusterings themselves. In addition to this general setting and overview, the second focus is used on discussions of the essential ingredients of the demographic cluster algorithm of IBM's Intelligent Miner, based Condorcet's criterion.</content></document><document><year>2004</year><authors>Pedro Domingos1 </authors><title>The Role of Occam's Razor in Knowledge Discovery</title><content>Many KDD systems incorporate an implicit or explicit preference for simpler models, but this use of Occam's razor has been strongly criticized by several authors (e.g., Schaffer, 1993; Webb, 1996). This controversy arises partly because Occam's razor has been interpreted in two quite different ways. The first interpretation (simplicity is a goal in itself) is essentially correct, but is at heart a preference for more comprehensible models. The second interpretation (simplicity leads to greater accuracy) is much more problematic. A critical review of the theoretical arguments for and against it shows that it is unfounded as a universal principle, and demonstrably false. A review of empirical evidence shows that it also fails as a practical heuristic. This article argues that its continued use in KDD risks causing significant opportunities to be missed, and should therefore be restricted to the comparatively few applications where it is appropriate. The article proposes and reviews the use of domain constraints as an alternative for avoiding overfitting, and examines possible methods for handling the accuracy&amp;#x2013;comprehensibility trade-off.</content></document><document><year>2004</year><authors>Frans Coenen1 | Graham Goulbourne1  | Paul Leng1 </authors><title>Tree Structures for Mining Association Rules</title><content>A well-known approach to Knowledge Discovery in Databases involves the identification of association rules linking database attributes. Extracting all possible association rules from a database, however, is a computationally intractable problem, because of the combinatorial explosion in the number of sets of attributes for which incidence-counts must be computed. Existing methods for dealing with this may involve multiple passes of the database, and tend still to cope badly with densely-packed database records. We describe here a class of methods we have introduced that begin by using a single database pass to perform a partial computation of the totals required, storing these in the form of a set enumeration tree, which is created in time linear to the size of the database. Algorithms for using this structure to complete the count summations are discussed, and a method is described, derived from the well-known Apriori algorithm. Results are presented demonstrating the performance advantage to be gained from the use of this approach. Finally, we discuss possible further applications of the method.</content></document><document><year>2004</year><authors>Daniel B. Carr1 | Anthony R. Olsen2 | Suzanne M. Pierson3  | Jean-Yves P. Courbois4 </authors><title>Using Linked Micromap Plots to Characterize Omernik Ecoregions      </title><content>The paper introduces linked micromap (LM) plots for presenting environmental summaries. The LM template includes parallel         sequences of micromap, label, and statistical summary graphics panels with attention paid to perceptual grouping, sorting         and linking of the summary components. The applications show LM plots for Omernik Level II Ecoregions. The summarized United         States continental data includes USGS digital elevation, 30-year normal precipitation and temperature, and 8 million AVHRR         pixels classified into 159 types of land cover. One LM plot uses a line-height glyph to represent all 159 land cover percentages         per ecoregion. LM plots represent new visualization methodology that is useful in the data and knowledge based pattern representation         and knowledge discovery process. The LM plots focus on providing an orienting overview. The overview provides a starting place         for subsequent drilling down to what could otherwise be viewed as an overwhelming mass of data. The overview also provides         a starting place to learn about the intellectual structure that lies behind the notion of ecoregions and begins to connect         this abstract structure to quantitative methods.      </content></document><document><year>2004</year><authors>Daniel Barbar&amp;aacute 1  | Ping Chen2 </authors><title>Using Self-Similarity to Cluster Large Data Sets</title><content>Clustering is a widely used knowledge discovery technique. It helps uncovering structures in data that were not previously known. The clustering of large data sets has received a lot of attention in recent years, however, clustering is a still a challenging task since many published algorithms fail to do well in scaling with the size of the data set and the number of dimensions that describe the points, or in finding arbitrary shapes of clusters, or dealing effectively with the presence of noise. In this paper, we present a new clustering algorithm, based in self-similarity properties of the data sets. Self-similarity is the property of being invariant with respect to the scale used to look at the data set. While fractals are self-similar at every scale used to look at them, many data sets exhibit self-similarity over a range of scales. Self-similarity can be measured using the fractal dimension. The new algorithm which we call Fractal Clustering (FC) places points incrementally in the cluster for which the change in the fractal dimension after adding the point is the least. This is a very natural way of clustering points, since points in the same cluster have a great degree of self-similarity among them (and much less self-similarity with respect to points in other clusters). FC requires one scan of the data, is suspendable at will, providing the best answer possible at that point, and is incremental. We show via experiments that FC effectively deals with large data sets, high-dimensionality and noise and is capable of recognizing clusters of arbitrary shape.</content></document><document><year>2004</year><authors>Bettina Berendt1</authors><title>Using Site Semantics to Analyze, Visualize, and Support Navigation</title><content>To satisfy potential customers of a Web site and to lead them to the goods offered by the site, one should support them in the course of navigation they have embarked on. This paper presents the tool STRATDYN, developed as an add-on module to the Web Usage Miner WUM. WUM not only discovers frequent sequences, but it also allows the inspection of the different paths through the site. STRATDYN extends these capabilities: It tests differences between navigation patterns, described by a number of measures of success and strategy, for statistical significance. This can help to single out the relevant differences between users' behaviors, and it can determine whether a change in the site's design has had the desired effect. STRATDYN also exploits the site's semantics in the classification of navigation behavior and in the visualization of results, displaying navigation patterns as alternative paths through a strategy space. This helps to understand the Web logs, and to communicate analysis results to non-experts. Two case studies investigate search in an online catalog and interaction with an electronic shopping agent in an online store. They show how the results of analysis can lead to proposals for improving a Web site. These highlight the importance of investigating measures not only of eventual success, but also of process, to help users navigate towards the site's offers.</content></document><document><year>2004</year><authors>MГЎrcia MacГЄdo1 | Dianne Cook2  | Timothy J. Brown3 </authors><title>Visual Data Mining In Atmospheric Science Data      </title><content>This paper discusses the use of simple visual tools to explore multivariate spatially-referenced data. It describes interactive         approaches such as linked brushing, and dynamic methods such as the grand tour, applied to studying the Comprehensive Ocean-Atmosphere         Data Set (COADS). This visual approach provides an alternative way to gain understanding of high-dimensional data. It also         provides cross-validation and visual adjuncts to the more computationally intensive data mining techniques.      </content></document><document><year>2004</year><authors>Juhnyoung Lee1 | Mark Podlaseck2 | Edith Schonberg3  | Robert Hoch4 </authors><title>Visualization and Analysis of Clickstream Data of Online Stores for Understanding Web Merchandising</title><content>Clickstreams are visitors' paths through a Web site. Analysis of clickstreams shows how a Web site is navigated and used by its visitors. Clickstream data of online stores contains information useful for understanding the effectiveness of marketing and merchandising efforts, such as how customers find the store, what products they see, and what products they purchase. In this paper, we present an interactive visualization system that provides users with greater abilities to interpret and explore clickstream data of online stores. This system visualizes the effectiveness of Web merchandising from two different points of view by using two different visualization techniques: visualization of sessions by using parallel coordinates and visualization of product performance by using starfield graphs. Furthermore, this system provides facilities for zooming, filtering, color-coding, dynamic querying and data sampling. It also provides summary information along with visualizations, and by maintaining a connection between visualizations and the source database, it dynamically updates the summary information. To demonstrate how the presented visualization system provides capabilities for examining online store clickstreams, we present a series of parallel coordinates and starfield visualizations that display clickstream data from an operating online retail store. A framework for understanding Web merchandising is briefly explained. A set of metrics referred to as micro-conversion rates, which are defined for Web merchandising analysis in our previous work (Lee et al., Electronic Markets, 2000), is also explained and used for the visualizations of online store effectiveness.</content></document><document><year>2004</year><authors>Ron Kohavi1| Brij Mas|2| Myra Spiliopoulou3 | Jaideep Srivastava4</authors><title>Web Mining</title><content>Without Abstract</content></document><document><year>2004</year><authors>Minos Garofalakis1 | Aristides Gionis2 | Rajeev Rastogi1 | S. Seshadri3  | Kyuseok Shim4 </authors><title>XTRACT: Learning Document Type Descriptors from XML Document Collections</title><content>XML is rapidly emerging as the new standard for data representation and exchange on the Web. Unlike HTML, tags in XML documents describe the semantics of the data and not how it is to be displayed. In addition, an XML document can be accompanied by a Document Type Descriptor (DTD) which plays the role of a schema for an XML data collection. DTDs contain valuable information on the structure of documents and thus have a crucial role in the efficient storage of XML data, as well as the effective formulation and optimization of XML queries. Despite their importance, however, DTDs are not mandatory, and it is frequently possible that documents in XML databases will not have accompanying DTDs. In this paper, we propose XTRACT, a novel system for inferring a DTD schema for a database of XML documents. Since the DTD syntax incorporates the full expressive power of regular expressions, naive approaches typically fail to produce concise and intuitive DTDs. Instead, the XTRACT inference algorithms employ a sequence of sophisticated steps that involve: (1) finding patterns in the input sequences and replacing them with regular expressions to generate general candidate DTDs, (2) factoring candidate DTDs using adaptations of algorithms from the logic optimization literature, and (3) applying the Minimum Description Length (MDL) principle to find the best DTD among the candidates. The results of our experiments with real-life and synthetic DTDs demonstrate the effectiveness of XTRACT's approach in inferring concise and semantically meaningful DTD schemas for XML databases.</content></document><document><year>2009</year><authors>Qiang-Li Zhao1 | Yan-Huang Jiang1  | Ming Xu1</authors><title>A fast ensemble pruning algorithm based on pattern mining process      </title><content>Ensemble pruning deals with the reduction of base classifiers prior to combination in order to improve generalization and         prediction efficiency. Existing ensemble pruning algorithms require much pruning time. This paper presents a fast pruning         approach: pattern mining based ensemble pruning (PMEP). In this algorithm, the prediction results of all base classifiers         are organized as a transaction database, and FP-Tree structure is used to compact the prediction results. Then a greedy pattern         mining method is explored to find the ensemble of size k. After obtaining the ensembles of all possible sizes, the one with the best accuracy is outputted. Compared with Bagging,         GASEN, and Forward Selection, experimental results show that PMEP achieves the best prediction accuracy and keeps the size         of the final ensemble small, more importantly, its pruning time is much less than other ensemble pruning algorithms.      </content></document><document><year>2009</year><authors>En Tzu Wang1  | Arbee L. P. Chen2 </authors><title>A novel hash-based approach for mining frequent itemsets over data streams requiring less memory space      </title><content>In recent times, data are generated as a form of continuous data streams in many applications. Since handling data streams         is necessary and discovering knowledge behind data streams can often yield substantial benefits, mining over data streams         has become one of the most important issues. Many approaches for mining frequent itemsets over data streams have been proposed.         These approaches often consist of two procedures including continuously maintaining synopses for data streams and finding         frequent itemsets from the synopses. However, most of the approaches assume that the synopses of data streams can be saved         in memory and ignore the fact that the information of the non-frequent itemsets kept in the synopses may cause memory utilization         to be significantly degraded. In this paper, we consider compressing the information of all the itemsets into a structure         with a fixed size using a hash-based technique. This hash-based approach skillfully summarizes the information of the whole         data stream by using a hash table, provides a novel technique to estimate the support counts of the non-frequent itemsets,         and keeps only the frequent itemsets for speeding up the mining process. Therefore, the goal of optimizing memory space utilization         can be achieved. The correctness guarantee, error analysis, and parameter setting of this approach are presented and a series         of experiments is performed to show the effectiveness and the efficiency of this approach.      </content></document><document><year>2009</year><authors>Dora Alvarez1  | Hugo Hidalgo1 </authors><title>Document analysis and visualization with zero-inflated poisson      </title><content>Data visualization is aimed at obtaining a graphic representation of high dimensional information. A data projection over         a lower dimensional space is pursued, looking for some structure on the projections. Among the several data projection based         methods available, the Generative Topographic Mapping (GTM) has become an important probabilistic framework to model data.         The application to document data requires a change in the original (Gaussian) model in order to consider binary or multinomial         variables. There have been several modifications on GTM to consider this kind of data, but the resulting latent projections         are all scattered on the visualization plane. A document visualization method is proposed in this paper, based on a generative         probabilistic model consisting of a mixture of Zero-inflated Poisson distributions. The performance of the method is evaluated         in terms of cluster forming for the latent projections with an index based on Fisher&amp;#8217;s classifier, and the topology preservation         capability is measured with the Sammon&amp;#8217;s stress error. A comparison with the GTM implementation with Gaussian, multinomial         and Poisson distributions and with a Latent Dirichlet model is presented, observing a greater performance for the proposed         method. A graphic presentation of the projections is also provided, showing the advantage of the developed method in terms         of visualization and class separation. A detailed analysis of some documents projected on the latent representation showed         that most of the documents appearing away from the corresponding cluster could be identified as outliers.      </content></document><document><year>2009</year><authors>C. I. Ezeife1  | Yi Liu1 </authors><title>Fast incremental mining of web sequential patterns with PLWAP tree      </title><content>Point and click at web pages generate continuous data sequences, which flow into the web log data, causing the need to update         previously mined web sequential patterns. Algorithms for mining web sequential patterns from scratch include WAP, PLWAP and         Apriori-based GSP. Reusing old patterns with only recent additional data sequences in an incremental fashion, when updating         patterns, would achieve fast response time with reasonable memory space usage. This paper proposes two algorithms, RePL4UP         (Revised PLWAP For UPdate), and PL4UP (PLWAP For UPdate), which use the PLWAP tree structure to incrementally update web sequential         patterns efficiently without scanning the whole database even when previous small items become frequent. The RePL4UP concisely         stores the position codes of small items in the database sequences in its metadata during tree construction. During mining,         RePL4UP scans only the new additional database sequences, revises the old PLWAP tree to restore information on previous small         items that have become frequent, while it deletes previous frequent items that have become small using the small item position         codes. PL4UP initially builds a bigger PLWAP tree that includes all sequences in the database using a tolerance support, t, that is lower than the regular minimum support, s. The position code features of the PLWAP tree are used to efficiently mine these trees to extract current frequent patterns         when the database is updated. These approaches more quickly update old frequent patterns without the need to re-scan the entire         updated database.      </content></document><document><year>2009</year><authors>C. I. Ezeife1  | Yi Liu1 </authors><title>Fast incremental mining of web sequential patterns with PLWAP tree      </title><content>Without Abstract</content></document><document><year>2009</year><authors>Sattar Hashemi1  | Ying Yang2 </authors><title>Flexible decision tree for data stream classification in the presence of concept change, noise and missing values      </title><content>In recent years, classification learning for data streams has become an important and active research topic. A major challenge         posed by data streams is that their underlying concepts can change over time, which requires current classifiers to be revised         accordingly and timely. To detect concept change, a common methodology is to observe the online classification accuracy. If         accuracy drops below some threshold value, a concept change is deemed to have taken place. An implicit assumption behind this         methodology is that any drop in classification accuracy can be interpreted as a symptom of concept change. Unfortunately however,         this assumption is often violated in the real world where data streams carry noise that can also introduce a significant reduction         in classification accuracy. To compound this problem, traditional noise cleansing methods are incompetent for data streams.         Those methods normally need to scan data multiple times whereas learning for data streams can only afford one-pass scan because         of data&amp;#8217;s high speed and huge volume. Another open problem in data stream classification is how to deal with missing values.         When new instances containing missing values arrive, how a learning model classifies them and how the learning model updates         itself according to them is an issue whose solution is far from being explored. To solve these problems, this paper proposes         a novel classification algorithm, flexible decision tree (FlexDT), which extends fuzzy logic to data stream classification.         The advantages are three-fold. First, FlexDT offers a flexible structure to effectively and efficiently handle concept change. Second, FlexDT is robust to noise. Hence it can prevent noise         from interfering with classification accuracy, and accuracy drop can be safely attributed to concept change. Third, it deals         with missing values in an elegant way. Extensive evaluations are conducted to compare FlexDT with representative existing         data stream classification algorithms using a large suite of data streams and various statistical tests. Experimental results         suggest that FlexDT offers a significant benefit to data stream classification in real-world scenarios where concept change,         noise and missing values coexist.      </content></document><document><year>2009</year><authors>Jens HГјhn1  | Eyke HГјllermeier1 </authors><title>FURIA: an algorithm for unordered fuzzy rule induction      </title><content>This paper introduces a novel fuzzy rule-based classification method called FURIA, which is short for Fuzzy Unordered Rule         Induction Algorithm. FURIA extends the well-known RIPPER algorithm, a state-of-the-art rule learner, while preserving its         advantages, such as simple and comprehensible rule sets. In addition, it includes a number of modifications and extensions.         In particular, FURIA learns fuzzy rules instead of conventional rules and unordered rule sets instead of rule lists. Moreover,         to deal with uncovered examples, it makes use of an efficient rule stretching method. Experimental results show that FURIA         significantly outperforms the original RIPPER, as well as other classifiers such as C4.5, in terms of classification accuracy.      </content></document><document><year>2009</year><authors>Han-Shen Huang1 | Bo-Hou Yang1| 2 | Yu-Ming Chang1  | Chun-Nan Hsu1 </authors><title>Global and componentwise extrapolations for accelerating training of Bayesian networks and conditional random fields      </title><content>The triple jump extrapolation method is an effective approximation of Aitken&amp;#8217;s acceleration that can accelerate the convergence         of many algorithms for data mining, including EM and generalized iterative scaling (GIS). It has two options&amp;#8212;global and componentwise         extrapolation. Empirical studies showed that neither can dominate the other and it is not known which one is better under         what condition. In this paper, we investigate this problem and conclude that, when the Jacobian is (block) diagonal, componentwise         extrapolation will be more effective. We derive two hints to determine the block diagonality. The first hint is that when         we have a highly sparse data set, the Jacobian of the EM mapping for training a Bayesian network will be block diagonal. The         second is that the block diagonality of the Jacobian of the GIS mapping for training CRF is negatively correlated with the         strength of feature dependencies. We empirically verify these hints with controlled and real-world data sets and show that         our hints can accurately predict which method will be superior. We also show that both global and componentwise extrapolation         can provide substantial acceleration. In particular, when applied to train large-scale CRF models, the GIS variant accelerated         by componentwise extrapolation not only outperforms its global extrapolation counterpart, as our hint predicts, but can also         compete with limited-memory BFGS (L-BFGS), the de facto standard for CRF training, in terms of both computational efficiency         and F-scores. Though none of the above methods are as fast as stochastic gradient descent (SGD), careful tuning is required         for SGD and the results given in this paper provide a useful foundation for automatic tuning.      </content></document><document><year>2009</year><authors>Aleks|er Kolcz1 | Dunja Mladenic2| Wray Buntine3| 4| Marko Grobelnik2 | John Shawe-Taylor5</authors><title>Guest editors&amp;#8217; introduction: special issue of selected papers from ECML PKDD 2009      </title><content>Without Abstract</content></document><document><year>2009</year><authors>Philipp Kranen1  | Thomas Seidl1 </authors><title>Harnessing the strengths of anytime algorithms for constant data streams      </title><content>Anytime algorithms have been proposed for many different applications, e.g., in data mining. Their strengths are the ability         to first provide a result after a very short initialization and second to improve their result with additional time. Therefore,         anytime algorithms have so far been used when the available processing time varies, e.g., on varying data streams. In this         paper we propose to employ anytime algorithms on constant data streams, i.e., for tasks with constant time allowance. We introduce         two approaches that harness the strengths of anytime algorithms on constant data streams and thereby improve the over all         quality of the result with respect to the corresponding budget;algorithm. We derive formulas for the expected performance         gain and demonstrate the effectiveness of our novel approaches using existing anytime algorithms on benchmark data sets.      </content></document><document><year>2009</year><authors>Jin Shieh1  | Eamonn Keogh1 </authors><title>         iSAX: disk-aware mining and indexing of massive time series datasets      </title><content>Current research in indexing and mining time series data has produced many interesting algorithms and representations. However,         the algorithms and the size of data considered have generally not been representative of the increasingly massive datasets         encountered in science, engineering, and business domains. In this work, we introduce a novel multi-resolution symbolic representation         which can be used to index datasets which are several orders of magnitude larger than anything else considered in the literature.         To demonstrate the utility of this representation, we constructed a simple tree-based index structure which facilitates fast         exact search and orders of magnitude faster, approximate search. For example, with a database of one-hundred million time         series, the approximate search can retrieve high quality nearest neighbors in slightly over a second, whereas a sequential         scan would take tens of minutes. Our experimental evaluation demonstrates that our representation allows index performance         to scale well with increasing dataset sizes. Additionally, we provide analysis concerning parameter sensitivity, approximate         search effectiveness, and lower bound comparisons between time series representations in a bit constrained environment. We         further show how to exploit the combination of both exact and approximate search as sub-routines in data mining algorithms,         allowing for the exact mining of truly massive real world datasets, containing tens of millions of time series.      </content></document><document><year>2009</year><authors>Matthijs van Leeuwen1 | Jilles Vreeken1  | Arno Siebes1 </authors><title>Identifying the components      </title><content>Most, if not all, databases are mixtures of samples from different distributions. Transactional data is no exception. For         the prototypical example, supermarket basket analysis, one also expects a mixture of different buying patterns. Households         of retired people buy different collections of items than households with young children. Models that take such underlying         distributions into account are in general superior to those that do not. In this paper we introduce two MDL-based algorithms         that follow orthogonal approaches to identify the components in a transaction database. The first follows a model-based approach,         while the second is data-driven. Both are parameter-free: the number of components and the components themselves are chosen         such that the combined complexity of data and models is minimised. Further, neither prior knowledge on the distributions nor         a distance metric on the data is required. Experiments with both methods show that highly characteristic components are identified.      </content></document><document><year>2009</year><authors>Guoliang Li1 | Jianhua Feng1 | Jianyong Wang1  | Lizhu Zhou1 </authors><title>Incremental sequence-based frequent query pattern mining from XML queries      </title><content>Existing algorithms of mining frequent XML query patterns (XQPs) employ a candidate generate-and-test strategy. They involve         expensive candidate enumeration and costly tree-containment checking. Further, most of existing methods compute the frequencies         of candidate query patterns from scratch periodically by checking the entire transaction database, which consists of XQPs         transferred from user query logs. However, it is not straightforward to maintain such discovered frequent patterns in real         XML databases as there may be frequent updates that may not only invalidate some existing frequent query patterns but also         generate some new frequent query patterns. Therefore, a drawback of existing methods is that they are rather inefficient for         the evolution of transaction databases. To address above-mentioned problems, this paper proposes an efficient algorithm ESPRIT to mine frequent XQPs without costly tree-containment checking. ESPRIT transforms XML queries into sequences using a one-to-one mapping technique and mines the frequent sequences to generate frequent         XQPs. We propose two efficient incremental algorithms, ESPRIT-i and ESPRIT-i         +, to incrementally mine frequent XQPs. We devise several novel optimization techniques of query rewriting, cache lookup, and         cache replacement to improve the answerability and the hit rate of caching. We have implemented our algorithms and conducted         a set of experimental studies on various datasets. The experimental results demonstrate that our algorithms achieve high efficiency         and scalability and outperform state-of-the-art methods significantly.      </content></document><document><year>2009</year><authors>Henrik Grosskreutz1  | Stefan RГјping1 </authors><title>On subgroup discovery in numerical domains      </title><content>Subgroup discovery is a Knowledge Discovery task that aims at finding subgroups of a population with high generality and distributional         unusualness. While several subgroup discovery algorithms have been presented in the past, they focus on databases with nominal         attributes or make use of discretization to get rid of the numerical attributes. In this paper, we illustrate why the replacement         of numerical attributes by nominal attributes can result in suboptimal results. Thereafter, we present a new subgroup discovery         algorithm that prunes large parts of the search space by exploiting bounds between related numerical subgroup descriptions.         The same algorithm can also be applied to ordinal attributes. In an experimental section, we show that the use of our new         pruning scheme results in a huge performance gain when more that just a few split-points are considered for the numerical         attributes.      </content></document><document><year>2009</year><authors>Leman Akoglu1  | Christos Faloutsos1 </authors><title>RTG: a recursive realistic graph generator using random typing      </title><content>We propose a new, recursive model to generate realistic graphs, evolving over time. Our model has the following properties:         it is (a) flexible, capable of generating the cross product of weighted/unweighted, directed/undirected, uni/bipartite graphs;         (b) realistic, giving graphs that obey eleven static and dynamic laws that real graphs follow (we formally prove that for several of the (power) laws and we estimate their         exponents as a function of the model parameters); (c) parsimonious, requiring only four parameters. (d) fast, being linear on the number of edges; (e) simple, intuitively leading to the generation of macroscopic         patterns. We empirically show that our model mimics two real-world graphs very well: Blognet (unipartite, undirected, unweighted)         with 27 K nodes and 125 K edges; and Committee-to-Candidate campaign donations (bipartite, directed, weighted) with 23 K nodes         and 880 K edges. We also show how to handle time so that edge/weight additions are bursty and self-similar.      </content></document><document><year>2009</year><authors>Hua Yan1| Keke Chen2 | Ling Liu3 | Zhang Yi4 </authors><title>SCALE: a scalable framework for efficiently clustering transactional data      </title><content>This paper presents SCALE, a fully automated transactional clustering framework. The SCALE design highlights three unique         features. First, we introduce the concept of Weighted Coverage Density as a categorical similarity measure for efficient clustering         of transactional datasets. The concept of weighted coverage density is intuitive and it allows the weight of each item in         a cluster to be changed dynamically according to the occurrences of items. Second, we develop the weighted coverage density         measure based clustering algorithm, a fast, memory-efficient, and scalable clustering algorithm for analyzing transactional         data. Third, we introduce two clustering validation metrics and show that these domain specific clustering evaluation metrics         are critical to capture the transactional semantics in clustering analysis. Our SCALE framework combines the weighted coverage         density measure for clustering over a sample dataset with self-configuring methods. These self-configuring methods can automatically         tune the two important parameters of our clustering algorithms: (1) the candidates of the best number K of clusters; and (2) the application of two domain-specific cluster validity measures to find the best result from the set         of clustering results. We have conducted extensive experimental evaluation using both synthetic and real datasets and our         results show that the weighted coverage density approach powered by the SCALE framework can efficiently generate high quality         clustering results in a fully automated manner.      </content></document><document><year>2009</year><authors>Pierre Hansen1 | Jack Brimberg2 | Dragan Uro&amp;#353 evi&amp;#263 3  | Nenad Mladenovi&amp;#263 4 </authors><title>Solving large p-median clustering problems by primal&amp;#8211;dual variable neighborhood search      </title><content>Data clustering methods are used extensively in the data mining literature to detect important patterns in large datasets         in the form of densely populated regions in a multi-dimensional Euclidean space. Due to the complexity of the problem and         the size of the dataset, obtaining quality solutions within reasonable CPU time and memory requirements becomes the central         challenge. In this paper, we solve the clustering problem as a large scale p-median model, using a new approach based on the variable neighborhood search (VNS) metaheuristic. Using a highly efficient         data structure and local updating procedure taken from the OR literature, our VNS procedure is able to tackle large datasets         directly without the need for data reduction or sampling as employed in certain popular methods. Computational results demonstrate         that our VNS heuristic outperforms other local search based methods such as CLARA and CLARANS even after upgrading these procedures         with the same efficient data structures and local search. We also obtain a bound on the quality of the solutions by solving         heuristically a dual relaxation of the problem, thus introducing an important capability to the solution process.      </content></document><document><year>2009</year><authors>V. Lipets1 | N. Vanetik1  | E. Gudes1 </authors><title>Subsea: an efficient heuristic algorithm for subgraph isomorphism      </title><content>We present a novel approach to the problem of finding all subgraphs and induced subgraphs of a (target) graph which are isomorphic         to another (pattern) graph. To attain efficiency we use a special representation of the pattern graph. We also combine our         search algorithm with some known bisection algorithms. Experimental comparison with other algorithms was performed on several         types of graphs. The comparison results suggest that the approach provided here is most effective when all instances of a         subgraph need to be found.      </content></document><document><year>2009</year><authors>Francesco Bonchi1 | Carlos Castillo1 | Debora Donato1  | Aristides Gionis1 </authors><title>Taxonomy-driven lumping for sequence mining      </title><content>Given a taxonomy of events and a dataset of sequences of these events, we study the problem of finding efficient and effective         ways to produce a compact representation of the sequences. We model sequences with Markov models whose states correspond to         nodes in the provided taxonomy, and each state represents the events in the subtree under the corresponding node. By lumping         observed events to states that correspond to internal nodes in the taxonomy, we allow more compact models that are easier         to understand and visualize, at the expense of a decrease in the data likelihood. We formally define and characterize our         problem, and we propose a scalable search method for finding a good trade-off between two conflicting goals: maximizing the         data likelihood, and minimizing the model complexity. We implement these ideas in Taxomo,;a taxonomy-driven modeler, which we apply in two different domains, query-log mining and mining of moving-object trajectories.         The empirical evaluation confirms the feasibility and usefulness of our approach.      </content></document><document><year>2009</year><authors>Ilkka Huopaniemi1 | Tommi Suvitaival1 | Janne NikkilГ¤1| 2 | Matej Ore&amp;#353 i&amp;#269 3  | Samuel Kaski1 </authors><title>Two-way analysis of high-dimensional collinear data      </title><content>We present a Bayesian model for two-way ANOVA-type analysis of high-dimensional, small sample-size datasets with highly correlated         groups of variables. Modern cellular measurement methods are a main application area; typically the task is differential analysis         between diseased and healthy samples, complicated by additional covariates requiring a multi-way analysis. The main complication         is the combination of high dimensionality and low sample size, which renders classical multivariate techniques useless. We         introduce a hierarchical model which does dimensionality reduction by assuming that the input variables come in similarly-behaving         groups, and performs an ANOVA-type decomposition for the set of reduced-dimensional latent variables. We apply the methods         to study lipidomic profiles of a recent large-cohort human diabetes study.      </content></document><document><year>2008</year><authors>Aida de Haro-GarcГ­a1  | NicolГЎs GarcГ­a-Pedrajas1 </authors><title>A divide-and-conquer recursive approach for scaling up instance selection algorithms      </title><content>Instance selection is becoming more and more relevant due to the huge amount of data that is being constantly produced. However,         although current algorithms are useful for fairly large datasets, scaling problems are found when the number of instances         is of hundreds of thousands or millions. In the best case, these algorithms are of efficiency O(n         2), n being the number of instances. When we face huge problems, scalability is an issue, and most algorithms are not applicable.         This paper presents a divide-and-conquer recursive approach to the problem of instance selection for instance based learning         for very large problems. Our method divides the original training set into small subsets where the instance selection algorithm         is applied. Then the selected instances are rejoined in a new training set and the same procedure, partitioning and application         of an instance selection algorithm, is repeated. In this way, our approach is based on the philosophy of divide-and-conquer         applied in a recursive manner. The proposed method is able to match, and even improve, for the case of storage reduction,         the results of well-known standard algorithms with a very significant reduction of execution time. An extensive comparison         in 30 datasets form the UCI Machine Learning Repository shows the usefulness of our method. Additionally, the method is applied         to 5 huge datasets with from 300,000 to more than a million instances, with very good results and fast execution time.      </content></document><document><year>2008</year><authors>Charu C. Aggarwal1  | Philip S. Yu2 </authors><title>A framework for condensation-based anonymization of string data      </title><content>In recent years, privacy preserving data mining has become an important problem because of the large amount of personal data         which is tracked by many business applications. An important method for privacy preserving data mining is the method of condensation.         This method is often used in the case of multi-dimensional data in which pseudo-data is generated to mask the true values         of the records. However, these methods are not easily applicable to the case of string data, since they require the use of         multi-dimensional statistics in order to generate the pseudo-data. String data are especially important in the privacy preserving         data-mining domain because most DNA and biological data are coded as strings. In this article, we will discuss a new method         for privacy preserving mining of string data with the use of simple template-based models. The template-based model turns         out to be effective in practice, and preserves important statistical characteristics of the strings such as intra-record distances.         We will explore the behavior in the context of a classification application, and show that the accuracy of the application         is not affected significantly by the anonymization process.      </content></document><document><year>2008</year><authors>Yu Xia1 </authors><title>A global optimization method for semi-supervised clustering      </title><content>In this paper, we adapt Tuy&amp;#8217;s concave cutting plane method to the semi-supervised clustering. We also give properties of local         optimal solutions of the semi-supervised clustering. Numerical examples show that this method can give a better solution than         other semi-supervised clustering algorithms do.      </content></document><document><year>2008</year><authors>GermГЎn Creamer1| 2  | Sal Stolfo1 </authors><title>A link mining algorithm for earnings forecast and trading      </title><content>The objective of this paper is to present and discuss a link mining algorithm called CorpInterlock and its application to         the financial domain. This algorithm selects the largest strongly connected component of a social network and ranks its vertices         using several indicators of distance and centrality. These indicators are merged with other relevant indicators in order to         forecast new variables using a boosting algorithm. We applied the algorithm CorpInterlock to integrate the metrics of an extended         corporate interlock (social network of directors and financial analysts) with corporate fundamental variables and analysts&amp;#8217;         predictions (consensus). CorpInterlock used these metrics to forecast the trend of the cumulative abnormal return and earnings         surprise of S&amp;amp;P 500 companies. The rationality behind this approach is that the corporate interlock has a direct effect on         future earnings and returns because these variables affect directors and managers&amp;#8217; compensation. The financial analysts engage         in what the agency theory calls the &amp;#8220;earnings game&amp;#8221;: Managers want to meet the financial forecasts of the analysts and analysts         want to increase their compensation or business of the company that they follow. Following the CorpInterlock algorithm, we         calculated a group of well-known social network metrics and integrated with economic variables using Logitboost. We used the         results of the CorpInterlock algorithm to evaluate several trading strategies. We observed an improvement of the Sharpe ratio         (risk-adjustment return) when we used &amp;#8220;long only&amp;#8221; trading strategies with the extended corporate interlock instead of the         basic corporate interlock before the regulation Fair Disclosure (FD) was adopted (1998&amp;#8211;2001). There was no major difference         among the trading strategies after 2001. Additionally, the CorpInterlock algorithm implemented with Logitboost showed a significantly         lower test error than when the CorpInterlock algorithm was implemented with logistic regression. We conclude that the CorpInterlock         algorithm showed to be an effective forecasting algorithm and supported profitable trading strategies.      </content></document><document><year>2008</year><authors>Adrian KГјgel1  | Enno Ohlebusch1 </authors><title>A space efficient solution to the frequent string mining problem for many databases      </title><content>The frequent string mining problem is to find all substrings of a collection of string databases which satisfy database specific         minimum and maximum frequency constraints. Our contribution improves the existing linear-time algorithm for this problem in         such a way that the peak memory consumption is a constant factor of the size of the largest database of strings. We show how         the results for each database can be stored implicitly in space proportional to the size of the database, making it possible         to traverse the results in lexicographical order. Furthermore, we present a linear-time algorithm which calculates the intersection         of the results of different databases. This algorithm is based on an algorithm to merge two suffix arrays, and our modification         allows us to also calculate the LCP table of the resulting suffix array during the merging.      </content></document><document><year>2008</year><authors>Nicolas Cebron1  | Michael R. Berthold1</authors><title>Active learning for object classification: from exploration to exploitation      </title><content>Classifying large datasets without any a-priori information poses a problem in numerous tasks. Especially in industrial environments,         we often encounter diverse measurement devices and sensors that produce huge amounts of data, but we still rely on a human         expert to help give the data a meaningful interpretation. As the amount of data that must be manually classified plays a critical         role, we need to reduce the number of learning episodes involving human interactions as much as possible. In addition for         real world applications it is fundamental to converge in a stable manner to a solution that is close to the optimal solution.         We present a new self-controlled exploration/exploitation strategy to select data points to be labeled by a domain expert         where the potential of each data point is computed based on a combination of its representativeness and the uncertainty of         the classifier. A new Prototype Based Active Learning (PBAC) algorithm for classification is introduced. We compare the results         to other active learning approaches on several benchmark datasets.      </content></document><document><year>2008</year><authors>Arnaud Soulet1  | Bruno CrГ©milleux2 </authors><title>Adequate condensed representations of patterns      </title><content>Patterns are at the core of the discovery of a lot of knowledge from data but their uses are limited due to their huge number         and their mining cost. During the last decade, many works addressed the concept of condensed representation w.r.t. frequency         queries. Such representations are several orders of magnitude smaller than the size of the whole collections of patterns,         and also enable us to regenerate the frequency information of any pattern. In this paper, we propose a framework for condensed         representations w.r.t. a large set of new and various queries named condensable functions based on interestingness measures (e.g., frequency, lift, minimum). Such condensed representations are achieved thanks to         new closure operators automatically derived from each condensable function to get adequate condensed representations. We propose a generic algorithm Mic Mac to efficiently mine the adequate condensed representations. Experiments show both the conciseness of the adequate condensed         representations and the efficiency of our algorithm.      </content></document><document><year>2008</year><authors>Tzu-Tsung Wong1 </authors><title>Alternative prior assumptions for improving the performance of naГЇve Bayesian classifiers      </title><content>The prior distribution of an attribute in a naГЇve Bayesian classifier is typically assumed to be a Dirichlet distribution,         and this is called the Dirichlet assumption. The variables in a Dirichlet random vector can never be positively correlated         and must have the same confidence level as measured by normalized variance. Both the generalized Dirichlet and the Liouville         distributions include the Dirichlet distribution as a special case. These two multivariate distributions, also defined on         the unit simplex, are employed to investigate the impact of the Dirichlet assumption in naГЇve Bayesian classifiers. We propose         methods to construct appropriate generalized Dirichlet and Liouville priors for naГЇve Bayesian classifiers. Our experimental         results on 18 data sets reveal that the generalized Dirichlet distribution has the best performance among the three distribution         families. Not only is the Dirichlet assumption inappropriate, but also forcing the variables in a prior to be all positively         correlated can deteriorate the performance of the naГЇve Bayesian classifier.      </content></document><document><year>2008</year><authors>Vineet Chaoji1 | Mohammad Al Hasan1 | Saeed Salem1  | Mohammed J. Zaki1 </authors><title>An integrated, generic approach to pattern mining: data mining template library      </title><content>Frequent pattern mining (FPM) is an important data mining paradigm to extract informative patterns like itemsets, sequences,         trees, and graphs. However, no practical framework for integrating the FPM tasks has been attempted. In this paper, we describe         the design and implementation of the Data Mining Template Library (DMTL) for FPM. DMTL utilizes a generic data mining approach, where all aspects of mining are controlled via a set of properties. It uses a novel pattern property hierarchy to define and mine different pattern types. This property hierarchy can be thought of as a systematic characterization of         the pattern space, i.e., a meta-pattern specification that allows the analyst to specify new pattern types, by extending this         hierarchy. Furthermore, in DMTL all aspects of mining are controlled by a set of different mining properties. For example, the kind of mining approach to use, the kind of data types and formats to mine over, the kind of back-end storage         manager to use, are all specified as a list of properties. This provides tremendous flexibility to customize the toolkit for         various applications. Flexibility of the toolkit is exemplified by the ease with which support for a new pattern can be added.         Experiments on synthetic and public dataset are conducted to demonstrate the scalability provided by the persistent back-end         in the library. DMTL been publicly released as open-source software (http://dmtl.sourceforge.net/), and has been downloaded by numerous researchers from all over the world.      </content></document><document><year>2008</year><authors>Nitesh V. Chawla1 | David A. Cieslak1 | Lawrence O. Hall2  | Ajay Joshi2 </authors><title>Automatically countering imbalance and its empirical relationship to cost      </title><content>Learning from imbalanced data sets presents a convoluted problem both from the modeling and cost standpoints. In particular,         when a class is of great interest but occurs relatively rarely such as in cases of fraud, instances of disease, and regions         of interest in large-scale simulations, there is a correspondingly high cost for the misclassification of rare events. Under         such circumstances, the data set is often re-sampled to generate models with high minority class accuracy. However, the sampling         methods face a common, but important, criticism: how to automatically discover the proper amount and type of sampling? To address this problem, we propose a wrapper paradigm that discovers the amount of re-sampling for a data set based on optimizing         evaluation functions like the f-measure, Area Under the ROC Curve (AUROC), cost, cost-curves, and the cost dependent f-measure.         Our analysis of the wrapper is twofold. First, we report the interaction between different evaluation and wrapper optimization         functions. Second, we present a set of results in a cost- sensitive environment, including scenarios of unknown or changing         cost matrices. We also compared the performance of the wrapper approach versus cost-sensitive learning methods&amp;#8212;MetaCost and         the Cost-Sensitive Classifiers&amp;#8212;and found the wrapper to outperform the cost-sensitive classifiers in a cost-sensitive environment.         Lastly, we obtained the lowest cost per test example compared to any result we are aware of for the KDD-99 Cup intrusion detection         data set.      </content></document><document><year>2008</year><authors>Manjeet Rege1 | Ming Dong1 | Farshad Fotouhi1</authors><title>Bipartite isoperimetric graph partitioning for data co-clustering      </title><content>Data co-clustering refers to the problem of simultaneous clustering of two data types. Typically, the data is stored in a         contingency or co-occurrence matrix C where rows and columns of the matrix represent the data types to be co-clustered. An entry C                     ij             of the matrix signifies the relation between the data type represented by row i and column j. Co-clustering is the problem of deriving sub-matrices from the larger data matrix by simultaneously clustering rows and         columns of the data matrix. In this paper, we present a novel graph theoretic approach to data co-clustering. The two data         types are modeled as the two sets of vertices of a weighted bipartite graph. We then propose Isoperimetric Co-clustering Algorithm         (ICA)&amp;#8212;a new method for partitioning the bipartite graph. ICA requires a simple solution to a sparse system of linear equations         instead of the eigenvalue or SVD problem in the popular spectral co-clustering approach. Our theoretical analysis and extensive         experiments performed on publicly available datasets demonstrate the advantages of ICA over other approaches in terms of the         quality, efficiency and stability in partitioning the bipartite graph.      </content></document><document><year>2008</year><authors>Michael Brydon1  | Andrew Gemino1</authors><title>Classification trees and decision-analytic feedforward control: a case study from the video game industry      </title><content>The objective of this paper is to use a challenging real-world problem to illustrate how a probabilistic predictive model         can provide the foundation for decision-analytic feedforward control. Commercial data mining software and sales data from         a market research firm are used to create a predictive model of market success in the video game industry. A procedure is         then described for transforming the classification trees into a decision-analytic model that can be solved to produce a value-maximizing         game development policy. The video game example shows how the compact predictive models created by data mining algorithms         can help to make decision-analytic feedforward control feasible, even for large, complex problems. However, the example also         highlights the bounds placed on the practicality of the approach due to combinatorial explosions in the number of contingencies         that have to be modeled. We show, for example, how the &amp;#8220;option value&amp;#8221; of sequels creates complexity that is effectively impossible         to address using conventional decision analysis tools.      </content></document><document><year>2008</year><authors>Jianyong Wang1 | Yuzhou Zhang1| Lizhu Zhou1| George Karypis2 | Charu C. Aggarwal3</authors><title>CONTOUR: an efficient algorithm for discovering discriminating subsequences      </title><content>In recent years we have witnessed several applications of frequent sequence mining, such as feature selection for protein         sequence classification and mining block correlations in storage systems. In typical applications such as clustering, it is         not the complete set but only a subset of discriminating frequent subsequences which is of interest. One approach to discovering         the subset of useful frequent subsequences is to apply any existing frequent sequence mining algorithm to find the complete         set of frequent subsequences. Then, a subset of interesting subsequences can be further identified. Unfortunately, it is very         time consuming to mine the complete set of frequent subsequences for large sequence databases. In this paper, we propose a         new algorithm, CONTOUR, which efficiently mines a subset of high-quality subsequences directly in order to cluster the input         sequences. We mainly focus on how to design some effective search space pruning methods to accelerate the mining process and         discuss how to construct an accurate clustering algorithm based on the result of CONTOUR. We conducted an extensive performance         study to evaluate the efficiency and scalability of CONTOUR, and the accuracy of the frequent subsequence-based clustering         algorithm.      </content></document><document><year>2008</year><authors>Ron Kohavi1 | Roger Longbotham1 | Dan Sommerfield1  | R|al M. Henne1 </authors><title>Controlled experiments on the web: survey and practical guide      </title><content>The web provides an unprecedented opportunity to evaluate ideas quickly using controlled experiments, also called randomized         experiments, A/B tests (and their generalizations), split tests, Control/Treatment tests, MultiVariable Tests (MVT) and parallel         flights. Controlled experiments embody the best scientific design for establishing a causal relationship between changes and         their influence on user-observable behavior. We provide a practical guide to conducting online experiments, where end-users         can help guide the development of features. Our experience indicates that significant learning and return-on-investment (ROI)         are seen when development teams listen to their customers, not to the Highest Paid Person&amp;#8217;s Opinion (HiPPO). We provide several         examples of controlled experiments with surprising results. We review the important ingredients of running controlled experiments,         and discuss their limitations (both technical and organizational). We focus on several areas that are critical to experimentation,         including statistical power, sample size, and techniques for variance reduction. We describe common architectures for experimentation         systems and analyze their advantages and disadvantages. We evaluate randomization and hashing techniques, which we show are         not as simple in practice as is often assumed. Controlled experiments typically generate large amounts of data, which can         be analyzed using data mining techniques to gain deeper understanding of the factors influencing the outcome of interest,         leading to new hypotheses and creating a virtuous cycle of improvements. Organizations that embrace controlled experiments         with clear evaluation criteria can evolve their systems with automated optimizations and real-time analyses. Based on our         extensive practical experience with multiple systems and organizations, we share key lessons that will help practitioners         in running trustworthy controlled experiments.      </content></document><document><year>2008</year><authors>Prithviraj Sen1  | Lise Getoor1 </authors><title>Cost-sensitive learning with conditional Markov networks      </title><content>There has been a recent, growing interest in classification and link prediction in structured domains. Methods such as conditional         random fields and relational Markov networks support flexible mechanisms for modeling correlations due to the link structure.         In addition, in many structured domains, there is an interesting structure in the risk or cost function associated with different         misclassifications. There is a rich tradition of cost-sensitive learning applied to unstructured (IID) data. Here we propose         a general framework which can capture correlations in the link structure and handle structured cost functions. We present two new cost-sensitive structured classifiers based on maximum entropy principles. The first determines         the cost-sensitive classification by minimizing the expected cost of misclassification. The second directly determines the         cost-sensitive classification without going through a probability estimation step. We contrast these approaches with an approach         which employs a standard 0/1-loss structured classifier to estimate class conditional probabilities followed by minimization         of the expected cost of misclassification and with a cost-sensitive IID classifier that does not utilize the correlations         present in the link structure. We demonstrate the utility of our cost-sensitive structured classifiers with experiments on         both synthetic and real-world data.      </content></document><document><year>2008</year><authors>Tao Pei1| 2 | Ajay Jasra3 | David J. H|4 | A.-Xing Zhu1| 5  | Chenghu Zhou1 </authors><title>DECODE: a new method for discovering clusters of different densities in spatial data      </title><content>When clusters with different densities and noise lie in a spatial point set, the major obstacle to classifying these data         is the determination of the thresholds for classification, which may form a series of bins for allocating each point to different         clusters. Much of the previous work has adopted a model-based approach, but is either incapable of estimating the thresholds         in an automatic way, or limited to only two point processes, i.e. noise and clusters with the same density. In this paper,         we present a new density-based cluster method (DECODE), in which a spatial data set is presumed to consist of different point         processes and clusters with different densities belong to different point processes. DECODE is based upon a reversible jump         Markov Chain Monte Carlo (MCMC) strategy and divided into three steps. The first step is to map each point in the data to         its mth nearest distance, which is referred to as the distance between a point and its mth nearest neighbor. In the second step, classification thresholds are determined via a reversible jump MCMC strategy. In         the third step, clusters are formed by spatially connecting the points whose mth nearest distances fall into a particular bin defined by the thresholds. Four experiments, including two simulated data         sets and two seismic data sets, are used to evaluate the algorithm. Results on simulated data show that our approach is capable         of discovering the clusters automatically. Results on seismic data suggest that the clustered earthquakes, identified by DECODE,         either imply the epicenters of forthcoming strong earthquakes or indicate the areas with the most intensive seismicity, this         is consistent with the tectonic states and estimated stress distribution in the associated areas. The comparison between DECODE         and other state-of-the-art methods, such as DBSCAN, OPTICS and Wavelet Cluster, illustrates the contribution of our approach:         although DECODE can be computationally expensive, it is capable of identifying the number of point processes and simultaneously         estimating the classification thresholds with little prior knowledge.      </content></document><document><year>2008</year><authors>Parvathi Chundi1  | Daniel J. Rosenkrantz2 </authors><title>Efficient algorithms for segmentation of item-set time series      </title><content>We propose a special type of time series, which we call an item-set time series, to facilitate the temporal analysis of software version histories, email logs, stock market data, etc. In an item-set time         series, each observed data value is a set of discrete items. We formalize the concept of an item-set time series and present         efficient algorithms for segmenting a given item-set time series. Segmentation of a time series partitions the time series         into a sequence of segments where each segment is constructed by combining consecutive time points of the time series. Each segment is associated with         an item set that is computed from the item sets of the time points in that segment, using a function which we call a measure function. We then define a concept called the segment difference, which measures the difference between the item set of a segment and the item sets of the time points in that segment. The         segment difference values are required to construct an optimal segmentation of the time series. We describe novel and efficient         algorithms to compute segment difference values for each of the measure functions described in the paper. We outline a dynamic         programming based scheme to construct an optimal segmentation of the given item-set time series. We use the item-set time         series segmentation techniques to analyze the temporal content of three different data sets&amp;#8211;Enron email, stock market data,         and a synthetic data set. The experimental results show that an optimal segmentation of item-set time series data captures         much more temporal content than a segmentation constructed based on the number of time points in each segment, without examining         the item set data at the time points, and can be used to analyze different types of temporal data.      </content></document><document><year>2008</year><authors>Li Wei1 | Eamonn Keogh1 | Xiaopeng Xi1  | Melissa Yoder2 </authors><title>Efficiently finding unusual shapes in large image databases      </title><content>Among the visual features of multimedia content, shape is of particular interest because humans can often recognize objects         solely on the basis of shape. Over the past three decades, there has been a great deal of research on shape analysis, focusing         mostly on shape indexing, clustering, and classification. In this work, we introduce the new problem of finding shape discords, the most unusual shapes in a collection. We motivate the problem by considering the utility of shape discords in diverse         domains including zoology, microscopy, anthropology, and medicine. While the brute force search algorithm has quadratic time         complexity, we avoid this untenable lethargy by using locality-sensitive hashing to estimate similarity between shapes which         enables us to reorder the search more efficiently and thus extract the maximum benefit from an admissible pruning strategy         we introduce. An extensive experimental evaluation demonstrates that our approach is empirically linear in time.      </content></document><document><year>2008</year><authors>Yasuki Kakishita1| 4 | Kazutoshi Sasahara2| 3 | Tetsuro Nishino1| Miki Takahasi2 | Kazuo Okanoya2</authors><title>Ethological data mining: an automata-based approach to extract behavioral units and rules      </title><content>We propose an efficient automata-based approach to extract behavioral units and rules from continuous sequential data of animal         behavior. By introducing novel extensions, we integrate two elemental methods&amp;#8212;the N-gram model and Angluin&amp;#8217;s machine learning algorithm into an ethological data mining framework. This allows us to obtain the         minimized automaton-representation of behavioral rules that accept (or generate) the smallest set of possible behavioral patterns         from sequential data of animal behavior. With this method, we demonstrate how the ethological data mining works using real         birdsong data; we use the Bengalese finch song and perform experimental evaluations of this method using artificial birdsong         data generated by a computer program. These results suggest that our ethological data mining works effectively even for noisy         behavioral data by appropriately setting the parameters that we introduce. In addition, we demonstrate a case study using         the Bengalese finch song, showing that our method successfully grasps the core structure of the singing behavior such as loops         and branches.      </content></document><document><year>2008</year><authors>Amol Ghoting1 | Srinivasan Parthasarathy2  | Matthew Eric Otey3 </authors><title>Fast mining of distance-based outliers in high-dimensional datasets      </title><content>Defining outliers by their distance to neighboring data points has been shown to be an effective non-parametric approach to         outlier detection. In recent years, many research efforts have looked at developing fast distance-based outlier detection         algorithms. Several of the existing distance-based outlier detection algorithms report log-linear time performance as a function         of the number of data points on many real low-dimensional datasets. However, these algorithms are unable to deliver the same         level of performance on high-dimensional datasets, since their scaling behavior is exponential in the number of dimensions.         In this paper, we present RBRP, a fast algorithm for mining distance-based outliers, particularly targeted at high-dimensional         datasets. RBRP scales log-linearly as a function of the number of data points and linearly as a function of the number of         dimensions. Our empirical evaluation demonstrates that we outperform the state-of-the-art algorithm, often by an order of         magnitude.      </content></document><document><year>2008</year><authors>Petteri Hintsanen1  | Hannu Toivonen1 </authors><title>Finding reliable subgraphs from large probabilistic graphs      </title><content>Reliable subgraphs can be used, for example, to find and rank nontrivial links between given vertices, to concisely visualize         large graphs, or to reduce the size of input for computationally demanding graph algorithms. We propose two new heuristics         for solving the most reliable subgraph extraction problem on large, undirected probabilistic graphs. Such a problem is specified         by a probabilistic graph G subject to random edge failures, a set of terminal vertices, and an integer K. The objective is to remove K edges from G such that the probability of connecting the terminals in the remaining subgraph is maximized. We provide some technical details         and a rough analysis of the proposed algorithms. The practical performance of the methods is evaluated on real probabilistic         graphs from the biological domain. The results indicate that the methods scale much better to large input graphs, both computationally         and in terms of the quality of the result.      </content></document><document><year>2008</year><authors>Shipra Agrawal1| 2| Jayant R. Haritsa1  | B. Aditya Prakash3</authors><title>FRAPP: a framework for high-accuracy privacy-preserving mining      </title><content>To preserve client privacy in the data mining process, a variety of techniques based on random perturbation of individual         data records have been proposed recently. In this paper, we present FRAPP, a generalized matrix-theoretic framework of random         perturbation, which facilitates a systematic approach to the design of perturbation mechanisms for privacy-preserving mining.         Specifically, FRAPP is used to demonstrate that (a) the prior techniques differ only in their choices for the perturbation         matrix elements, and (b) a symmetric positive-definite perturbation matrix with minimal condition number can be identified,         substantially enhancing the accuracy even under strict privacy requirements. We also propose a novel perturbation mechanism         wherein the matrix elements are themselves characterized as random variables, and demonstrate that this feature provides significant         improvements in privacy at only a marginal reduction in accuracy. The quantitative utility of FRAPP, which is a general-purpose         random-perturbation-based privacy-preserving mining technique, is evaluated specifically with regard to association and classification         rule mining on a variety of real datasets. Our experimental results indicate that, for a given privacy requirement, either         substantially lower modeling errors are incurred as compared to the prior techniques, or the errors are comparable to those         of direct mining on the true database.      </content></document><document><year>2008</year><authors>Gary M. Weiss1 | Bianca Zadrozny2  | Maytal Saar-Tsechansky3 </authors><title>Guest editorial: special issue on utility-based data mining      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Walter Daelemans1| Bart Goethals2  | Katharina Morik3</authors><title>Guest Editors&amp;#8217; Introduction: Special issue of Selected Papers from ECML PKDD 2008      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Mehrdad Mahdavi1  | Hassan Abolhassani1| 2 </authors><title>Harmony K-means algorithm for document clustering      </title><content>Fast and high quality document clustering is a crucial task in organizing information, search engine results, enhancing web         crawling, and information retrieval or filtering. Recent studies have shown that the most commonly used partition-based clustering         algorithm, the K-means algorithm, is more suitable for large datasets. However, the K-means algorithm can generate a local optimal solution. In this paper we propose a novel Harmony K-means Algorithm (HKA) that deals with document clustering based on Harmony Search (HS) optimization method. It is proved         by means of finite Markov chain theory that the HKA converges to the global optimum. To demonstrate the effectiveness and         speed of HKA, we have applied HKA algorithms on some standard datasets. We also compare the HKA with other meta-heuristic         and model-based document clustering approaches. Experimental results reveal that the HKA algorithm converges to the best known         optimum faster than other methods and the quality of clusters are comparable.      </content></document><document><year>2008</year><authors>Shumeet Baluja1  | Michele Covell1 </authors><title>Learning to hash: forgiving hash functions and applications      </title><content>The problem of efficiently finding similar items in a large corpus of high-dimensional data points arises in many real-world         tasks, such as music, image, and video retrieval. Beyond the scaling difficulties that arise with lookups in large data sets,         the complexity in these domains is exacerbated by an imprecise definition of similarity. In this paper, we describe a method         to learn a similarity function from only weakly labeled positive examples. Once learned, this similarity function is used         as the basis of a hash function to severely constrain the number of points considered for each lookup. Tested on a large real-world         audio dataset, only a tiny fraction of the points (~0.27%) are ever considered for each lookup. To increase efficiency, no         comparisons in the original high-dimensional space of points are required. The performance far surpasses, in terms of both         efficiency and accuracy, a state-of-the-art Locality-Sensitive-Hashing-based (LSH) technique for the same problem and data         set.      </content></document><document><year>2008</year><authors>Chedy RaГЇssi1| 3 | Toon Calders2  | Pascal Poncelet3 </authors><title>Mining conjunctive sequential patterns      </title><content>In this paper we aim at extending the non-derivable condensed representation in frequent itemset mining to sequential pattern         mining. We start by showing a negative example: in the context of frequent sequences, the notion of non-derivability is meaningless.         Therefore, we extend our focus to the mining of conjunctions of sequences. Besides of being of practical importance, this         class of patterns has some nice theoretical properties. Based on a new unexploited theoretical definition of equivalence classes         for sequential patterns, we are able to extend the notion of a non-derivable itemset to the sequence domain. We present a         new depth-first approach to mine non-derivable conjunctive sequential patterns and show its use in mining association rules         for sequences. This approach is based on a well known combinatorial theorem: the MГ¶bius inversion. A performance study using         both synthetic and real datasets illustrates the efficiency of our mining algorithm. These new introduced patterns have a         high-potential for real-life applications, especially for network monitoring and biomedical fields with the ability to get         sequential association rules with all the classical statistical metrics such as confidence, conviction, lift etc.      </content></document><document><year>2008</year><authors>Jukka Cor|er1 | Magnus Ekdahl2 | Timo Koski3</authors><title>Parallell interacting MCMC for learning of topologies of graphical models      </title><content>Automated statistical learning of graphical models from data has attained a considerable degree of interest in the machine         learning and related literature. Many authors have discussed and/or demonstrated the need for consistent stochastic search         methods that would not be as prone to yield locally optimal model structures as simple greedy methods. However, at the same         time most of the stochastic search methods are based on a standard Metropolis&amp;#8211;Hastings theory that necessitates the use of         relatively simple random proposals and prevents the utilization of intelligent and efficient search operators. Here we derive         an algorithm for learning topologies of graphical models from samples of a finite set of discrete variables by utilizing and         further enhancing a recently introduced theory for non-reversible parallel interacting Markov chain Monte Carlo-style computation.         In particular, we illustrate how the non-reversible approach allows for novel type of creativity in the design of search operators.         Also, the parallel aspect of our method illustrates well the advantages of the adaptive nature of search operators to avoid         trapping states in the vicinity of locally optimal network topologies.      </content></document><document><year>2008</year><authors>Lior Rokach1 | Lihi Naamani2  | Armin Shmilovici1 </authors><title>Pessimistic cost-sensitive active learning of decision trees for profit maximizing targeting campaigns      </title><content>In business applications such as direct marketing, decision-makers are required to choose the action which best maximizes         a utility function. Cost-sensitive learning methods can help them achieve this goal. In this paper, we introduce Pessimistic         Active Learning (PAL). PAL employs a novel pessimistic measure, which relies on confidence intervals and is used to balance         the exploration/exploitation trade-off. In order to acquire an initial sample of labeled data, PAL applies orthogonal arrays         of fractional factorial design. PAL was tested on ten datasets using a decision tree inducer. A comparison of these results         to those of other methods indicates PAL&amp;#8217;s superiority.      </content></document><document><year>2008</year><authors>Tom Fawcett1 </authors><title>PRIE: a system for generating rulelists to maximize ROC performance      </title><content>Rules are commonly used for classification because they are modular, intelligible and easy to learn. Existing work in classification         rule learning assumes the goal is to produce categorical classifications to maximize classification accuracy. Recent work         in machine learning has pointed out the limitations of classification accuracy: when class distributions are skewed, or error         costs are unequal, an accuracy maximizing classifier can perform poorly. This paper presents a method for learning rules directly         from ROC space when the goal is to maximize the area under the ROC curve (AUC). Basic principles from rule learning and computational         geometry are used to focus the search for promising rule combinations. The result is a system that can learn intelligible         rulelists with good ROC performance.      </content></document><document><year>2008</year><authors>George Forman1 </authors><title>Quantifying counts and costs via classification      </title><content>Many business applications track changes over time, for example, measuring the monthly prevalence of influenza incidents.         In situations where a classifier is needed to identify the relevant incidents, imperfect classification accuracy can cause         substantial bias in estimating class prevalence. The paper defines two research challenges for machine learning. The &amp;#8216;quantification&amp;#8217;         task is to accurately estimate the number of positive cases (or class distribution) in a test set, using a training set that         may have a substantially different distribution. The &amp;#8216;cost quantification&amp;#8217; variant estimates the total cost associated with         the positive class, where each case is tagged with a cost attribute, such as the expense to resolve the case. Quantification         has a very different utility model from traditional classification research. For both forms of quantification, the paper describes         a variety of methods and evaluates them with a suitable methodology, revealing which methods give reliable estimates when         training data is scarce, the testing class distribution differs widely from training, and the positive class is rare, e.g.,         1% positives. These strengths can make quantification practical for business use, even where classification accuracy is poor.      </content></document><document><year>2008</year><authors>Szymon Jaroszewicz1 | Tobias Scheffer2  | Dan A. Simovici3 </authors><title>Scalable pattern mining with Bayesian networks as background knowledge      </title><content>We study a discovery framework in which background knowledge on variables and their relations within a discourse area is available         in the form of a graphical model. Starting from an initial, hand-crafted or possibly empty graphical model, the network evolves         in an interactive process of discovery. We focus on the central step of this process: given a graphical model and a database,         we address the problem of finding the most interesting attribute sets. We formalize the concept of interestingness of attribute         sets as the divergence between their behavior as observed in the data, and the behavior that can be explained given the current         model. We derive an exact algorithm that finds all attribute sets whose interestingness exceeds a given threshold. We then         consider the case of a very large network that renders exact inference unfeasible, and a very large database or data stream.         We devise an algorithm that efficiently finds the most interesting attribute sets with prescribed approximation bound and         confidence probability, even for very large networks and infinite streams. We study the scalability of the methods in controlled         experiments; a case-study sheds light on the practical usefulness of the approach.      </content></document><document><year>2008</year><authors>Apostolos N. Papadopoulos1 | Apostolos Lyritsis1  | Yannis Manolopoulos1 </authors><title>SkyGraph: an algorithm for important subgraph discovery in relational graphs      </title><content>A significant number of applications require effective and efficient manipulation of relational graphs, towards discovering         important patterns. Some example applications are: (i) analysis of microarray data in bioinformatics, (ii) pattern discovery         in a large graph representing a social network, (iii) analysis of transportation networks, (iv) community discovery in Web         data. The basic approach followed by existing methods is to apply mining techniques on graph data to discover important patterns,         such as subgraphs that are likely to be useful. However, in some cases the number of mined patterns is large, posing difficulties         in selecting the most important ones. For example, applying frequent subgraph mining on a set of graphs the system returns         all connected subgraphs whose frequency is above a specified (usually user-defined) threshold. The number of discovered patterns         may be large, and this number depends on the data characteristics and the frequency threshold specified. It would be more         convenient for the user if &amp;#8220;goodness&amp;#8221; criteria could be set to evaluate the usefulness of these patterns, and if the user         could provide preferences to the system regarding the characteristics of the discovered patterns. In this paper, we propose         a methodology to support such preferences by applying subgraph discovery in relational graphs towards retrieving important         connected subgraphs. The importance of a subgraph is determined by: (i) the order of the subgraph (the number of vertices)         and (ii) the subgraph edge connectivity. The performance of the proposed technique is evaluated by using real-life as well         as synthetically generated data sets.      </content></document><document><year>2008</year><authors>Erik Linstead1 | Sushil Bajracharya1 | Trung Ngo1 | Paul Rigor1 | Cristina Lopes1  | Pierre Baldi1 </authors><title>Sourcerer: mining and searching internet-scale software repositories      </title><content>Large repositories of source code available over the Internet, or within large organizations, create new challenges and opportunities         for data mining and statistical machine learning. Here we first develop Sourcerer, an infrastructure for the automated crawling,         parsing, fingerprinting, and database storage of open source software on an Internet-scale. In one experiment, we gather 4,632         Java projects from SourceForge and Apache totaling over 38;million lines of code from 9,250 developers. Simple statistical         analyses of the data first reveal robust power-law behavior for package, method call, and lexical containment distributions.         We then develop and apply unsupervised, probabilistic, topic and author-topic (AT) models to automatically discover the topics         embedded in the code and extract topic-word, document-topic, and AT distributions. In addition to serving as a convenient         summary for program function and developer activities, these and other related distributions provide a statistical and information-theoretic         basis for quantifying and analyzing source file similarity, developer similarity and competence, topic scattering, and document         tangling, with direct applications to software engineering an software development staffing. Finally, by combining software         textual content with structural information captured by our CodeRank approach, we are able to significantly improve software         retrieval performance, increasing the area under the curve (AUC) retrieval metric to 0.92&amp;#8211; roughly 10&amp;#8211;30% better than previous         approaches based on text alone. A prototype of the system is available at: http://sourcerer.ics.uci.edu.      </content></document><document><year>2008</year><authors>Pauli Miettinen1 </authors><title>The Boolean column and column-row matrix decompositions      </title><content>Matrix decompositions are used for many data mining purposes. One of these purposes is to find a concise but interpretable         representation of a given data matrix. Different decomposition formulations have been proposed for this task, many of which         assume a certain property of the input data (e.g., nonnegativity) and aim at preserving that property in the decomposition.         In this paper we propose new decomposition formulations for binary matrices, namely the Boolean CX and CUR decompositions.         They are natural combinations of two previously presented decomposition formulations. We consider also two subproblems of         these decompositions and present a rigorous theoretical study of the subproblems. We give algorithms for the decompositions         and for the subproblems, and study their performance via extensive experimental evaluation. We show that even simple algorithms         can give accurate and intuitive decompositions of real data, thus demonstrating the power and usefulness of the proposed decompositions.      </content></document><document><year>2008</year><authors>C. Whitrow1 | D. J. H|1| 2| P. Juszczak1| D. Weston1 | N. M. Adams2</authors><title>Transaction aggregation as a strategy for credit card fraud detection      </title><content>The problem of preprocessing transaction data for supervised fraud classification is considered. It is impractical to present         an entire series of transactions to a fraud detection system, partly because of the very high dimensionality of such data         but also because of the heterogeneity of the transactions. Hence, a framework for transaction aggregation is considered and         its effectiveness is evaluated against transaction-level detection, using a variety of classification methods and a realistic         cost-based performance measure. These methods are applied in two case studies using real data. Transaction aggregation is         found to be advantageous in many but not all circumstances. Also, the length of the aggregation period has a large impact         upon performance. Aggregation seems particularly effective when a random forest is used for classification. Moreover, random         forests were found to perform better than other classification methods, including SVMs, logistic regression and KNN. Aggregation         also has the advantage of not requiring precisely labeled data and may be more robust to the effects of population drift.      </content></document><document><year>2008</year><authors>Jimeng Sun1 | Charalampos E. Tsourakakis2| Evan Hoke3| Christos Faloutsos2 | Tina Eliassi-Rad4</authors><title>Two heads better than one: pattern discovery in time-evolving multi-aspect data      </title><content>Data stream values are often associated with multiple aspects. For example each value observed at a given time-stamp from environmental sensors may have an associated type (e.g., temperature,         humidity, etc.) as well as location. Time-stamp, type and location are the three aspects, which can be modeled using a tensor         (high-order array). However, the time aspect is special, with a natural ordering, and with successive time-ticks having usually         correlated values. Standard multiway analysis ignores this structure. To capture it, we propose 2 Heads Tensor Analysis (2-heads), which provides a qualitatively different treatment on time. Unlike most existing approaches that use a PCA-like         summarization scheme for all aspects, 2-heads treats the time aspect carefully. 2-heads combines the power of classic multilinear         analysis with wavelets, leading to a powerful mining tool. Furthermore, 2-heads has several other advantages as well: (a)         it can be computed incrementally in a streaming fashion, (b) it has a provable error guarantee and, (c) it achieves significant         compression ratio against competitors. Finally, we show experiments on real datasets, and we illustrate how 2-heads reveals         interesting trends in the data. This is an extended abstract of an article published in the Data Mining and Knowledge Discovery         journal.      </content></document><document><year>2008</year><authors>Daniel SГЎnchez1| 2 | JosГ© MarГ­a Serrano3| Ignacio Blanco2| Maria Jose MartГ­n-Bautista2 | MarГ­a-Amparo Vila2</authors><title>Using association rules to mine for strong approximate dependencies      </title><content>In this paper we deal with the problem of mining for approximate dependencies (AD) in relational databases. We introduce a         definition of AD based on the concept of association rule, by means of suitable definitions of the concepts of item and transaction.         This definition allow us to measure both the accuracy and support of an AD. We provide an interpretation of the new measures         based on the complexity of the theory (set of rules) that describes the dependence, and we employ this interpretation to compare         the new measures with existing ones. A methodology to adapt existing association rule mining algorithms to the task of discovering         ADs is introduced. The adapted algorithms obtain the set of ADs that hold in a relation with accuracy and support greater         than user-defined thresholds. The experiments we have performed show that our approach performs reasonably well over large         databases with real-world data.      </content></document><document><year>2008</year><authors>Ian Davidson1  | S. S. Ravi2 </authors><title>Using instance-level constraints in agglomerative hierarchical clustering: theoretical and empirical results      </title><content>Clustering with constraints is a powerful method that allows users to specify background knowledge and the expected cluster         properties. Significant work has explored the incorporation of instance-level constraints into non-hierarchical clustering         but not into hierarchical clustering algorithms. In this paper we present a formal complexity analysis of the problem and         show that constraints can be used to not only improve the quality of the resultant dendrogram but also the efficiency of the         algorithms. This is particularly important since many agglomerative style algorithms have running times that are quadratic         (or faster growing) functions of the number of instances to be clustered. We present several bounds on the improvement in         the running times of algorithms obtainable using constraints.      </content></document><document><year>2006</year><authors>Anthony Bagnall1 | Chotirat &amp;#8220 Ann&amp;#8221  Ratanamahatana2 | Eamonn Keogh3 | Stefano Lonardi3  | Gareth Janacek1 </authors><title>A Bit Level Representation for Time Series Data Mining with Shape Based Similarity      </title><content>Clipping is the process of transforming a real valued series into a sequence of bits representing whether each data is above         or below the average. In this paper, we argue that clipping is a useful and flexible transformation for the exploratory analysis         of large time dependent data sets. We demonstrate how time series stored as bits can be very efficiently compressed and manipulated         and that, under some assumptions, the discriminatory power with clipped series is asymptotically equivalent to that achieved         with the raw data. Unlike other transformations, clipped series can be compared directly to the raw data series. We show that         this means we can form a tight lower bounding metric for Euclidean and Dynamic Time Warping distance and hence efficiently         query by content. Clipped data can be used in conjunction with a host of algorithms and statistical tests that naturally follow         from the binary nature of the data. A series of experiments illustrate how clipped series can be used in increasingly complex         ways to achieve better results than other popular representations. The usefulness of the proposed representation is demonstrated         by the fact that the results with clipped data are consistently better than those achieved with a Wavelet or Discrete Fourier         Transformation at the same compression ratio for both clustering and query by content. The flexibility of the representation         is shown by the fact that we can take advantage of a variable Run Length Encoding of clipped series to define an approximation         of the Kolmogorov complexity and hence perform Kolmogorov based clustering.      </content></document><document><year>2006</year><authors>Min Wang1 | Yee Leung2| Chenhu Zhou3| Tao Pei3 | Jiancheng Luo3</authors><title>A Mathematical Morphology Based Scale Space Method for the Mining of Linear Features in Geographic Data      </title><content>This paper presents a spatial data mining method MCAMMO and its extension L_MCAMMO designed for discovering linear and near         linear features in spatial databases. L_MCAMMO can be divided into two basic steps: first, the most suitable re-segmenting         scale is found by MCAMMO, which is a scale space method with mathematical morphology operators; second, the segmented result         at this scale is re-segmented to obtain the final linear belts. These steps are essentially a multi-scale binary image segmentation         process, and can also be treated as hierarchical clustering if we view the points under each connected component as one cluster.         The final number of clusters is the one which survives (relatively, not absolutely) the longest scale range, and the clustering         which first realizes this number of clusters is the most suitable segmentation. The advantages of MCAMMO in general and L_MCAMMO         in particular, are: no need to pre-specify the number of clusters, a small number of simple inputs, capable of extracting         clusters with arbitrary shapes, and robust to noise. The effectiveness of the proposed method is substantiated by the real-life         experiments in the mining of seismic belts in China.      </content></document><document><year>2006</year><authors>Michael Hahsler1 </authors><title>A Model-Based Frequency Constraint for Mining Associations from Transaction Data      </title><content>Mining frequent itemsets is a popular method for finding associated items in databases. For this method, support, the co-occurrence         frequency of the items which form an association, is used as the primary indicator of the associations's significance. A single,         user-specified support threshold is used to decided if associations should be further investigated. Support has some known         problems with rare items, favors shorter itemsets and sometimes produces misleading associations.                     In this paper we develop a novel model-based frequency constraint as an alternative to a single, user-specified minimum support.               The constraint utilizes knowledge of the process generating transaction data by applying a simple stochastic mixture model               (the NB model) which allows for transaction data's typically highly skewed item frequency distribution. A user-specified precision               threshold is used together with the model to find local frequency thresholds for groups of itemsets. Based on the constraint               we develop the notion of NB-frequent itemsets and adapt a mining algorithm to find all NB-frequent itemsets in a database.               In experiments with publicly available transaction databases we show that the new constraint provides improvements over a               single minimum support threshold and that the precision threshold is more robust and easier to set and interpret by the user.            </content></document><document><year>2006</year><authors>Laura M&amp;#259 ru&amp;#351 ter1 | A. J. M. M. (TON) Weijters2 | Wil M. P. Van Der Aalst2  | Antal Van Den Bosch3 </authors><title>A Rule-Based Approach for Process Discovery: Dealing with Noise and Imbalance in Process Logs      </title><content>Effective information systems require the existence of explicit process models. A completely specified process design needs         to be developed in order to enact a given business process. This development is time consuming and often subjective and incomplete.         We propose a method that constructs the process model from process log data, by determining the relations between process         tasks. To predict these relations, we employ machine learning technique to induce rule sets. These rule sets are induced from         simulated process log data generated by varying process characteristics such as noise and log size. Tests reveal that the         induced rule sets have a high predictive accuracy on new data. The effects of noise and imbalance of execution priorities         during the discovery of the relations between process tasks are also discussed. Knowing the causal, exclusive, and parallel         relations, a process model expressed in the Petri net formalism can be built. We illustrate our approach with real world data         in a case study.      </content></document><document><year>2006</year><authors>Didier Dubois1 | Eyke HГјllermeier2  | Henri Prade1 </authors><title>A systematic approach to the assessment of fuzzy association rules      </title><content>In order to allow for the analysis of data sets including numerical attributes, several generalizations of association rule         mining based on fuzzy sets have been proposed in the literature. While the formal specification of fuzzy associations is more         or less straightforward, the assessment of such rules by means of appropriate quality measures is less obvious. Particularly,         it assumes an understanding of the semantic meaning of a fuzzy rule. This aspect has been ignored by most existing proposals,         which must therefore be considered as ad-hoc to some extent. In this paper, we develop a systematic approach to the assessment         of fuzzy association rules. To this end, we proceed from the idea of partitioning the data stored in a database into examples         of a given rule, counterexamples, and irrelevant data. Evaluation measures are then derived from the cardinalities of the         corresponding subsets. The problem of finding a proper partition has a rather obvious solution for standard association rules         but becomes less trivial in the fuzzy case. Our results not only provide a sound justification for commonly used measures         but also suggest a means for constructing meaningful alternatives.      </content></document><document><year>2006</year><authors>Jakob J. Verbeek1 | Jan R. J. Nunnink2  | Nikos Vlassis2 </authors><title>Accelerated EM-based clustering of large data sets</title><content>Motivated by the poor performance (linear complexity) of the EM algorithm in clustering large data sets, and inspired by the successful accelerated versions of related algorithms like k-means, we derive an accelerated variant of the EM algorithm for Gaussian mixtures that: (1) offers speedups that are at least linear in the number of data points, (2) ensures convergence by strictly increasing a lower bound on the data log-likelihood in each learning step, and (3) allows ample freedom in the design of other accelerated variants. We also derive a similar accelerated algorithm for greedy mixture learning, where very satisfactory results are obtained. The core idea is to define a lower bound on the data log-likelihood based on a grouping of data points. The bound is maximized by computing in turn (i) optimal assignments of groups of data points to the mixture components, and (ii) optimal re-estimation of the model parameters based on average sufficient statistics computed over groups of data points. The proposed method naturally generalizes to mixtures of other members of the exponential family. Experimental results show the potential of the proposed method over other state-of-the-art acceleration techniques.</content></document><document><year>2006</year><authors>Jian Pei1 | Jiawei Han2  | Laks V. S. Lakshmanan3 </authors><title>An Erratum on #x201C;Pushing Convertible Constraints in Frequent Itemset Mining#x201D;      </title><content>Without Abstract</content></document><document><year>2006</year><authors>XINGQUAN ZHU1 | XINDONG WU1  | QIJUN CHEN1 </authors><title>Bridging Local and Global Data Cleansing: Identifying Class Noise in Large, Distributed Data Datasets</title><content>To cleanse mislabeled examples from a training dataset for efficient and effective induction, most existing approaches adopt a major set oriented scheme: the training dataset is separated into two parts (a major set and a minor set). The classifiers learned from the major set are used to identify noise in the minor set. The obvious drawbacks of such a scheme are twofold: (1) when the underlying data volume keeps growing, it would be either physically impossible or time consuming to load the major set into the memory for inductive learning; and (2) for multiple or distributed datasets, it can be either technically infeasible or factitiously forbidden to download data from other sites (for security or privacy reasons). Therefore, these approaches have severe limitations in conducting effective global data cleansing from large, distributed datasets.In this paper, we propose a solution to bridge the local and global analysis for noise cleansing. More specifically, the proposed effort tries to identify and eliminate mislabeled data items from large or distributed datasets through local analysis and global incorporation. For this purpose, we make use of distributed datasets or partition a large dataset into subsets, each of which is regarded as a local subset and is small enough to be processed by an induction algorithm at one time to construct a local model for noise identification. We construct good rules from each subset, and use the good rules to evaluate the whole dataset. For a given instance I                                    k                , two error count variables are used to count the number of times it has been identified as noise by all data subsets. The instance with higher error values will have a higher probability of being a mislabeled example. Two threshold schemes, majority and non-objection, are used to identify and eliminate the noisy examples. Experimental results and comparative studies on both real-world and synthetic datasets are reported to evaluate the effectiveness and efficiency of the proposed approach.</content></document><document><year>2006</year><authors>Xiaozhe Wang1 | Kate Smith1  | Rob Hyndman2 </authors><title>Characteristic-Based Clustering for Time Series Data</title><content>With the growing importance of time series clustering research, particularly for similarity searches amongst long time series such as those arising in medicine or finance, it is critical for us to find a way to resolve the outstanding problems that make most clustering methods impractical under certain circumstances. When the time series is very long, some clustering algorithms may fail because the very notation of similarity is dubious in high dimension space; many methods cannot handle missing data when the clustering is based on a distance metric.This paper proposes a method for clustering of time series based on their structural characteristics. Unlike other alternatives, this method does not cluster point values using a distance metric, rather it clusters based on global features extracted from the time series. The feature measures are obtained from each individual series and can be fed into arbitrary clustering algorithms, including an unsupervised neural network algorithm, self-organizing map, or hierarchal clustering algorithm.</content></document><document><year>2006</year><authors>PETER J. ROUSSEEUW1  | KATRIEN VAN DRIESSEN2 </authors><title>Computing LTS Regression for Large Data Sets      </title><content>Data mining aims to extract previously unknown patterns or substructures from large databases. In statistics, this is what         methods of robust estimation and outlier detection were constructed for, see e.g. Rousseeuw and Leroy (1987). Here we will focus on least trimmed squares (LTS) regression, which is based on the subset of h cases (out of n) whose least squares fit possesses the smallest sum of squared residuals. The coverage h may be set between n/2 and n. The computation time of existing LTS algorithms grows too much with the size of the data set, precluding their use for data         mining. In this paper we develop a new algorithm called FAST-LTS. The basic ideas are an inequality involving order statistics         and sums of squared residuals, and techniques which we call &amp;#8216;selective iteration&amp;#8217; and &amp;#8216;nested extensions&amp;#8217;. We also use an         intercept adjustment technique to improve the precision. For small data sets FAST-LTS typically finds the exact LTS, whereas         for larger data sets it gives more accurate results than existing algorithms for LTS and is faster by orders of magnitude.         This allows us to apply FAST-LTS to large databases.      </content></document><document><year>2006</year><authors>ABDELHAMID BOUCHACHIA1  | WITOLD PEDRYCZ2 </authors><title>Data Clustering with Partial Supervision      </title><content>Clustering with partial supervision finds its application in situations where data is neither entirely nor accurately labeled.         This paper discusses a semi-supervised clustering algorithm based on a modified version of the fuzzy C-Means (FCM) algorithm.         The objective function of the proposed algorithm consists of two components. The first concerns traditional unsupervised clustering         while the second tracks the relationship between classes (available labels) and the clusters generated by the first component.         The balance between the two components is tuned by a scaling factor. Comprehensive experimental studies are presented. First,         the discrimination of the proposed algorithm is discussed before its reformulation as a classifier is addressed. The induced         classifier is evaluated on completely labeled data and validated by comparison against some fully supervised classifiers,         namely support vector machines and neural networks. This classifier is then evaluated and compared against three semi-supervised         algorithms in the context of learning from partly labeled data. In addition, the behavior of the algorithm is discussed and         the relation between classes and clusters is investigated using a linear regression model. Finally, the complexity of the         algorithm is briefly discussed.      </content></document><document><year>2006</year><authors>Charles X. Ling1  | Qiang Yang2 </authors><title>Discovering Classification from Data of Multiple Sources</title><content>In many large e-commerce organizations, multiple data sources are often used to describe the same customers, thus it is important to consolidate data of multiple sources for intelligent business decision making. In this paper, we propose a novel method that predicts the classification of data from multiple sources without class labels in each source. We test our method on artificial and real-world datasets, and show that it can classify the data accurately. From the machine learning perspective, our method removes the fundamental assumption of providing class labels in supervised learning, and bridges the gap between supervised and unsupervised learning.</content></document><document><year>2006</year><authors>Jianlin Cheng1 | Michael J. Sweredoski1  | Pierre Baldi1 </authors><title>DOMpro: Protein Domain Prediction Using Profiles, Secondary Structure, Relative Solvent Accessibility, and Recursive Neural         Networks      </title><content>Protein domains are the structural and functional units of proteins. The ability to parse protein chains into different domains         is important for protein classification and for understanding protein structure, function, and evolution. Here we use machine         learning algorithms, in the form of recursive neural networks, to develop a protein domain predictor called DOMpro. DOMpro         predicts protein domains using a combination of evolutionary information in the form of profiles, predicted secondary structure,         and predicted relative solvent accessibility. DOMpro is trained and tested on a curated dataset derived from the CATH database.         DOMpro correctly predicts the number of domains for 69% of the combined dataset of single and multi-domain chains. DOMpro         achieves a sensitivity of 76% and specificity of 85% with respect to the single-domain proteins and sensitivity of 59% and         specificity of 38% with respect to the two-domain proteins. DOMpro also achieved a sensitivity and specificity of 71% and         71% respectively in the Critical Assessment of Fully Automated Structure Prediction 4 (CAFASP-4) (Fisher et al., 1999; Saini and Fischer, 2005) and was ranked among the top ab initio domain predictors. The DOMpro server, software, and dataset are available at http://www.igb.uci.edu/servers/psss.html.               </content></document><document><year>2006</year><authors>Matthew Eric Otey1 | Amol Ghoting1  | Srinivasan Parthasarathy1 </authors><title>Fast Distributed Outlier Detection in Mixed-Attribute Data Sets</title><content>Efficiently detecting outliers or anomalies is an important problem in many areas of science, medicine and information technology. Applications range from data cleaning to clinical diagnosis, from detecting anomalous defects in materials to fraud and intrusion detection. Over the past decade, researchers in data mining and statistics have addressed the problem of outlier detection using both parametric and non-parametric approaches in a centralized setting. However, there are still several challenges that must be addressed. First, most approaches to date have focused on detecting outliers in a continuous attribute space. However, almost all real-world data sets contain a mixture of categorical and continuous attributes. Categorical attributes are typically ignored or incorrectly modeled by existing approaches, resulting in a significant loss of information. Second, there have not been any general-purpose distributed outlier detection algorithms. Most distributed detection algorithms are designed with a specific domain (e.g. sensor networks) in mind. Third, the data sets being analyzed may be streaming or otherwise dynamic in nature. Such data sets are prone to concept drift, and models of the data must be dynamic as well. To address these challenges, we present a tunable algorithm for distributed outlier detection in dynamic mixed-attribute data sets.</content></document><document><year>2006</year><authors>Hui Xiong1 | Pang-Ning Tan2  | Vipin Kumar3 </authors><title>Hyperclique pattern discovery      </title><content>Existing algorithms for mining association patterns often rely on the support-based pruning strategy to prune a combinatorial         search space. However, this strategy is not effective for discovering potentially interesting patterns at low levels of support.         Also, it tends to generate too many spurious patterns involving items which are from different support levels and are poorly         correlated. In this paper, we present a framework for mining highly-correlated association patterns called hyperclique patterns.         In this framework, an objective measure called h-confidence is applied to discover hyperclique patterns. We prove that the         items in a hyperclique pattern have a guaranteed level of global pairwise similarity to one another as measured by the cosine         similarity (uncentered Pearson's correlation coefficient). Also, we show that the h-confidence measure satisfies a cross-support         property which can help efficiently eliminate spurious patterns involving items with substantially different support levels.         Indeed, this cross-support property is not limited to h-confidence and can be generalized to some other association measures.         In addition, an algorithm called hyperclique miner is proposed to exploit both cross-support and anti-monotone properties         of the h-confidence measure for the efficient discovery of hyperclique patterns. Finally, our experimental results show that         hyperclique miner can efficiently identify hyperclique patterns, even at extremely low levels of support.      </content></document><document><year>2006</year><authors>Parvathi Chundi1  | Daniel J. Rosenkrantz2 </authors><title>Information Preserving Time Decompositions of Time Stamped Documents*               </title><content>Extraction of sequences of events from news and other documents based on the publication times of these documents has been         shown to be extremely effective in tracking past events. This paper addresses the issue of constructing an optimal information preserving decomposition of the time period associated with a given document set, i.e., a decomposition with the smallest number of         subintervals, subject to no loss of information. We introduce the notion of the compressed interval decomposition, where each subinterval consists of consecutive time points having identical information content. We define optimality, and         show that any optimal information preserving decomposition of the time period is a refinement of the compressed interval decomposition.         We define several special classes of measure functions (functions that measure the prevalence of keywords in the document set and assign them numeric values), based on their effect         on the information computed as document sets are combined. We give algorithms, appropriate for different classes of measure         functions, for computing an optimal information preserving decomposition of a given document set. We studied the effectiveness         of these algorithms by computing several compressed interval and information preserving decompositions for a subset of the         Reuters&amp;#8211;21578 document set. The experiments support the obvious conclusion that the temporal information gleaned from a document         set is strongly dependent on the measure function used and on other user-defined parameters.      </content></document><document><year>2006</year><authors>Olivier de Vel1 </authors><title>Learning Semi-Structured Document Categorization Using Bounded-Length Spectrum Sub-Sequence Kernels</title><content>In this paper we report an investigation into the learning of semi-structured document categorization. We automatically discover low-level, short-range byte data structure patterns from a document data stream by extracting all byte sub-sequences within a sliding window to form an augmented (or bounded-length) string spectrum feature map and using a modified suffix trie data structure (called the coloured generalized suffix tree or CGST) to efficiently store and manipulate the feature map. Using the CGST we are able to efficiently compute the stream's bounded-length sequence spectrum kernel. We compare the performance of two classifier algorithms to categorize the data streams, namely, the SVM and Naive Bayes (NB) classifiers. Experiments have provided good classification performance results on a variety of document byte streams, particularly when using the NB classifier under certain parameter settings. Results indicate that the bounded-length kernel is superior to the standard fixed-length kernel for semi-structured documents.</content></document><document><year>2006</year><authors>Jun Yan1 | Ning Liu2 | Qiang Yang3 | Benyu Zhang4 | Qiansheng Cheng1  | Zheng Chen4 </authors><title>Mining Adaptive Ratio Rules from Distributed Data Sources</title><content>Different from traditional association-rule mining, a new paradigm called Ratio Rule (RR) was proposed recently. Ratio rules are aimed at capturing the quantitative association knowledge, We extend this framework to mining ratio rules from distributed and dynamic data sources. This is a novel and challenging problem. The traditional techniques used for ratio rule mining is an eigen-system analysis which can often fall victim to noise. This has limited the application of ratio rule mining greatly. The distributed data sources impose additional constraints for the mining procedure to be robust in the presence of noise, because it is difficult to clean all the data sources in real time in real-world tasks. In addition, the traditional batch methods for ratio rule mining cannot cope with dynamic data. In this paper, we propose an integrated method to mining ratio rules from distributed and changing data sources, by first mining the ratio rules from each data source separately through a novel robust and adaptive one-pass algorithm (which is called Robust and Adaptive Ratio Rule (RARR)), and then integrating the rules of each data source in a simple probabilistic model. In this way, we can acquire the global rules from all the local information sources adaptively. We show that the RARR technique can converge to a fixed point and is robust as well. Moreover, the integration of rules is efficient and effective. Both theoretical analysis and experiments illustrate that the performance of RARR and the proposed information integration procedure is satisfactory for the purpose of discovering latent associations in distributed dynamic data source.</content></document><document><year>2006</year><authors>Ying Yang1 | Xindong Wu2  | Xingquan Zhu2 </authors><title>Mining in Anticipation for Concept Change: Proactive-Reactive Prediction in Data Streams</title><content>Prediction in streaming data is an important activity in the modern society. Two major challenges posed by data streams are (1) the data may grow without limit so that it is difficult to retain a long history of raw data; and (2) the underlying concept of the data may change over time. The novelties of this paper are in four folds. First, it uses a measure of conceptual equivalence to organize the data history into a history of concepts. This contrasts to the common practice that only keeps recent raw data. The concept history is compact while still retains essential information for learning. Second, it learns concept-transition patterns from the concept history and anticipates what the concept will be in the case of a concept change. It then proactively prepares a prediction model for the future change. This contrasts to the conventional methodology that passively waits until the change happens. Third, it incorporates proactive and reactive predictions. If the anticipation turns out to be correct, a proper prediction model can be launched instantly upon the concept change. If not, it promptly resorts to a reactive mode: adapting a prediction model to the new data. Finally, an efficient and effective system RePro is proposed to implement these new ideas. It carries out prediction at two levels, a general level of predicting each oncoming concept and a specific level of predicting each instance's class. Experiments are conducted to compare RePro with representative existing prediction methods on various benchmark data sets that represent diversified scenarios of concept change. Empirical evidence offers inspiring insights and demonstrates the proposed methodology is an advisable solution to prediction in data streams.</content></document><document><year>2006</year><authors>Shichao Zhang1| 2  | Mohammed J. Zaki3 </authors><title>Mining Multiple Data Sources: Local Pattern Analysis</title><content>Without Abstract</content></document><document><year>2006</year><authors>Raymond Chi-Wing Wong1  | Ada Wai-Chee Fu1 </authors><title>Mining top-K frequent itemsets from data streams      </title><content>Frequent pattern mining on data streams is of interest recently. However, it is not easy for users to determine a proper frequency         threshold. It is more reasonable to ask users to set a bound on the result size. We study the problem of mining top K frequent itemsets in data streams. We introduce a method based on the Chernoff bound with a guarantee of the output quality         and also a bound on the memory usage. We also propose an algorithm based on the Lossy Counting Algorithm. In most of the experiments         of the two proposed algorithms, we obtain perfect solutions and the memory space occupied by our algorithms is very small.         Besides, we also propose the adapted approach of these two algorithms in order to handle the case when we are interested in         mining the data in a sliding window. The experiments show that the results are accurate.      </content></document><document><year>2006</year><authors>Charu C. Aggarwal1 </authors><title>On the use of Human-Computer Interaction for Projected Nearest Neighbor Search      </title><content>Nearest Neighbor search is an important and widely used technique in a number of important application domains. In many of         these domains, the dimensionality of the data representation is often very high. Recent theoretical results have shown that         the concept of proximity or nearest neighbors may not be very meaningful for the high dimensional case. Therefore, it is often         a complex problem to find good quality nearest neighbors in such data sets. Furthermore, it is also difficult to judge the         value and relevance of the returned results. In fact, it is hard for any fully automated system to satisfy a user about the         quality of the nearest neighbors found unless he is directly involved in the process. This is especially the case for high         dimensional data in which the meaningfulness of the nearest neighbors found is questionable. In this paper, we address the         complex problem of high dimensional nearest neighbor search from the user perspective by designing a system which uses effective         cooperation between the human and the computer. The system provides the user with visual representations of carefully chosen         subspaces of the data in order to repeatedly elicit his preferences about the data patterns which are most closely related         to the query point. These preferences are used in order to determine and quantify the meaningfulness of the nearest neighbors.         Our system is not only able to find and quantify the meaningfulness of the nearest neighbors, but is also able to diagnose         situations in which the nearest neighbors found are truly not meaningful.      </content></document><document><year>2006</year><authors>Jia Hu1  | Ning Zhong1 </authors><title>Organizing Multiple Data Sources for Developing Intelligent e-Business Portals</title><content>Enterprise applications usually involve huge, complex, and persistent data to work on, together with business rules and processes. In order to represent, integrate, and use the information coming from the huge, distributed, multiple sources, we present a conceptual model with dynamic multi-level workflows corresponding to a mining-grid centric multi-layer grid architecture, for multi-aspect analysis in building an e-business portal on the Wisdom Web. We show that this integrated model will help to dynamically organize status-based business processes that govern enterprise application integration.We also present two case studies to demonstrate the effectiveness of the proposed model in the real world. The first case study is about how to organize and mine multiple data sources for behavior-based online customer segmentation, which is the first crucial step of personalization and one-to-one marketing. The second case study is about how to evaluate and monitor data quality, which in return can optimize the knowledge discovery process for intelligent decision making. The proposed methodology attempts to orchestrate various mining agents on the mining-grid for integrating data and knowledge in a unified portal developed by a service-oriented architecture.</content></document><document><year>2006</year><authors>GUI-RONG XUE1 | YONG YU1 | DOU SHEN2 | QIANG YANG2 | HUA-JUN ZENG3  | ZHENG CHEN3 </authors><title>Reinforcing Web-object Categorization Through Interrelationships</title><content>Existing categorization algorithms deal with homogeneous Web objects, and consider interrelated objects as additional features when taking the interrelationships with other types of objects into account. However, focusing on any single aspect of the inter-object relationship is not sufficient to fully reveal the true categories of Web objects. In this paper, we propose a novel categorization algorithm, called the Iterative Reinforcement Categorization Algorithm (IRC), to exploit the full interrelationship between different types of Web objects on the Web, including Web pages and queries. IRC classifies the interrelated Web objects by iteratively reinforcing the individual classification results of different types of objects via their interrelationship. Experiments on a clickthrough-log dataset from the MSN search engine show that, in terms of the F1 measure, IRC achieves a 26.4% improvement over a pure content-based classification method. It also achieves a 21% improvement over a query-metadata-based method, as well as a 16.4% improvement on F1 measure over the well-known virtual document-based method. Our experiments show that IRC converges fast enough to be applicable to real world applications.</content></document><document><year>2006</year><authors>Arindam Banerjee1  | Joydeep Ghosh2 </authors><title>Scalable Clustering Algorithms with Balancing Constraints</title><content>Clustering methods for data-mining problems must be extremely scalable. In addition, several data mining applications demand that the clusters obtained be balanced, i.e., of approximately the same size or importance. In this paper, we propose a general framework for scalable, balanced clustering. The data clustering process is broken down into three steps: sampling of a small representative subset of the points, clustering of the sampled data, and populating the initial clusters with the remaining data followed by refinements. First, we show that a simple uniform sampling from the original data is sufficient to get a representative subset with high probability. While the proposed framework allows a large class of algorithms to be used for clustering the sampled set, we focus on some popular parametric algorithms for ease of exposition. We then present algorithms to populate and refine the clusters. The algorithm for populating the clusters is based on a generalization of the stable marriage problem, whereas the refinement algorithm is a constrained iterative relocation scheme. The complexity of the overall method is O(kN log N) for obtaining k balanced clusters from N data points, which compares favorably with other existing techniques for balanced clustering. In addition to providing balancing guarantees, the clustering performance obtained using the proposed framework is comparable to and often better than the corresponding unconstrained solution. Experimental results on several datasets, including high-dimensional (&amp;gt;20,000) ones, are provided to demonstrate the efficacy of the proposed framework.</content></document><document><year>2006</year><authors>Hye-Chung Kum1 | Joong Hyuk Chang2  | Wei Wang1 </authors><title>Sequential Pattern Mining in Multi-Databases via Multiple Alignment</title><content>To efficiently find global patterns from a multi-database, information in each local database must first be mined and summarized at the local level. Then only the summarized information is forwarded to the global mining process. However, conventional sequential pattern mining methods based on support cannot summarize the local information and is ineffective for global pattern mining from multiple data sources. In this paper, we present an alternative local mining approach for finding sequential patterns in the local databases of a multi-database. We propose the theme of approximate sequential pattern mining roughly defined as identifying patterns approximately shared by many sequences. Approximate sequential patterns can effectively summerize and represent the local databases by identifying the underlying trends in the data. We present a novel algorithm, ApproxMAP, to mine approximate sequential patterns, called consensus patterns, from large sequence databases in two steps. First, sequences are clustered by similarity. Then, consensus patterns are mined directly from each cluster through multiple alignment. We conduct an extensive and systematic performance study over synthetic and real data. The results demonstrate that ApproxMAP is effective and scalable in mining large sequences databases with long patterns. Hence, ApproxMAP can efficiently summarize a local database and reduce the cost for global mining. Furthremore, we present an elegant and uniform model to identify both high vote sequential patterns and exceptional sequential patterns from the collection of these consensus patterns from each local databases.</content></document><document><year>2006</year><authors>D. Bouchaffra1  | J. Tan1</authors><title>Structural Hidden Markov Models Using a Relation of Equivalence: Application to Automotive Designs      </title><content>Standard hidden Markov models (HMM's) have been studied extensively in the last two decades. It is well known that these models         assume state conditional independence of the observations. Therefore, they are inadequate for classification of complex and         highly structured patterns. Nowadays, the need for new statistical models that are capable to cope with structural time series         data is increasing. We propose in this paper a novel paradigm that we named &amp;#8220;structural hidden Markov model&amp;#8221; (SHMM). It extends         traditional HMM's by partitioning the set of observation sequences into classes of equivalences. These observation sequences are related in the sense they all contribute to produce a particular local structure. We describe four basic problems that are assigned to a structural hidden Markov model: (1) probability evaluation, (2) statistical         decoding, (3) local structure decoding, and (4) parameter estimation. We have applied SHMM in order to mine customers' preferences         for automotive designs. The results reported in this application show that SHMM's outperform the traditional hidden Markov         model with a 9% of increase in accuracy.      </content></document><document><year>2006</year><authors>Michail Vlachos1 | Philip S. Yu1| Vittorio Castelli1 | Christopher Meek2</authors><title>Structural Periodic Measures for Time-Series Data      </title><content>This work motivates the need for more flexible structural similarity measures between time-series sequences, which are based         on the extraction of important periodic features. Specifically, we present non-parametric methods for accurate periodicity         detection and we introduce new periodic distance measures for time-series sequences. We combine these new measures with an         effective metric tree index structure for efficiently answering k-Nearest-Neighbor queries. The goal of these tools and techniques are to assist in detecting, monitoring and visualizing structural         periodic changes. It is our belief that these methods can be directly applicable in the manufacturing industry for preventive         maintenance and in the medical sciences for accurate classification and anomaly detection.      </content></document><document><year>2006</year><authors>N. Vanetik1 | S. E. Shimony1  | E. Gudes1 </authors><title>Support measures for graph data*               </title><content>The concept of support is central to data mining. While the definition of support in transaction databases is intuitive and         simple, that is not the case in graph datasets and databases. Most mining algorithms require the support of a pattern to be         no greater than that of its subpatterns, a property called anti-monotonicity, or admissibility. This paper examines the requirements for admissibility of a support measure. Support measures for mining         graphs are usually based on the notion of an instance graph---a graph representing all the instances of the pattern in a database         and their intersection properties. Necessary and sufficient conditions for support measure admissibility, based on operations         on instance graphs, are developed and proved. The sufficient conditions are used to prove admissibility of one support measure&amp;#8212;the         size of the independent set in the instance graph. Conversely, the necessary conditions are used to quickly show that some         other support measures, such as weighted count of instances, are not admissible.      </content></document><document><year>2006</year><authors>Gregor Leban1 | Bla&amp;#382  Zupan1| 2 | Gaj Vidmar3  | Ivan Bratko1| 4 </authors><title>VizRank: Data Visualization Guided by Machine Learning      </title><content>Data visualization plays a crucial role in identifying interesting patterns in exploratory data analysis. Its use is, however,         made difficult by the large number of possible data projections showing different attribute subsets that must be evaluated         by the data analyst. In this paper, we introduce a method called VizRank, which is applied on classified data to automatically         select the most useful data projections. VizRank can be used with any visualization method that maps attribute values to points         in a two-dimensional visualization space. It assesses possible data projections and ranks them by their ability to visually         discriminate between classes. The quality of class separation is estimated by computing the predictive accuracy of k-nearest neighbor classifier on the data set consisting of x and y positions of the projected data points and their class information. The paper introduces the method and presents experimental         results which show that VizRank's ranking of projections highly agrees with subjective rankings by data analysts. The practical         use of VizRank is also demonstrated by an application in the field of functional genomics.      </content></document><document><year>2007</year><authors>Xin Wang1  | Ata KabГЎn1 </authors><title>A dynamic bibliometric model for identifying online communities      </title><content>Predictive modelling of online dynamic user-interaction recordings and community identification from such data becomes more         and more important with the widespread use of online communication technologies. Despite of the time-dependent nature of the         problem, existing approaches of community identification are based on static or fully observed network connections. Here we         present a new, dynamic generative model for the inference of communities from a sequence of temporal events produced through         online computer- mediated interactions. The distinctive feature of our approach is that it tries to model the process in a         more realistic manner, including an account for possible random temporal delays between the intended connections. The inference         of these delays from the data then forms an integral part of our state-clustering methodology, so that the most likely communities         are found on the basis of the likely intended connections rather than just the observed ones. We derive a maximum likelihood         estimation algorithm for the identification of our model, which turns out to be computationally efficient for the analysis         of historical data and it scales linearly with the number of non-zero observed (L +; 1)-grams, where L is the Markov memory length. In addition, we also derive an incremental version of the algorithm, which could be used for         real-time analysis. Results obtained on both synthetic and real-world data sets demonstrate the approach is flexible and able         to reveal novel and insightful structural aspects of online interactions. In particular, the analysis of a full day worth         synchronous Internet relay chat participation sequence, reveals the formation of an extremely clear community structure.      </content></document><document><year>2007</year><authors>Elaine P. M. de Sousa1 | Caetano Traina Jr.1 | Agma J. M. Traina1 | Leejay Wu2  | Christos Faloutsos2 </authors><title>A fast and effective method to find correlations among attributes in databases      </title><content>The problem of identifying meaningful patterns in a database lies at the very heart of data mining. A core objective of data         mining processes is the recognition of inter-attribute correlations. Not only are correlations necessary for predictions and         classifications &amp;#8211; since rules would fail in the absence of pattern &amp;#8211; but also the identification of groups of mutually correlated         attributes expedites the selection of a representative subset of attributes, from which existing mappings allow others to         be derived. In this paper, we describe a scalable, effective algorithm to identify groups of correlated attributes. This algorithm         can handle non-linear correlations between attributes, and is not restricted to a specific family of mapping functions, such         as the set of polynomials. We show the results of our evaluation of the algorithm applied to synthetic and real world datasets,         and demonstrate that it is able to spot the correlated attributes. Moreover, the execution time of the proposed technique         is linear on the number of elements and of correlations in the dataset.      </content></document><document><year>2007</year><authors>Anna M. Manning1 | David J. Haglin2  | John A. Keane1 </authors><title>A recursive search algorithm for statistical disclosure assessment      </title><content>A new algorithm, SUDA2, is presented which finds minimally unique itemsets i.e., minimal itemsets of frequency one. These         itemsets, referred to as Minimal Sample Uniques (MSUs), are important for statistical agencies who wish to estimate the risk         of disclosure of their datasets. SUDA2 is a recursive algorithm which uses new observations about the properties of MSUs to         prune and traverse the search space. Experimental comparisons with previous work demonstrate that SUDA2 is several orders         of magnitude faster, enabling datasets of significantly more columns to be addressed. The ability of SUDA2 to identify the         boundaries of the search space for MSUs is clearly demonstrated.      </content></document><document><year>2007</year><authors>R. Vilalta1 | T. Stepinski2  | M. Achari1 </authors><title>An efficient approach to external cluster assessment with an application to martian topography      </title><content>Automated tools for knowledge discovery are frequently invoked in databases where objects already group into some known (i.e.,         external) classification scheme. In the context of unsupervised learning or clustering, such tools delve inside large databases         looking for alternative classification schemes that are meaningful and novel. An assessment of the information gained with         new clusters can be effected by looking at the degree of separation between each new cluster and its most similar class. Our         approach models each cluster and class as a multivariate Gaussian distribution and estimates their degree of separation through         an information theoretic measure (i.e., through relative entropy or Kullback&amp;#8211;Leibler distance). The inherently large computational         cost of this step is alleviated by first projecting all data over the single dimension that best separates both distributions         (using Fisher&amp;#8217;s Linear Discriminant). We test our algorithm on a dataset of Martian surfaces using the traditional division         into geological units as external classes and the new, hydrology-inspired, automatically performed division as novel clusters.         We find the new partitioning constitutes a formally meaningful classification that deviates substantially from the traditional         classification.      </content></document><document><year>2007</year><authors>S. Prabakaran1 | Rajendra Sahu1 | Sekher Verma1</authors><title>Classification of multi class dataset using wavelet power spectrum      </title><content>Data mining techniques are widely used in many fields. One of the applications of data mining in the field of the Bioinformatics         is classification of tissue samples. In the present work, a wavelet power spectrum based approach has been presented for feature         selection and successful classification of the multi class dataset. The proposed method was applied on SRBCT and the breast         cancer datasets which are multi class cancer datasets. The selected features are almost those selected in previous works.         The method was able to produce almost 100% accurate classification results. The method is very simple and robust to noise.         No extensive preprocessing is required. The classification was performed with comparatively very lesser number of features         than those used in the original works. No information is lost due to the initial pruning of the data usually performed using         a threshold in other methods. The method utilizes the inherent nature of the data in performing various tasks. So, the method         can be used for a wide range of data.      </content></document><document><year>2007</year><authors>Girish Keshav Palshikar1  | Manoj M. Apte2 </authors><title>Collusion set detection using graph clustering      </title><content>Many mal-practices in stock market trading&amp;#8212;e.g., circular trading and price manipulation&amp;#8212;use the modus operandi of collusion. Informally, a set of traders is a candidate collusion set when they have &amp;#8220;heavy trading&amp;#8221; among themselves,         as compared to their trading with others. We formalize the problem of detection of collusion sets, if any, in the given trading         database. We show that naГЇve approaches are inefficient for real-life situations. We adapt and apply two well-known graph         clustering algorithms for this problem. We also propose a new graph clustering algorithm, specifically tailored for detecting         collusion sets. A novel feature of our approach is the use of Dempster&amp;#8211;Schafer theory of evidence to combine the candidate         collusion sets detected by individual algorithms. Treating individual experiments as evidence, this approach allows us to         quantify the confidence (or belief) in the candidate collusion sets. We present detailed simulation experiments to demonstrate         effectiveness of the proposed algorithms.      </content></document><document><year>2007</year><authors>Eamonn Keogh1 | Stefano Lonardi1 | Chotirat Ann Ratanamahatana2 | Li Wei1 | Sang-Hee Lee3  | John H|ley4 </authors><title>Compression-based data mining of sequential data      </title><content>The vast majority of data mining algorithms require the setting of many input parameters. The dangers of working with parameter-laden         algorithms are twofold. First, incorrect settings may cause an algorithm to fail in finding the true patterns. Second, a perhaps         more insidious problem is that the algorithm may report spurious patterns that do not really exist, or greatly overestimate         the significance of the reported patterns. This is especially likely when the user fails to understand the role of parameters         in the data mining process. Data mining algorithms should have as few parameters as possible. A parameter-light algorithm         would limit our ability to impose our prejudices, expectations, and presumptions on the problem at hand, and would let the         data itself speak to us. In this work, we show that recent results in bioinformatics, learning, and computational theory hold great promise         for a parameter-light data-mining paradigm. The results are strongly connected to Kolmogorov complexity theory. However, as         a practical matter, they can be implemented using any off-the-shelf compression algorithm with the addition of just a dozen         lines of code. We will show that this approach is competitive or superior to many of the state-of-the-art approaches in anomaly/interestingness         detection, classification, and clustering with empirical tests on time series/DNA/text/XML/video datasets. As a further evidence         of the advantages of our method, we will demonstrate its effectiveness to solve a real world classification problem in recommending         printing services and products.      </content></document><document><year>2007</year><authors>Michail Vlachos1 | Kun-Lung Wu1| Shyh-Kwei Chen1 | Philip S. Yu1</authors><title>Correlating burst events on streaming stock market data      </title><content>We address the problem of monitoring and identification of correlated burst patterns in multi-stream time series databases.         We follow a two-step methodology: first we identify the burst sections in our data and subsequently we store them for easy         retrieval in an efficient in-memory index. The burst detection scheme imposes a variable threshold on the examined data and         takes advantage of the skewed distribution that is typically encountered in many applications. The detected bursts are compacted         into burst intervals and stored in an interval index. The index facilitates the identification of correlated bursts by performing         very efficient overlap operations on the stored burst regions. We present the merits of the proposed indexing scheme through         a thorough analysis of its complexity. We also manifest the real-time response of our burst indexing technique, and demonstrate         the usefulness of the approach for correlating surprising volume trading events using historical stock data of the NY stock         exchange. While the focus of this work is on financial data, the proposed methods and data-structures can find applications         for anomaly or novelty detection in telecommunication, network traffic and medical data.      </content></document><document><year>2007</year><authors>Xiaoxin Yin1 | Jiawei Han1  | Philip S. Yu2 </authors><title>CrossClus: user-guided multi-relational clustering      </title><content>Most structured data in real-life applications are stored in relational databases containing multiple semantically linked         relations. Unlike clustering in a single table, when clustering objects in relational databases there are usually a large         number of features conveying very different semantic information, and using all features indiscriminately is unlikely to generate         meaningful results. Because the user knows her goal of clustering, we propose a new approach called CrossClus, which performs multi-relational clustering under user&amp;#8217;s guidance. Unlike semi-supervised clustering which requires the user         to provide a training set, we minimize the user&amp;#8217;s effort by using a very simple form of user guidance. The user is only required         to select one or a small set of features that are pertinent to the clustering goal, and CrossClus searches for other pertinent features in multiple relations. Each feature is evaluated by whether it clusters objects in         a similar way with the user specified features. We design efficient and accurate approaches for both feature selection and         object clustering. Our comprehensive experiments demonstrate the effectiveness and scalability of CrossClus.      </content></document><document><year>2007</year><authors>Gregory Piatetsky-Shapiro1 </authors><title>Data mining and knowledge discovery 1996 to 2005: overcoming the hype and moving from &amp;#8220;university&amp;#8221; to &amp;#8220;business&amp;#8221; and &amp;#8220;analytics&amp;#8221;      </title><content>I survey the transformation of the data mining and knowledge discovery field over the last 10;years from the unique vantage         point of KDnuggets as a leading chronicler of the field. Analysis of the most frequent words in KDnuggets News leads to revealing         observations.      </content></document><document><year>2007</year><authors>Lucia Sacchi1 | Cristiana Larizza1| Carlo Combi2 | Riccardo Bellazzi1</authors><title>Data mining with Temporal Abstractions: learning rules from time series      </title><content>A large volume of research in temporal data mining is focusing on discovering temporal rules from time-stamped data. The majority         of the methods proposed so far have been mainly devoted to the mining of temporal rules which describe relationships between         data sequences or instantaneous events and do not consider the presence of complex temporal patterns into the dataset. Such         complex patterns, such as trends or up and down behaviors, are often very interesting for the users. In this paper we propose         a new kind of temporal association rule and the related extraction algorithm; the learned rules involve complex temporal patterns         in both their antecedent and consequent. Within our proposed approach, the user defines a set of complex patterns of interest         that constitute the basis for the construction of the temporal rule; such complex patterns are represented and retrieved in         the data through the formalism of knowledge-based Temporal Abstractions. An Apriori-like algorithm looks then for meaningful         temporal relationships (in particular, precedence temporal relationships) among the complex patterns of interest. The paper         presents the results obtained by the rule extraction algorithm on a simulated dataset and on two different datasets related         to biomedical applications: the first one concerns the analysis of time series coming from the monitoring of different clinical         variables during hemodialysis sessions, while the other one deals with the biological problem of inferring relationships between         genes from DNA microarray data.      </content></document><document><year>2007</year><authors>Qingfeng Chen1 | Yi-Ping Phoebe Chen1| 2  | Chengqi Zhang3 </authors><title>Detecting inconsistency in biological molecular databases using ontologies      </title><content>The rapid growth of life science databases demands the fusion of knowledge from heterogeneous databases to answer complex         biological questions. The discrepancies in nomenclature, various schemas and incompatible formats of biological databases,         however, result in a significant lack of interoperability among databases. Therefore, data preparation is a key prerequisite         for biological database mining. Integrating diverse biological molecular databases is an essential action to cope with the         heterogeneity of biological databases and guarantee efficient data mining. However, the inconsistency in biological databases         is a key issue for data integration. This paper proposes a framework to detect the inconsistency in biological databases using         ontologies. A numeric estimate is provided to measure the inconsistency and identify those biological databases that are appropriate         for further mining applications. This aids in enhancing the quality of databases and guaranteeing accurate and efficient mining         of biological databases.      </content></document><document><year>2007</year><authors>Chia Huey Ooi1 | Madhu Chetty1  | Shyh Wei Teng1 </authors><title>Differential prioritization in feature selection and classifier aggregation for multiclass microarray datasets      </title><content>The high dimensionality of microarray datasets endows the task of multiclass tissue classification with various difficulties&amp;#8212;the         main challenge being the selection of features deemed relevant and non-redundant to form the predictor set for classifier         training. The necessity of varying the emphases on relevance and redundancy, through the use of the degree of differential         prioritization (DDP) during the search for the predictor set is also of no small importance. Furthermore, there are several         types of decomposition technique for the feature selection (FS) problem&amp;#8212;all-classes-at-once, one-vs.-all (OVA) or pairwise         (PW). Also, in multiclass problems, there is the need to consider the type of classifier aggregation used&amp;#8212;whether non-aggregated         (a single machine), or aggregated (OVA or PW). From here, first we propose a systematic approach to combining the distinct         problems of FS and classification. Then, using eight well-known multiclass microarray datasets, we empirically demonstrate         the effectiveness of the DDP in various combinations of FS decomposition types and classifier aggregation methods. Aided by         the variable DDP, feature selection leads to classification performance which is better than that of rank-based or equal-priorities         scoring methods and accuracies higher than previously reported for benchmark datasets with large number of classes. Finally,         based on several criteria, we make general recommendations on the optimal choice of the combination of FS decomposition type         and classifier aggregation method for multiclass microarray datasets.      </content></document><document><year>2007</year><authors>G. Niklas NorГ©n1| 2 | Rol| Orre3| Andrew Bate1 | I. Ralph Edwards1</authors><title>Duplicate detection in adverse drug reaction surveillance      </title><content>The WHO Collaborating Centre for International Drug Monitoring in Uppsala, Sweden, maintains and analyses the world&amp;#8217;s largest         database of reports on suspected adverse drug reaction (ADR) incidents that occur after drugs are on the market. The presence         of duplicate case reports is an important data quality problem and their detection remains a formidable challenge, especially         in the WHO drug safety database where reports are anonymised before submission. In this paper, we propose a duplicate detection         method based on the hit-miss model for statistical record linkage described by Copas and Hilton, which handles the limited         amount of training data well and is well suited for the available data (categorical and numerical rather than free text).         We propose two extensions of the standard hit-miss model: a hit-miss mixture model for errors in numerical record fields and         a new method to handle correlated record fields, and we demonstrate the effectiveness both at identifying the most likely         duplicate for a given case report (94.7% accuracy) and at discriminating true duplicates from random matches (63% recall with         71% precision). The proposed method allows for more efficient data cleaning in post-marketing drug safety data sets, and perhaps         other knowledge discovery applications as well.      </content></document><document><year>2007</year><authors>Geoffrey I. Webb1 </authors><title>Editorial      </title><content>Without Abstract</content></document><document><year>2007</year><authors>James Cheng1 | Yiping Ke1  | Wilfred Ng1 </authors><title>Effective elimination of redundant association rules      </title><content>It is well-recognized that the main factor that hinders the applications of Association Rules (ARs) is the huge number of ARs returned by the mining process. In this paper, we propose an effective solution that presents         concise mining results by eliminating the redundancy in the set of ARs. We adopt the concept of &amp;#948;         tolerance to define the set of &amp;#948;-Tolerance ARs (&amp;#948;-TARs), which is a concise representation for the set of ARs. The notion of &amp;#948;-tolerance is a relaxation on the closure defined on the support of frequent itemsets, thus allowing us to effectively prune         the redundant ARs. We devise a set of inference rules, with which we prove that the set of &amp;#948;-TARs is a non-redundant representation of ARs. In addition, we prove that the set of ARs that is derived from the &amp;#948;-TARs by the inference rules is sound and complete. We also develop a compact tree structure called the &amp;#948;-TAR tree, which facilitates the efficient generation of the &amp;#948;-TARs and derivation of other ARs. Experimental results verify the efficiency of using the &amp;#948;-TAR tree to generate the &amp;#948;-TARs and to query the ARs. The set of &amp;#948;-TARs is shown to be significantly smaller than the state-of-the-art concise representations of ARs. In addition, the approximation         on the support and confidence of the ARs derived from the &amp;#948;-TARs are highly accurate.      </content></document><document><year>2007</year><authors>Fabian MГ¶rchen1  | Alfred Ultsch2 </authors><title>Efficient mining of understandable patterns from multivariate interval time series      </title><content>We present a new method for the understandable description of local temporal relationships in multivariate data, called Time Series Knowledge Mining (TSKM). We define the Time Series Knowledge Representation (TSKR) as a new language for expressing temporal knowledge in time interval data. The patterns have a hierarchical structure,         with levels corresponding to the temporal concepts duration, coincidence, and partial order. The patterns are very compact,         but offer details for each element on demand. In comparison with related approaches, the TSKR is shown to have advantages         in robustness, expressivity, and comprehensibility. The search for coincidence and partial order in interval data can be formulated         as instances of the well known frequent itemset problem. Efficient algorithms for the discovery of the patterns are adapted         accordingly. A novel form of search space pruning effectively reduces the size of the mining result to ease interpretation         and speed up the algorithms. Human interaction is used during the mining to analyze and validate partial results as early         as possible and guide further processing steps. The efficacy of the methods is demonstrated using two real life data sets.         In an application to sports medicine the results were recognized as valid and useful by an expert of the field.      </content></document><document><year>2007</year><authors>Marko Robnik-&amp;#352 ikonja1  | Koen Vanhoof2 </authors><title>Evaluation of ordinal attributes at value level      </title><content>We propose a novel context sensitive algorithm for evaluation of ordinal attributes which exploits the information hidden         in ordering of attributes&amp;#8217; and class&amp;#8217; values and provides a separate score for each value of the attribute. Similar to feature         selection algorithm ReliefF, the proposed algorithm exploits the contextual information via selection of nearest instances.         The ordEval algorithm outputs probabilistic factors corresponding to the effect an increase/decrease of attribute&amp;#8217;s value has on the         class value. While the ordEval algorithm is general and can be used for analysis of any survey with graded answers, we show         its utility on an important marketing problem of customer (dis)satisfaction. We develop a visualization technique and show         how we can use it to detect and confirm several findings from marketing theory.      </content></document><document><year>2007</year><authors>Jessica Lin1 | Eamonn Keogh2 | Li Wei2  | Stefano Lonardi2 </authors><title>Experiencing SAX: a novel symbolic representation of time series      </title><content>Many high level representations of time series have been proposed for data mining, including Fourier transforms, wavelets,         eigenwaves, piecewise polynomial models, etc. Many researchers have also considered symbolic representations of time series,         noting that such representations would potentiality allow researchers to avail of the wealth of data structures and algorithms         from the text processing and bioinformatics communities. While many symbolic representations of time series have been introduced         over the past decades, they all suffer from two fatal flaws. First, the dimensionality of the symbolic representation is the         same as the original data, and virtually all data mining algorithms scale poorly with dimensionality. Second, although distance         measures can be defined on the symbolic approaches, these distance measures have little correlation with distance measures         defined on the original time series.                     In this work we formulate a new symbolic representation of time series. Our representation is unique in that it allows dimensionality/numerosity               reduction, and it also allows distance measures to be defined on the symbolic approach that lower bound corresponding distance               measures defined on the original series. As we shall demonstrate, this latter feature is particularly exciting because it               allows one to run certain data mining algorithms on the efficiently manipulated symbolic representation, while producing identical               results to the algorithms that operate on the original data. In particular, we will demonstrate the utility of our representation               on various data mining tasks of clustering, classification, query by content, anomaly detection, motif discovery, and visualization.            </content></document><document><year>2007</year><authors>Raghu Ramakrishnan1  | Bee-Chung Chen1 </authors><title>Exploratory mining in cube space      </title><content>Data Mining has evolved as a new discipline at the intersection of several existing areas, including Database Systems, Machine         Learning, Optimization, and Statistics. An important question is whether the field has matured to the point where it has originated         substantial new problems and techniques that distinguish it from its parent disciplines. In this paper, we discuss a class         of new problems and techniques that show great promise for exploratory mining, while synthesizing and generalizing ideas from         the parent disciplines. While the class of problems we discuss is broad, there is a common underlying objective&amp;#8212;to look beyond         a single data-mining step (e.g., data summarization or model construction) and address the combined process of data selection         and transformation, parameter and algorithm selection, and model construction. The fundamental difficulty lies in the large         space of alternative choices at each step, and good solutions must provide a natural framework for managing this complexity.         We regard this as a grand challenge for Data Mining, and see the ideas discussed here as promising initial steps towards a         rigorous exploratory framework that supports the entire process.      </content></document><document><year>2007</year><authors>Jiawei Han1 | Hong Cheng1| Dong Xin1 | Xifeng Yan1</authors><title>Frequent pattern mining: current status and future directions      </title><content>Frequent pattern mining has been a focused theme in data mining research for over a decade. Abundant literature has been dedicated         to this research and tremendous progress has been made, ranging from efficient and scalable algorithms for frequent itemset         mining in transaction databases to numerous research frontiers, such as sequential pattern mining, structured pattern mining,         correlation mining, associative classification, and frequent pattern-based clustering, as well as their broad applications.         In this article, we provide a brief overview of the current status of frequent pattern mining and discuss a few promising         research directions. We believe that frequent pattern mining research has substantially broadened the scope of data analysis         and will have deep impact on data mining methodologies and applications in the long run. However, there are still some challenging         research issues that need to be solved before frequent pattern mining can claim a cornerstone approach in data mining applications.      </content></document><document><year>2007</year><authors>Hans-Peter Kriegel1 | Karsten M. Borgwardt1| Peer KrГ¶ger1| Alexey Pryakhin1| Matthias Schubert1 | Arthur Zimek1</authors><title>Future trends in data mining      </title><content>Over recent years data mining has been establishing itself as one of the major disciplines in computer science with growing         industrial impact. Undoubtedly, research in data mining will continue and even increase over coming decades. In this article,         we sketch our vision of the future of data mining. Starting from the classic definition of &amp;#8220;data mining&amp;#8221;, we elaborate on         topics that &amp;#8212; in our opinion &amp;#8212; will set trends in data mining.      </content></document><document><year>2007</year><authors>A. K. A. de Medeiros1 | A. J. M. M. Weijters1 | W. M. P. van der Aalst1</authors><title>Genetic process mining: an experimental evaluation      </title><content>One of the aims of process mining is to retrieve a process model from an event log. The discovered models can be used as objective starting points during the deployment of process-aware information systems (Dumas et;al., eds., Process-Aware Information         Systems: Bridging People and Software Through Process Technology. Wiley, New York, 2005) and/or as a feedback mechanism to         check prescribed models against enacted ones. However, current techniques have problems when mining processes that contain         non-trivial constructs and/or when dealing with the presence of noise in the logs. Most of the problems happen because many         current techniques are based on local information in the event log. To overcome these problems, we try to use genetic algorithms to mine process models. The main         motivation is to benefit from the global search performed by this kind of algorithms. The non-trivial constructs are tackled by choosing an internal representation that         supports them. The problem of noise is naturally tackled by the genetic algorithm because, per definition, these algorithms         are robust to noise. The main challenge in a genetic approach is the definition of a good fitness measure because it guides         the global search performed by the genetic algorithm. This paper explains how the genetic algorithm works. Experiments with         synthetic and real-life logs show that the fitness measure indeed leads to the mining of process models that are complete (can reproduce all the behavior in the log) and precise (do not allow for extra behavior that cannot be derived from the event log). The genetic algorithm is implemented as a plug-in         in the ProM framework.      </content></document><document><year>2007</year><authors>Tao Li1 | Chang-Shing Perng2  | Sheng Ma3 </authors><title>Guest editorial: special issue on temporal data mining: theory, algorithms and applications      </title><content>This special issue provides a leading forum for timely, in-depth presentation of recent advances in algorithms, theories and         applications in temporal data mining. The selected papers underwent a rigorous refereeing and revision process.      </content></document><document><year>2007</year><authors>Carlotta Domeniconi1 | Dimitrios Gunopulos2| Sheng Ma3| Bojun Yan1| Muna Al-Razgan1 | Dimitris Papadopoulos2</authors><title>Locally adaptive metrics for clustering high dimensional data      </title><content>Clustering suffers from the curse of dimensionality, and similarity functions that use all input features with equal relevance         may not be effective. We introduce an algorithm that discovers clusters in subspaces spanned by different combinations of         dimensions via local weightings of features. This approach avoids the risk of loss of information encountered in global dimensionality         reduction techniques, and does not assume any data distribution model. Our method associates to each cluster a weight vector,         whose values capture the relevance of features within the corresponding cluster. We experimentally demonstrate the gain in         perfomance our method achieves with respect to competitive methods, using both synthetic and real datasets. In particular,         our results show the feasibility of the proposed technique to perform simultaneous clustering of genes and conditions in gene         expression data, and clustering of very high-dimensional data such as text data.      </content></document><document><year>2007</year><authors>Gary M. Weiss1  | Ye Tian1</authors><title>Maximizing classifier utility when there are data acquisition and modeling costs      </title><content>Classification is a well-studied problem in data mining. Classification performance was originally gauged almost exclusively         using predictive accuracy, but as work in the field progressed, more sophisticated measures of classifier utility that better         represented the value of the induced knowledge were introduced. Nonetheless, most work still ignored the cost of acquiring         training examples, even though this cost impacts the total utility of the data mining process. In this article we analyze the relationship between the number of acquired training examples and the utility of the data         mining process and, given the necessary cost information, we determine the number of training examples that yields the optimum         overall performance. We then extend this analysis to include the cost of model induction&amp;#8212;measured in terms of the CPU time         required to generate the model. While our cost model does not take into account all possible costs, our analysis provides         some useful insights and a template for future analyses using more sophisticated cost models. Because our analysis is based         on experiments that acquire the full set of training examples, it cannot directly be used to find a classifier with optimal         or near-optimal total utility. To address this issue we introduce two progressive sampling strategies that are empirically         shown to produce classifiers with near-optimal total utility.      </content></document><document><year>2007</year><authors>Edward C. Malthouse1 </authors><title>Mining for trigger events with survival analysis      </title><content>This paper discusses a new application of data mining, quantifying the importance of responding to trigger events with reactive         contacts. Trigger events happen during a customer&amp;#8217;s lifecycle and indicate some change in the relationship with the company.         If detected early, the company can respond to the problem and retain the customer; otherwise the customer may switch to another         company. It is usually easy to identify many potential trigger events. What is needed is a way of prioritizing which events         demand interventions. We conceptualize the trigger event problem and show how survival analysis can be used to quantify the         importance of addressing various trigger events. The method is illustrated on four real data sets from different industries         and countries.      </content></document><document><year>2007</year><authors>Hong Yao1  | Howard J. Hamilton1 </authors><title>Mining functional dependencies from data      </title><content>In this paper, we propose an efficient rule discovery algorithm, called FD_Mine, for mining functional dependencies from data.         By exploiting Armstrong&amp;#8217;s Axioms for functional dependencies, we identify equivalences among attributes, which can be used         to reduce both the size of the dataset and the number of functional dependencies to be checked. We first describe four effective         pruning rules that reduce the size of the search space. In particular, the number of functional dependencies to be checked         is reduced by skipping the search for FDs that are logically implied by already discovered FDs. Then, we present the FD_Mine         algorithm, which incorporates the four pruning rules into the mining process. We prove the correctness of FD_Mine, that is,         we show that the pruning does not lead to the loss of useful information. We report the results of a series of experiments.         These experiments show that the proposed algorithm is effective on 15 UCI datasets and synthetic data.      </content></document><document><year>2007</year><authors>Lijie Wen1 | Wil M. P. van der Aalst2 | Jianmin Wang1  | Jiaguang Sun1 </authors><title>Mining process models with non-free-choice constructs      </title><content>Process mining aims at extracting information from event logs to capture the business process as it is being executed. Process         mining is particularly useful in situations where events are recorded but there is no system enforcing people to work in a         particular way. Consider for example a hospital where the diagnosis and treatment activities are recorded in the hospital         information system, but where health-care professionals determine the &amp;#8220;careflow.&amp;#8221; Many process mining approaches have been         proposed in recent years. However, in spite of many researchers&amp;#8217; persistent efforts, there are still several challenging problems         to be solved. In this paper, we focus on mining non-free-choice constructs, i.e., situations where there is a mixture of choice         and synchronization. Although most real-life processes exhibit non-free-choice behavior, existing algorithms are unable to         adequately deal with such constructs. Using a Petri-net-based representation, we will show that there are two kinds of causal         dependencies between tasks, i.e., explicit and implicit ones. We propose an algorithm that is able to deal with both kinds         of dependencies. The algorithm has been implemented in the ProM framework and experimental results shows that the algorithm         indeed significantly improves existing process mining techniques.      </content></document><document><year>2007</year><authors>Florian Verhein1  | Sanjay Chawla1 </authors><title>Mining spatio-temporal patterns in object mobility databases      </title><content>With the increasing use of wireless communication devices and the ability to track people and objects cheaply and easily,         the amount of spatio-temporal data is growing substantially. Many of these applications cannot easily locate the exact position         of objects, but they can determine the region in which each object is contained. Furthermore, the regions are fixed and may         vary greatly in size. Examples include mobile/cell phone networks, RFID tag readers and satellite tracking. This demands techniques         to mine such data. These techniques must also correct for the bias produced by different sized regions. We provide a comprehensive         definition of Spatio-Temporal Association Rules (STARs) that describe how objects move between regions over time. We also present other patterns that are useful for mobility data;         stationary regions and high traffic regions. The latter consists of sources, sinks and thoroughfares. These patterns describe important temporal characteristics of regions and we show that they can be considered as special         STARs. We define spatial support to effectively deal with the problem of different sized regions. We provide an efficient algorithm&amp;#8212;STAR-Miner&amp;#8212;to find these patterns by exploiting several pruning properties.      </content></document><document><year>2007</year><authors>Toon Calders1| 2  | Bart Goethals2 </authors><title>Non-derivable itemset mining      </title><content>All frequent itemset mining algorithms rely heavily on the monotonicity principle for pruning. This principle allows for excluding         candidate itemsets from the expensive counting phase. In this paper, we present sound and complete deduction rules to derive         bounds on the support of an itemset. Based on these deduction rules, we construct a condensed representation of all frequent         itemsets, by removing those itemsets for which the support can be derived, resulting in the so called Non-Derivable Itemsets (NDI) representation. We also present connections between our proposal and recent other proposals for condensed representations         of frequent itemsets. Experiments on real-life datasets show the effectiveness of the NDI representation, making the search         for frequent non-derivable itemsets a useful and tractable alternative to mining all frequent itemsets.      </content></document><document><year>2007</year><authors>Christos Faloutsos1  | Vasileios Megalooikonomou2 </authors><title>On data mining, compression, and Kolmogorov complexity      </title><content>Will we ever have a theory of data mining analogous to the relational algebra in databases? Why do we have so many clearly         different clustering algorithms? Could data mining be automated? We show that the answer to all these questions is negative,         because data mining is closely related to compression and Kolmogorov complexity; and the latter is undecidable. Therefore,         data mining will always be an art, where our goal will be to find better models (patterns) that fit our datasets as best as         possible.      </content></document><document><year>2007</year><authors>SГ©bastien Gambs1 | BalГЎzs KГ©gl1  | Esma AГЇmeur1 </authors><title>Privacy-preserving boosting      </title><content>We describe two algorithms, BiBoost (Bipartite Boosting) and MultBoost (Multiparty Boosting), that allow two or more participants to construct a boosting classifier without explicitly sharing         their data sets. We analyze both the computational and the security aspects of the algorithms. The algorithms inherit the         excellent generalization performance of AdaBoost. Experiments indicate that the algorithms are better than AdaBoost executed separately by the participants, and that, independently of the number of participants, they perform close to AdaBoost executed using the entire data set.      </content></document><document><year>2007</year><authors>Muneaki Ohshima1| Ning Zhong1 | YiYu Yao2 | Chunnian Liu3</authors><title>Relational peculiarity-oriented mining      </title><content>Peculiarity rules are a new type of useful knowledge that can be discovered by searching the relevance among peculiar data.         A main task in mining such knowledge is peculiarity identification. Previous methods for finding peculiar data focus on attribute         values. By extending to record-level peculiarity, this paper investigates relational peculiarity-oriented mining. Peculiarity         rules are mined, and more importantly explained, in a relational mining framework. Several experiments are carried out and         the results show that relational peculiarity-oriented mining is effective.      </content></document><document><year>2007</year><authors>Ian Davidson1  | S. S. Ravi1 </authors><title>The complexity of non-hierarchical clustering with instance and cluster level constraints      </title><content>Recent work has looked at extending clustering algorithms with instance level must-link (ML) and cannot-link (CL) background         information. Our work introduces &amp;#948; and &amp;#949; cluster level constraints that influence inter-cluster distances and cluster composition.         The addition of background information, though useful at providing better clustering results, raises the important feasibility question: Given a collection of constraints and a set of data, does there exist at least one partition of the data set satisfying         all the constraints? We study the complexity of the feasibility problem for each of the above constraints separately and also         for combinations of constraints. Our results clearly delineate combinations of constraints for which the feasibility problem         is computationally intractable (i.e., NP-complete) from those for which the problem is efficiently solvable (i.e., in the computational class P). We also consider the ML and CL constraints in conjunctive and disjunctive normal forms (CNF and DNF respectively). We show         that for ML constraints, the feasibility problem is intractable for CNF but efficiently solvable for DNF. Unfortunately, for         CL constraints, the feasibility problem is intractable for both CNF and DNF. This effectively means that CL-constraints in         a non-trivial form cannot be efficiently incorporated into clustering algorithms. To overcome this, we introduce the notion         of a choice-set of constraints and prove that the feasibility problem for choice-sets is efficiently solvable for both ML         and CL constraints. We also present empirical results which indicate that the feasibility problem occurs extensively in real         world problems.      </content></document><document><year>2007</year><authors>Pedro Domingos1 </authors><title>Toward knowledge-rich data mining      </title><content>This position paper proposes knowledge-rich data mining as a focus of research, and describes initial steps in pursuing it.      </content></document><document><year>2007</year><authors>Wilson Wong1 | Wei Liu1  | Mohammed Bennamoun1 </authors><title>Tree-Traversing Ant Algorithm for term clustering based on featureless similarities      </title><content>Many conventional methods for concepts formation in ontology learning have relied on the use of predefined templates and rules,         and static resources such as WordNet. Such approaches are not scalable, difficult to port between different domains and incapable         of handling knowledge fluctuations. Their results are far from desirable, either. In this paper, we propose a new ant-based         clustering algorithm, Tree-Traversing Ant (TTA), for concepts formation as part of an ontology learning system. With the help of Normalized Google Distance (NGD) and nВ° of Wikipedia (nВ°W) as measures for similarity and distance between terms, we attempt to achieve an adaptable clustering method that is highly         scalable and portable across domains. Evaluations with an seven datasets show promising results with an average lexical overlap         of 97% and ontological improvement of 48%. At the same time, the evaluations demonstrated several advantages that are not         simultaneously present in standard ant-based and other conventional clustering methods.      </content></document><document><year>2007</year><authors>Colin Fyfe1 </authors><title>Two topographic maps for data visualisation      </title><content>We review a new form of self-organizing map which is based on a nonlinear projection of latent points into data space, identical         to that performed in the Generative Topographic Mapping (GTM) [Bishop et;al. (1997) Neurl Comput 10(1): 215&amp;#8211;234]. But whereas         the GTM is an extension of a mixture of experts, our new model is an extension of a product of experts [Hinton (2000) Technical         report GCNU TR 2000-004, Gatsby Computational Neuroscience Unit, University College, London]. We show visualisation results         on some real data sets and compare with the GTM. We then introduce a second mapping based on harmonic averages and show that         it too creates a topographic mapping of the data. We compare these mappings on real and artificial data sets.      </content></document><document><year>2007</year><authors>Abdelaziz Berrado1  | George C. Runger1 </authors><title>Using metarules to organize and group discovered association rules      </title><content>The high dimensionality of massive data results in the discovery of a large number of association rules. The huge number of         rules makes it difficult to interpret and react to all of the rules, especially because many rules are redundant and contained         in other rules. We discuss how the sparseness of the data affects the redundancy and containment between the rules and provide         a new methodology for organizing and grouping the association rules with the same consequent. It consists of finding metarules, rules that express the associations between the discovered rules themselves. The information provided by the metarules is         used to reorganize and group related rules. It is based only on data-determined relationships between the rules. We demonstrate         the suggested approach on actual manufacturing data and show its effectiveness on several benchmark data sets.      </content></document><document><year>2007</year><authors>F. Masseglia1 | P. Poncelet2 | M. Teisseire3  | A. Marascu1 </authors><title>Web usage mining: extracting unexpected periods from web logs      </title><content>Existing Web usage mining techniques are currently based on an arbitrary division of the data (e.g. &amp;#8220;one log per month&amp;#8221;) or         guided by presumed results (e.g. &amp;#8220;what is the customers&amp;#8217; behaviour for the period of Christmas purchases?&amp;#8221;). These approaches         have two main drawbacks. First, they depend on the above-mentioned arbitrary organization of data. Second, they cannot automatically         extract &amp;#8220;seasonal peaks&amp;#8221; from among the stored data. In this paper, we propose a specific data mining process (in particular,         to extract frequent behaviour patterns) in order to reveal the densest periods automatically. From the whole set of possible         combinations, our method extracts the frequent sequential patterns related to the extracted periods. A period is considered         to be dense if it contains at least one frequent sequential pattern for the set of users connected to the website in that         period. Our experiments show that the extracted periods are relevant and our approach is able to extract both frequent sequential         patterns and the associated dense periods.      </content></document><document><year>2005</year><authors>Elisa Bertino1 | Igor Nai Fovino2  | Loredana Parasiliti Provenza2 </authors><title>A Framework for Evaluating Privacy Preserving Data Mining Algorithms*               </title><content>Recently, a new class of data mining methods, known as privacy preserving data mining (PPDM) algorithms, has been developed by the research community working on security and knowledge discovery. The aim of these         algorithms is the extraction of relevant knowledge from large amount of data, while protecting at the same time sensitive         information. Several data mining techniques, incorporating privacy protection mechanisms, have been developed that allow one         to hide sensitive itemsets or patterns, before the data mining process is executed. Privacy preserving classification methods,         instead, prevent a miner from building a classifier which is able to predict sensitive data. Additionally, privacy preserving         clustering techniques have been recently proposed, which distort sensitive numerical attributes, while preserving general         features for clustering analysis. A crucial issue is to determine which ones among these privacy-preserving techniques better         protect sensitive information. However, this is not the only criteria with respect to which these algorithms can be evaluated.         It is also important to assess the quality of the data resulting from the modifications applied by each algorithm, as well         as the performance of the algorithms. There is thus the need of identifying a comprehensive set of criteria with respect to         which to assess the existing PPDM algorithms and determine which algorithm meets specific requirements.                     In this paper, we present a first evaluation framework for estimating and comparing different kinds of PPDM algorithms. Then,               we apply our criteria to a specific set of algorithms and discuss the evaluation results we obtain. Finally, some considerations               about future work and promising directions in the context of privacy preservation in data mining are discussed.            </content></document><document><year>2005</year><authors>Jianlin Cheng1| Michael J. Sweredoski1 | Pierre Baldi1 </authors><title>Accurate Prediction of Protein Disordered Regions by Mining Protein Structure Data      </title><content>Intrinsically disordered regions in proteins are relatively frequent and important for our understanding of molecular recognition         and assembly, and protein structure and function. From an algorithmic standpoint, flagging large disordered regions is also         important for ab initio protein structure prediction methods. Here we first extract a curated, non-redundant, data set of protein disordered regions         from the Protein Data Bank and compute relevant statistics on the length and location of these regions. We then develop an         ab initio predictor of disordered regions called DISpro which uses evolutionary information in the form of profiles, predicted secondary         structure and relative solvent accessibility, and ensembles of 1D-recursive neural networks. DISpro is trained and cross validated         using the curated data set. The experimental results show that DISpro achieves an accuracy of 92.8% with a false positive         rate of 5%. DISpro is a member of the SCRATCH suite of protein data mining tools available through http://www.igb.uci.edu/servers/psss.html.               </content></document><document><year>2005</year><authors>Rakesh Agrawal1 | Johannes Gehrke2 | Dimitrios Gunopulos3  | Prabhakar Raghavan4 </authors><title>Automatic Subspace Clustering of High Dimensional Data</title><content>Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate clusters in large high dimensional datasets.</content></document><document><year>2005</year><authors>Raymond Chi-Wing Wong1 | Ada Wai-Chee Fu1  | Ke Wang2 </authors><title>Data Mining for Inventory Item Selection with Cross-Selling Considerations</title><content>Association rule mining, studied for over ten years in the literature of data mining, aims to help enterprises with sophisticated decision making, but the resulting rules typically cannot be directly applied and require further processing. In this paper, we propose a method for actionable recommendations from itemset analysis and investigate an application of the concepts of association rules&amp;#x2014;maximal-profit item selection with cross-selling effect (MPIS). This problem is about choosing a subset of items which can give the maximal profit with the consideration of cross-selling effect. A simple approach to this problem is shown to be NP-hard. A new approach is proposed with consideration of the loss rule&amp;#x2014;a rule similar to the association rule&amp;#x2014;to model the cross-selling effect. We show that MPIS can be approximated by a quadratic programming problem. We also propose a greedy approach and a genetic algorithm to deal with this problem. Experiments are conducted, which show that our proposed approaches are highly effective and efficient.</content></document><document><year>2005</year><authors>Ben Kao1 | Minghua Zhang1 | Chi-Lap Yip1 | David W. Cheung1  | Usama Fayyad1</authors><title>Efficient Algorithms for Mining and Incremental Update of Maximal Frequent Sequences</title><content>We study two problems: (1) mining frequent sequences from a transactional database, and (2) incremental update of frequent sequences when the underlying database changes over time. We review existing sequence mining algorithms including GSP, PrefixSpan, SPADE, and ISM. We point out the large memory requirement of Pref ixSpan, SPADE, and ISM, and evaluate the performance of GSP. We discuss the high I/O cost of GSP, particularly when the database contains long frequent sequences. To reduce the I/O requirement, we propose an algorithm MFS, which could be considered as a generalization of GSP. The general strategy of MFS is to first find an approximate solution to the set of frequent sequences and then perform successive refinement until the exact set of frequent sequences is obtained. We show that this successive refinement approach results in a significant improvement in I/O cost. We discuss how MFS can be applied to the incremental update problem. In particular, the result of a previous mining exercise can be used (by MFS) as a good initial approximate solution for the mining of an updated database. This results in an I/O efficient algorithm. To improve processing efficiency, we devise pruning techniques that, when coupled with GSP or MFS, result in algorithms that are both CPU and I/O efficient.</content></document><document><year>2005</year><authors>Guimei Liu1 | Hongjun Lu1 | Wenwu Lou1 | Yabo Xu2  | Jeffrey Xu Yu2 </authors><title>Efficient Mining of Frequent Patterns Using Ascending Frequency Ordered Prefix-Tree      </title><content>Mining frequent patterns, including mining frequent closed patterns or maximal patterns, is a fundamental and important problem         in data mining area. Many algorithms adopt the pattern growth approach, which is shown to be superior to the candidate generate-and-test         approach, especially when long patterns exist in the datasets. In this paper, we identify the key factors that influence the         performance of the pattern growth approach, and optimize them to further improve the performance. Our algorithm uses a simple         while compact data structure&amp;#8212;ascending frequency ordered prefix-tree (AFOPT) to store the conditional databases, in which         we use arrays to store single branches to further save space. The AFOPT structure is traversed in top-down depth-first order.         Our analysis and experiment results show that the combination of the top-down traversal strategy and the ascending frequency         order achieves significant performance improvement over previous works.      </content></document><document><year>2005</year><authors>Guimei Liu1 | Hongjun Lu1 | Wenwu Lou1 | Yabo Xu2  | Jeffrey Xu Yu2 </authors><title>Efficient Mining of Frequent Patterns Using Ascending Frequency Ordered Prefix-Tree</title><content>Mining frequent patterns, including mining frequent closed patterns or maximal patterns, is a fundamental and important problem in data mining area. Many algorithms adopt the pattern growth approach, which is shown to be superior to the candidate generate-and-test approach, especially when long patterns exist in the datasets. In this paper, we identify the key factors that influence the performance of the pattern growth approach, and optimize them to further improve the performance. Our algorithm uses a simple while compact data structure&amp;#x2014;ascending frequency ordered prefix-tree (AFOPT) to store the conditional databases, in which we use arrays to store single branches to further save space. The AFOPT structure is traversed in top-down depth-first order. Our analysis and experiment results show that the combination of the top-down traversal strategy and the ascending frequency order achieves significant performance improvement over previous works.</content></document><document><year>2005</year><authors>Michihiro Kuramochi1  | George Karypis1 </authors><title>Finding Frequent Patterns in a Large Sparse Graph*               </title><content>Graph-based modeling has emerged as a powerful abstraction capable of capturing in a single and unified framework many of         the relational, spatial, topological, and other characteristics that are present in a variety of datasets and application         areas. Computationally efficient algorithms that find patterns corresponding to frequently occurring subgraphs play an important         role in developing data mining-driven methodologies for analyzing the graphs resulting from such datasets. This paper presents         two algorithms, based on the horizontal and vertical pattern discovery paradigms, that find the connected subgraphs that have         a sufficient number of edge-disjoint embeddings in a single large undirected labeled sparse graph. These algorithms use three         different methods for determining the number of edge-disjoint embeddings of a subgraph and employ novel algorithms for candidate         generation and frequency counting, which allow them to operate on datasets with different characteristics and to quickly prune         unpromising subgraphs. Experimental evaluation on real datasets from various domains show that both algorithms achieve good         performance, scale well to sparse input graphs with more than 120,000 vertices or 110,000 edges, and significantly outperform         previously developed algorithms.      </content></document><document><year>2005</year><authors>Masakazu Seno1  | George Karypis1 </authors><title>Finding Frequent Patterns Using Length-Decreasing Support Constraints</title><content>Finding prevalent patterns in large amount of data has been one of the major problems in the area of data mining. Particularly, the problem of finding frequent itemset or sequential patterns in very large databases has been studied extensively over the years, and a variety of algorithms have been developed for each problem. The key feature in most of these algorithms is that they use a constant support constraint to control the inherently exponential complexity of these two problems. In general, patterns that contain only a few items will tend to be interesting if they have a high support, whereas long patterns can still be interesting even if their support is relatively small. Ideally, we want to find all the frequent patterns whose support decreases as a function of their length without having to find many uninteresting infrequent short patterns. Developing such algorithms is particularly challenging because the downward closure property of the constant support constraint cannot be used to prune short infrequent patterns.In this paper we present two algorithms, LPMiner and SLPMiner. Given a length-decreasing support constraint, LPMiner finds all the frequent itemset patterns from an itemset database, and SLPMiner finds all the frequent sequential patterns from a sequential database. Each of these two algorithms combines a well-studied efficient algorithm for constant-support-based pattern discovery with three effective database pruning methods that dramatically reduce the runtime. Our experimental evaluations show that both LPMiner and SLPMiner, by effectively exploiting the length-decreasing support constraint, are up to two orders of magnitude faster, and their runtime increases gradually as the average length of the input patterns increases.</content></document><document><year>2005</year><authors>Karam Gouda1  | Mohammed J. Zaki2 </authors><title>GenMax: An Efficient Algorithm for Mining Maximal Frequent Itemsets      </title><content>We present GenMax, a backtrack search based algorithm for mining maximal frequent itemsets. GenMax uses a number of optimizations         to prune the search space. It uses a novel technique called progressive focusing to perform maximality checking, and diffset propagation to perform fast frequency computation. Systematic experimental comparison with previous work indicates that different methods         have varying strengths and weaknesses based on dataset characteristics. We found GenMax to be a highly efficient method to         mine the exact set of maximal patterns.      </content></document><document><year>2005</year><authors>Ying Zhao1 | George Karypis1  | Usama Fayyad1</authors><title>Hierarchical Clustering Algorithms for Document Datasets</title><content>Fast and high-quality document clustering algorithms play an important role in providing intuitive navigation and browsing mechanisms by organizing large amounts of information into a small number of meaningful clusters. In particular, clustering algorithms that build meaningful hierarchies out of large document collections are ideal tools for their interactive visualization and exploration as they provide data-views that are consistent, predictable, and at different levels of granularity. This paper focuses on document clustering algorithms that build such hierarchical solutions and (i) presents a comprehensive study of partitional and agglomerative algorithms that use different criterion functions and merging schemes, and (ii) presents a new class of clustering algorithms called constrained agglomerative algorithms, which combine features from both partitional and agglomerative approaches that allows them to reduce the early-stage errors made by agglomerative methods and hence improve the quality of clustering solutions. The experimental evaluation shows that, contrary to the common belief, partitional algorithms always lead to better solutions than agglomerative algorithms; making them ideal for clustering large document collections due to not only their relatively low computational requirements, but also higher clustering quality. Furthermore, the constrained agglomerative methods consistently lead to better solutions than agglomerative methods alone and for many cases they outperform partitional methods, as well.</content></document><document><year>2005</year><authors>Hassan A. Artail1 </authors><title>HIL-Tree: A Hierarchical Structure for Guiding Search into Test and Measurement Data Archives</title><content>This paper describes a novel algorithm that uses discontinuity detection to discover index vectors in test and measurement data archives containing multidimensional data. The index vectors are generated from individual data series in the archive and hold location information about jumps and changes in trends (discontinuities). They are related in a hierarchical manner to form a tree-like structure based on the alignment of the location information across the vectors. We call such trees Hierarchical Index Locations trees (HIL-trees), which are useful in guiding navigation into the raw data and in speeding up the process of retrieving data subsets based on given criteria. To demonstrate the practical value of the algorithm, we present a case study through which the algorithm is applied to real automotive emission test data archives, and show how it works. We also compare the HIL-tree to the well-known R-tree index structure and show how HIL-trees are advantageous in many aspects.</content></document><document><year>2005</year><authors>Geoffrey I. Webb1  | Songmao Zhang2 </authors><title>K-Optimal Rule Discovery</title><content>K-optimal rule discovery finds the K rules that optimize a user-specified measure of rule value with respect to a set of sample data and user-specified constraints. This approach avoids many limitations of the frequent itemset approach of association rule discovery. This paper presents a scalable algorithm applicable to a wide range of K-optimal rule discovery tasks and demonstrates its efficiency.</content></document><document><year>2005</year><authors>Hwanjo Yu1 | Jiong Yang2 | Jiawei Han3  | Xiaolei Li3 </authors><title>Making SVMs Scalable to Large Data Sets using Hierarchical Cluster Indexing      </title><content>Support vector machines (SVMs) have been promising methods for classification and regression analysis due to their solid mathematical         foundations, which include two desirable properties: margin maximization and nonlinear classification using kernels. However,         despite these prominent properties, SVMs are usually not chosen for large-scale data mining problems because their training         complexity is highly dependent on the data set size. Unlike traditional pattern recognition and machine learning, real-world         data mining applications often involve huge numbers of data records. Thus it is too expensive to perform multiple scans on         the entire data set, and it is also infeasible to put the data set in memory. This paper presents a method, Clustering-Based SVM (CB-SVM), that maximizes the SVM performance for very large data sets given a limited amount of resource, e.g., memory. CB-SVM applies         a hierarchical micro-clustering algorithm that scans the entire data set only once to provide an SVM with high quality samples.         These samples carry statistical summaries of the data and maximize the benefit of learning. Our analyses show that the training         complexity of CB-SVM is quadratically dependent on the number of support vectors, which is usually much less than that of         the entire data set. Our experiments on synthetic and real-world data sets show that CB-SVM is highly scalable for very large         data sets and very accurate in terms of classification.      </content></document><document><year>2005</year><authors>Ke Wang Wong1 | Senqiang Zhou1 | Qiang Yang2  | Jack Man Shun Yeung3 </authors><title>Mining Customer Value: From Association Rules to Direct Marketing</title><content>Direct marketing is a modern business activity with an aim to maximize the profit generated from marketing to a selected group of customers. A key to direct marketing is to select a subset of customers so as to maximize the profit return while minimizing the cost. Achieving this goal is difficult due to the extremely imbalanced data and the inverse correlation between the probability that a customer responds and the dollar amount generated by a response. We present a solution to this problem based on a creative use of association rules. Association rule mining searches for all rules above an interestingness threshold, as opposed to some rules in a heuristic-based search. Promising association rules are then selected based on the observed value of the customers they summarize. Selected association rules are used to build a model for predicting the value of a future customer. On the challenging KDD-CUP-98 dataset, this approach generates 41% more profit than the KDD-CUP winner and 35% more profit than the best result published thereafter, with 57.7% recall on responders and 78.0% recall on non-responders. The average profit per mail is 3.3 times that of the KDD-CUP winner.</content></document><document><year>2005</year><authors>Mohammed J. Zaki1 </authors><title>Mining Non-Redundant Association Rules      </title><content>The traditional association rule mining framework produces many redundant rules. The extent of redundancy is a lot larger         than previously suspected. We present a new framework for associations based on the concept of closed frequent itemsets. The number of non-redundant rules produced by the new approach is exponentially (in the length of the         longest frequent itemset) smaller than the rule set from the traditional approach. Experiments using several &amp;#8220;hard&amp;#8221; as well         as &amp;#8220;easy&amp;#8221; real and synthetic databases confirm the utility of our framework in terms of reduction in the number of rules presented         to the user, and in terms of time.      </content></document><document><year>2005</year><authors>C.I. Ezeife1  | Yi Lu1</authors><title>Mining Web Log Sequential Patterns with Position Coded Pre-Order Linked WAP-Tree</title><content>Sequential mining is the process of applying data mining techniques to a sequential database for the purposes of discovering the correlation relationships that exist among an ordered list of events. An important application of sequential mining techniques is web usage mining, for mining web log accesses, where the sequences of web page accesses made by different web users over a period of time, through a server, are recorded. Web access pattern tree (WAP-tree) mining is a sequential pattern mining technique for web log access sequences, which first stores the original web access sequence database on a prefix tree, similar to the frequent pattern tree (FP-tree) for storing non-sequential data. WAP-tree algorithm then, mines the frequent sequences from the WAP-tree by recursively re-constructing intermediate trees, starting with suffix sequences and ending with prefix sequences.This paper proposes a more efficient approach for using the WAP-tree to mine frequent sequences, which totally eliminates the need to engage in numerous re-construction of intermediate WAP-trees during mining. The proposed algorithm builds the frequent header node links of the original WAP-tree in a pre-order fashion and uses the position code of each node to identify the ancestor/descendant relationships between nodes of the tree. It then, finds each frequent sequential pattern, through progressive prefix sequence search, starting with its first prefix subsequence event. Experiments show huge performance gain over the WAP-tree technique.</content></document><document><year>2005</year><authors>Victor M. Becerra1 | Roberto K. H. Galv&amp;atilde o2  | Magda Abou-Seada3 </authors><title>Neural and Wavelet Network Models for Financial Distress Classification</title><content>This work analyzes the use of linear discriminant models, multi-layer perceptron neural networks and wavelet networks for corporate financial distress prediction. Although simple and easy to interpret, linear models require statistical assumptions that may be unrealistic. Neural networks are able to discriminate patterns that are not linearly separable, but the large number of parameters involved in a neural model often causes generalization problems. Wavelet networks are classification models that implement nonlinear discriminant surfaces as the superposition of dilated and translated versions of a single &amp;#x201c;mother wavelet&amp;#x201d; function. In this paper, an algorithm is proposed to select dilation and translation parameters that yield a wavelet network classifier with good parsimony characteristics. The models are compared in a case study involving failed and continuing British firms in the period 1997&amp;#x2013;2000. Problems associated with over-parameterized neural networks are illustrated and the Optimal Brain Damage pruning technique is employed to obtain a parsimonious neural model. The results, supported by a re-sampling study, show that both neural and wavelet networks may be a valid alternative to classical linear discriminant models.</content></document><document><year>2005</year><authors>Charu C. Aggarwal1 | Jiawei Han2 | Jianyong Wang3  | Philip S. Yu4 </authors><title>On High Dimensional Projected Clustering of Data Streams</title><content>The data stream problem has been studied extensively in recent years, because of the great ease in collection of stream data. The nature of stream data makes it essential to use algorithms which require only one pass over the data. Recently, single-scan, stream analysis methods have been proposed in this context. However, a lot of stream data is high-dimensional in nature. High-dimensional data is inherently more complex in clustering, classification, and similarity search. Recent research discusses methods for projected clustering over high-dimensional data sets. This method is however difficult to generalize to data streams because of the complexity of the method and the large volume of the data streams.In this paper, we propose a new, high-dimensional, projected data stream clustering method, called HPStream. The method incorporates a fading cluster structure, and the projection based clustering methodology. It is incrementally updatable and is highly scalable on both the number of dimensions and the size of the data streams, and it achieves better clustering quality in comparison with the previous stream clustering methods. Our performance study with both real and synthetic data sets demonstrates the efficiency and effectiveness of our proposed framework and implementation methods.</content></document><document><year>2005</year><authors>Charu C. Aggarwal1  | Paul S. Bradley1</authors><title>On the Use of Wavelet Decomposition for String Classification</title><content>In recent years, the technological advances in mapping genes have made it increasingly easy to store and use a wide variety of biological data. Such data are usually in the form of very long strings for which it is difficult to determine the most relevant features for a classification task. For example, a typical DNA string may be millions of characters long, and there may be thousands of such strings in a database. In many cases, the classification behavior of the data may be hidden in the compositional behavior of certain segments of the string which cannot be easily determined apriori. Another problem which complicates the classification task is that in some cases the classification behavior is reflected in global be havior of the string, whereas in others it is reflected in local patterns. Given the enormous variation in the behavior of the strings over different data sets, it is useful to develop an approach which is sensitive to both the global and local behavior of the strings for the purpose of classification. For this purpose, we will exploit the multi-resolution property of wavelet decomposition in order to create a scheme which can mine classification characteristics at different levels of granularity. The resulting scheme turns out to be very effective in practice on a wide range of problems.</content></document><document><year>2005</year><authors>Josep Domingo-Ferrer1  | VicenГ§ Torra2 </authors><title>Ordinal, Continuous and Heterogeneous k-Anonymity Through Microaggregation      </title><content>         k-Anonymity is a useful concept to solve the tension between data utility and respondent privacy in individual data (microdata)         protection. However, the generalization and suppression approach proposed in the literature to achieve k-anonymity is not equally suited for all types of attributes: (i) generalization/suppression is one of the few possibilities         for nominal categorical attributes; (ii) it is just one possibility for ordinal categorical attributes which does not always         preserve ordinality; (iii) and it is completely unsuitable for continuous attributes, as it causes them to lose their numerical         meaning. Since attributes leading to disclosure (and thus needing k-anonymization) may be nominal, ordinal and also continuous, it is important to devise k-anonymization procedures which preserve the semantics of each attribute type as much as possible. We propose in this paper         to use categorical microaggregation as an alternative to generalization/suppression for nominal and ordinal k-anonymization; we also propose continuous microaggregation as the method for continuous k-anonymization.      </content></document><document><year>2005</year><authors>Stephen E. Fienberg1  | Aleks|ra B. Slavkovic2 </authors><title>Preserving the Confidentiality of Categorical Statistical Data Bases When Releasing Information for Association Rules*               </title><content>In the statistical literature, there has been considerable development of methods of data releases for multivariate categorical         data sets, where the releases come in the form of marginal tables corresponding to subsets of the categorical variables. Very         recently some of the ideas have been extended to allow for the release of combinations of mixtures of marginal tables and         conditional tables for subsets of variables. Association rules can be viewed as conditional tables. In this paper we consider         possible inferences an intruder can make about confidential categorical data following the release of information on one or         more association rules. We illustrate this with several examples.      </content></document><document><year>2005</year><authors>Josep Domingo-Ferrer1  | VicenГ§ Torra2 </authors><title>Privacy in Data Mining      </title><content>Without Abstract</content></document><document><year>2005</year><authors>Josep M. Mateo-Sanz1 | Josep Domingo-Ferrer1  | Francesc SebГ©1 </authors><title>Probabilistic Information Loss Measures in Confidentiality Protection of Continuous Microdata      </title><content>Inference control for protecting the privacy of microdata (individual data) should try to optimize the tradeoff between data         utility (low information loss) and protection against disclosure (low disclosure risk). Whereas risk measures are bounded         between 0 and 1, information loss measures proposed in the literature for continuous data are unbounded, which makes it awkward         to trade off information loss for disclosure risk. We propose in this paper to use probabilities to define bounded information         loss measures for continuous microdata.      </content></document><document><year>2005</year><authors>Mark Girolami1  | Ata Kab&amp;aacute n2 </authors><title>Sequential Activity Profiling: Latent Dirichlet Allocation of Markov Chains</title><content>To provide a parsimonious generative representation of the sequential activity of a number of individuals within a population there is a necessary tradeoff between the definition of individual specific and global representations. A linear-time algorithm is proposed that defines a distributed predictive model for finite state symbolic sequences which represent the traces of the activity of a number of individuals within a group. The algorithm is based on a straightforward generalization of latent Dirichlet allocation to time-invariant Markov chains of arbitrary order. The modelling assumption made is that the possibly heterogeneous behavior of individuals may be represented by a relatively small number of simple and common behavioral traits which may interleave randomly according to an individual-specific distribution. The results of an empirical study on three different application domains indicate that this modelling approach provides an efficient low-complexity and intuitively interpretable representation scheme which is reflected by improved prediction performance over comparable models.</content></document><document><year>2005</year><authors>Wojciech Grohman1</authors><title>Using Convex Sets for Exploratory Data Analysis and Visualization      </title><content>In this paper a new, abstract method for analysis and visualization of multidimensional data sets in pattern recognition problems         is introduced. It can be used to determine the properties of an unknown, complex data set and to assist in finding the most         appropriate recognition algorithm. Additionally, it can be employed to design layers of a feedforward artificial neural network         or to visualize the higher-dimensional problems in 2-D and 3-D without losing relevant data set information. The method is         derived from the convex set theory and works by considering convex subsets within the data and analyzing their respective         positions in the original dimension. Its ability to describe certain set features that cannot be explicitly projected into         lower dimensions sets it apart from many other visualization techniques. Two classical multidimensional problems are analyzed         and the results show the usefulness of the presented method and underline its strengths and weaknesses.      </content></document><document><year>2005</year><authors>Wojciech Grohman1</authors><title>Using Convex Sets for Exploratory Data Analysis and Visualization</title><content>In this paper a new, abstract method for analysis and visualization of multidimensional data sets in pattern recognition problems is introduced. It can be used to determine the properties of an unknown, complex data set and to assist in finding the most appropriate recognition algorithm. Additionally, it can be employed to design layers of a feedforward artificial neural network or to visualize the higher-dimensional problems in 2-D and 3-D without losing relevant data set information. The method is derived from the convex set theory and works by considering convex subsets within the data and analyzing their respective positions in the original dimension. Its ability to describe certain set features that cannot be explicitly projected into lower dimensions sets it apart from many other visualization techniques. Two classical multidimensional problems are analyzed and the results show the usefulness of the presented method and underline its strengths and weaknesses.</content></document><document><year>2005</year><authors>Seong Keon Lee1 | Hyun-Cheol Kang2 | Sang-Tae Han2  | Kwang-Hwan Kim3 </authors><title>Using Generalized Estimating Equation to Learn Decision Tree with Multivariate Responses      </title><content>Previous decision tree algorithms have used Mahalanobis distance for multiple continuous longitudinal response or generalized         entropy index for multiple binary responses. However, these methods are limited to either continuous or binary responses.         In this paper, we suggest a new tree-based method that can analyze any type of multiple responses by using a statistical approach,         called GEE (generalized estimating equations). The value of this new technique is demonstrated with reference to an application         using web-usage survey.      </content></document></documents>