<?xml version="1.0" encoding="UTF-8" standalone="no"?><documents><document><year>2009</year><authors>Lifeng He1 | Yuyan Chao2| Yuka Shimajiri1| Hirohisa Seki1 | Hidenori Itoh1</authors><title>         A-SATCHMORE: SATCHMORE with availability checking      </title><content>We present an improvement of SATCHMORE, calledA-SATCHMORE, by incorporating availability checking into relevancy. Because some atoms unavailable to the further computation         are also marked relevant, SATCHMORE suffers from a potential explosion of the search space. Addressing this weakness of SATCHMORE,         we show that an atom does not need to be marked relevant unless it is available to the further computation and no non-Horn         clause needs to be selected unless all its consequent atoms are marked availably relevant, i.e., unless it is totally availably         relevant. In this way,A-SATCHMORE is able to further restrict the ues of non-Horn clauses (therefore to reduce the search space) and makes the proof         more goal-oriented. Our theorem prover,A-SATCHMORE, can be simply implemented in PROLOG based on SATCHMORE. We discuss how to incorporate availability cheeking into         relevancy, describe our improvement and present the implementation. We also prove that our theorem prover is sound and complete,         and provide examples to show the power of our availability approach.      </content></document><document><year>2009</year><authors>Haruki Nakamura1 | Susumu Date2 | Hideo Matsuda2  | Shinji Shimojo3 </authors><title>A challenge towards next-generation research infrastructure for advanced life science      </title><content>Recently, life scientists have expressed a strong need for computational power sufficient to complete their analyses within         a realistic time as well as for a computational power capable of seamlessly retrieving biological data of interest from multiple         and diverse bio-related databases for their research infrastructure. This need implies that life science strongly requires         the benefits of advanced IT. In Japan, the Biogrid project has been promoted since 2002 toward the establishment of a next-generation         research infrastructure for advanced life science. In this paper, the Biogrid strategy toward these ends is detailed along         with the role and mission imposed on the Biogrid project. In addition, we present the current status of the development of         the project as well as the future issues to be tackled.      </content></document><document><year>2009</year><authors>Z. M. Ariola1| B. C. Massey1| M. Sami1 | E. Tick1</authors><title>A common intermediate language and its use in partitioning concurrent declarative programs      </title><content>The plethora of concurrent declarative language families, each with subtly different semantics, makes the design and implementation         of static analyses for these languages a demanding task. However, many of the languages share underlying structure, and if         this structure can be exploited, static analysis techniques can be shared across language families. These techniques can thus         provide a common kernel for the implementation of quality compilers for this entire language class.                     The purpose of this paper is to exploit the similarities of non-strict functional and concurrent logic languages in the design               of a common intermediate language (CIL). The CIL is introduced incrementally, giving at each step the rationale for its extension.               As an application, we present, in CIL form, some state-of-the-art static partitioning algorithms from the literature. This               allows us to &amp;#8220;uncover&amp;#8221; the relative advantages and disadvantages of the analyses, and determine promising directions for improving               static partitioning.            </content></document><document><year>2009</year><authors>E. Tick1  | X. Zhong1</authors><title>A compile-time granularity analysis algorithm and its performance evaluation      </title><content>A major implementation problem with implicitly parallel languages, is that small grain size can lead to execution overheads         and reduced performance. We present a new granularity-analysis scheme that produces estimators, at compile time, of the relative         execution weight of each procedure invocation. These estimators can be cheaply evaluated at runtime to approximate the relative         task granularities, enabling intelligent scheduling decisions. Our method seeks to balance tradeoffs between analysis complexity,         estimator accuracy, and runtime overhead of evaluating the estimator. To this end, rather than analyze data size or dependencies,         we introduceiteration parameters to handle recursive procedures. This simplification facilitates solving the recurrence equations that describe the granularity         estimators, and reduces the runtime overhead of evaluating these estimators. The algorithm is described in the context of         concurrent logic programming languages, although the concepts are applicable to functional languages in general. We show,         for a benchmark suite, that the method accurately estimates cost. Multiprocessor simulation results quantify the advantage         of dynamically scheduling tasks with the granularity information.      </content></document><document><year>2009</year><authors>Zahran Halim1</authors><title>A data-driven machine for OR-parallel evaluation of logic programs      </title><content>Research in the area of parallel evaluation mechanisms for logic programs have led to the proposal of a number of schemes         exploiting various forms of parallelism. Many of the early models have been based on the conventional approach of organising         concurrent components of computations as communicating processes. More recently, however, models based on more novel computation         organisations, in particular, data-driven organisations, have been proposed. This paper describes the development of one such         model, its implementation and the design of a data-driven machine to support it. The model exploits a form of parallelism         known as OR-parallelism and is particularly suited to database applications, although it would also support general applications.         It is envisaged that the proposed machine may be refined into an efficient database engine, which can then be a component         of a more powerful and integrated logic programming machine.      </content></document><document><year>2009</year><authors>Wolfgang Bibel1| 2</authors><title>A deductive solution for plan generation      </title><content>A new deductive method for solving robot problems is presented in this paper. The descriptions of initial and goal situations         are given by logical formulas. The primitive actions are described by rules, i. e. logical formulas as well. Altogether this         results in a problem description like in logic programming or in program synthesis. A solution is generated by a proof of         this description like in program synthesis, except that here proofs have to be strictly linear. This restriction is the clue         of our solution; it can be easily added as an option to any theorem prover such as one based on the connection method used         hereafter. In fact this restriction speeds up the proof search considerably. At the same time our approach offers an elegant         solution of the frame problem.      </content></document><document><year>2009</year><authors>Teijiro Isokawa1 | Ferdin| Peper2 | Shin&amp;#8217 ya Kowada1| Naotake Kamiura1  | Nobuyuki Matsui1 </authors><title>A Defect Localization Scheme for Cellular Nanocomputers      </title><content>Computers with device feature sizes of a few nanometers&amp;#8212;so-called nanocomputers&amp;#8212;are expected within a few decades, but this         expectation is accompanied by the realization that the boundary conditions of such systems differ substantially from those         of current VLSI-based computers. Prominent among the concerns is the increased degree of permanent defects that will affect         nanocomputers, such as defects caused by imperfections at the manufacturing stage, but also defects occurring later, possibly         even during the use of these systems. New techniques to deal with defects are called for, but given the huge number of devices         involved, such techniques may need to be self-contained: they need be applicable at local levels without outside control,         even while computations continue to take place. This paper proposes an important element in such techniques, i.e. the localization         of defects among a huge number of devices. It employs a cellular automaton-based architecture, and uses statistical techniques         combined with randomly moving configurations in the cellular space to estimate defect locations.      </content></document><document><year>2009</year><authors>Kohei Noshita1  | Xiao -Xun He2</authors><title>A fast algorithm for translating combinator expressions with BC-chains      </title><content>A fast algorithm is presented for translating lambda expressions to combinator trees with BC-chains. The time complexity of         this algorithm is O (n log n) in the worst case, where n is the length of an input expression. Furthermore it requires only         O (n log n) working space. This result achieves a substantial improvement to the previously known algorithm having the quadratic         complexity. The basic idea of the algorithm may be applied to practical processing systems, whether they use BC-chains or         not.      </content></document><document><year>2009</year><authors>Kazuhisa Kawai1 | Riichiro Mizoguchi2| Osamu Kakusho2 | Jun&amp;#8217 ichi Toyoda2</authors><title>A framework for ICAI systems based on inductive inference and logic programming      </title><content>The main components of an Intelligent Computer-Assisted Instruction (ICAI) system are the expertise, the student model and         tutoring strategies. The student model manages what the student dose and dose not understand, and the performance of an ICAI         system depends largely on how well the student model approximates the human student. We propose a new framework for ICAI systems         which uses the inductive inference for constructing the student model from the student&amp;#8217;s behavior. In the framework, both         the expertise and the student model are represented as Prolog programs, which enables to express the meta-knowledge that is         the knowledge of how to use the knowledge. Since the construction of the student models is performed independently of the         expertise, the framework is domain-independent. Therefore, an ICAI system for any subject area can be built with the framework.         As an example, the ICAI system teaching chemical reaction is presented together with a sample performance. The authors believe         that the new framework for ICAI systems based on logic programming and inductive inference could be a breakthrough of the         future ICAI systems.      </content></document><document><year>2009</year><authors>Eduardo Huedo1 | Ugo Bastolla1 | RubГ©n S. Montero2  | Ignacio M. Llorente2| 1 </authors><title>A framework for protein structure prediction on the grid      </title><content>The large number of protein sequences, provided by genomic projects at an increasing pace, constitutes a challenge for large         scale computational studies of protein structure and thermodynamics. Grid technology is very suitable to face this challenge,         since it provides a way to access the resources needed in compute and data intensive applications. In this paper, we show         the procedure to adapt to the Grid an algorithm for the prediction of protein thermodynamics, using the GridWay tool. GridWay         allows the resolution of large computational experiments by reacting to events dynamically generated by both the Grid and         the application.      </content></document><document><year>2009</year><authors>A. K. Goswami1 | L. M. Patnaik2</authors><title>A functional style of programming with CSP-like communication mechanisms      </title><content>This paper introduces CSP-like communication mechanisms into Backus&amp;#8217; Functional Programming (FP) systems extended by nondeterministic         constructs. Several new functionals are used to describe nondeterminism and communication in programs. The functionals union         and restriction are introduced into FP systems to develop a simple algebra of programs with nondeterminism. The behaviour         of other functionals proposed in this paper are characterized by the properties of union and restriction. The axiomatic semantics         of communication constructs are presented. Examples show that it is possible to reason about a communicating program by first         transforming it into a non-communicating program by using the axioms of communication, and then reasoning about the resulting         non-communicating version of the program. It is also shown that communicating programs can be developed from non-communicating         programs given as specifications by using a transformational approach.      </content></document><document><year>2009</year><authors>Masayuki Ishinishi1 | Yuhsuke Koyama2 | Hiroshi Deguchi2  | Hajime Kita3 </authors><title>A futures market-based model for dynamic network resource allocation      </title><content>Management of telecommunication network requires quick, continuous and decentralized allocation of network bandwidth to various         sorts of demands. So as to achieve the efficient network resource allocation, this paper describes a market-based model combining         futures market with the agent-based approach. That is, utilization time is divided into many timeslots, and futures markets         in hereafter use of bandwidth are opened. In our model, all market participants (software agents) observe only market prices         and decide to buy or sell bandwidth trying to maximize their utilities over time so that they can secure enough network resources.         The authors discuss network resource allocation through simulation using the proposed model.      </content></document><document><year>2009</year><authors>Shigemasa Suganuma1 | Van-Nam Huynh1 | Yoshiteru Nakamori1  | Shouyang Wang2</authors><title>A Fuzzy set based approach to generalized landscape theory of aggregation      </title><content>In this paper, we firstly reformulate the landscape theory of aggregation (Axelrod and Bennett, 1993) in terms of an optimization         problem, and then straightforwardly propose a fuzzy-set-theoretic based extension for it. To illustrate efficiency of the         proposal, we make a simulation with the proposed framework for the international alignment of the Second World War in Europe.         It is shown that the obtained results are essentially comparable to those given by the original theory. Consequently, the         fuzzy-set-theoretic based extension of landscape theory can allow us to analyze a wide variety of aggregation processes in         politics, economics, and society in a more flexible manner.      </content></document><document><year>2009</year><authors>Maurice Bruynooghe1 | Danny De Schreye1 | Bern Martens1</authors><title>A general criterion for avoiding infinite unfolding during partial deduction      </title><content>Well-founded orderings are a commonly used tool for proving the termination of programs. We introduce related concepts specialised         to SLD-trees. Based on these concepts, we formulate formal and practical criteria for controlling the unfolding during the         construction of SLD-trees that form the basis of a partial deduction. We provide algorithms that allow to use these criteria         in a constructive way. In contrast to the many ad hoc techniques proposed in the literature, our technique provides both a         formal and practically applicable framework.      </content></document><document><year>2009</year><authors>Juan Carlos Augusto1| 2 </authors><title>A general framework for reasoning about change      </title><content>The capability to represent and use concepts like time and events in computer science is essential to solve a wide class of         problems characterized by the notion of change. Real-time, databases and multimedia are just a few of several areas which         needs good tools to deal with time. Another area where this concepts are essential is artificial intelligence because an agent         must be able to reason about a dynamic environment.                     In this work a formalism is proposed which allows the representation and use of several features that had been recognized               as useful in the attempts to solve such class of problems. A general framework based on a many-sorted logic is proposed centering               our attention in issues such as the representation of time, actions, properties, events and causality. The proposal is compared               with related work from the temporal logic and artificial intelligence areas. This work complements and enhances previously               related efforts on formalizing temporal concepts with the same purpose.            </content></document><document><year>2009</year><authors>Thomas Wilmes1 </authors><title>A generalized approach to metaprogramming in logic grammars      </title><content>This paper extends Definite Clause Grammars (DCGs) by a metaprogramming facility which supports sequences of items (terminals,         nonterminals, Prolog goals, etc.) as values of variables. Variables may be employed themselves as a new sort of item in the         right-hand side of a rule, in order to activate the respective item sequences they are instantiated to. In addition, so-called         assignments render it possible to save the terminal string derived from an item (or even from an item sequence) in a variable         for later manipulation, or to unify it with another string.                     The new approach, called Assignment Metagrammars (AMGs), is compared with H. Abramson&amp;#8217;s meta-nonterminals and the Discontinuous               Grammars (DGs) or Gapping Grammars (GGs) proposed by V. Dahl and H. Abramson. AMGs are implemented by a compiler that translates               them into efficient Prolog programs.            </content></document><document><year>2009</year><authors>Hiroaki Imade1| Ryohei Morishita1| Isao Ono1| Norihiko Ono1 | Masahiro Okamoto2 </authors><title>A grid-oriented genetic algorithm framework for bioinformatics      </title><content>In this paper, we propose a framework for enabling for researchers of genetic algorithms (GAs) to easily develop GAs running         on the Grid, named &amp;#8220;Grid-Oriented Genetic algorithms (GOGAs)&amp;#8221;, and actually &amp;#8220;Gridify&amp;#8221; a GA for estimating genetic networks,         which is being developed by our group, in order to examine the usability of the proposed GOGA framework. We also evaluate         the scalability of the &amp;#8220;Gridified&amp;#8221; GA by applying it to a five-gene genetic network estimation problem on a grid testbed constructed         in our laboratory.      </content></document><document><year>2009</year><authors>Ken Kaneiwa1 </authors><title>A hybrid reasoning system for terminologies and first-order clauses in knowledge bases      </title><content>Description Logics (DLs) theoretically explore knowledge representation and reasoning in concept languages. However, since         they are conceptually oriented, they are not equipped with rule-based reasoning mechanisms for assertional knowledge bases         &amp;#8212; specifically, rules and facts in Logic Programming (LP), or the interaction of rules and facts with terminological knowledge.         To combine rule-based reasoning with terminological knowledge, this paper presents a hybrid reasoning system for DL knowledge         bases (TBox and ABox) and first-order clause sets. The primary result of this study involves the design of a sound and completeresolution method for thecomposed knowledge bases, and this method possesses features of an effective deduction procedure such as Robinson&amp;#8217;s Resolution Principle.      </content></document><document><year>2009</year><authors>Taizo Miyachi1 | Susumu Kunifuji1| Hajime Kitakami1| Koichi Furukawa1| Akikazu Takeuchi1 | Haruo Yokota1</authors><title>A knowledge assimilation method for logic databases      </title><content>In this paper we consider a deductive question-answering system for relational databases as a logic database system, and propose         a knowledge assimilation method suitable for such a system. The concept of knowledge assimilation for deductive logic is constructed         in an implementable form based on the notion of amalgamating object language and metalanguage. This concept calls for checks         to be conducted on four subconcepts, provability, contradiction, redundancy, independency, and their corresponding internal         database updates. We have implemented this logic database knowledge assimilation program in PROLOG, a logic programming language,         and have found PROLOG suitable for knowledge assimilation implementation.      </content></document><document><year>2009</year><authors>Katsumi Nitta1| Juntaro Nagao2 | Tetsuya Mizutori3</authors><title>A knowledge representation and inference system for procedural law      </title><content>KRIP-2 is a name of a software tool for building expert systems of a legal problem. It was developed to build an expert system         for the Patent Law. Laws can be classified into the substantive laws and the procedural laws, and the Patent Law contains         both of them. As these laws have different features, it is inconvenient to develop the knowledge base of these in the same         knowledge representation. To develop a knowledge base of laws, a knowledge representation language KRIP/L was introduced.         KRIP/L was an integration of the object oriented concept and extended Prolog, and has useful mechanisms to describe the phenomena         occured in the legal problem. KRIP/L-2 is the second version of KRIP/L. KRIP-2 is an implementation of KRIP/L-2, and composed         of some utility modules. KRIP-2 is implemented in Prolog, and an expert system for the Patent Law is developed in KRIP-2.      </content></document><document><year>2009</year><authors>Stephen Taylor1| Evyatar Av-Ron1 | Ehud Shapiro1</authors><title>A layered method for process and code mapping      </title><content>The mapping problem has been shown to be computationally equivalent to the graph isomorphism problem; as such it is unlikely         that a polynomial time algosrithm exists for its solution. Practical algorithms of general applicability and low computational         complexity have not been found and are unlikely to appear.                     This paper describes a layered method to support specialised process and code mapping strategies. The method separates the               task of mapping a problem to a virtual machine from the task of mapping a virtual machine to a physical machine. It allows               multiple virtual machines to execute concurrently and many applications to run on a single virtual machine.            </content></document><document><year>2009</year><authors>Ikuo Takeuchi1| Hiroshi Okuno1 | Nobuyasu Ohsato1</authors><title>A list processing language TAO with multiple programming paradigms      </title><content>This paper describes an interpreter-centered list processing language TAO which supports the logic programming paradigm and         the object-oriented programming paradigm together with the conventional procedural programming paradigm in the framework of         the Lisp language. TAO allows the user to mix these programming paradigms in solving complicated and multifaceted AI problems.         The fundamentals of these programing paradigms, namely, unification, message passing and function call can nest each other         in an expression. Thus, the user can use the result of a function call or a message passing in a unification straightforwardly         and vice versa. TAO also supports the concurrent programming. The implementation of the TAO interpreter on a Lisp machine         called ELIS achieves a remarkable efficiency.      </content></document><document><year>2009</year><authors>Yasutaka Takeda1 | Hiroshi Nakashima1| Kanae Masuda1| Takashi Chikayama2 | Kazuo Taki2</authors><title>A load balancing mechanism for large scale multiprocessor systems and its implementation      </title><content>In large scale multiprocessor systems, the distance between processors should be taken into account by software to reduce         the network traffic and the communication overhead. A load balancing method based on P3 (Processing Power Plane) model is proposed to enable programmers to specify distributing computational load, keeping the         locality of the computation. In this method, a process is allocated to a rectangle on a hypothetical processing power plane.         The size of the rectangle represents the processing power given to the process, and the distance between rectangles represents         the communication cost between them. This plane is divided to processors, and the region of the processor may be dynamically         reshaped to alleviate imbalance on P3. Mechanism for realization of the method has been implemented on the Multi-PSI/version, 2, which is a parallel processing         system with 64 processing elements connected to form a 2-dimensional mesh network. A packet transmission mechanism of the         Multi-PSI/version 2 is described, which realizes the process distribution along with the balancing method.      </content></document><document><year>2009</year><authors>Seif Haridi1 </authors><title>A logic programming language based on the Andorra model      </title><content>The Andorra model is a parallel execution model of logic programs which exploits the dependent and-parallelism and or-parallelism         inherent in logic programming. We present a flat subset of a language based on the Andorra model, henceforth called Andorra         Prolog, that is intended to subsume both Prolog and the committed choice languages. Flat Andorra, in addition todon&amp;#8217;t know anddon&amp;#8217;t care nondeterminism, supports control of or-parallel split, synchronisation on variables, and selection of clauses. We show the         operational semantics of the language, and its applicability in the domain of committed choice languages. As an examples of         the expressiveness of the language, we describe a method for communication between objects by time-stamped messages, which         is suitable for expressing distributed discrete event simulation applications. This method depends critically on the ability         to expressdon&amp;#8217;t know nondeterminism and thus cannot easily be expressed in a committed choice language.      </content></document><document><year>2009</year><authors>Robert Kowalski1 | Marek Sergot1</authors><title>A logic-based calculus of events      </title><content>We outline an approach for reasoning about events and time within a logic programming framework. The notion of event is taken         to be more primitive than that of time and both are represented explicitly by means of Horn clauses augmented with negation         by failure.                     The main intended applications are the updating of databases and narrative understanding. In contrast with conventional databases               which assume that updates are made in the same order as the corresponding events occur in the real world, the explicit treatment               of events allows us to deal with updates which provide new information about the past.            </content></document><document><year>2009</year><authors>Alvaro A. A. Fern|es1 | M. Howard Williams2 | Norman W. Paton3</authors><title>A logic-based integration of active and deductive databases      </title><content>A logic-based approach to the specification of active database functionality is presented which not only endows active databases         with a well-defined and well-understood formal semantics, but also tightly integrates them with deductive databases. The problem         of endowing deductive databases with rule-based active behaviour has been addressed in different ways. Typical approaches         include accounting for active behaviour by extending the operational semantics of deductive databases, or, conversely, accounting         for deductive capabilities by constraining the operational semantics of active databases. The main contribution of the paper         is an alternative approach in which a class of active databases is defined whose operational semantics is naturally integrated         with the operational semantics of deductive databases without either of them strictly subsuming the other. The approach is         demonstrated via the formalization of the syntax and semantics of an active-rule language that can be smoothly incorporated         into existing deductive databases, due to the fact that the standard formalization of deductive databases is reused, rather         than altered or extended. One distinctive feature of the paper is its use of ahistory, as defined in the Kowalski-Sergot event-calculus, to define event occurrences, database states and actions on these. This         has proved to be a suitable foundation for a comprehensive logical account of the concept set underpinning active databases.         The paper thus contributes a logical perspective to the ongoing task of developing a formal theory of active databases.      </content></document><document><year>2009</year><authors>Howard Bowman1</authors><title>A LOTOS based tutorial on formal methods for object-oriented distributed systems      </title><content>The majority of formal methods for distributed systems have their origins in the 1980&amp;#8217;s and were targeted at the early generations         of distributed systems. However, modern distributed systems have new features not found in the early systems, e.g. they areobject-oriented, havemobile components, aretime sensitive and are constructed according to advanced system development architectures, e.g.viewpoints models. A major topic of current research is thus, how to enhance the existing formal techniques in order to support these new features.         This paper gives a tutorial level review of this research area. We particularly focus on the process algebra LOTOS and consider         how the technique can be reconciled with these new features.      </content></document><document><year>2009</year><authors>Setsuo Arikawa1 | Satoru Miyano1| Ayumi Shinohara1| Satoru Kuhara2| Yasuhito Mukouchi3 | Takeshi Shinohara4</authors><title>A machine discovery from amino acid sequences by decision trees over regular patterns      </title><content>This paper describes a machine learning system that discovered a &amp;#8220;negative motif&amp;#8221;, in transmembrane domain identification         from amino acid sequences, and reports its experiments on protein data using PIR database. We introduce a decision tree whose         nodes are labeled with regular patterns. As a hypothesis, the system produces such a decision tree for a small number of randomly         chosen positive and negative examples from PIR. Experiments show that our system finds reasonable hypotheses very successfully.         As a theoretical foundation, we show that the class of languages defined by decesion trees of depth at mostd overk-variable regular patterns is polynomial-time learnable in the sense of probably approximately correct (PAC) learning for         any fixedd, k&amp;#8805;0.      </content></document><document><year>2009</year><authors>F. A. Murzin1  | V. A. Sluev2</authors><title>A memory organization for parallel computers      </title><content>In this paper a computer memory system intended for storing an arbitrary sequence of multidimensional arrays is described.         This memory system permits a parallel access to the cuts distinguished in the given array by fixing one of the coordinates         and to the large set of parallelepipeds which are the same dimension subarrays of the given arrays.      </content></document><document><year>2009</year><authors>W. F. Clocksin1 | H. Alshawi2</authors><title>A method for efficiently executing horn clause programs using multiple processors      </title><content>Previous investigations have suggested the use of multiple communicating processors for executing logic programs. However,         this strategy lacks efficiency due to competition for memory and communication bandwidth, and this is a problem that has been         largely neglected. In this paper we propose a realistic model for executing logic programs with low overhead on multiple processors.         Our proposal does not involve shared memory or copying computation state between processors. The model organises computations         over the nondeterministic proof tree so that different processors explore unique deterministic computation paths independently,         in order to exploit the &amp;#8220;OR-parallelism&amp;#8221; present in a program. We discuss the advantages of this approach over previous ones,         and suggest control strategies for making it effective in practice.      </content></document><document><year>2009</year><authors>Takuichi Nishimura1 | Hiroaki Yabe1 | Ryuichi Oka1</authors><title>A method of model improvement for spotting recognition of gestures using an image sequence      </title><content>We have developed a real-time gesture recognition system whose models can be taught by only one instruction. Therefore the         system can adapt to new gesture performer quickly but it can not raise the recognition rates even if we teach gestures many         times. That is because the system could not utilize all the teaching data. In order to cope with the problem, averages of         teaching data are calculated. First, the best frame correspondence of the teaching data and the model is obtained by Continuous         DP. Next the averages and variations are calculated for each frame of the model. We show the effectiveness of our method in         the experiments.      </content></document><document><year>2009</year><authors>Yuzuru Tanaka1</authors><title>A multiport page-memory architecture and a multiport disk-cache system      </title><content>Everlasting demands for solutions to ever growing computation problems and demands for efficient means to manage and utilize         sophisticated information have caused an increase in the amount of data necessary to handle a job, while drastic reduction         in CPU prices is encouraging massive parallel architectures for gigantic data processing. These trends are increasing the         importance of a large shared buffer memory with 103&amp;#8764;104 simultaneously accessible ports. This paper proposes a multiport page buffer architecture that allows 103&amp;#8764;104 concurrent accesses and causes no access conflict nor suspension. It consists of a set of memory banks and multistaged switching         networks with controllers that control each row of the networks. Consecutive words in each page are stored orthogonally across         banks. Memory interleaving may be applied to improve access rate in consecutive retrievals. When used as a disk cache memory,         it decreases the number of disk accesses and increases both the page transfer rate and the maximum number of concurrent page         accesses.      </content></document><document><year>2009</year><authors>Kazuhiro Fuchi1 </authors><title>A new beginning      </title><content>Without Abstract</content></document><document><year>2009</year><authors>Steffen HГ¶lldobler1  | Josef Schneeberger2</authors><title>A new deductive approach to planning      </title><content>We introduce a new deductive approach to planning which is based on Horn clauses. Plans as well as situations are represented         as terms and, thus, are first-class objects. We do neither need frame axioms nor state-literals. The only rule of inference         is the SLDE-resolution rule, i.e. SLD-resolution, where the traditional unification algorithm has been replaced by anE-unification procedure. We exemplify the properties of our method such as forward and backward reasoning, plan checking, and         the integration of general theories. Finally, we present the calculus and show that it is sound and complete.      </content></document><document><year>2009</year><authors>Nobuyuki Ichiyoshi1 | Kazuaki Rokusawa1| Katsuto Nakajima1 | Yu Inamura1</authors><title>A new external reference management and distributed unification for KL1      </title><content>This paper describes a new external reference management scheme for KL1, a committed choice logic programming language based         on GHC. The significance of the new scheme is that it realizes incremental inter-processor garbage collection. Previous distributed         implementations of committed choice languages had not seriously addressed inter-processor garbage collection.                     Incremental inter-precessor garbage collection is realized by the Weighted Export Counting (WEC). It is a first attempt to               use the weighted reference counting technique in logic programming language implementation, and is also new in that it has               introduced export and import tables for making independent local garbage collection possible and reducing the number of inter-processor               read requests.            </content></document><document><year>2009</year><authors>FranГ§ois Fages1 </authors><title>A new fixpoint semantics for general logic programs compared with the well-founded and the stable model semantics      </title><content>We study a new fixpoint semantics for logic programs with negation. Our construction is intermediate between Van Gelder&amp;#8217;s         well-founded model and Gelfond and Lifschitz&amp;#8217;s stable model semantics. We show first that the stable models of a logic programP are exactly the well-supported models ofP, i.e. the supported models with loop-free finite justifications. Then we associate to any logic programP a non-monotonic operator over the semilattice of justified interpretations, and we define in an abstract form its ordinal         powers. We show that the fixpoints of this operator are the stable models ofP, and that its ordinal powers after some ordinala are extensions of the well-founded partial model ofP. In particular ifP has a well-founded model then that canonical model is also an ordinal power and the unique fixpoint of our operator. We show         with examples of logic programs which have a unique stable model but no well-founded model that the converse is false. We         relate also our work to Doyle&amp;#8217;s truth maintenance system and some implementations of rule-based expert systems.      </content></document><document><year>2009</year><authors>Kazuo Iwama1| 2  | Akinori Kawachi1| 2 </authors><title>A new quantum claw-finding algorithm for three functions      </title><content>Fork functionsf         1, ...f         k, ak-tuple (x         1, ...x         k) such thatf         1(x         1)=...=f         k(x         k) is called a claw off         1, ...,f         k. In this paper, we construct a new quantum claw-finding algorithm for three functions that is efficient when the numberM of intermediate solutions is small. The known quantum claw-finding algorithm for three functions requiresO(N         7/8 logN) queries to find a claw, but our algorithm requiresO(N         3/4 logN) queries ifM &amp;#8804; &amp;#8730;N andO(N         7/12         M         1/3 logN) queries otherwise. Thus, our algorithm is more efficient ifM&amp;#8804;N         7/8.      </content></document><document><year>2009</year><authors>Khayri A. M. Ali1</authors><title>A parallel copying garbage collection scheme for shared-memory multiprocessors      </title><content>Earlier work on parallel copying garbage collection is based on parallelization of sequential schemes with breadth-first traversing         of live data. Recently it has been demonstrated that sequential copying schemes with depth-first traversing of live data are         more flexible and more efficient than the corresponding ones with breadth-first traversal. A clear advantage of the former         is that they work with no extra space overheads on non-contiguous memory blocks, which allows more flexible implementation.                     This paper shows how to parallelize an efficient depth-first copying scheme while retaining its high efficiency and flexibility.</content></document><document><year>2009</year><authors>Yuji Matsumoto1 </authors><title>A parallel parsing system for natural language analysis      </title><content>The paper presents a parallel parsing system for Definite Clause Grammars suitable for committed-choice parallel logic programming         languages. Grammatical elements such as words and nonterminal symbols are defined as parallel processes. Parsing is done by         the processcommunication. The advantage of the system is that all the grammar rules are compiled into the parallel logic programming         language and the program has neither any side-effect nor duplicated computation.      </content></document><document><year>2009</year><authors>Carl Vogel1  | Fred Popowich2</authors><title>A parametric definition for a family of inheritance reasoners      </title><content>This paper gives a declarative specification of a popular inheritance system and shows how simple changes to this specification         can result in different path-based reasoners. This parameterized definition provides a deeper understanding of the fundamental         differences between some of the more popular path-based inheritance reasoners. In particular, it allows the clarification         of some of the results on the complexity of reasoning in the various systems. The uniform framework also allows definition         of novel systems which constitute intermediate points in the space of possible reasoners, and facilitates perspicuous Prolog         implementation.      </content></document><document><year>2009</year><authors>Raf Venken1  | Bart Demoen2</authors><title>A partial evaluation system for Prolog: some practical considerations      </title><content>An introduction to the basic concepts of partial evaluation is followed by a short description of its use as a program transformation         tool.                     A number of difficulties in building a partial evaluation tool, related to particular features of the language Prolog, are               discussed.            </content></document><document><year>2009</year><authors>Hidehiko Masuhara1  | Akinori Yonezawa2 </authors><title>A portable approach to dynamic optimization in run-time specialization      </title><content>This paper proposes arun-time bytecode specialization (BCS) technique that analyzes programs and generates specialized programs at run-time in an intermediate language. By using         an intermediate language for code generation, a back-end system canoptimize the specialized programs after specialization. The system uses Java virtual machine language (JVML) as the intermediate language, which allows the system to easily achieve         practicalportability and to use existing sophisticated just-in-time (JIT) compilers as its back-end. The binding-time analysis algorithm is based         on a type system, and covers a non-object-oriented subset of JVML. The specializer generates programs on a per-instruction         basis, and can performmethod inlining at run-time. Our performance measurements show that a non-trivial application program specialized at run-time by BCS runs approximately         3&amp;#8211;4 times faster than the unspecialized one. Despite the large overhead of JIT compilation of specialized code, we observed         that the overall performance of the application can be improved.      </content></document><document><year>2009</year><authors>Antonio Brogi1 | Simone Contiero1</authors><title>A program specialiser for meta-level compositions of logic programs      </title><content>Metal-level compositions of object logic programs are naturally implemented by means of meta-programming techniques. Metainterpreters         defining program compositions however suffer from a computational overhead that is due partly to the interpretation layer         present in all meta-programs, and partly to the specific interpretation layer needed to deal with program compositions.                     We show that meta-interpreters implementing compositions of object programs can be fruitfully specialised w.r.t. meta-level               queries of the form Demo (E, G), where E denotes a program expression and G denotes a (partially instantiated) object level               query. More precisely, we describe the design and implementation of declarative program specialiser that suitably transforms               such meta-interpreters so as to sensibly reduce &amp;#8212; if not to completely remove &amp;#8212; the overhead due to the handling of program               compositions. In many cases the specialiser succeeds in eliminating also the overhead due to meta-interpretation.            </content></document><document><year>2009</year><authors>Hitoshi Aida1 | Hidehiko Tanaka1 | Tohru Moto-Oka1</authors><title>A prolog extension for handling negative knowledge      </title><content>Pure prolog has certain defects as a programming language. To resolve such problems, some extensions of prolog which go beyond         horn logic, or alternatively, some non-logical mechanisms need to be introduced. Non-logical mechanisms, however, often do         not work as the programmer expects.                     In this paper, a simple extension of prolog based on the procedural interpretation of ordinary prolog is discussed. The extended               prolog can deal with negative knowledge as well as positive in a unified manner, and improve the terminability of programs.            </content></document><document><year>2009</year><authors>AndrГ©s CordГіn-Franco1 | Miguel A. GutiГ©rrez-Naranjo1 | Mario J. PГ©rez-JimГ©nez1  | Fern|o Sancho-Caparrini1 </authors><title>A Prolog simulator for deterministic P systems with active membranes      </title><content>In this paper we propose a new way to represent P systems with active membranes based on Logic Programming techniques. This         representation allows us to express the set of rules and the configuration of the P system in each step of the evolution as         literals of an appropriate language of first order logic. We provide a Prolog program to simulate, the evolution of these         P systems and present some auxiliary tools to simulate the evolution of a P system with active membranes using 2-division         which solves the SAT problem following the techniques presented in Reference.10               </content></document><document><year>2009</year><authors>Mark E. Stickel1 </authors><title>A Prolog technology theorem prover      </title><content>An extension of Prolog, based on the model elimination theorem-proving procedure, would permit production of a logically complete         Prolog technology theorem prover capable of performing inference operations at a rate approaching that of Prolog itself.      </content></document><document><year>2009</year><authors>Hiroshi Maruyama1 | Akinori Yonezawa2</authors><title>A Prolog-based natural language front-end system      </title><content>A Prolog-based natural language front-end system is described with the following major issues of discussion: Domain independence         of the syntax analyser was achieved by the &amp;#8216;generate-and-test&amp;#8217; notion and the domain independent semantic representation;         Determiners were treated as higher order predicates; A technique called &amp;#8216;syntactic feature&amp;#8217; was employed to write a readable         parser in Prolog.      </content></document><document><year>2009</year><authors>Harvey Abramson1</authors><title>A prological definition of HASL a purely functional language with unification based conditional binding expressions      </title><content>We present a definition in Prolog of a new purely funtional (applicative) language HASL (HArvey&amp;#8217;s StaticLanguage). HASL is a descendant of Turner&amp;#8217;s SASL and differs from the latter in several significant points: it includes Abramson&amp;#8217;s         unification based conditional binding constructs; it restricts each clause in a definition of a HASL function to have the         same arity, thereby complicating somewhat the compilation of clauses to combinators, but simplifying considerably the HASL         reduction machine; and it includes the single element domain {fail} as a component of the domain of HASL data structures.         It is intended to use HASL to express the functional dependencies in a translator writing system based on denotational semantics,         and to study the feasibility of using HASL as a functional sublanguage of Prolog or some other logic programming language.         Regarding this latter application we suggest that since a reduction mechanism exists for HASL, it may be easier to combine         it with a logic programming language than it was for Robinson and Siebert to combine LISP and LOGIC into LOGLISP: in that         case a fairly complex mechanism had to be invented to reduce uninterpreted LOGIC terms to LISP values.                     The definition is divided into four parts. The first part defines the lexical structure of the language by means of a simple               Definite Clause Grammar which relates character strings to &amp;#8220;token&amp;#8221; strings. The second part defines the syntactic structure               of the language by means of a more complex Definite Clause Grammar and relates token strings to a parse tree. The third part               is semantic in nature and translates the parse tree definitions and expressions to a variable-free string of combinators and               global names. The fourth part of the definition consists of a set of Prolog predicates which specifies how strings of combinators               and global names are reduced to &amp;#8220;values&amp;#8221;, ie., integers, truth values, characters, lists, functions, fail, and has an operational               flavour: one can think of this fourth part as the definition of a normal order reduction machine.            </content></document><document><year>2009</year><authors>Jun Gu1| 2| Xiaofei Huang1 | Bin Du1</authors><title>A quantitative solution to Constraint Satisfaction Problem (CSP)      </title><content>Constraint Satisfaction Problem (CSP) is an important problem in artificial intelligence and operations research. Many practical         problems can be formulated as CSP, i.e., finding a consistent value assignment to variables subject to a set of constraints.         In this paper, we give a quantitative approach to solve the CSPs which deals uniformly with binary constraints as well as         high order,k-ary (k &amp;#8805; 2) constraints. In this quantitative approach, using variable transformation and constraint transformation, a CSP is transformed         into a satisfiability (SAT) problem. The SAT problem is then solved within a continuous search space. We will evaluate the         performance of this method based on randomly generated SAT problem instances and regularly generatedk-ary (k &amp;#8805; 2) CSP problem instances.      </content></document><document><year>2009</year><authors>Kazuo Ohta1 | Tetsuro Nishino1 | Seiya Okubo2  | Noboru Kunihiro1 </authors><title>A quantum algorithm using NMR computers to break secret-key cryptosystems      </title><content>In this paper, we discuss quantum algorithms that, for a given plaintextm         o and a given ciphertextc         o, will find a secret key,k         o, satisfyingc         o=E(k         o,m         o), where an encryption algorithm,E, is publicly available. We propose a new algorithm suitable for an NMR (Nuclear Magnetic Resonance) computer based on the         technique used to solve the counting problem. The complexity of, our algorithm decreases as the measurement accuracy of the         NMR computer increases. We discuss the possibility that the proposed algorithm is superior to Grover&amp;#8217;s algorithm based on         initial experimental results.      </content></document><document><year>2009</year><authors>Shigeki Shibayama1 | Takeo Kakuta1| Nobuyoshi Miyazaki1| Haruo Yokota1 | Kunio Murakami1</authors><title>A relational database machine with large semiconductor disk and hardware relational algebra processor      </title><content>This paper describes the basic concepts, design and implementation decisions, standpoints and significance of the database         machine Delta in the scope of Japan&amp;#8217;s Fifth Generation Computer Project. Delta is planned to be operational in 1985 for researchers&amp;#8217;         use as a backend database machine for logic programming software development. Delta is basically a relational database machine         system. It combines hardware facilities for efficient relational database operations, which are typically represented by relational         algebra, and software which deals with hardware control and actual database management requirements. Notable features include         attribute-based internal schema in accordance with the characteristics found in the relation access from logic programming         environment. This is also useful for the hardware relational algebra manipulation algorithm based on merge-sorting of attributes         by hardware and a large capacity Semiconductor Disk for fast access to databases. Various implementation decisions of database         management requirements are made in this novel system configuration, which will be meaningful to give an example for constructing         a hardware and software combination of a relational database machine. Delta is in the stage between detailed design and implementation.      </content></document><document><year>2009</year><authors>Hideyuki Nakashima1 | Doug Degroot2</authors><title>A Report on 1985 International Symposium on Logic Programming      </title><content>Without Abstract</content></document><document><year>2009</year><authors>Setsuo Arikawa1  | Takeshi Shinohara2</authors><title>A run-time efficient realization of Aho-Corasick pattern matching machines      </title><content>Realizations of Aho-Corasick pattern matching machines which deal with several keywords at a time are studied from the viewpoint         of run-time and space complexity. New realizations by means of dividing character codes and transition tables are introduced         and shown to be efficient. A time-space trade-off in such realizations is pointed out. Experimental results on run-time of         our realizations are shown and compared with those of some other well-known pattern matching techniques. Applications of our         realizations to sorting and searching for keywords are also discussed.      </content></document><document><year>2009</year><authors>Hiroshi Fujita1  | Koichi Furukawa1</authors><title>A self-applicable partial evaluator and its use in incremental compilation      </title><content>This paper presents an experimental implementation of a self-applicable partial evaluator in Prolog used for compiler generation         and compiler generator generation. The partial evaluator is an extension of a simple meta interpreter for Prolog programs,         and its self-application is straightforward because of its simplicity. A method of incremental compilation is also described         as a promising application of the partial evaluator for knowledge-based systems.      </content></document><document><year>2009</year><authors>Masahito Yamamoto1| 2 | Atsushi Kameda1| Nobuo Matsuura2| Toshikazu Shiba2| Yumi Kawazoe2 | Azuma Ohuchi2</authors><title>A separation method for DNA computing based on concentration control      </title><content>A separation method for DNA computing based on concentration control is presented. The concentration control method was earlier         developed and has enabled us to use DNA concentrations as input data and as filters to extract target DNA. We have also applied         the method to the shortest path problems, and have shown the potential of concentration control to solve large-scale combinatorial         optimization problems. However, it is still quite difficult to separate different DNA with the same length and to quantify         individual DNA concentrations. To overcome these difficulties, we use DGGE and CDGE in this paper. We demonstrate that the         proposed method enables us to separate different DNA with the same length efficiently, and we actually solve an instance of         the shortest path problems.      </content></document><document><year>2009</year><authors>Khayri A. M. Ali1</authors><title>A simple generational real-time garbage collection scheme      </title><content>This paper presents and empirically evaluates a generational real-time garbage collection scheme, which is based on combining         Baker&amp;#8217;s real-time scheme with a simple generational scheme by Andrew W. Appel.      </content></document><document><year>2009</year><authors>Takayasu Ito1 </authors><title>A step towards complementary programming      </title><content>There are various theoretical and practical approaches to single out fundamental concepts and issues of programming in the         research of Program Development Methodology and Automatic Programming. We can observe that the current program development         methods have been directed towards semantics-based programming to seek basic disciplines of programming processes and semantic         frameworks. However none of them gives a general and reasonable framework, since any current programming methodology does         not employ a general semantic discipline for program derivation, manipulation and understanding. In this article we discuss         a framework of complementary programming as a modest proposal for reliable/semantics-based programming.      </content></document><document><year>2009</year><authors>Hidenori Kawamura1 | Yasushi Okada1 | Azuma Ohuchi1  | Koichi Kurumatani2 </authors><title>A study on effects of market systems in an artificial market      </title><content>In an artificial market approach with multi-agent systems, the static equilibrium concept is often used in market systems         to approximate continuous market auctions. However, differences between the static equilibrium concept and continuous auctions         have not been discussed in the context of an artificial market study. In this paper, we construct an artificial market model         with both of them, namely, the Itayose and Zaraba method, and show simple characteristic differences between these methods         based on computer simulations. The result indicates the further need to model the market system by studying artificial markets.      </content></document><document><year>2009</year><authors>Arun Krishnan1 </authors><title>A survey of life sciences applications on the grid      </title><content>The availability of powerful microprocessors and improvements in the performance of networks has enabled high performance         computing on wide-area, distributed systems. Computational grids, by integrating diverse, geographically distributed and essentially         heterogeneous resources provide the infrastructure for solving large-scale problems. However, heterogeneity, on the one hand         allows for scalability, but on the other hand makes application development and deployment for such an environment extremely         difficult.                     The field of life sciences has been an explosion in data over the past decade. The data acquired needs to be processed, interpreted               and analyzed to be useful. The large resource needs of bioinformatics allied to the large number of data-parallel applications               in this field and the availability of a powerful, high performance, computing grid environment lead naturally to opportunities               for developing grid-enabled applications. This survey, done as part of the Life Sciences Research Group (a research group               belonging to the Global Grid Forum) attempts to collate information regarding grid-enabled applications in this field.            </content></document><document><year>2009</year><authors>D. W. Shin1| 2| J. H. Nang1| S. R. Maeng1 | J. W. Cho1</authors><title>A typed functional extension of logic programming      </title><content>A logic language is suitable for specification if it is equipped with features for data abstraction and modularization. In         this paper, an effective mechanism to incorporate function and type into logic programming is presented as the means to embed         data abstraction mechanism into logic programming. This incorporation is essentially based on Horn clause logic with equality         and a polymorphic type system that is an extension of Mycroft and O&amp;#8217;Keefe&amp;#8217;s system. This paper also presents an implementation         based on Warren Abstract Machine (WAM) and shows the performance, along with a comparison with WAM.      </content></document><document><year>2009</year><authors>Olivier Danvy1 | Bernd Grobauer1  | Morten Rhiger1 </authors><title>A unifying approach to goal-directed evaluation      </title><content>Goal-directed evaluation, as embodied in Icon and Snobol, is built on the notions of backtracking and of generating successive         results, and therefore it has always been something of a challenge to specify and implement. In this article, we address this         challenge using computational monads and partial evaluation.                     We consider a subset of Icon and we specify it with a monadic semantics and a list monad. We then consider a spectrum of monads               that also fit the bill, and we relate them to each other. For example, we derive a continuation monad as a Church encoding               of the list monad. The resulting semantics coincides with Gudeman&amp;#8217;s continuation semantics of Icon.            </content></document><document><year>2009</year><authors>William J. Dally1 </authors><title>A universal parallel computer architecture      </title><content>Advances in interconnection network performance and interprocessor interaction mechanisms enable the construction of fine-grain         parallel computers in which the nodes are physically small and have a small amount of memory. This class of machines has a         much higher ratio of processor to memory area and hence provides greater processor throughput and memory bandwidth per unit         cost relative to conventional memory-dominated machines. This paper describes the technology and architecture trends motivating         fine-grain architecture and the enabling technologies of high-performance interconnection networks and low-overhead interaction         mechanisms. We conclude with a discussion of our experiences with the J-Machine, a prototype fine-grain concurrent computer.      </content></document><document><year>2009</year><authors>Bernard A. Galler1 </authors><title>A view of Artificial Intelligence      </title><content>Without Abstract</content></document><document><year>2009</year><authors>Paul J. Voda1 </authors><title>A view of programming languages as symbiosis of meaning and computations      </title><content>Function and logic programming languages are understood as terms, resp. formulas, of a first order theory. This theory gives         meaning to programs and allows reasoning about programs within full predicate logic possibly using quantifiers and induction.         The operational semantics of programming languages is given by deductively restricted subtheories of the meaning theory in         such a way that the computation sequences are in a one-to-one correspondence with proofs in subtheories. Moreover, meaning         is invariant to computations as everything provable in a subtheory is required to be a theorem of the meaning theory. The         questions of deadlocks and termination of programs are thus reduced to the proof-theoretical questions of existence of proofs         in the subtheories.      </content></document><document><year>2009</year><authors>Yuzuru Tanaka1 </authors><title>A VLSI algorithm for sorting variable-length character strings      </title><content>This paper proposes a sorting hardware module that can directly cope with variable length character strings. It gives a pipelined         heap sort algorithm for a set of variable-length character strings, and a VLSI architecture that implements this algorithm.         The hardware consists of a specially designed single chip module and an external memory bank. This special chip module is         called a V-Sort Engine Core. The number of words in the external memory bank should be larger than the total length of strings         to be sorted. A hardware module that can sort no more than 2            L             strings uses a V-Sort Engine core consisting ofL levels. Thei-th level of a V-Sort Engine Core has a logic cell and a memory bank with 2            i             words. Each word consists of three fields and a mark bit, i. e., level number, character, and path number. A triple (j, c, i) consisting of these field values denotes thej+1st characterc of thei-th input string. Concurrent execution of the external memory bank and all the level logic cells of the V-Sort Engine Core         allows the hardware module to receive a sequence of strings sequentially character by character, and to begin the sequential         output of the sort result immediately after receiving the last input character. It requires no extra time other than those         required for sequential data transfer to and from itself.      </content></document><document><year>2009</year><authors>Kemal Ebcio&amp;#285 lu1  | Manoj Kumar1</authors><title>A wide instruction word architecture for parallel execution of logic programs coded in BSL      </title><content>This paper begins by describing BSL, a new logic programming language fundamentally different from Prolog. BSL is a nondeterministic         Algol-class language whose programs have a natural translation to first order logic; executing a BSL program without free         variables amounts to proving the corresponding first order sentence. A new approach is proposed for parallel execution of         logic programs coded in BSL, that relies on advanced compilation techniques for extracting fine grain parallelism from sequential         code. We describe a new &amp;#8220;Very Long Instruction Word&amp;#8221; (VLIW) architecture for parallel execution of BSL programs. The architecture, now being designed at the IBM Thomas J. Watson         Research Center, avoids the synchronization and communication delays (normally associated with parallel execution of logic         programs on multiprocessors), by determining data dependences between operations at compile time, and by coupling the processing         elements very tightly, via a single central shared register file. A simulator for the architecture has been implemented and         some simulation results are reported in the paper, which are encouraging.      </content></document><document><year>2009</year><authors>Antonis C. Kakas1  | Fabrizio Riguzzi2 </authors><title>Abductive concept learning      </title><content>We investigate how abduction and induction can be integrated into a common learning framework. In particular, we consider         an extension of Inductive Logic Programming (ILP) for the case in which both the background and the target theories are abductive         logic programs and where an abductive notion of entailment is used as the basic coverage relation for learning. This extended         learning framework has been called Abductive Concept Learning (ACL). In this framework, it is possible to learn with incomplete         background information about the training examples by exploiting the hypothetical reasoning of abduction. We also study how         the ACL framework can be used as a basis for multiple predicate learning.                     An algorithm for ACL is developed by suitably extending the top-down ILP method: the deductive proof procedure of Logic Programming               is replaced by an abductive proof procedure for Abductive Logic Programming. This algorithm also incorporates a phase for               learning integrity constraints by suitably employing a system that learns from interpretations like ICL. The framework of               ACL thus integrates the two ILP settings of explanatory (predictive) learning and confirmatory (descriptive) learning. The               above algorithm has been implemented into a system also called ACL Several experiments have been performed that show the effectiveness               of the ACL framework in learning from incomplete data and its appropriate use for multiple predicate learning.            </content></document><document><year>2009</year><authors/></document><document><year>2008</year><authors>Takashi Washio1 | Koutarou Nakanishi1 | Hiroshi Motoda1</authors><title>A Classification Method Based on Subspace Clustering and Association Rules      </title><content>Class Association Rule (CAR) based classification is a growing topic in recent datamining study for its high interpretability         and accuracy. However, most of the approaches have not intensively addressed the classification of instances including numeric         attributes. In this paper, a levelwise subspace clustering deriving hyper-rectangular clusters is proposed to efficiently         provide quantitative, interpretative and accurate CARs. Significant performance of the proposed approach has been demonstrated         through the tests on UCI repository data.      </content></document><document><year>2008</year><authors>Mohamed Medhat Gaber1  | Philip S. Yu2 </authors><title>A Holistic Approach for Resource-aware Adaptive Data Stream Mining      </title><content>Mining data streams is a field of increasing interest due to the importance of its applications and dissemination of data         stream sources. Most of the streaming techniques developed so far have not addressed the need for resource-aware computing         in data stream analysis. The fact that streaming information is often generated or received onboard resource-constrained computational         devices such as sensor nodes and mobile devices motivates the need for resource-awareness in data stream processing systems.         In this paper, we propose a generic framework that enables resource-awareness in streaming computation using algorithm granularity         settings in order to change the resource consumption patterns periodically. This generic framework is applied to a novel threshold-based         micro-clustering algorithm to test its validity and feasibility. We have termed this algorithm as RA-Cluster. RA-Custer is         the first data stream clustering algorithm that can adapt to the changing availability of different resources. The experimental         results show the applicability of the framework and the algorithm in terms of resource-awareness and accuracy.      </content></document><document><year>2008</year><authors>Futoshi Iwama1  | Naoki Kobayashi1 </authors><title>A New Type System for JVM Lock Primitives      </title><content>A bytecode verifier for the Java virtual machine language (JVML) statically checks that bytecode does not cause any fatal         error. However, the present verifier does not check correctness of the usage of lock primitives. To solve this problem, we         extend Stata and Abadi&amp;#8217;s type system for JVML by augmenting types with information about how each object is locked and unlocked.         The resulting type system guarantees that when a thread terminates, it has released all the locks it has acquired and that         a thread releases a lock only if it has acquired the lock previously. We have implemented a prototype Java bytecode verifier         based on the type system. We have tested the verifier for several classes in the Java run time library and confirmed that         the verifier runs efficiently and gives correct answers.      </content></document><document><year>2008</year><authors>Ichiro Kobayashi1  | Mai Saito2</authors><title>A Study on an Information Recommendation System that Provides Topical Information Related to User&amp;#8217;s Inquiry for Information         Retrieval      </title><content>As the Internet has been commonly used in our everyday lives, we have been able to obtain large amount of information from         it, whereas we have simultaneously had a problem that it is difficult to find proper information for us from the large amount         of information on the Web. Although many information recommendation methods have been proposed in order to solve this problem,         most recommendation methods are based on a large amount of user&amp;#8217;s personal data such as operation log, schedule, etc &amp;#8211; which         means that we have to manage a large amount of personal data in the system in order to provide proper information to users,         and it would be expensive to construct such a system.                     With this background, in this study, against aiming to construct a sophisticated information recommendation system based on               large personal data, we propose a handy and not expensive information recommendation method, working beside a normal search               engine, which does not depend on user profile data, but on topical news information.            </content></document><document><year>2008</year><authors>Elvira Albert1 | GermГЎn Puebla2 | Manuel Hermenegildo3</authors><title>Abstraction-Carrying Code: a Model for Mobile Code Safety      </title><content>         Proof-Carrying Code (PCC) is a general approach to mobile code safety in which programs are augmented with a certificate (or proof). The intended         benefit is that the program consumer can locally validate the certificate w.r.t. the &amp;#8220;untrusted&amp;#8221; program by means of a certificate         checker&amp;#8212;a process which should be much simpler, efficient, and automatic than generating the original proof. The practical         uptake of PCC greatly depends on the existence of a variety of enabling technologies which allow both proving programs correct         and replacing a costly verification process by an efficient checking procedure on the consumer side. In this work we propose         Abstraction-Carrying Code (ACC), a novel approach which uses abstract interpretation as enabling technology. We argue that the large body of applications         of abstract interpretation to program verification is amenable to the overall PCC scheme. In particular, we rely on an expressive         class of safety policies which can be defined over different abstract domains. We use an abstraction (or abstract model) of the program computed by standard static analyzers as a certificate. The validity of the abstraction         on the consumer side is checked in a single pass by a very efficient and specialized abstract-interpreter. We believe that         ACC brings the expressiveness, flexibility and automation which is inherent in abstract interpretation techniques to the area         of mobile code safety.      </content></document><document><year>2008</year><authors>Masayuki Numao1  | Tu-Bao Ho2 </authors><title>Active Mining      </title><content>Without Abstract</content></document><document><year>2008</year><authors>JoГЈo Gama1 | Pedro Rodrigues1  | JesГєs Aguilar-Ruiz2 </authors><title>An Overview on Learning from Data Streams      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Janusz Sobecki1 </authors><title>Ant Colony Metaphor Applied in User Interface Recommendation      </title><content>In this paper web&amp;#8211;based system user interface hybrid recommendation method based on the ant colony metaphor is presented.         We apply the ontology&amp;#8211;based user and user interface modeling. The user model is represented as a tuple and user interface         model is represented by a set of connected nodes, what enables suitable user interface design, an interface personalization         and recommendation. The recommendation is performed using ant colony metaphor for selection the most optimal path in the user         interface graph that specifies the user interface parameters for the specified user.      </content></document><document><year>2008</year><authors>Hirohito Shibata1| 2  | Koichi Hori1 </authors><title>Cognitive Support for the Organization of Writing      </title><content>This paper describes a novel framework and a system to support writing professional documents. Our framework focuses on aiding         the process of organizing ideas and constructing logical connections between text chunks in formal documents. Most previous         approaches dealing with these issues have utilized one of three types of representations: a tree, a map, and a network. However,         they all have their strengths and weaknesses. The tree is superior in helping the writers to grasp the hierarchical structure         of a document, but it is difficult to express vague relationships among parts of a document. The map is easy to operate, but         it restricts freedom in spatial organization. The network has flexibility to express what writers want, but it requires additional         graphical objects to specify relationships among parts and it cannot be naturally transformed into a linear document. In our         approach, the tree and the map are integrated: the local structure of a document is defined by the map while the global structure         of the document is defined by the tree. Combining both representations has the following advantages: (1) it leverages the         benefits of both representations, (2) it seamlessly supports both top-down organization and bottom-up organization, and (3)         it smoothly supports writing from an ill-organized state to a well-organized state. Based on the proposed framework, we have         built a system called iWeaver. We describe experiments in which six users wrote six technical documents using the system. The results show that our framework         is an effective aid for constructing long, structured documents.      </content></document><document><year>2008</year><authors>Zhaohui Ding1 | Xiaohui Wei1 | Yuan Luo1| Da ma1 | Peter W. Arzberger2  | Wilfred W. Li3 </authors><title>Customized Plug-in Modules in Metascheduler CSF4 for Life Sciences Applications      </title><content>As more and more life science researchers start to take advantages of grid technologies in their work, the demand increases         for a robust yet easy to use metascheduler or resource broker. In this paper, we have extended the metascheduler CSF4 by providing         a Virtual Job Model (VJM) to synchronize the resource co-allocation for cross-domain parallel jobs. The VJM eliminates dead-locks         and improves resource usage for multi-cluster parallel applications compiled with MPICH-G2. Taking advantage of the extensible         scheduler plug-in model of CSF4, one may develop customized metascheduling policies for life sciences applications. As an         example, an array-job scheduler plug-in is developed for pleasantly parallel applications such as AutoDock and Blast. The         performance of the VJM is evaluated through experiments with mpiBLAST-g2 using a Gfarm data grid testbed. Furthermore, a CSF4         portlet has been released to provide a graphical user interface for transparent grid access, with the use of Gfarm for data         staging and simplified data management. The platform is open source at sourceforge.net/projects/gcsf/ and has been deployed         in life science gateways by projects such as My WorkSphere, and PRAGMA Biosciences Portal. The VJM enables the development         of support for more sophisticated workflows and metascheduling policies in the near future.      </content></document><document><year>2008</year><authors>Teijiro Isokawa1 | Shin&amp;#8217 ya Kowada1 | Yousuke Takada2 | Ferdin| Peper3 | Naotake Kamiura4  | Nobuyuki Matsui4 </authors><title>Defect-Tolerance in Cellular Nanocomputers      </title><content>For the manufacturing of computers built by nanotechnology, defects are expected to be a major problem. This paper explores         this issue for nanocomputers based on cellular automata. Known for their regular structure, such architectures promise cost-effective         manufacturing based on molecular self-organization. We show how a cellular automaton can detect defects in a self-contained         way, and how it configures circuits on its cells while avoiding the defects. The employed cellular automaton is asynchronous,         i.e., it does not require a central clock to synchronize the updates of its cells. This mode of timing is especially suitable         for the high integration densities of nanotechnology implementations, since it potentially causes less heat dissipation.      </content></document><document><year>2008</year><authors>Ken Kaneiwa1 </authors><title>Description Logics with Contraries, Contradictories, and Subcontraries      </title><content>Several constructive description logics,12) in which classical negation was replaced by strong negation as a component to treat negative atomic information have been         proposed as intuitionistic variants of description logics. For conceptual representation, strong negation alone and in a combination         with classical negation seems to be useful and necessary due to their respective predicate denial (e.g., not happy) and predicate         term negation (e.g., unhappy) properties. In this paper, we propose an alternative description logic  with classical negation and strong negation. We adhere in particular to the notions of contraries, contradictories, and subcontraries         (as discussed in 6)), generated from conceivable statement types using predicate denial and predicate term negation. To capture these notions,         our formalization includes a semantics that suitably interprets various combinations of classical negation and strong negation.         We show that our semantics preserves contradictoriness and contrariness for -concepts, but the semantics of constructive description logic  with Heyting negation and strong negation cannot preserve the property for -concepts.      </content></document><document><year>2008</year><authors>Masami Hagiya1 </authors><title>Designing Chemical and Biological Systems      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Tsuyoshi Murata1 </authors><title>Detection of Breaking News from Online Web Search Queries      </title><content>Many people enter queries to Google or Yahoo! in order to search useful information from the Web. Queries given to search         engines can be regarded as the resources for detecting people&amp;#8217;s information needs. It is often reported that many people perform         search intensively after worldwide disasters or accidents. This paper describes a method for detecting such breaking news         from search queries that are available online. In our method, real time search queries are obtained and filtered with news         words extracted from a news site. Experimental results show that our method has abilities of detecting breaking news from         more than 25 million search queries for six months.      </content></document><document><year>2008</year><authors>Ryuzo Azuma1 | Ryo Umetsu1 | Shingo Ohki1 | Fumikazu Konishi1 | Sumi Yoshikawa1 | Akihiko Konagaya1  | Kazumi Matsumura2 </authors><title>Discovering Dynamic Characteristics of Biochemical Pathways using Geometric Patterns among Parameter-Parameter Dependencies         in Differential Equations      </title><content>This paper proposes a novel approach to the analysis and validation of mathematical models using two-dimensional geometrical         patterns representing parameter-parameter dependencies (PPD) in dynamic systems. A geometrical pattern is obtained by calculating         moment values, such as the area under the curve (AUC), area under the moment curve (AUMC), and mean residence time (MRT),         for a series of simulations with a wide range of parameter values. In a mathematical model of the metabolic pathways of the         cancer drug irinotecan (CPT11), geometrical patterns can be classified into three major categories:         &amp;#8220;independent,&amp;#8221; &amp;#8220;hyperbolic,&amp;#8221; and &amp;#8220;complex.&amp;#8221; These categories characterize substructures arising in differential equations,         and are helpful for understanding the behavior of large-scale mathematical models. The Open Bioinformatics Grid (OBIGrid)         provides a cyber-infrastructure for users to share these data as well as computational resources.      </content></document><document><year>2008</year><authors>Tsuyoshi Murata1 </authors><title>Discovery of User Communities Based on Terms of Web Log Data      </title><content>The Web is a huge network composed of Web pages and hyperlinks. It is often reported that related Web pages are densely linked         with each other. Finding groups of such related pages, which are called Web communities, is important for information retrieval         from the Web. Several attempts have been made for the discovery of Web communities such as Kumar&amp;#8217;s trawling and Flake&amp;#8217;s method.         In addition to the communities of related Web pages, there are communities of users sharing common interests. Finding the         latter communities, which we called user communities in this paper, is also important for clarifying the behaviors of Web         users. It is expected that the characteristics of user communities in the Web correspond to those in real human communities.         A method for discovering user communities is described in this paper. Client-level log data (Web audience measurement data)         is used as the data of users&amp;#8217; Web watching behaviors. Maximal complete bipartite graphs are searched from term-user graph         obtained from the log data without analyzing the contents of Web pages. Experimental results show that our method succeeds         in discovering many interesting user communities with labels that characterize the communities.      </content></document><document><year>2008</year><authors>Hisashi Shimosaka1 | Tomoyuki Hiroyasu2  | Mitsunori Miki2 </authors><title>Distributed Workflow Management System based on Publish-Subscribe Notification for Web Services      </title><content>In recent years, Grid technologies have been standardized based on Web service specifications. Of these specifications, the         WS-Resources Framework and WS-Notification have attracted a great deal of attention. This paper focuses on scientific applications         integration on the wide area network. We propose and implement a new distributed workflow management system called the &amp;#8220;Application         Igniting System.&amp;#8221; This system is based on the publish-subscribe notification defined by the WS-Notification specification         and realizes a flexible and loosely coupled workflow control by introducing some Web services, which handle message exchange.         By applying to a typical bioinformatics workflow, we concluded that the overhead time related to message exchange is very         short.      </content></document><document><year>2008</year><authors>Satoshi Murata1  | Milan N. Stojanovic2 </authors><title>DNA-based Nanosystems      </title><content>DNA-based nanosystems have emerged as an interdisciplinary field that draws on computer science, biochemistry, material science,         and engineering. Although the field is still in its infancy, fundamental methodologies to build up large-scale complex nanosystems         have been already established. In this paper, we review several recent topics in the DNA-based nanosystems, as they were presented         on Kavli Japanese-American Frontier of Science meeting by Erik Winfree, Satoshi Murata, and Milan Stojanovic.      </content></document><document><year>2008</year><authors>Li-Ning Xing1 | Ying-Wu Chen1 | Ke-Wei Yang1</authors><title>Double Layer ACO Algorithm for the Multi-Objective FJSSP      </title><content>Scheduling for the flexible job shop is very important in both fields of production management and combinatorial optimization.         In this work, a double layer Ant Colony Optimization (ACO) algorithm is proposed for the Flexible Job Shop Scheduling Problem         (FJSSP). In the proposed algorithm, two different ACO algorithms are applied to solve the FJSSP with a hierarchical way. The         primary mission of upper layer ACO algorithm is achieving an excellent assignment of operations to machines. The leading task         of lower layer ACO algorithm is obtaining the optimal sequencing of operations on each machine. Experimental results suggest         that the proposed algorithm is a feasible and effective approach for the multi-objective FJSSP.      </content></document><document><year>2008</year><authors>Stefano Lonardi1 | Jessica Lin2 | Eamonn Keogh1  | Bill &amp;#8216 Yuan-chi&amp;#8217  Chiu1 </authors><title>Efficient Discovery of Unusual Patterns in Time Series      </title><content>The problem of finding a specified pattern in a time series database (i.e., query by content) has received much attention         and is now a relatively mature field. In contrast, the important problem of enumerating all surprising or interesting patterns         has received far less attention. This problem requires a meaningful definition of &amp;#8220;surprise&amp;#8221;, and an efficient search technique.         All previous attempts at finding surprising patterns in time series use a very limited notion of surprise, and/or do not scale         to massive datasets. To overcome these limitations we propose a novel technique that defines a pattern surprising if the frequency         of its occurrence differs substantially from that expected by chance, given some previously seen data. This notion has the         advantage of not requiring the user to explicitly define what is a surprising pattern, which may be hard, or perhaps impossible,         to elicit from a domain expert. Instead, the user gives the algorithm a collection of previously observed &amp;#8220;normal&amp;#8221; data. Our         algorithm uses a suffix tree to efficiently encode the frequency of all observed patterns and allows a Markov model to predict         the expected frequency of previously unobserved patterns. Once the suffix tree has been constructed, a measure of surprise         for all the patterns in a new database can be determined in time and space linear in the size of the database. We demonstrate         the utility of our approach with an extensive experimental evaluation.      </content></document><document><year>2008</year><authors>Siegfried Benkner1 | Gerhard Engelbrecht1| Stuart E. Middleton2 | Ivona Br|ic1 | Rainer Schmidt1</authors><title>End-to-End QoS Support for a Medical Grid Service Infrastructure      </title><content>Quality of Service support is an important prerequisite for the adoption of Grid technologies for medical applications. The         GEMSS Grid infrastructure addressed this issue by offering end-to-end QoS in the form of explicit timeliness guarantees for         compute-intensive medical simulation services. Within GEMSS, parallel applications installed on clusters or other HPC hardware         may be exposed as QoS-aware Grid services for which clients may dynamically negotiate QoS constraints with respect to response         time and price using Service Level Agreements. The GEMSS infrastructure and middleware is based on standard Web services technology         and relies on a reservation based approach to QoS coupled with application specific performance models. In this paper we present         an overview of the GEMSS infrastructure, describe the available QoS and security mechanisms, and demonstrate the effectiveness         of our methods with a Grid-enabled medical imaging service.      </content></document><document><year>2008</year><authors>Mihai Oltean1  | Oana Muntean1</authors><title>Exact Cover with Light      </title><content>We suggest a new optical solution for solving the YES/NO version of the Exact Cover problem by using the massive parallelism         of light. The idea is to build an optical device which can generate all possible solutions of the problem and then to pick         the correct one. In our case the device has a graph-like representation and the light is traversing it by following the routes         given by the connections between nodes. The nodes are connected by arcs in a special way which lets us to generate all possible         covers (exact or not) of the given set. For selecting the correct solution we assign to each item, from the set to be covered,         a special integer number. These numbers will actually represent delays induced to light when it passes through arcs. The solution         is represented as a subray arriving at a certain moment in the destination node. This will tell us if an exact cover does         exist or not.      </content></document><document><year>2008</year><authors>Tu-Bao Ho1 | Canh-Hao Nguyen1 | Saori Kawasaki1 | Si-Quang Le1  | Katsuhiko Takabayashi2 </authors><title>Exploiting Temporal Relations in Mining Hepatitis Data      </title><content>Various data mining methods have been developed last few years for hepatitis study using a large temporal and relational database         given to the research community. In this work we introduce a novel temporal abstraction method to this study by detecting         and exploiting temporal patterns and relations between events in viral hepatitis such as &amp;#8220;event A slightly happened before         event B and B simultaneously ended with event C&amp;#8221;. We developed algorithms to first detect significant temporal patterns in         temporal sequences and then to identify temporal relations between these temporal patterns. Many findings by data mining methods         applied to transactions/graphs of temporal relations shown to be significant by physician evaluation and matching with published         in Medline.      </content></document><document><year>2008</year><authors>Daniela Godoy1| 2  | AnalГ­a Am|i1| 2 </authors><title>Exploiting User Interests to Characterize Navigational Patterns in Web Browsing Assistance      </title><content>In order to be capable of exploiting context for pro-active information recommendation, agents need to extract and understand         user activities based on their knowledge of the user interests. In this paper, we propose a novel approach for context-aware         recommendation in browsing assistants based on the integration of user profiles, navigational patterns and contextual elements.         In this approach, user profiles built using an unsupervised Web page clustering algorithm are used to characterize user ongoing         activities and behavior patterns. Experimental evidence show that using longer-term interests to explain active browsing goals         user assistance is effectively enhanced.      </content></document><document><year>2008</year><authors>Satoshi Fujishima1 | Yoshimasa Takahashi1 | Katsumi Nishikori1 | Hiroaki Kato1  | Takashi Okada2 </authors><title>Extended Study of the Classification of Dopamine Receptor Agonists and Antagonists using a TFS-based Support Vector Machine      </title><content>We previously investigated the classification and prediction of dopamine D1 receptor agonists and antagonists using a topological         fragment spectra (TFS)-based support vector machine (SVM), in which the dataset contained noise compounds that had no D1 receptor         activity. This work extended the dataset to seven activity classes (dopamine D1, D2, and auto-receptor agonists, and D1, D2,         D3, and D4 antagonists) and increased the noise ratio to ten times that of active compounds. In total, this study used 16,008         compounds for training and 1,779 compounds for prediction. The TFS-based SVM gave good, stable results for both classification         and prediction, even in the case that included ten times the noise data. The resulting model correctly predicted 97.6% of         the prediction set of 1,779 compounds.      </content></document><document><year>2008</year><authors>Kazuo Hara1  | Yuji Matsumoto1 </authors><title>Extracting Clinical Trial Design Information from MEDLINE Abstracts      </title><content>Evidence-based medicine (EBM) requires medical practitioners to select appropriate treatments for individual patients based         on the current best evidence, and the results of phase III clinical trials are the major source of such evidence. In this         paper, we report results of experiment in extracting important information for EBM from the abstracts of phase III clinical         trials, in an effort to investigate how far the existing natural language processing (NLP) techniques could support EBM using         MEDLINE database.      </content></document><document><year>2008</year><authors/></document></documents>