GUID:5B8BE7BD-979A-4F7B-A61D-591023A38C23
LCount:697
CCount:140
ClCount:7
ClNames: computational complexity; database systems; Human Computer Interaction; Machine Learning; Multiagent Systems; neural networks; nonlinear control systems;
L:1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 829 830 831 832 833 834 835 836 837 
C:689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 
ID:1
CLASS:1
Title: Type two computational complexity
Abstract: A programming language for the partial computable functionals is used as the basis for a definition of the computational complexity of functionals (type2 functions). An axiomatic account in the spirit of Blum is then provided. The novel features of this approach are justified by applying it to problems in abstract complexity, specifically operator speed-up, and by using it to define the illusive notion of the polynomial degree of an arbitrary function. New results are obtained for these degrees.
ID:2
CLASS:1
Title: On fundamental tradeoffs between delay bounds and computational complexity in packet scheduling algorithms
Abstract: In this work, we clarify, extend and solve an open problem concerning the computational complexity for packet scheduling algorithms to achieve tight end-to-end delay bounds. We first focus on the difference between the time a packet finishes service in a scheduling algorithm and its virtual finish time under a GPS (General Processor Sharing) scheduler, called GPS-relative delay. We prove that, under a slightly restrictive but reasonable computational model, the lower bound computational complexity of any scheduling algorithm that guarantees O(1) GPS-relative delay bound is &#937; (log2 n) (widely believed as a "folklore theorem" but never proved). We also discover that, surprisingly, the complexity lower bound remains the same even if the delay bound is relaxed to O(na) for 0&#8249;a&#8949;1. This implies that the delay-complexity tradeoff curve is "flat" in the "interval" [O(1), O(n)). We later extend both complexity results (for O(1) or O(na) delay) to a much stronger computational model. Finally, we show that the same complexity lower bounds are conditionally applicable to guaranteeing tight end-to-end delay bounds. This is done by untangling the relationship between the GPS-relative delay bound and the end-to-end delay bound.
ID:3
CLASS:1
Title: Complexity of computations
Abstract: The framework for research in the theory of complexity of computations is described, emphasizing the interrelation between seemingly diverse problems and methods. Illustrative examples of practical and theoretical significance are given. Directions for new research are discussed.
ID:4
CLASS:1
Title: An overview of computational complexity
Abstract: An historical overview of computational complexity is presented. Emphasis is on the fundamental issues of defining the intrinsic computational complexity of a problem and proving upper and lower bounds on the complexity of problems. Probabilistic and parallel computation are discussed.
ID:5
CLASS:1
Title: Fixpoint logics, relational machines, and computational complexity
Abstract: We establish a general connection between fixpoint logic and complexity. On one side, we have fixpoint logic, parameterized by the choices of 1st-order operators (inflationary or noninflationary) and iteration constructs (deterministic, nondeterministic, or alternating). On the other side, we have the complexity classes between P and EXPTIME. Our parameterized fixpoint logics capture the complexity classes P, NP, PSPACE, and EXPTIME, but equally is achieved only over ordered structures.There is, however, an inherent mismatch between complexity and logic&mdash;while computational devices work on encodings of problems, logic is applied directly to the underlying mathematical structures. To overcome this mismatch, we use a theory of relational complexity, which bridges the gap between standard complexity and fixpoint logic. On one hand, we show that questions about containments among standard complexity classes can be translated to questions about containments among relational complexity classes. On the other hand, the expressive power of fixpoint logic can be precisely characterized in terms of relational complexity classes. This tight, three-way relationship among fixpoint logics, relational complexity and standard complexity yields in a uniform way logical analogs to all containments among the complexity classes P, NP, PSPACE, and EXPTIME. The logical formulation shows that some of the most tantalizing questions in complexity theory boil down to a single question: the relative power of inflationary vs. noninflationary 1st-order operators.
ID:6
CLASS:1
Title: On fundamental tradeoffs between delay bounds and computational complexity in packet scheduling algorithms
Abstract: We clarify, extend, and solve a long-standing open problem concerning the computational complexity for packet scheduling algorithms to achieve tight end-to-end delay bounds. We first focus on the difference between the time a packet finishes service in a scheduling algorithm and its virtual finish time under a GPS (General Processor Sharing) scheduler, called GPS-relative delay. We prove that, under a slightly restrictive but reasonable computational model, the lower bound computational complexity of any scheduling algorithm that guarantees O(1) GPS-relative delay bound is &#937;(log n). We also discover that, surprisingly, the complexity lower bound remains the same even if the delay bound is relaxed to O(na) for O &lt; a &lt; 1. This implies that the delay-complexity tradeoff curve is flat in the "interval" [O(1), O(n)). We later conditionally extend both complexity results (for O(1) or O(na) delay) to a much stronger computational model, the linear decision tree. Finally, we show that the same complexity lower bounds are conditionally applicable to guaranteeing tight end-to-end delay bounds, if the delay bounds are provided through the Latency Rate (LR) framework.
ID:7
CLASS:1
Title: Counting complexity classes for numeric computations II: algebraic and semialgebraic sets
Abstract: We define counting classes #PR and #PC in the Blum-Shub-Smale setting of computations over the real or complex numbers, respectively. The problems of counting the number of solutions of systems of polynomial inequalities over R, or of systems of polynomial equalities over C, respectively, turn out to be natural complete problems in these classes. We investigate to what extent the new counting classes capture the complexity of computing basic topological invariants of semialgebraic sets (over R) and algebraic sets (over C). We prove that the problem to compute the (modified) Euler characteristic of semialgebraic sets is FPR#P RR-complete, and that the problem to compute the geometric degree of complex algebraic sets is FPR#PCC-complete. We also define new counting complexity classes GCR and GCC in the classical Turing model via taking Boolean parts of the classes above, and show that the problems to compute the Euler characteristic and the geometric degree of (semi)algebraic sets given by integer polynomials are complete in these classes. We complement the results in the Turing model by proving, for all k &#8712; N, the FPSPACE-hardness of the problem of computing the kth Betti number of the set of real zeros of a given integer polynomial. This holds with respect to the singular homology as well as for the Borel-Moore homology.
ID:8
CLASS:1
Title: Teaching Computational Complexity: Resources to Treasure
Abstract: This article describes and discusses textbooks, monographs, and collections that are excellent resources from which to teach courses on computational complexity theory.
ID:9
CLASS:1
Title: Exponential separation of quantum and classical online space complexity
Abstract: The main objective of quantum computation is to exploit the natural parallelism of quantum mechanics to solve problems using less computational resources than classical computers. Although quantum algorithms realizing an exponential time speed-up over the best known classical algorithms exist, no quantum algorithm is known performing computation using less space resources than classical algorithms. In this paper, we study, for the first time explicitly, spacebounded quantum algorithms for computational problems where the input is given not as a whole, but bit by bit. We show that there exist such problems that a quantum computer can solve using exponentiallyless work space than a classical computer. More precisely, we introduce a very natural and simple model of a space-bounded quantum online machine and prove an exponential separation of classical and quantum online space complexity, in the bounded-error setting and for a total language. The language we consider is inspired bya communication problem that Buhrman, Cleve and Wigderson used to show an almost quadratic separation of quantum and classical bounded-error communication complexity. We prove that, in the framework of online space complexity, the separation becomes exponential.
ID:10
CLASS:1
Title: The computational complexity of some julia sets
Abstract: Although numerous computer programs have been written to compute sets of points which claim to approximate Julia sets, no reliable high precision pictures of non-trivial Julia sets are currently known. Usually, no error estimates are added and even those algorithms which work reliably in theory, become unreliable in practice due to rounding errors and the use of fixed length floating point numbers.In this paper we prove the existence of polynomial time algorithms to approximate the Julia sets of complex functions f(z)=z2+c for |c|&lt;1/4. We will give a strict computable error estimation w.r.t. the Hausdorff metric dH which means that the set is recursive [10]. Although these and many more Julia sets J are proven to be recursive [12] and furthermore recursive compact subsets of the Euclidean plane are known to have a computable Turing machine time complexity [10], hardly anything is known about the computational complexity of non-trivial examples. The algorithms given in this paper compute the Julia sets locally in time O(k2&#8226; M(k)) (where M(k) is a time bound for multiplication of two k-bit integers). Roughly speaking, the local time complexity is the number of Turing machine steps to decide for a single point whether it belongs to a grid Kk&#8838; (2-k&#8226; Z)2 such that dH(Kk,J)&#8804; 2-k.
ID:11
CLASS:1
Title: Computational complexity of current GPSG theory
Abstract: An important goal of computational linguistics has been to use linguistic theory to guide the construction of computationally efficient real-world natural language processing systems. At first glance, generalized phrase structure grammar (GPSG) appears to be a blessing on two counts. First, the precise formalisms of GPSG might be a direct and transparent guide for parser design and implementation. Second, since GPSG has weak context-free generative power and context-free languages can be parsed in O(n3) by a wide range of algorithms, GPSG parsers would appear to run in polynomial time. This widely-assumed GPSG "efficient parsability" result is misleading: here we prove that the universal recognition problem for current GPSG theory is exponential-polynomial time hard, and assuredly intractable. The paper pinpoints sources of complexity (e.g. metarules and the theory of syntactic features) in the current GPSG theory and concludes with some linguistically and computationally motivated restrictions on GPSG.
ID:12
CLASS:1
Title: Unsolvability considerations in computational complexity
Abstract: The study of Computational Complexity began with the investigation of Turing machine computations with limits on the amounts of tape or time which could be used. Latter a set of general axioms for measures of resource limiting was presented and this instigated much study of the properties of these general measures. Many interesting results were shown, but the general axioms allowed measures with undesirable properties and many attempts have been made to tighten up the axioms so that only desirable measures will be defined. In this paper several undecidability aspects of complexity classes and several sets associated with them will be examined. These sets will be classified by their degree of unsolvability and restrictions will be placed on measures so that these degrees are identical. This gives rise to a new criterion for the &ldquo;naturalness&rdquo; of measures and to suggestions for strengthening the measures of complexity.
ID:13
CLASS:1
Title: The computational complexity of knot and link problems
Abstract: We consider the problem of deciding whether a polygonal knot in 3-dimensional Euclidean space is unknotted, ie., capable of being continuously deformed without self-intersection so that it lies in a plane. We show that this problem, UNKNOTTING PROBLEM is in NP. We also consider the problem, SPLITTING PROBLEM of determining whether two or more such polygons can be split, or continuously deformed without self-intersection so that they occupy both sides of a plane without intersecting it. We show that it also is in NP. Finally, we show that the problem of determining the genus of a polygonal knot (a generalization of the problem of determining whether it is unknotted) is in PSPACE. We also give exponential worst-case running time bounds for deterministic algorithms to solve each of these problems. These algorithms are based on the use of normal surfaces and decision procedures due to W. Haken, with recent extensions by W. Jaco and J. L. Tollefson.
ID:14
CLASS:1
Title: The computational complexity of nash equilibria in concisely represented games
Abstract: Games may be represented in many different ways, and different representations of games affect the complexity of problems associated with games, such as finding a Nash equilibrium. The traditional method of representing a game is to explicitly list all the payoffs, but this incurs an exponential blowup as the number of agents grows. We study two models of concisely represented games: circuit games, where the payoffs are computed by a given boolean circuit, and graph games, where each agent's payoff is a function of only the strategies played by its neighbors in a given graph. For these two models, we study the complexity of four questions: determining if a given strategy is a Nash equilibrium, finding a Nash equilibrium, determining if there exists a pure Nash equilibrium, and determining if there exists a Nash equilibrium in which the payoffs to the players meet some given guarantees. In many cases, we obtain tight results, showing that the problems are complete for various complexity classes.
ID:15
CLASS:1
Title: Size-time complexity of Boolean networks for prefix computations
Abstract: The prefix problem consists of computing all the products x0x1 &hellip; xj (j = 0, &hellip; , N - 1), given a sequence x = (x0, x1, &hellip; , xN-1) of elements in a semigroup. In this paper we completely characterize the size-time complexity of computing prefixes with Boolean networks, which are synchronized interconnections of Boolean gates and one-bit storage devices. This complexity crucially depends upon two properties of the underlying semigroup, which we call cycle-freedom (no cycle of length greater than one in the Cayley graph of the semigroup), and memory-induciveness (arbitrarily long products of semigroup elements are true functions of all their factors). A nontrivial characterization is given of non-memory-inducive semigroups as those whose recurrent subsemigroup (formed by the elements with self-loops in the Cayley graph) is the direct product of a left-zero semigroup and a right-zero semigroup. Denoting by S and T size and computation time, respectively, we have S = &THgr;((N/T)log(N/T)) for memory-inducive non-cycle-free semigroups, and S = &THgr;(N/T) for all other semigroups. We have T &egr; [&OHgr;(log N), &Ogr;(N)] for all semigroups, with the exception of those whose recurrent subsemigroup is a right-zero semigroup, for which T &egr; [&OHgr;(1), &Ogr;(N)]. The preceding results are also extended to the VLSI model of computation. Area-time optimal circuits are obtained for both boundary and nonboundary I/O protocols.
ID:16
CLASS:1
Title: Open questions in the theory of semifeasible computation
Abstract: The study of semifeasible algorithms was initiated by Selman's work a quarter of century ago [Sel79,Sel81,Sel82]. Informally put, this research stream studies the power of those sets L for which there is a deterministic (or in some cases, the function may belong to one of various nondeterministic function classes) polynomial-time function f such that when at least one of x and y belongs to L, then f(x, y) &isin; L &cap; {x, y}. The intuition here is that it is saying: "Regarding membership in L, if you put a gun to my head and forced me to bet on one of x or y as belonging to L, my money would be on f(x, y) ."In this article, we present a number of open problems from the theory of semifeasible algorithms. For each we present its background and review what partial results, if any, are known.
ID:17
CLASS:1
Title: Communication complexity of secure computation (extended abstract)
Abstract: A secret-ballot vote for a single proposition is an example of a secure distributed computation. The goal is for m participants to jointly compute the output of some n-ary function (in this case, the sum of the votes), while protecting their individual inputs against some form of misbehavior.In this paper, we initiate the investigation of the communication complexity of unconditionally secure multi-party computation, and its relation with various fault-tolerance models. We present upper and lower bounds on communication, as well as tradeoffs among resources.First, we consider the &ldquo;direct sum problem&rdquo; for communications complexity of perfectly secure protocols: Can the communication complexity of securely computing   a single function f : Fn &rarr; F at k sets of inputs be smaller if all are computed simultaneously than if each is computed individually? We show that the answer depends on the failure model. A factor of O(n/log n) can be gained in the privacy model (where processors are curious but correct); specifically, when f is n-ary addition (mod 2), we show a lower bound of &OHgr;(n2 log n) for computing f O(n) times simultaneously. No gain is possible in a slightly stronger fault model (fail-stop mode); specifically, when f is n-ary  addition over  GF(q), we show an exact bound of &THgr;(kn2 log q) for computing f at k sets of inputs simultaneously (for any k &ge; 1).However, if one is willing to pay an additive cost in fault tolerance (from t to t-k+1), then a variety of known non-cryptographic protocols (including &ldquo;provably unparallelizable&rdquo; protocols from above!) can be systematically compiled to compute one function at k sets of inputs with no increase in communication complexity. Our compilation technique is based on a new compression idea of polynomial-based  multi-secret  sharing.Lastly, we show how to compile private protocols into error-detecting protocols at a big savings of a factor of O(n3) (up to a log factor) over the best known error-correcting protocols. This is a new notion of fault-tolerant protocols, and is especially useful when malicious behavior is infrequent, since error-detection implies error-correction in this case.
ID:18
CLASS:1
Title: Randomized computations on large data sets: tight lower bounds
Abstract: We study the randomized version of a computation model (introduced in [9, 10]) that restricts random access to external memory and internal memory space. Essentially, this model can be viewed as a powerful version of a data stream model that puts no cost on sequential scans of external memory (as other models for data streams) and, in addition, (like other external memory models, but unlike streaming models), admits several large external memory devices that can be read and written to in parallel.We obtain tight lower bounds for the decision problems set equality, multiset equality, and checksort. More precisely, we show that any randomized one-sided-error bounded Monte Carlo algorithm for these problems must perform &#937;(logN) random accesses to external memory devices, provided that the internal memory size is at most O(4&#8730;N/logN), where N denotes the size of the input data.From the lower bound on the set equality problem we can infer lower bounds on the worst case data complexity of query evaluation for the languages XQuery, XPath, and relational algebra on streaming data. More precisely, we show that there exist queries in XQuery, XPath, and relational algebra, such that any (randomized) Las Vegas algorithm that evaluates these queries must perform &#937;(logN) random accesses to external memory devices, provided that the internal memory size is at most O(4&#8730;N/logN).
ID:19
CLASS:1
Title: The computational complexity of continued fractions
Abstract: The Knuth-Sch&ouml;nhage algorithm for expanding a quolynomial into a continued fraction is shown to be essentially optimal with respect to the number of multiplications/divisions used, uniformly in the inputs.
ID:20
CLASS:1
Title: The relationship between computational circuit complexity and radix
Abstract: The relationship between computational circuit complexity and radix is analyzed. Circuit complexity is measured by the computation time or the number of (d, r) modules in the circuit. The effect of radix on complexity is determined by showing what happens to lower bounds on computation time and number of modules when the radix is varied. If the radix is changed from d1 to d2, the lower bound on computation time is changed by the additive factor logr (logd2d1); the lower bound on number of modules is multiplied by (logd2d1) or log(d2d1)2.
ID:21
CLASS:1
Title: The complexity of computing a Nash equilibrium
Abstract: We resolve the question of the complexity of Nash equilibrium by showing that the problem of computing a Nash equilibrium in a game with 4 or more players is complete for the complexity class PPAD. Our proof uses ideas from the recently-established equivalence between polynomial time solvability of normal form games and graphical games, establishing that these kinds of games can simulate a PPAD-complete class of Brouwer functions.
ID:22
CLASS:1
Title: Computational complexity versus virtual worlds
Abstract: The ability to simulate complex physical situations in real-time is a critical element of any "virtual world" scenario, as well as being key for many engineering and robotics applications. Unfortunately the computation cost of standard physical simulation methods increases rapidly as the situation becomes more complex. The result is that even when using the fastest supercomputers we are still able to interactively simulate only small, toy worlds. To solve this problem I propose changing the way we represent and simulate physics in order to reduce the computational complexity of physical simulation, thus making possible interactive simulation of complex situations.
ID:23
CLASS:1
Title: The future of computational complexity theory: part I
Abstract: As you probably already know, there is an active discussion going on---in forums ranging from lunch-table conversations to workshops on "strategic directions" to formal reports---regarding the future of theoretical computer science. Since your complexity columnist does not know The Answer, I've asked a number of people to contribute their comments on the narrower issue of the future of complexity theory. The only ground rule was a loose 1-page limit; each contributor could choose what aspect(s) of the future to address, and the way in which to address them. The first installment of contributions appears in this issue, and one or two more installments will appear among the next few issues.Also coming during the next few issues: the search for the perfect theory journal, and (for the sharp-eyed) Lance Fortnow dons a clown suit. Finally, let me mention that work of Russell Impagliazzo resolves one of the open questions from Complexity Theory Column 11.
ID:24
CLASS:1
Title: The computational complexity of the correct-prefix property for TAGs
Abstract: A new upper bound is presented for the computational complexity of the parsing problem for TAGs, under the constraint that input is read from left to right in such a way that errors in the input are observed as soon as possible, which is called the "correct-prefix property." The former upper bound, O(n9), is now improved to O(n6), which is the same as that of practical parsing algorithms for TAGs without the additional constraint of the correct-prefix property.
ID:25
CLASS:1
Title: On non-linear lower bounds in computational complexity
Abstract: The purpose of this paper is to explore the possibility that purely graph-theoretic reasons may account for the superlinear complexity of wide classes of computational problems. The results are therefore of two kinds: reductions to graph theoretic conjectures on the one hand, and graph theoretic results on the other. We show that the graph of any algorithm for any one of a number of arithmetic problems (e.g. polynomial multiplication, discrete Fourier transforms, matrix multiplication) must have properties closely related to concentration networks.
ID:26
CLASS:1
Title: Complexity Of Computations
Abstract: Construction of algorithms is a time honored mathematical activity. Euclid's algorithm for finding the greatest common divisor of two integers, as well as the many constructions by a ruler and compass are some of the fruits of the search for algorithms by the Greek mathematicians. In our days, we have the whole field of Numerical Analysis devoted to finding a variety of algorithms for numerical integration of differential equations.
ID:27
CLASS:1
Title: On the complexity of polynomial matrix computations
Abstract: We study the link between the complexity of polynomial matrix multiplication and the complexity of solving other basic linear algebra problems on polynomial matrices. By polynomial matrices we mean ntimes n matrices in K[x] of degree bounded by d, with K a commutative field. Under the straight-line program model we show that multiplication is reducible to the problem of computing the coefficient of degree d of the determinant. Conversely, we propose algorithms for minimal approximant computation and column reduction that are based on polynomial matrix multiplication; for the determinant, the straight-line program we give also relies on matrix product over K[x] and provides an alternative to the determinant algorithm of [16, 17]. We further show that all these problems can be solved in particular in O (&#969;) operations in K. Here the "soft O" notation O  indicates some missing log (nd) factors and &#969; is the exponent of matrix multiplication over K.
ID:28
CLASS:1
Title: The computational complexity of boolean and stochastic agent design problems
Abstract: The Agent Design problem involves determining whether or not it is possible to construct an agent capable of satisfying a given task specification in a particular environment. The simplest examples of such specifications are where an agent is required to bring about some goal or where an agent is required to maintain some invariant condition. Both cases can be viewed as identifying a set of states with a single propositional variable, &khgr;, so that the tasks are specified by formulae &khgr; (reach one of the states represented by &khgr;) or barx (avoid all of the states represented by &khgr;). In previous work, the complexity of agent design problems expressed by single argument propositional formulae has been systematically investigated for a range of different types of environments. It was shown that the complexity of the problem varies from nl-complete in the best case to undecidable in the worst. In this paper, we extend these previous results by investigating the effect on problem complexity of allowing tasks for agents to be specified as arbitrary propositional formulae of &eegr; variables. We show that, in those settings where Agent Design is PSPACE or NP-complete the general problem is "no more difficult", i.e., remains in PSPACE or NP. In contrast, a number of the settings for which there were polynomial-time methods become NP-complete when considering more general task specifications. Finally, we investigate the complexity of stochastic agent design problems, where we require an agent that has a "better than evens" chance of success: we show that complexity results for the "guaranteed success" cases carries across to stochastic instances.
ID:29
CLASS:1
Title: A model of computation for VLSI with related complexity results
Abstract: A new model of computation for VLSI, based on the assumption that time for propagating information is at least linear in the distance, is proposed. While accommodating for basic laws of physics, the model is designed to be general and technology independent. Thus, from a complexity viewpoint, it is especially suited for deriving lower bounds and trade-offs. New results for a number of problems, including fan-in, transitive functions, matrix multiplication, and sorting are presented. As regards upper bounds, it must be noted that, because of communication costs, the model clearly favors regular and pipelined architectures (e.g., systolic arrays).
ID:30
CLASS:1
Title: The computational complexity of component selection in simulation reuse
Abstract: Simulation composability has been much more difficult to realize than some initially imagined. We believe that success lies in explicit considerations for the adaptability of components. In this paper we show that the complexity of optimal component selection for adaptable components is NP-complete. However, our approach allows for the efficient adaptation of components to construct a complex simulation in the most flexible manner while allowing the greatest opportunity to meet all requirements, all the while reducing time and costs. We demonstrate that complexity can vary from polynomial, to NP, and even to exponential as a function of seemingly simple decisions made about the nature of dependencies among components. We generalize these results to show that regardless of the types or reasons for dependencies in component selection, just their mere existence makes this problem very difficult to solve optimally.
ID:31
CLASS:1
Title: The future of computational complexity theory: part II
Abstract: This is the final part of a 2-part column on the future of computational complexity theory. The grounds rules were that the contributors had no restrictions (except a 1-page limit). For readers interested in more formal reports, in addition to the two URLs mentioned in the previous issue (ftp://ftp.cs.washington.edu/tr/1996/O3/UW-CSE-96-O3-O3.PS.Z and http://theory.lcs.mit.edu/-oded/toc-sp.html) I would also point to the recent "Strategic Directions" report (http://geisel.csl.uiuc.edu/-loui/complete.html).Coming during the next few issues: the search for the perfect theory journal; Thomas Jefferson exposed as a theoretical computer scientist; and Mitsunori Ogihara's survey of DNA-based computation. (The "Lance Fortnow in a clown suit" article promised in the previous column actually ran stand-alone last issue due to scheduling; it probably is still available at a library near you!) Finally, some recent work by Edith Hemaspaandra, Harald Hempel, and myself, and of Buhrman and Fortnow, partially resolves one of the open questions from Complexity Theory Column 11.
ID:32
CLASS:1
Title: Complexity reduction of biochemical networks
Abstract: This paper discusses two broad approaches for reducing the complexity of large cellular network models. The first approach involves exploiting conservation and time-scale separation and allows the dimension of the model to be significantly reduced. The second approach involves identifying subnetworks that carry out well defined functions and replacing these with simpler representations. Examples include identification of functional subnetworks such as oscillators or bistable switches and replacing these with a simplified mathematical construct. This enables complex networks to be rationalized as a series of hierarchical modules and greatly simplifies our ability to understand the dynamics of complex networks.
ID:33
CLASS:1
Title: A model of computation for VLSI with related complexity results
Abstract:  We propose a new model of computation for VLSI which is a refinement of previous models and makes the additional assumption that the time for propagating information is linear in the distance. Our approach is motivated by the failure of previous models to allow for realistic asymptotic analysis. While accommodating for basic laws of physics, this model tries to be most general and technology-independent. Thus, from a complexity viewpoint, it is especially suited for deriving lower bounds and trade-offs. We present new results for a number of problems including fan-in, addition, transitive functions, matrix multiplication, and sorting. 
ID:34
CLASS:1
Title: Computational complexity of computing polynomials over the fields of real and complex numbers
Abstract: Fast computation of polynomials of 1 variable in the fields R and C of real and complex numbers is considered. The optimal schemes of computation with preconditioning (that is, the schemes involving the minimal number of arithmetic operations without counting preliminary treatment of coefficients) for evaluation in C are presented. The schemes which are close to optimal ones are presented for evaluation in R. The difference between the complexity of computation in R and in C is established. A new generalization of the problem is presented.
ID:35
CLASS:1
Title: Reducing computational complexity with array predicates
Abstract: This article describes how array predicates were used to reduce the computational complexity of four APL primitive functions when one of their arguments is a permutation vector. The search primitives, indexof and set membership, and the sorting primitives, upgrade and downgrade, execute in linear time on such arguments. Our contribution, a method for static determination of array properties, lets us generate code that is optimized for special cases of primitives. Our approach eliminates runtime checks which would otherwise slow down the execution of all cases of the effected primitives. We use the same analysis technique to reduce the type complexity of certain array primitives.
ID:36
CLASS:1
Title: Size-time complexity of Boolean networks for prefix computations
Abstract: The prefix problem consists of computing all the products x0x1&hellip;xj (j=0, &hellip;, N - 1), given a sequence x = (x0, x1, &hellip;, xN - 1) of elements in a semigroup. In this paper we completely characterize the size-time complexity of computing prefixes with boolean networks, which are synchronized interconnections of Boolean gates and one-bit storage devices. This complexity crucially depends upon a property of the underlying semigroup, which we call cycle-freedom (no cycle of length greater than one in the Cayley graph of the semigroup). Denoting by S and T size and computation time, respectively, we have S = &THgr;((N/T) log(N/T)), for non-cycle-free semigroups, and S = &THgr;(N/T), for cycle-free semigroups. In both cases, T &isin; [&OHgr;(logN), O(N)].
ID:37
CLASS:1
Title: Exploring an information-based approach to computation and computational complexity
Abstract: We present the background and justification for a new approach to studying computation and computational complexity. We focus on categories of problems and categories of solutions which provide the logical definition on which to base an algorithm. Computational capability is introduced via a formalization of computation termed a model of computation. The concept of algorithm is formalized using the methods of Traub, Wasilkowski and Wo&zacute;niakowski, from which we can formalize the differences between deterministic, non-deterministic, and heuristic algorithms. Finally, we introduce our measure of complexity: the Hartley entropy measure. We provide many examples to amplify the concepts introduced.
ID:38
CLASS:1
Title: A technique for reducing normal-form games to compute a Nash equilibrium
Abstract: We present a technique for reducing a normal-form (aka. (bi)matrix) game, O, to a smaller normal-form game, R, for the purpose of computing a Nash equilibrium. This is done by computing a Nash equilibrium for a subcomponent, G, of O for which a certain condition holds. We also show that such a subcomponent G on which to apply the technique can be found in polynomial time (if it exists), by showing that the condition that G needs to satisfy can be modeled as a Horn satisfiability problem. We show that the technique does not extend to computing Pareto-optimal or welfare-maximizing equilibria. We present a class of games, which we call ALAGIU (Any Lower Action Gives Identical Utility) games, for which recursive application of (special cases of) the technique is sufficient for finding a Nash equilibrium in linear time. Finally, we discuss using the technique to compute approximate Nash equilibria.
ID:39
CLASS:1
Title: Towards understanding the predictability of stock markets from the perspective of computational complexity
Abstract: This paper initiates a study into the century-old issue of market predictability from the perspective of computational complexity. We develop a simple agent-based model for a stock market where the agents are traders equipped with simple trading strategies, and their trades together determine the stock prices. Computer simulations show that a basic case of this model is already capable of generating price graphs which are visually similar to the recent price movements of high tech stocks. In the general model, we prove that if there are a large number of traders but they employ a relatively small number of strategies, then there is a polynomial-time algorithm for predicting future price movements with high accuracy. On the other hand, if the number of strategies is large, market prediction becomes complete in two new computational complexity classes CPP and BCPP, where PNP[&Ogr;(log n)] e BCPP e CPP = PP. These computational completeness results open up a novel possibility that the price graph of a actual stock could be sufficiently deterministic for various prediction goals but appear random to all polynomial-time prediction algorithms.
ID:40
CLASS:1
Title: On the complexity of global computation in the presence of link failures: the case of uni-directional faults
Abstract: We consider distributed computations in an asynchronous communication model with undetectable link failures. The computational tasks we consider are obtaining the value of a predetermined function of the local inputs scattered in the network (e.g., the sum of all local values). We call this task Global Computation.A trivial protocol for Global Computation consists of each processor sending its local input to all processors via flooding. Our aim is to justify the use of this simple protocol, in the presence of faulty links, by proving matching lower bounds on the message complexity (i.e., total number of messages sent) of Global Computation.In this paper we concentrate on the case in which the communication links are either unidirectional or fail in a  uni-directional manner. Our main result states that for every n and m, the message complexity of Global Computation on such networks is at least n.m/PolyLog (n) where n is the number of processors and m is the number of links. Hence, in the presence of unidirectional link failures, the simple flooding algorithm is optimal up to a polylogarithmic factor.
ID:41
CLASS:1
Title: Some complexity questions related to distributive computing(Preliminary Report)
Abstract: Let M &equil; {0, 1, 2, ..., m&mdash;1} , N &equil; {0, 1, 2,..., n&mdash;1} , and f:M &times; N &rarr; {0, 1} a Boolean-valued function. We will be interested in the following problem and its related questions. Let i &egr; M, j &egr; N be integers known only to two persons P1 and P2, respectively. For P1 and P2 to determine cooperatively the value f(i, j), they send information to each other alternately, one bit at a time, according to some algorithm. The quantity of interest, which measures the information exchange necessary for computing f, is the minimum number of bits exchanged in any algorithm. For example, if f(i, j) &equil; (i + j) mod 2. then 1 bit of information (conveying whether i is odd) sent from P1 to P2 will enable P2 to determine f(i, j), and this is clearly the best possible. The above problem is a variation of a model of Abelson [1] concerning information transfer in distributive computions.
ID:42
CLASS:1
Title: Computational complexity and numerical stability
Abstract: Limiting consideration to algorithms satisfying various numerical stability requirements may change lower bounds for computational complexity and/or make lower bounds easier to prove. We will show that, under a sufficiently strong restriction upon numerical stability, any algorithm for multiplying two n&times;n matrices using only +, &minus; and &times; requires at least n3 multiplications. We conclude with a survey of results concerning the numerical stability of several algorithms which have been considered by complexity theorists.
ID:43
CLASS:1
Title: On the complexity of computing the measure of \&\#8746;[a<sub>i</sub>,b<sub>i</sub>]
Abstract: The decision tree complexity of computing the measure of the union of n (possibly overlapping) intervals is shown to be &OHgr;(n log n), even if comparisons between linear functions of the interval endpoints are allowed. The existence of an &OHgr;(n log n) lower bound to determine whether any two of n real numbers are within &isin; of each other is also demonstrated. These problems provide an excellent opportunity for discussing the effects of the computational model on the ease of analysis and on the results produced.
ID:44
CLASS:1
Title: The cost of social agents
Abstract: In this paper we follow the BOID (Belief, Obligation, Intention, Desire) architecture to describe agents and agent types in Defeasible Logic. We argue that the introduction of obligations can provide a new reading of the concepts of intention and intentionality. Then we examine the notion of social agent (i.e., an agent where obligations prevail over intentions) and discuss some computational and philosophical issues related to it. We show that the notion of social agent either requires more complex computations or has some philosophical drawbacks.
ID:45
CLASS:1
Title: On the complexity of computing peer agreements for consistent query answering in peer-to-peer data integration systems
Abstract: Peer-to-Peer (P2P) data integration systems have recently attracted significant attention for their ability to manage and share data dispersed over different peer sources. While integrating data for answering user queries, it often happens that inconsistencies arise, because some integrity constraints specified on peers' global schemas may be violated. In these cases, we may give semantics to the inconsistent system by suitably "repairing" the retrieved data, as typically done in the context of traditional data integration systems. However, some specific features of P2P systems, such as peer autonomy and peer preferences (e.g., different source trusting), should be properly addressed to make the whole approach effective. In this paper, we face these issues that were only marginally considered in the literature. We first present a formal framework for reasoning about autonomous peers that exploit individual preference criteria in repairing the data. The idea is that queries should be answered over the best possible database repairs with respect to the preferences of all peers, i.e., the states on which they are able to find an agreement. Then, we investigate the computational complexity of dealing with peer agreements and of answering queries in P2P data integration systems. It turns out that considering peer preferences makes these problems only mildly harder than in traditional data integration systems.
ID:46
CLASS:1
Title: Multithreading decoupled architectures for complexity-effective general purpose computing
Abstract: Decoupled architectures have not traditionally been used in the context of general purpose computing because of their inability to tolerate control-intensive code that exists across a wide range of applications. This work investigates the possibility of using multithreading to overcome the loss of decoupling dependencies that represent the cause of this main limitation in decoupled architectures. A proposal for a multithreaded decoupled control/access/execute architecture is presented as a platform for achieving high performance on general purpose workloads. It is argued that such a decoupled architecture is more complexity-effective and scalable than comparable superscalar processors, which incorporate enormous amounts of complexity for modest performance gains.
ID:47
CLASS:1
Title: Hierarchies based on computational complexity and irregularities of class determining measured sets (Preliminary Report)
Abstract: We consider here the problem of building transfinite hierarchies of computable functions on the basis of their difficulty of computation. Previous hierarchies of functions through the constructive ordinals have had two major problems, each apparently caused by not having techniques to restrict the classes considered at limit ordinals. These problems are, first that every function occurs at some name for &ohgr;, or some other small ordinal, and second that two names for the same constructive ordinal have two different classes of functions associated with them. The axioms of computational complexity introduced by Blum[1] and several theorems by Blum[1],and by McCreight and Meyer[9], particularly the union theorem, lead to a very natural method of hierarchy building that partially avoids the first of these problems. The other problem is not overcome, however. In attempting to understand the problem of obtaining unique, or nearly unique classes of functions we have encountered difficulties because the class determining measured sets of McCreight and Meyer[9] lack several properties that might reasonably be desired.
ID:48
CLASS:1
Title: Computational Complexity of One-Tape Turing Machine Computations
Abstract: The quantitative aspects of one-tape Turing machine computations are considered. It is shown, for instance, that there exists a sharp time bound which must be reached for the recognition of nonregular sets of sequences. It is shown that the computation time can be used to characterize the complexity of recursive sets of sequences, and several results are obtained about this classification. These results are then applied to the recognition speed of context-free languages and it is shown, among other things, that it is recursively undecidable how much time is required to recognize a nonregular context-free language on a one-tape Turing machine. Several unsolved problems are discussed.
ID:49
CLASS:1
Title: Computational complexity of itemset frequency satisfiability
Abstract: Computing frequent itemsets is one of the most prominent problems in data mining. We introduce a new, related problem, called FREQSAT: given some itemset-interval pairs, does there exist a database such that for every pair the frequency of the itemset falls in the interval? It is shown in this paper that FREQSAT is not finitely axiomatizable and that it is NP-complete. We also study cases in which other characteristics of the database are given as well. These characteristics can complicate FREQSAT even more. For example, when the maximal number of duplicates of a transaction is known, FREQSAT becomes PP-hard. We describe applications of FREQSAT in frequent itemset mining algorithms and privacy in data mining.
ID:50
CLASS:1
Title: Approaches to complexity reduction in a systems biology research environment (SYCAMORE)
Abstract: Due to the complexity of biochemical reaction networks, so-called complexity reduction algorithms play a crucial role for making simulations efficient and for dissecting biochemical networks into meaningful subnetworks for analysis. Here, different approaches are presented, which we are developing in the context of a computational research environment for systems biology (SYCAMORE). These approaches are based on time-scale decomposition, sensitivity analysis, and hybrid simulation methods.
ID:51
CLASS:1
Title: Computational complexity of probabilistic disambiguation by means of tree-grammars
Abstract: This paper studies the computational complexity of disambiguation under probabilistic tree-grammars as in (Bod, 1992; Schabes and Waters, 1993). It presents a proof that the following problems are NP-hard: computing the Most Probable Parse from a sentence or from a word-graph, and computing the Most Probable Sentence (MPS) from a word-graph. The NP-hardness of computing the MPS from a word-graph also holds for Stochastic Context-Free Grammars (SCFGs).
ID:52
CLASS:1
Title: Computational complexity of probabilistic Turing machines
Abstract: Probabilistic Turing machines are Turing machines with the ability to flip coins in order to make random decisions. We allow probabilistic Turing machines small but nonzero error probability in computing number-theoretic functions. An example is given of a function computable more quickly by probabilistic Turing machines than by deterministic Turing machines. It is shown how probabilistic linear-bounded automata can simulate nondeterministic linear-bounded automata.
ID:53
CLASS:1
Title: An incremental algorithm for computing ranked full disjunctions
Abstract: The full disjunction is a variation of the join operator that maximally combines tuples from connected relations, while preserving all information in the relations. The full disjunction can be seen as a natural extension of the binary outerjoin operator to an arbitrary number of relations and is a useful operator for information integration. This paper presents the algorithm INCREMENTALFD for computing the full disjunction of a set of relations. INCREMENTALFD improves upon previous algorithms for computing the full disjunction in three ways. First, it has a lower total run-time when computing the full result and a lower runtime when computing only k tuples of the result, for any constant k. Second, for a natural class of ranking functions, INCREMENTALFD returns tuples in ranking order. Third, INCREMENTALFD can be adapted to have a block-based execution, instead of a tuple-based execution.
ID:54
CLASS:1
Title: Hybrid quantum-classical computing with applications to computer graphics
Abstract: Quantum computing (QC) has become an important area of research in computer science because of its potential to provide more efficient algorithmic solutions to certain problems than are possible with classical computing (CC). In particular, QC is able to exploit the special properties of quantum superposition to achieve computational parallelism beyond what can be achieved with parallel CC computers. However, these special properties are not applicable for general computation. Therefore, we propose the use of "hybrid quantum computers" (HQCs) that combine both classical and quantum computing architectures in order to leverage the benefits of both. We demonstrate how an HQC can exploit quantum search to support general database operations more efficiently than is possible with CC. Our solution is based on new quantum results that are of independent significance to the field of quantum computing. More specifically, we demonstrate that the most restrictive implications of the quantum No-Cloning Theorem can be avoided through the use of semiclones. In this paper we discuss specific applications of quantum search to problems in computational geometry, simulation, and computer graphics.
ID:55
CLASS:1
Title: On the complexity of nonrecursive XQuery and functional query languages on complex values
Abstract: This article studies the complexity of evaluating functional query languages for complex values such as monad algebra and the recursion-free fragment of XQuery. We show that monad algebra, with equality restricted to atomic values, is complete for the class TA[2O(n), O(n)] of problems solvable in linear exponential time with a linear number of alternations if the query is assumed to be part of the input. The monotone fragment of monad algebra with atomic value equality but without negation is NEXPTIME-complete. For monad algebra with deep value equality, that is, equality of complex values, we establish TA[2O(n), O(n)] lower and exponential-space upper bounds. We also study a fragment of XQuery, Core XQuery, that seems to incorporate all the features of a query language on complex values that are traditionally deemed essential. A close connection between monad algebra on lists and Core XQuery (with &ldquo;child&rdquo; as the only axis) is exhibited. The two languages are shown expressively equivalent up to representation issues. We show that Core XQuery is just as hard as monad algebra with respect to query and combined complexity. As Core XQuery is NEXPTIME-hard, the best-known techniques for processing such problems require exponential amounts of working memory and doubly exponential time in the worst case. We present a property of queries---the lack of a certain form of composition---that virtually all real-world XQueries have and that allows for query evaluation in PSPACE and thus singly exponential time. Still, we are able to show for an important special case---Core XQuery with equality testing restricted to atomic values---that the composition-free language is just as expressive as the language with composition. Thus, under widely-held complexity-theoretic assumptions, the language with composition is an exponentially more succinct version of the composition-free language.
ID:56
CLASS:1
Title: A logic programming approach to knowledge-state planning: Semantics and complexity
Abstract: We propose a new declarative planning language, called K, which is based on principles and methods of logic programming. In this language, transitions between states of knowledge can be described, rather than transitions between completely described states of the world, which makes the language well suited for planning under incomplete knowledge. Furthermore, our formalism enables the use of default principles in the planning process by supporting negation as failure. Nonetheless, K also supports the representation of transitions between states of the world (i.e., states of complete knowledge) as a special case, which shows that the language is very flexible. As we demonstrate on particular examples, the use of knowledge states may allow for a natural and compact problem representation. We then provide a thorough analysis of the computational complexity of K, and consider different planning problems, including standard planning and secure planning (also known as conformant planning) problems. We show that these problems have different complexities under various restrictions, ranging from NP to NEXPTIME in the propositional case. Our results form the theoretical basis for the DLVk system, which implements the language K on top of the DLV logic programming system.
ID:57
CLASS:1
Title: Typing and querying XML documents: some complexity bounds
Abstract: We study the complexity bound of validating XML documents, viewed as labeled unranked ordered trees, against various typing systems like DTDs, XML schemas, tree automata ... We also consider query evaluation complexities for various fragments of XPath. For both problems, validation and query evaluation, we consider data and combined complexity bounds.
ID:58
CLASS:1
Title: Computational properties of metaquerying problems
Abstract: Metaquerying is a data mining technology by which hidden dependencies among several database relations can be discovered. This tool has already been successfully applied to several real-world applications, but only preliminary results about the complexity of metaquerying can be found in the literature. In this article, we define several variants of metaquerying that encompass, as far as we know, all the variants that have been defined in the literature. We study both the combined complexity and the data complexity of these variants. We show that under the combined complexity measure metaquerying is generally intractable (unless P = NP), lying sometimes quite high in the complexity hierarchies (as high as NPPP), depending on the characteristics of the plausibility index. Nevertheless, we are able to single out some tractable and interesting metaquerying cases, whose combined complexity is LOGCFL-complete. As for the data complexity of metaquerying, we prove that, in general, it is within TC0, but lies within AC0 in some simpler cases. Finally, we discuss the implementation of metaqueries by providing algorithms that answer them.
ID:59
CLASS:1
Title: Cosmological lower bound on the circuit complexity of a small problem in logic
Abstract: An exponential lower bound on the circuit complexity of deciding the weak monadic second-order theory of one successor (WS1S) is proved. Circuits are built from binary operations, or 2-input gates, which compute arbitrary Boolean functions. In particular, to decide the truth of logical formulas of length at most 610 in this second-order language requires a circuit containing at least 10125 gates. So even if each gate were the size of a proton, the circuit would not fit in the known universe. This result and its proof, due to both authors, originally appeared in 1974 in the Ph.D. thesis of the first author. In this article, the proof is given, the result is put in historical perspective, and the result is extended to probabilistic circuits.&ast;
ID:60
CLASS:1
Title: Some facets of complexity theory and cryptography: A five-lecture tutorial
Abstract: In this tutorial, selected topics of cryptology and of computational complexity theory are presented. We give a brief overview of the history and the foundations of classical cryptography, and then move on to modern public-key cryptography. Particular attention is paid to cryptographic protocols and the problem of constructing key components of protocols such as one-way functions. A function is one-way if it is easy to compute, but hard to invert. We discuss the notion of one-way functions both in a cryptographic and in a complexity-theoretic setting. We also consider interactive proof systems and present some interesting zero-knowledge protocols. In a zero-knowledge protocol, one party can convince the other party of knowing some secret information without disclosing any bit of this information. Motivated by these protocols, we survey some complexity-theoretic results on interactive proof systems and related complexity classes.
ID:61
CLASS:1
Title: Complexity and expressive power of logic programming
Abstract: This article surveys various complexity and expressiveness results on different forms of logic programming. The main focus is on decidable forms of logic programming, in particular, propositional logic programming and datalog, but we also mention general logic programming with function symbols. Next to classical results on plain logic programming (pure Horn clause programs), more recent results on various important extensions of logic programming are surveyed. These include logic programming with different forms of negation, disjunctive logic programming, logic programming with equality, and constraint logic programming.
ID:62
CLASS:1
Title: The complexity of acyclic conjunctive queries
Abstract: This paper deals with the evaluation of acyclic Booleanconjunctive queries in relational databases. By well-known resultsof Yannakakis[1981], this problem is solvable in polynomial time;its precise complexity, however, has not been pinpointed so far. Weshow that the problem of evaluating acyclic Boolean conjunctivequeries is complete for LOGCFL, the class of decision problems thatare logspace-reducible to a context-free language. Since LOGCFL iscontained in AC1 and NC2, the evaluation problem of acyclic Booleanconjunctive queries is highly parallelizable. We present a paralleldatabase algorithm solving this problem with alogarithmic number ofparallel join operations. The algorithm is generalized to computingthe output of relevant classes of non-Boolean queries. We also showthat the acyclic versions of the following well-known database andAI problems are all LOGCFL-complete: The Query Output Tuple problemfor conjunctive queries, Conjunctive Query Containment, ClauseSubsumption, and Constraint Satisfaction. The LOGCFL-completenessresult is extended to the class of queries of bounded tree widthand to other relevant query classes which are more general than theacyclic queries.
ID:63
CLASS:1
Title: Proof-complexity results for nonmonotonic reasoning
Abstract: It is well-known that almost all nonmonotonic formalisms have a higher worst-case complexity than classical reasoning. In some sense, this observation denies one of the original motivations of nonmonotonic systems, which was the expectation taht nonmonotonic rules should help to speed-up the reasoning process, and not make it more difficult. In this paper, we look at this issue from a proof-theoretical perspective. We consider analytic calculi for certain nonmonotonic logis and analyze to what extent the presence of nonmonotonic rules can simplify the search for proofs. In particular, we show that there are classes of first-order formulae which have only extremely long &ldquo;classical&rdquo; proofs, i.e., proofs without applications of nonmonotonic rules, but there are short proofs using nonmonotonic inferences. Hence,despite the increase of complexity in the worst case, there are instances where nonmonotonic reasoning can be much simpler than classical (cut-free) reasoning.
ID:64
CLASS:1
Title: Complexity of finite-horizon Markov decision process problems
Abstract:  Controlled stochastic systems occur in science engineering, manufacturing, social sciences, and many other cntexts. If the systems is modeled as a Markov decision process (MDP) and will run ad infinitum, the optimal control policy can be computed in polynomial time using linear programming. The problems considered here assume that the time that the process will run is finite, and based on the size of the input. There are mny factors that compound the complexity of computing the optimal policy. For instance, there are many factors that compound the complexity of this computation. For instance, if the controller does not have complete information about the state of the system, or if the system is represented in some very succint manner, the optimal policy is provably    not computable in time polynomial in the size of the input. We analyze the computational complexity of evaluating policies and of determining whether a sufficiently good policy exists for a MDP, based on a number of confounding factors, including the observability of the system state; the succinctness of the representation; the type of policy; even the number of actions relative to the number of states. In almost every case, we show that the decision problem is complete for some known complexity class. Some of these results are familiar from work by Papadimitriou and Tsitsiklis and others, but some, such as our PL-completeness proofs, are surprising. We include proofs of completeness for natural problems in the as yet little-studied classes NPPP.
ID:65
CLASS:1
Title: Complexity estimates depending on condition and round-off error
Abstract: This paper has two agendas. One is to develop the foundations of round-off in computation. The other is to describe an algorithm for deciding feasibility for polynomial systems of equations and inequalities together with its complexity analysis and its round-off properties. Each role reinforces the other.
ID:66
CLASS:1
Title: Making computation count: arithmetic circuits in the nineties
Abstract: This issue's expert guest column is by Eric Allender, who has just taken over the Structural Complexity Column in the Bulletin of the EATCS.Regarding "Journals to Die For" (SIGACT News Complexity Theory Column 16), Joachim von zur Gathen, the editor-in-chief of computational complexity, has written me pointing out, quite correctly, that I cheated his journal of a point. His journal, in the issue checked, does include the email address of each author. I apologize for the missing point. Joachim also asked me to mention that, while the journal does not require the "alpha" citation style, it does strongly advise authors to use that style.
ID:67
CLASS:1
Title: On the combinatorial and algebraic complexity of quantifier elimination
Abstract: In this paper, a new algorithm for performing quantifier elimination from first order formulas over real closed fields in given. This algorithm improves the complexity of the asymptotically fastest algorithm for this problem, known to this data. A new feature of this algorithm is that the role of the algebraic part (the dependence on the degrees of the imput polynomials) and the combinatorial part (the dependence on the number of polynomials) are sparated. Another new feature is that the degrees of the polynomials in the equivalent quantifier-free formula that is output, are independent of the number of input polynomials. As special cases of this algorithm new and improved algorithms for deciding a sentence in the first order theory over real closed fields, and also for solving the existential problem in the first order theory over real closed fields, are obtained.
ID:68
CLASS:1
Title: The complexity of logic-based abduction
Abstract: Abduction is an important form of nonmonotonic reasoning allowing one to find explanations for certain symptoms or manifestations. When the application domain is described by a logical theory, we speak about logic-based abduction. Candidates for abductive explanations are usually subjected to minimality criteria such as subset-minimality, minimal cardinality, minimal weight, or minimality under prioritization of individual hypotheses. This paper presents a comprehensive complexity analysis of relevant decision and search problems related to abduction on propositional theories. Our results indicate that abduction is harder than deduction. In particular, we show that with the most basic forms of abduction the relevant decision problems are complete for complexity classes at the second level of the polynomial hierarchy, while the use of prioritization raises the complexity to the third level in certain cases.
ID:69
CLASS:1
Title: On the complexity of nonrecursive XQuery and functional query languages on complex values
Abstract: This paper studies the complexity of evaluating functional query languages for complex values such as monad algebra and the recursion-free fragment of XQuery.We show that monad algebra with equality restricted to atomic values is complete for the class TA[2o(n), O(n)] of problems solvable in linear exponential time with a linear number of alternations. The monotone fragment of monad algebra with atomic value equality but without negation is complete for nondeterministic exponential time. For monad algebra with deep equality, we establish TA[2o(n), O(n)] lower and exponential-space upper bounds.Then we study a fragment of XQuery, Core XQuery, that seems to incorporate all the features of a query language on complex values that are traditionally deemed essential. A close connection between monad algebra on lists and Core XQuery (with "child" as the only axis) is exhibited, and it is shown that these languages are expressively equivalent up to representation issues. We show that Core XQuery is just as hard as monad algebra w.r.t. combined complexity, and that it is in TC0 if the query is assumed fixed.
ID:70
CLASS:1
Title: The complexity of XPath query evaluation and XML typing
Abstract: We study the complexity of two central XML processing problems. The first is XPath 1.0 query processing, which has been shown to be in PTIME in previous work. We prove that both the data complexity and the query complexity of XPath 1.0 fall into lower (highly parallelizable) complexity classes, while the combined complexity is PTIME-hard. Subsequently, we study the sources of this hardness and identify a large and practically important fragment of XPath 1.0 for which the combined complexity is LOGCFL-complete and, therefore, in the highly parallelizable complexity class NC2. The second problem is the complexity of validating XML documents against various typing schemes like Document Type Definitions (DTDs), XML Schema Definitions (XSDs), and tree automata, both with respect to data and to combined complexity. For data complexity, we prove that validation is in LOGSPACE and depends crucially on how XML data is represented. For the combined complexity, we show that the complexity ranges from LOGSPACE to LOGCFL, depending on the typing scheme.
ID:71
CLASS:1
Title: Complexity of propositional nested circumscription and nested abnormality theories
Abstract: Circumscription has been recognized as an important principle for knowledge representation and common-sense reasoning. The need for a circumscriptive formalism that allows for simple yet elegant modular problem representation has led Lifschitz (AIJ, 1995) to introduce nested abnormality theories (NATs) as a tool for modular knowledge representation, tailored for applying circumscription to minimize exceptional circumstances. Abstracting from this particular objective, we propose LCIRC, which is an extension of generic propositional circumscription by allowing propositional combinations and nesting of circumscriptive theories. As shown, NATs are naturally embedded into this language, and are in fact of equal expressive capability. We then analyze the complexity of LCIRC and NATs, and in particular the effect of nesting. The latter is found to be a source of complexity, which climbs the Polynomial Hierarchy as the nesting depth increases and reaches PSPACE-completeness in the general case. We also identify meaningful syntactic fragments of NATs which have lower complexity. In particular, we show that the generalization of Horn circumscription in the NAT framework remains coNP-complete, and that Horn NATs without fixed letters can be efficiently transformed into an equivalent Horn CNF, which implies polynomial solvability of principal reasoning tasks. Finally, we also study extensions of NATs and briefly address the complexity in the first-order case. Our results give insight into the &ldquo;cost&rdquo; of using LCIRC (respectively, NATs) as a host language for expressing other formalisms such as action theories, narratives, or spatial theories.
ID:72
CLASS:1
Title: Lower bounds on the bounded coefficient complexity of bilinear maps
Abstract: We prove lower bounds of order n log n for both the problem of multiplying polynomials of degree n, and of dividing polynomials with remainder, in the model of bounded coefficient arithmetic circuits over the complex numbers. These lower bounds are optimal up to order of magnitude. The proof uses a recent idea of R. Raz [Proc. 34th STOC 2002] proposed for matrix multiplication. It reduces the linear problem of multiplying a random circulant matrix with a vector to the bilinear problem of cyclic convolution. We treat the arising linear problem by extending J. Morgenstern's bound [J. ACM 20, pp. 305--306, 1973] in a unitarily invariant way. This establishes a new lower bound on the bounded coefficient complexity of linear forms in terms of the singular values of the corresponding matrix. In addition, we extend these lower bounds for linear and bilinear maps to a model of circuits that allows a restricted number of unbounded scalar multiplications.
ID:73
CLASS:1
Title: Secure multiparty computation of approximations
Abstract: Approximation algorithms can sometimes provide efficient solutions when no efficient exact computation is known. In particular, approximations are often useful in a distributed setting where the inputs are held by different parties and may be extremely large. Furthermore, for some applications, the parties want to compute a function of their inputs securely without revealing more information than necessary. In this work, we study the question of simultaneously addressing the above efficiency and security concerns via what we call secure approximations.We start by extending standard definitions of secure (exact) computation to the setting of secure approximations. Our definitions guarantee that no additional information is revealed by the approximation beyond what follows from the output of the function being approximated. We then study the complexity of specific secure approximation problems. In particular, we obtain a sublinear-communication protocol for securely approximating the Hamming distance and a polynomial-time protocol for securely approximating the permanent and related &num;P-hard problems.
ID:74
CLASS:1
Title: SIGACT news complexity theory column 48
Abstract: Here is a real gift to the field from David Johnson: After a thirteen year intermission, David is restarting his NP-completeness column. His column will now appear about twice yearly in ACM Transactions on Algorithms. Welcome back David, and thanks! And for those for whom a diet of two per year wont do, meals past can be found at http://www.research.att.com/~dsj/columns.html.
ID:75
CLASS:1
Title: Exact GPS simulation with logarithmic complexity, and its application to an optimally fair scheduler
Abstract: Generalized Processor Sharing (GPS) is a fluid scheduling policy providing perfect fairness. The minimum deviation (lead/lag) with respect to the GPS service achievable by a packet scheduler is one packet size. To the best of our knowledge, the only packet scheduler guaranteeing such minimum deviation is Worst-case Fair Weighted Fair Queueing (WF2Q), that requires on-line GPS simulation. Existing algorithms to perform GPS simulation have O(N) complexity per packet transmission (N being the number of competing flows). Hence WF2Q has been charged for O(N) complexity too. Schedulers with lower complexity have been devised, but at the price of at least O(N) deviation from the GPS service, which has been shown to be detrimental for real-time adaptive applications and feedback based applications. Furthermore, it has been proven that the lower bound complexity to guarantee O(1) deviation is &#937;(log N), yet a scheduler achieving such result has remained elusive so far.In this paper we present an algorithm that performs exact GPS simulation with O(log N) worst-case complexity and small constants. As such it improves the complexity of all the packet schedulers based on GPS simulation. In particular, using our algorithm within WF2Q, we achieve the minimum deviation from the GPS service with O(log N) complexity, thus matching the aforementioned lower bound. Furthermore, we assess the effectiveness of the proposed solution by simulating real-world scenarios.
ID:76
CLASS:1
Title: Complexity-distortion tradeoffs in variable complexity 2-D DCT
Abstract: Variable complexity algorithms (VCAs) (i.e. algorithms which take a variable, input-dependent amount of time to complete a task) have been proposed to reduce the average computational complexity of compression algorithms for images and videos. In this paper we introduce a new 2-D variable-complexity DCT that can be used to replace the regular discrete cosine transform (DCT), if only a part of DCT coefficients need to be computed. Consequently, the computational complexity of the DCT can be reduced at the cost in degradation of the reconstructed image quality. We investigate fine-grained complexity-distortion tradeoffs for the proposed variable-complexity, separable DCT (VS-DCT). The evaluation includes a theoretical computational complexity analysis of the VS-DCT in terms of the number of additions and multiplications (note in this paper computational complexity does not refer to the polynomial order of operations) and empirical complexity-distortion curves of the VS-DCT running on two distinct platforms, including a desktop personal computer and an embedded system. The results of evaluation show that our VS-DCT can reduce the computational complexity of a regular DCT by up to 10% for every 3dB decrease in the PSNR (peak signal-to-noise ratio) of the reconstructed images.
ID:77
CLASS:1
Title: Two applications of information complexity
Abstract: We show the following new lower bounds in two concrete complexity models:&lt;ol&gt;(1) In the two-party communication complexity model, we show that the tribes function on n inputs[6] has two-sided error randomized complexity &#937;(n), while its nondeterminstic complexity and co-nondeterministic complexity are both &#920;(&#8730;n). This separation between randomized and nondeterministic complexity is the best possible and it settles an open problem in Kushilevitz and Nisan[17], which was also posed by Beame and Lawry[5].(2) In the Boolean decision tree model, we show that the recursive majority-of-three function on 3h inputs has randomized complexity &#937;((7/3)h). The deterministic complexity of this function is &#920;(3h), and the nondeterministic complexity is &#920;(2h). Our lower bound on the randomized complexity is a substantial improvement over any lower bound for this problem that can be obtained via the techniques of Saks and Wigderson [23], Heiman and Wigderson[14], and Heiman, Newman, and Wigderson[13]. Recursive majority is an important function for which a class of natural algorithms known as directional algorithms does not achieve the best randomized decision tree upper bound.&lt;/ol.These lower bounds are obtained using generalizations of information complexity, which quantifies the minimum amount of information that will have to be revealed about the inputs by every correct algorithm in a given model of computation.
ID:78
CLASS:1
Title: SIGACT News complexity theory column 32
Abstract: We describe some recent or not-so-recent results in the model of learning known as exact learning from queries. Some results are chosen because they led to a result in complexity theory, and others are chosen because they seem to indicate an interesting line of research at the common ground of learning and complexity. More precisely, we survey: (1) an improvement of the Karp-Lipton collapse that stemmed from a learning algorithm for Boolean circuits, (2) recent advances in distinguishing information-theoretic from complexity-theoretic difficulty of learning, and (3) recent results on learning small-depth circuits with modular gates that call for a systematic study of the expressive power of so-called Multiplicity Automata.
ID:79
CLASS:1
Title: Dynamic parallel complexity of computational circuits
Abstract: The dynamic parallel complexity of general computational circuits (defined in introduction) is discussed. We exhibit some relationships between parallel circuit evaluation and some uniform closure properties of a certain class of unary functions and present a systematic method for the design of processor efficient parallel algorithms for circuit evaluation. Using this method: (1) we improve the algorithm for parallel Boolean circuit evaluation; (2) we give a nontrivial upper bound for parallel min-max-plus circuit evaluation; (3) we partially answer the first open question raised in [MiRK85] by showing that all circuits over finite noncommutative semi-ring and circuits over infinite non-commutative semi-ring which has finite dimension over a commutative semi-ring can be evaluated in polylogarithmic time in its size and degree using M(n) processors. Moreover, we develop a theory for determining closure properties of certain classes of unary functions.
ID:80
CLASS:1
Title: Strategic directions in research in theory of computing
Abstract: This report focuses oll two core areas of theory of computing: discrete algorithms and computational complexity theory. The report reviews the purposes and goals of theoretical research, summarizes selected past and recent achievements, explains the importance of sustaining core research, and identifies promising opportunities for filture research. Some research opportunities build bridges between theory of computing and other areas of computer science, and other science and engineering disciplines.
ID:81
CLASS:1
Title: Dyn-FO (preliminary version): a parallel, dynamic complexity class
Abstract: Traditionally, computational complexity has considered only static problems. Classical Complexity Classes such as NC, P, NP, and PSPACE are defined in terms of the complexity of checking&mdash;upon presentation of an entire input&mdash;whether the input satisfies a certain property.For many, if not most, applications of computers including: databases, text editors, program development, it is more appropriate to model the process as a dynamic one. There is a fairly large object being worked on over a period of time. The object is repeatedly modified by users and computations are performed.Thus a dynamic algorithm for a certain class of queries is one that can maintain an input object, e.g. a database, and process changes to the database as well as answering  queries about the current database.Here, we introduce the complexity class, Dynamic First-Order Logic (Dyn-FO). This is the class of properties S, for which there is an algorithm that can perform inserts, deletes and queries from S, such that each unit insert, delete, or query is first-order computable. This corresponds to the sets of properties that can be maintained and queried in first-order logic, i.e. relational calculus, on a relational database.We investigate the complexity class Dyn-FO. We show that many interesting properties are in Dyn-FO including, among others, graph connectivity, k-edge connectivity, and the computation of minimum spanning trees. Furthermore, we show that several NP complete optimization problems admit approximation algorithms in Dyn-FO. Note that none of these  problems is in static FO, and this fact has been used to justify increasing the power of query languages beyond first-order. It is thus striking that these problems are indeed dynamic first-order, and thus, were computable in first-order database languages all along.We also define &ldquo;bounded expansion reductions&rdquo; which honor dynamic complexity classes. We prove that certain standard complete problems for static complexity classes, such as AGAP for P remain complete via these new reductions. On the other hand, we prove that other such problems including GAP for NL and 1GAP for L are no longer complete via bounded expansion reductions. Furthermore, we show that a version of AGAP called AGAP+ is not in Dyn-FO unless all of P is contained in parallel linear  time.Our results shed light on some of the interesting differences between static and dynamic complexity.
ID:82
CLASS:1
Title: Complexity results on DPLL and resolution
Abstract: DPLL and resolution are two popular methods for solving the problem of propositional satisfiability. Rather than algorithms, they are families of algorithms, as their behavior depends on some choices they face during execution: DPLL depends on the choice of the literal to branch on; resolution depends on the choice of the pair of clauses to resolve at each step. The complexity of making the optimal choice is analyzed in this article. Extending previous results, we prove that choosing the optimal literal to branch on in DPLL is &Delta;p2[log n]-hard, and becomes NPPP-hard if branching is only allowed on a subset of variables. Optimal choice in regular resolution is both NP-hard and coNP-hard. The problem of determining the size of the optimal proofs is also analyzed: it is coNP-hard for DPLL, and &Delta;p2[log n]-hard if a conjecture we make is true. This problem is coNP-hard for regular resolution.
ID:83
CLASS:1
Title: The arithmetical complexity of dimension and randomness
Abstract: Constructive dimension and constructive strong dimension are effectivizations of the Hausdorff and packing dimensions, respectively. Each infinite binary sequence A is assigned a dimension dim(A) &isin; &lsqb;0,1&rsqb; and a strong dimension Dim(A) &isin; &lsqb;0,1&rsqb;.
ID:84
CLASS:1
Title: The complexity of homomorphism and constraint satisfaction problems seen from the other side
Abstract: We give a complexity theoretic classification of homomorphism problems for graphs and, more generally, relational structures obtained by restricting the left hand side structure in a homomorphism. For every class C of structures, let HOM(C,&minus;) be the problem of deciding whether a given structure A &isin;C has a homomorphism to a given (arbitrary) structure &szlig;. We prove that, under some complexity theoretic assumption from parameterized complexity theory, HOM(C,&minus;) is in polynomial time if and only if C has bounded tree width modulo homomorphic equivalence.
ID:85
CLASS:1
Title: On minimal-program complexity measures
Abstract: Brief consideration is given to some properties of three measures of complexity based on the length of minimal descriptive programs. Although the measures explicitly deal with finite sequences, the complexity of an infinite sequence can be regarded as a function mapping each positive integer n to the complexity of the initial segment of length n. Some properties of a complexity hierarchy of infinite sequences with respect to one of the measures is considered.
ID:86
CLASS:1
Title: Partial information classes
Abstract: In this survey we present partial information classes, which have been studied under different names and in different contexts in the literature. They are defined in terms of partial information algorithms. Such algorithms take a word tuple as input and yield a small set of possibilities for its characteristic string as output. We define a unified framework for the study of partial information classes and show how previous notions fit into the framework. The framework allows us to study the relationship of a large variety of partial information classes in a uniform way. We survey how partial information classes are related to other complexity theoretic notions like advice classes, lowness, bi-immunity, NP-completeness, and decidability.
ID:87
CLASS:1
Title: ACM SIGACT news distributed computing column 9
Abstract: The Distributed Computing Column covers the theory of systems that are composed of a number of interacting computing elements. These include problems of communication and networking, databases, distributed shared memory, multiprocessor architectures, operating systems, verification, internet, and the web.This issue consists of the paper "Incentives and Internet Computation" by Joan Feigenbaum and Scott Shenker. Many thanks to them for contributing to this issue.
ID:88
CLASS:1
Title: Uniform characterizations of complexity classes
Abstract: In the past few years, generalized operators (a. k. a. leaf languages) in the context of polynomial-time machines, and gates computing arbitrary groupoidal functions in the context of Boolean circuits, have raised much interest. We survey results from both areas, point out connections between them, and present relationships to a generalized quantifier concept from finite model theory.
ID:89
CLASS:1
Title: Computing radical expressions for roots of unity
Abstract: We present an improvement of an algorithm given by GAUSS to compute a radical expression for a p-th root of unity. The time complexity of the algorithm is O (p3m6 log p), where m is the largest prime factor of p - 1.On current workstations the given implementation of the algorithm can be used to obtain radical expressions for p-th roots of unity for p up to about 100 in the general case and p up to about 250 if p - 1 has only small prime factors.Moreover, the algorithm allows the use of coarse grained parallel computation in various ways that would allow one to compute bigger examples on a network of workstations.
ID:90
CLASS:1
Title: SIGACT news complexity theory column 52
Abstract: For those who are big fans of the fantastic complexity textbook by Daniel Bovet and Pierluigi Crescenzi [BC93] (and I myself certainly am), there is great news. The authors have made their book available online, free of charge for noncommercial use. It can be found at via Pilu's web site (start at http://www.algoritmica.org/piluc, then click on the "Books" section, and then click on "Introduction to the Theory of Complexity").
ID:91
CLASS:1
Title: Autonomic computing: emerging trends and open problems
Abstract: The increasing heterogeneity, dynamism and interconnectivity in software applications, services and networks led to complex, unmanageable and insecure systems. Coping with such a complexity necessitates to investigate a new paradigm namely Autonomic Computing. Although academic and industry efforts are beginning to proliferate in this research area, there are still a lots of open issues that remain to be solved. This paper proposes a categorization of complexity in I/T systems and presents an overview of autonomic computing research area. The paper also discusses a summary of the major autonomic computing systems that have been already developed both in academia and industry, and finally outlines the underlying research issues and challenges from a practical as well as a theoretical point of view.
ID:92
CLASS:1
Title: Guest Column: NP-complete problems and physical reality
Abstract: Can NP-complete problems be solved efficiently in the physical universe? I survey proposals including soap bubbles, protein folding, quantum computing, quantum advice, quantum adiabatic algorithms, quantum-mechanical nonlinearities, hidden variables, relativistic time dilation, analog computing, Malament-Hogarth spacetimes, quantum gravity, closed timelike curves, and "anthropic computing." The section on soap bubbles even includes some "experimental" results. While I do not believe that any of the proposals will let us solve NP-complete problems efficiently, I argue that by studying them, we can learn something not only about computation but also about physics.
ID:93
CLASS:1
Title: Efficient computation of the topology of level sets
Abstract: This paper introduces two efficient algorithms that compute the Contour Tree of a 3D scalar field F and its augmented version with the Betti numbers of each isosurface. The Contour Tree is a fundamental data structure in scientific visualization that is used to preprocess the domain mesh to allow optimal computation of isosurfaces with minimal overhead storage. The Contour Tree can also be used to build user interfaces reporting the complete topological characterization of a scalar field, as shown in Figure 1.The first part of the paper presents a new scheme that augments the Contour Tree with the Betti numbers of each isocontour in linear time. We show how to extend the scheme introduced in [3] with the Betti number computation without increasing its complexity. Thus, we improve on the time complexity from our previous approach [10] from O(m log m) to O(n log n + m), where m is the number of tetrahedra and n is the number of vertices in the domain of F.The second part of the paper introduces a new divide-and-conquer algorithm that computes the Augmented Contour Tree with improved efficiency. The central part of the scheme computes the output Contour Tree by merging two intermediate Contour Trees and is independent of the interpolant. In this way we confine any knowledge regarding a specific interpolant to an oracle that computes the tree for a single cell. We have implemented this oracle for the trilinear interpolant and plan to replace it with higher order interpolants when needed. The complexity of the scheme is O(n + t log n), where t is the number of critical points of F. For the first time we can compute the Contour Tree in linear time in many practical cases when t = O(n1 - &epsilon;).Lastly, we report the running times for a parallel implementation of our algorithm, showing good scalability with the number of processors.
ID:94
CLASS:1
Title: SIGACT News Complexity Theory Column 12
Abstract: Please don't be shy about sending even vague pointers to people who may have complete or partial resolutions of the problems mentioned in any of the open questions columns that have appeared as earlier complexity theory columns. Though I don't give a dollar for each solved problem (what do you expect on a theoretician's salary!), I'll be delighted to report in this space on resolutions (such as the thrilling resolution by Cai and Sivakumar that was mentioned in the previous column). Without further delay, here is this issue's guest column, by Mihir Bellare.
ID:95
CLASS:1
Title: Hard-core theorems for complexity classes
Abstract: Nancy Lynch proved that if a decision problem A is not solvable in polynomial time, then there exists an infinite recursive subset X of its domain on which the decision is almost everywhere complex. In this paper, general theorems of this kind that can be applied to several well-known automata-based complexity classes, including a common class of randomized algorithms, are proved.
ID:96
CLASS:1
Title: Partitioning and ordering large radiosity computations
Abstract: We describe a system that computes radiosity solutions for polygonal environments much larger than can be stored in main memory. The solution is stored in and retrieved from a database as the computation proceeds. Our system is based on two ideas: the use of visibility oracles to find source and blocker surfaces potentially visible to a receiving surface; and the use of hierarchical techniques to represent interactions between large surfaces efficiently, and to represent the computed radiosity solution compactly. Visibility information allows the environment to be partitioned into subsets, each containing all the information necessary to transfer light to a cluster of receiving polygons. Since the largest subset needed for any particular cluster is much smaller than the total size of the environment, these subset computations can be performed in much less memory than can classical or hierarchical radiosity. The computation is then ordered for further efficiency. Careful ordering of energy transfers minimizes the number of database reads and writes. We report results from large solutions of unfurnished and furnished buildings, and show that our implementation's observed running time scales nearly linearly with both local and global model complexity.
ID:97
CLASS:1
Title: Polynomial time algorithm for computing the top Betti numbers of semi-algebraic sets defined by quadratic inequalities
Abstract: For any fixed l &gt; 0, we present an algorithm which takes as input a semi-algebraic set, S, defined by P1 &#8805; 0,...,Ps &#8805; 0, where each Pi &#8712; R[X1,...,Xk] has degree &#8804; 2, and computes the top l Betti numbers of S, bk-1(S), ..., bk-l(S), in polynomial time. The complexity of the algorithm, stated more precisely, is &#931;i=0l+2 (si k2O((l,s)). For fixed l, the complexity of the algorithm can be expressed as sl+2 k2O(l), which is polynomial in the input parameters s and k. To our knowledge this is the first polynomial time algorithm for computing non-trivial topological invariants of semi-algebraic sets in Rk defined by polynomial inequalities, where the number of inequalities is not fixed and the polynomials are allowed to have degree greater than one. For fixed s, we obtain by letting l = k, an algorithm for computing all the Betti numbers of S whose complexity is k2O(s)
ID:98
CLASS:1
Title: The complexity of mining maximal frequent itemsets and maximal frequent patterns
Abstract: Mining maximal frequent itemsets is one of the most fundamental problems in data mining. In this paper we study the complexity-theoretic aspects of maximal frequent itemset mining, from the perspective of counting the number of solutions. We present the first formal proof that the problem of counting the number of distinct maximal frequent itemsets in a database of transactions, given an arbitrary support threshold, is #P-complete, thereby providing strong theoretical evidence that the problem of mining maximal frequent itemsets is NP-hard. This result is of particular interest since the associated decision problem of checking the existence of a maximal frequent itemset is in P.We also extend our complexity analysis to other similar data mining problems dealing with complex data structures, such as sequences, trees, and graphs, which have attracted intensive research interests in recent years. Normally, in these problems a partial order among frequent patterns can be defined in such a way as to preserve the downward closure property, with maximal frequent patterns being those without any successor with respect to this partial order. We investigate several variants of these mining problems in which the patterns of interest are subsequences, subtrees, or subgraphs, and show that the associated problems of counting the number of maximal frequent patterns are all either #P-complete or #P-hard.
ID:99
CLASS:1
Title: Lower bounds for algebraic computation trees
Abstract: A topological method is given for obtaining lower bounds for the height of algebraic computation trees, and algebraic decision trees. Using this method we are able to generalize, and present in a uniform and easy way, almost all the known nonlinear lower bounds for algebraic computations. Applying the method to decision trees we extend all the apparently known lower bounds for linear decision trees to bounded degree algebraic decision trees, thus answering the open questions raised by Steele and Yao [20]. We also show how this new method can be used to establish lower bounds on the complexity of constructions with ruler and compass in plane Euclidean geometry.
ID:100
CLASS:2
Title: Heterogeneous distributed database systems for production use
Abstract: It is increasingly important for organizations to achieve additional coordination of diverse computerized operations. To do so, it is necessary to have database systems that can operate over a distributed network and can encompass a heterogeneous mix of computers, operating systems, communications links, and local database management systems. This paper outlines approaches to various aspects of heterogeneous distributed data management and describes the characteristics and architectures of seven existing heterogeneous distributed database systems developed for production use. The objective is a survey of the state of the art in systems targeted for production environments as opposed to research prototypes.
ID:101
CLASS:2
Title: Federated database systems for managing distributed, heterogeneous, and autonomous databases
Abstract: A federated database system (FDBS) is a collection of cooperating database systems that are autonomous and possibly heterogeneous. In this paper, we define a reference architecture for distributed database management systems from system and schema viewpoints and show how various FDBS architectures can be developed. We then define a methodology for developing one of the popular architectures of an FDBS. Finally, we discuss critical issues related to developing and operating an FDBS.
ID:102
CLASS:2
Title: Highly available systems for database applications
Abstract: As users entrust more and more of their applications to computersystems, the need for systems that are continuously operational (24hours per day) has become even greater. This paper presents asurvey and analysis of representative architectures and techniquesthat have been developed for constructing highly available systemsfor database applications. It then proposes a design of adistributed software subsystem that can serve as a unifiedframework for constructing database application systems that meetvarious requirements for high availability.
ID:103
CLASS:2
Title: Virtual memory management for database systems
Abstract: Over the last several years, a number of hardware and software systems have been developed which map entire files directly into the virtual memory address spaces used by programs. Since all file contents are directly addressable, there is no need for a programmer to issue explicit file system actions, such as Read or Write. In addition, all of the buffer management problems are eliminated, since programmers do not have to squeeze pieces of large files into small virtual spaces. Although these advantages are tempting, we find that database systems have gone their own way. In this paper, we will look at two particular approaches to database system design, and see how (and why) they interface to file systems as they do. We will then look at the potential advantages and implications of working more closely with virtual memory management, and describe some of the functions and constraints that would have to be supported by a generalized page manager.
ID:104
CLASS:2
Title: An on-line simulator and database system for management of a commercial fish farm
Abstract: We have developed an on-line system to aid in the management of enclosed, multi-tank fish farms with water filtration, heating and recirculation, where fish are grown from fingerling through to market size (about 250 gr.). The manager has a high degree of control over tank loadings and fish rearing conditions, but he must maintain high production rates within biological constraints on water quality and fish density and within system constraints imposed by the physical plant. A multi-level interactive simulator permits exploration of management strategies, with nested levels of constraints. A database is used for storage of both real and simulated data on management interventions and their consequences on fish growth and water quality. Interaction between the simulators and the database aids model calibration and validation and permits storage of promising management strategies.
ID:105
CLASS:2
Title: Aspects of a trigger subsystem in an integrated database system
Abstract: This paper considers the specifications and design of a trigger subsystem in a database management system. The use of triggers as extended assertions and as a means to materialize virtual data objects are discussed. The functional requirements of a trigger subsystem and different implementation issues are studied. We also examine the relationships between a trigger subsystem and the rest of the database system, in particular the authorization and locking subsystems.
ID:106
CLASS:2
Title: Consistency and correctness of duplicate database systems
Abstract: Solutions to the duplicate database update problem are considered, and a formal validation technique using the theory of L systems is developed and applied to the problem. The paper shows some particular solutions but is primarily concerned with general properties of the problem, convenient representational techniques, and formal proof procedures which are general enough to apply to this and to a number of other problems in parallel processing and synchronization.
ID:107
CLASS:2
Title: Design and implementation of a portable database system for small computers
Abstract: This paper describes the design and implementation of PEDMS. Part B describes design objectives, resulting system architecture and interfaces to user and database administrator. Part C concentrates on the implementation of PEDMS, discussing the difficulties imposed by the environment, and the ways of circumventing these difficulties.
ID:108
CLASS:2
Title: SPSL/SPSA a minicomputer database system for structured systems analysis and design
Abstract: The field of computer-aided systems analysis and design is still very young and therefore, as yet, there are few automated aids in existence. Progress, however, is now being made in computer-aided techniques to be used in the development of information systems. Most of the limited number of automated aids available require large computer systems for their operations. This paper describes a computer-aided systems analysis and documentation system which has been implemented on a PDP-11/40 computer running under the UNIX operating system. In particular, the database design and implementation considerations with in a small minicomputer environment of the SPSL/SPSA system are presented. Also the experiences gained in the use of this system are reported. Finally, several extensions to the existing SPSL/SPSA system currently under development are described.
ID:109
CLASS:2
Title: The structure and operation of a relational database system in a cell-oriented integrated circuit design system
Abstract: An important use for a database management system is in the storage and handling of information for engineering design, particularly integrated circuit design. However, most discussions on this topic have concentrated on the layout of shapes necessary to form the various circuit elements, or connections between user-defined cells. Equally important, but often disregarded, is the necessity to support other design tools in addition to graphics for circuit layout. These include simulators and automatic layout programs that take a description of a circuit at one level and convert it to a lower level. In addition, if cells are part of a library defined and maintained by others, operations must be included to handle the maintenance of generations or versions of a cell design. These aspects of a database management system for engineering design are discussed in light of the tools being developed at the University of Utah and an extended version of System R, developed at the IBM San Jose Research Laboratory. The Utah approach emphasizes the use of previously designed and tested cells, with interconnects at fixed locations, placed on a grid. Because it is unlikely that the designers of circuits designed all (or any) of the cells used in their circuits, special database management operations are necessary to assure that a consistent, working circuit results.
ID:110
CLASS:2
Title: DBMiner: a system for data mining in relational databases and data warehouses
Abstract: A data mining system, DBMiner, has been developed for interactive mining of multiple-level knowledge in large relational databases and data warehouses. The system implements a wide spectrum of data mining functions, including characterization, comparison, association, classification, prediction, and clustering. By incorporating several interesting data mining techniques, including OLAP and attribute-oriented induction, statistical analysis, progressive deepening for mining multiple-level knowledge, and meta-rule guided mining, the system provides a user-friendly, interactive data mining environment with good performance.
ID:111
CLASS:2
Title: Query optimization in a memory-resident domain relational calculus database system
Abstract: We present techniques for optimizing queries in memory-resident database systems. Optimization techniques in memory-resident database systems differ significantly from those in conventional disk-resident database systems. In this paper we address the following aspects of query optimization in such systems and present specific solutions for them: (1) a new approach to developing a CPU-intensive cost model; (2) new optimization strategies for main-memory query processing; (3) new insight into join algorithms and access structures that take advantage of memory residency of data; and (4) the effect of the operating system's scheduling algorithm on the memory-residency assumption. We present an interesting result that a major cost of processing queries in memory-resident database systems is incurred by evaluation of predicates. We discuss optimization techniques using the Office-by-Example (OBE) that has been under development at IBM Research. We also present the results of performance measurements, which prove to be excellent in the current state of the art. Despite recent work on memory-resident database systems, query optimization aspects in these systems have not been well studied. We believe this paper opens the issues of query optimization in memory-resident database systems and presents practical solutions to them.
ID:112
CLASS:2
Title: Semantic integrity support in SQL:1999 and commercial (object-)relational database management systems
Abstract: The correctness of the data managed by database systems is vital to any application that utilizes data for business, research, and decision-making purposes. To guard databases against erroneous data not reflecting real-world data or business rules, semantic integrity constraints can be specified during database design. Current commercial database management systems provide various means to implement mechanisms to enforce semantic integrity constraints at database run-time.In this paper, we give an overview of the semantic integrity support in the most recent SQL-standard SQL:1999, and we show to what extent the different concepts and language constructs proposed in this standard can be found in major commercial (object-)relational database management systems. In addition, we discuss general design guidelines that point out how the semantic integrity features provided by these systems should be utilized in order to implement an effective integrity enforcing subsystem for a database.
ID:113
CLASS:2
Title: An experimental object-based sharing system for networked databases
Abstract: An approach and mechanism for the transparent sharing of objects in an environment of interconnected (networked), autonomous database systems is presented. An experimental prototype system has been designed and implemented, and an analysis of its performance conducted. Previous approaches to sharing in this environment typically rely on the use of a global, integrated conceptual database schema; users and applications must pose queries at this new level of abstraction to access remote information. By contrast, our approach provides a mechanism that allows users to import remote objects directly into their local database transparently; access to remote objects is virtually the same as access to local objects. The experimental prototype system that has been designed and implemented is based on the Iris and Omega object-based database management systems; this system supports the sharing of data and meta-data objects (information units) as well as units of behavior. The results of experiments conducted to evaluate the performance of our mechanism demonstrate the feasibility of database transparent object sharing in a federated environment, and provide insight into the performance overhead and tradeoffs involved.
ID:114
CLASS:2
Title: Data placement in shared-nothing parallel database systems
Abstract: Data placement in shared-nothing database systems has been studied extensively in the past and various placement algorithms have been proposed. However, there is no consensus on the most efficient data placement algorithm and placement is still performed manually by a database administrator with periodic reorganization to correct mistakes. This paper presents the first comprehensive simulation study of data placement issues in a shared-nothing system. The results show that current hardware technology trends have significantly changed the performance tradeoffs considered in past studies. A simplistic data placement strategy based on the new results is developed and shown to perform well for a variety of workloads.
ID:115
CLASS:2
Title: Analysis of locking behavior in three real database systems
Abstract: Concurrency control is essential to the correct functioning of a database due to the need for correct, reproducible results. For this reason, and because concurrency control is a well-formulated problem, there has developed an enormous body of literature studying the performance of concurrency control algorithms. Most of this literature uses either analytic modeling or random number-driven simulation, and explicitly or implicitly makes certain assumptions about the behavior of transactions and the patterns by which they set and unset locks. Because of the difficulty of collecting suitable measurements, there have been only a few studies which use trace-driven simulation, and still less study directed toward the characterization of concurrency control behavior of real workloads. In this paper, we present a study of three database workloads, all taken from IBM DB2 relational database systems running commercial applications in a production environment. This study considers topics such as frequency of locking and unlocking, deadlock and blocking, duration of locks, types of locks, correlations between applications of lock types, two-phase versus non-two-phase locking, when locks are held and released, etc. In each case, we evaluate the behavior of the workload relative to the assumptions commonly made in the research literature and discuss the extent to which those assumptions may or may not lead to erroneous conclusions.
ID:116
CLASS:2
Title: Integrating symbolic images into a multimedia database system using classification and abstraction approaches
Abstract: Symbolic images are composed of a finite set of symbols that have a semantic meaning. Examples of symbolic images include maps (where the semantic meaning of the symbols is given in the legend), engineering drawings, and floor plans. Two approaches for supporting queries on symbolic-image databases that are based on image content are studied. The classification approach preprocesses all symbolic images and attaches a semantic classification and an associated certainty factor to each object that it finds in the image. The abstraction approach describes each object in the symbolic image by using a vector consisting of the values of some of its features (e.g., shape, genus, etc.). The approaches differ in the way in which responses to queries are computed. In the classification approach, images are retrieved on the basis of whether or not they contain objects that have the same classification as the objects in the query. On the other hand, in the abstraction approach, retrieval is on the basis of similarity of feature vector values of these objects. Methods of integrating these two approaches into a relational multimedia database management system so that symbolic images can be stored and retrieved based on their content are described. Schema definitions and indices that support query specifications involving spatial as well as contextual constraints are presented. Spatial constraints may be based on both locational information (e.g., distance) and relational information (e.g., north of). Different strategies for image retrieval for a number of typical queries using these approaches are described. Estimated costs are derived for these strategies. Results are reported of a comparative study of the two approaches in terms of image insertion time, storage space, retrieval accuracy, and retrieval time.
ID:117
CLASS:2
Title: A flexible and recoverable client/server database event notification system
Abstract: A software architecture is presented that allows client application programs to interact with a DBMS server in a flexible and powerful way, using either direct, volatile messages, or messages sent via recoverable queues. Normal requests from clients to the server and replies from the server to clients can be transmitted using direct or recoverable messages. In addition, an application event notification mechanism is provided, whereby client applications running anywhere on the network can register for events, and when those events are raised, the clients are notified. A novel parameter passing mechanism allows a set of tuples to be included in an event notification. The event mechanism is particularly useful in an active DBMS, where events can be raised by triggers to signal running application programs.
ID:118
CLASS:2
Title: Secure buffering in firm real-time database systems
Abstract: Many real-time database applications arise in electronic financial services, safety-critical installations and military systems where enforcing is crucial to the success of the enterprise. We investigate here the performance implications, in terms of killed transactions, of guaranteeing multi-level secrecy in a real-time database system supporting applications with firm deadlines. In particular, we focus on the buffer management aspects of this issue.Our main contributions are the following. First, we identify the importance and difficulties of providing secure buffer management in the real-time database environment. Second, we present , a novel buffer management algorithm that provides covert-channel-free security. SABRE employs a fully dynamic one-copy allocation policy for efficient usage of buffer resources. It also incorporates several optimizations for reducing the overall number of killed transactions and for decreasing the unfairness in the distribution of killed transactions across security levels. Third, using a detailed simulation model, the real-time performance of SABRE is evaluated against unsecure conventional and real-time buffer management policies for a variety of security-classified transaction workloads and system configurations. Our experiments show that SABRE provides security with only a modest drop in real-time performance. Finally, we evaluate SABRE's performance when augmented with the GUARD adaptive admission control policy. Our experiments show that this combination provides close to ideal fairness for real-time applications that can tolerate covert-channel bandwidths of up to one bit per second (a limit specified in military standards).
ID:119
CLASS:2
Title: A hierarchical access control model for video database systems
Abstract: Content-based video database access control is becoming very important, but it depends on the progresses of the following related research issues: (a) efficient video analysis for supporting semantic visual concept representation; (b) effective video database indexing structure; (c) the development of suitable video database models; and (d) the development of access control models tailored to the characteristics of video data. In this paper, we propose a novel approach to support multilevel access control in video databases. Our access control technique combines a video database indexing mechanism with a hierarchical organization of visual concepts (i.e., video database indexing units), so that different classes of users can access different video elements or even the same video element with different quality levels according to their permissions. These video elements, which, in our access control mechanism, are used for specifying the authorization objects, can be a semantic cluster, a subcluster, a video scene, a video shot, a video frame, or even a salient object (i.e., region of interest). In the paper, we first introduce our techniques for obtaining these multilevel video access units. We also propose a hierarchical video database indexing technique to support our multilevel video access control mechanism. Then, we present an innovative access control model which is able to support flexible multilevel access control to video elements. Moreover, the application of our multilevel video database modeling, representation, and indexing for MPEG-7 is discussed.
ID:120
CLASS:2
Title: Description logics for semantic query optimization in object-oriented database systems
Abstract: Semantic query optimization uses semantic knowledge (i.e., integrity constraints) to transform a query into an equivalent one that may be answered more efficiently. This article proposes a general method for semantic query optimization in the framework of Object-Oriented Database Systems. The method is effective for a large class of queries, including conjunctive recursive queries expressed with regular path expressions and is based on three ingredients. The first is a Description Logic, ODLRE, providing a type system capable of expressing: class descriptions, queries, views, integrity constraint rules and inference techniques, such as incoherence detection and subsumption computation. The second is a semantic expansion function for queries, which incorporates restrictions logically implied by the query and the schema (classes + rules) in one query. The third is an optimal rewriting method of a query with respect to the schema classes that rewrites a query into an equivalent one, by determining more specialized classes to be accessed and by reducing the number of factors. We implemented the method in a tool providing an ODMG-compliant interface that allows a full interaction with OQL queries, wrapping underlying Description Logic representation and techniques to the user.
ID:121
CLASS:2
Title: Buffer management in relational database systems
Abstract: The hot-set model, characterizing the buffer requirements of relational queries, is presented. This model allows the system to determine the optimal buffer space to be allocated to a query; it can also be used by the query optimizer to derive efficient execution plans accounting for the available buffer space, and by a query scheduler to prevent thrashing. The hot-set model is compared with the working-set model. A simulation study is presented.
ID:122
CLASS:2
Title: Transaction management in the R* distributed database management system
Abstract: This paper deals with the transaction management aspects of the R* distributed database system. It concentrates primarily on the description of the R* commit protocols, Presumed Abort (PA) and Presumed Commit (PC). PA and PC are extensions of the well-known, two-phase (2P) commit protocol. PA is optimized for read-only transactions and a class of multisite update transactions, and PC is optimized for other classes of multisite update transactions. The optimizations result in reduced intersite message traffic and log writes, and, consequently, a better response time. The paper also discusses R*'s approach toward distributed deadlock detection and resolution.
ID:123
CLASS:2
Title: Performance of a two-headed disk system when serving database queries under the scan policy
Abstract: Disk drives with movable two-headed arms are now commercially available. The two heads are separated by a fixed number of cylinders. A major problem for optimizing disk head movement, when answering database requests, is the specification of the optimum number of cylinders separating the two heads. An earlier analytical study assumed a FCFS model and concluded that the optimum separation distance should be equal to 0.44657 of the number of cylinders N of the disk. This paper considers that the SCAN scheduling policy is used in file access, and it applies combinatorial analysis to derive exact formulas for the expected head movement. Furthermore, it is proven that the optimum separation distance is N/2 - 1 (&lceil;N/2 - 1&rceil; and &lfloor;N/2 - 1&rfloor;) if N is even (odd). In addition, a comparison with a single-headed disk system operating under the same scheduling policy shows that if the two heads are optimally spaced, then the mean seek distance is less than one-half of the value obtained with one head. In fact that the SCAN policy is used for many database applications (for example,batching and secondary key retrieval) demonstrates the potential of two-headed disk systems for improving the performance of database systems.
ID:124
CLASS:2
Title: Cactis: a self-adaptive, concurrent implementation of an object-oriented database management system
Abstract: Cactis is an object-oriented, multiuser DBMS developed at the University of Colorado. The system supports functionally-defined data and uses techniques based on attributed graphs to optimize the maintenance of functionally-defined data. The implementation is self-adaptive in that the physical organization and the update algorithms dynamically change in order to reduce disk access. The system is also concurrent. At any given time there are some number of computations that must be performed to bring the database up to date; these computations are scheduled independently and are performed when the expected cost to do so is minimal. The DBMS runs in the Unix/C Sun workstation environment. Cactis is designed to support applications that require rich data modeling capabilities and the ability to specify functionally-defined data, but that also demand good performance. Specifically, Cactis is intended for use in the support of such applications as VLSI and PCB design, and software environments.
ID:125
CLASS:2
Title: The design and implementation of an extendible deductive database system
Abstract: This paper presents the design and implementation of a deductive database system. RDL1, is the production rule language for this system. This language is an extension of logic based languages towards the support of updates and complex domains. Different features of this language are described with its applications. The general architecture of the system is presented highlighting integration with a relational database system. Then, three original features of the system are discussed: extensibility through the definition of end user data types, query optimization techniques, and the extensibility of the control strategy. Future improvements and extensions conclude the paper.
ID:126
CLASS:2
Title: A data model for flexible hypertext database systems
Abstract: Hypertext and other page-oriented databases cannot be schematized in the same manner as record-oriented databases. As a result, most hypertext databases implicitly employ a data model based on a simple, unrestricted graph. This paper presents a hypergraph model for maintaining page-oriented databases in such a way that some of the functionality traditionally provided by database schemes can be available to hypertext databases. In particular, the model formalizes identification of commonality in the structure, set-at-a-time database access, and definition of user-specific views. An efficient implementation of the model is also discussed.
ID:127
CLASS:2
Title: Set query optimization in distributed database systems
Abstract: This paper addresses the problem of optimizing queries that involve set operations (set queries) in a distributed relational database system. A particular emphasis is put on the optimization of such queries in horizontally partitioned database systems. A mathematical programming model of the set query problem is developed and its NP-completeness is proved. Solution procedures are proposed and computational results presented. One of the main results of the computational experiments is that, for many queries, the solution procedures are not sensitive to errors in estimating the size of results of set operations.
ID:128
CLASS:2
Title: The design of a relational database system with abstract data types for domains
Abstract: An extension to the relational model is described in which domains can he arbitrarily defined as abstract data types. Operations on these data types include primitive operations, aggregates, and transformations. It is shown that these operations make the query language complete in the sense of Chandra and Harel. The system has been designed in such a way that new data types and their operations can be defined with a minimal amount of interaction with the database management system.
ID:129
CLASS:2
Title: A problem-oriented inferential database system
Abstract: Recently developed inferential database systems face some common problems: a very fast growth of search space and difficulties in recognizing inference termination (especially for recursive axioms). These shortcomings stem mainly from the fact that the inference process is usually separated from database operations. A problem-oriented inferential system i8 described which refers to the database prior to query (or subquery) processing, so that the inference from the very beginning is directed by data relevant to the query. A multiprocessor implementation of the system is presented based on a computer network conforming to database relations and axioms. The system provides an efficient indication of query termination, and is complete in the sense that it produces all correct answers to a query in a finite time.
ID:130
CLASS:2
Title: Join processing in database systems with large main memories
Abstract: We study algorithms for computing the equijoin of two relations in a system with a standard architecture hut with large amounts of main memory. Our algorithms are especially efficient when the main memory available is a significant fraction of the size of one of the relations to he joined; but they can be applied whenever there is memory equal to approximately the square root of the size of one relation. We present a new algorithm which is a hybrid of two hash-based algorithms and which dominates the other algorithms we present, including sort-merge. Even in a virtual memory environment, the hybrid algorithm dominates all the others we study.Finally, we describe how three popular tools to increase the efficiency of joins, namely filters, Babb arrays, and semijoins, can he grafted onto any of our algorithms.
ID:131
CLASS:2
Title: Robust transaction routing in distributed database systems
Abstract: In this paper, we examine the issue of robust transaction routing in distributed database systems. A class of dynamic routing strategies which use estimated response times to make routing decisions is studied in details. Since response time estimation and decision making depend on the assumed transaction model and parameters, it is important to examine the robustness or sensitivity to the inaccuracy in the assumptions and parameter values. Through simulations, we find that the dynamic routing strategy based strictly on response time is too aggressive in sharing loads and makes too many non-preferred system routings. It is robust with respect to change in the number of database calls per transaction, but is relatively sensitive to the distribution of database calls. Two refinements are proposed which improve system performance as well as robustness of routing decisions.
ID:132
CLASS:2
Title: Integrating an object-oriented programming system with a database system
Abstract: There are two major issues to address to achieve integration of an object-oriented programming system with a database system. One is the language issue: an object-oriented programming language must be augmented with semantic data modeling concepts to provide a robust set of data modeling concepts to allow modeling of entities for important real-world applications. Another is the computational-model issue: application programmers should be able to access and manipulate objects as though the objects are in an infinite virtual memory; in other words, they should not have to be aware of the existence of a database system in their computations with the data structures the programming language allows. This paper discusses these issues and presents the solutions which we have incorporated into the ORION object-oriented database system at MCC.
ID:133
CLASS:2
Title: Statistical profile estimation in database systems
Abstract: A statistical profile summarizes the instances of a database. It describes aspects such as the number of tuples, the number of values, the distribution of values, the correlation between value sets, and the distribution of tuples among secondary storage units. Estimation of database profiles is critical in the problems of query optimization, physical database design, and database performance prediction. This paper describes a model of a database of profile, relates this model to estimating the cost of database operations, and surveys methods of estimating profiles. The operators and objects in the model include build profile, estimate profile, and update profile. The estimate operator is classified by the relational algebra operator (select, project, join), the property to be estimated (cardinality, distribution of values, and other parameters), and the underlying method (parametric, nonparametric, and ad-hoc). The accuracy, overhead, and assumptions of methods are discussed in detail. Relevant research in both the database and the statistics disciplines is incorporated in the detailed discussion.
ID:134
CLASS:2
Title: Fibonacci: a programming language for object databases
Abstract: Fibonacci is an object-oriented database programming language characterized by static and strong typing, and by new mechanisms for modeling databases in terms of objects with roles, classes, and associations. A brief introduction to the language is provided to present those features, which are particularly suited to modeling complex databases. Examples of the use of Fibonacci are given with reference to the prototype implementation of the language.
ID:135
CLASS:2
Title: An introduction to spatial database systems
Abstract: We propose a definition of a spatial database system as a database system that offers spatial data types in its data model and query language, and supports spatial data types in its implementation, providing at least spatial indexing and spatial join methods. Spatial database systems offer the underlying database technology for geographic information systems and other applications. We survey data modeling, querying, data structures and algorithms, and system architecture for such systems. The emphasis is on describing known technology in a coherent manner, rather than listing open problems.
ID:136
CLASS:2
Title: The aditi deductive database system
Abstract: Deductive databases generalize relational databases by providing support for recursive views and non-atomic data. Aditi is a deductive system based on the client-server model; it is inherently multi-user and capable of exploiting parallelism on shared-memory multiprocessors. The back-end uses relational technology for efficiency in the management of disk-based data and uses optimization algorithms especially developed for the bottom-up evaluation of logical queries involving recursion. The front-end interacts with the user in a logical language that has more expressive power than relational query languages. We present the structure of Aditi, discuss its components in some detail, and present performance figures.
ID:137
CLASS:2
Title: DECLARE and SDS: early efforts to commercialize deductive database technology
Abstract: The Smart Data System (SDS) and its declarative query language, Declarative Reasoning, represent the first large-scale effort to commercialize deductive database technology. SDS offers the functionality of deductive reasoning in a distributed, heterogeneous database environment. In this article we discuss several interesting aspects of the query compilation and optimization process. The emphasis is on the query execution plan data structure and its transformations by the optimizing rule compiler. Through detailed case studies we demonstrate that efficient and very compact runtime code can be generated. We also discuss our experiences gained from a large pilot application (the MVV-expert) and report on several issues of practical interest in engineering such a complex system, including the migration from Lisp to C. We argue that heuristic knowledge and control should be made an integral part of deductive databases.
ID:138
CLASS:2
Title: The CORAL deductive system
Abstract: CORAL is a deductive system that supports a rich declarative language, and an interface to C++, which allows for a combination of declarative and imperative programming. A CORAL declarative program can be organized as a collection of interacting modules. CORAL supports a wide range of evaluation strategies, and automatically chooses an efficient strategy for each module in the program. Users can guide query optimization by selecting from a wide range of control choices. The CORAL system provides imperative constructs to update, insert, and delete facts. Users can program in a combination of declarative CORAL and C++ extended with CORAL primitives. A high degree of extensibility is provided by allowing C++ programmers to use the class structure of C++ to enhance the CORAL implementation. CORAL provides support for main-memory data and, using the EXODUS storage manager, disk-resident data. We present a comprehensive view of the system from broad design goals, the language, and the architecture, to language interfaces and implementation details.
ID:139
CLASS:2
Title: The glue-nail deductive database system: design, implementation, and evaluation
Abstract: We describe the design and implementation of the Glue-Nail deductive database system. Nail is a purely declarative query language; Glue is a procedural language used for non-query activities. The two languages combined are sufficient to write a complete application. Nail and Glue code are both compiled into the target language IGlue. The Nail compiler uses variants of the magic sets algorithm and supports well-founded models. The Glue compiler's static optimizer uses peephole techniques and data flow analysis to improve code. The IGlue interpreter features a run-time adaptive optimizer that reoptimizes queries and automatically selects indexes. We also describe the Glue-Nail benchmark suite, a set of applications developed to evaluate the Glue-Nail language and to measure the performance of the system.
ID:140
CLASS:2
Title: Federated databases and systems: part II --- a tutorial on their resource consolidation
Abstract: The issues and solutions for the interoperability of a class of heterogeneous databases and their database systems are expounded in two parts. Part I presented the data-sharing issues in federated databases and systems (Hsiao, 1992). The present article explores resource-consolidation issues. Interoperability in this context refers to data sharing among heterogeneous databases, and to resource consolidation of computer hardware, system software, and support personnel. Resource consolidation requires the presence of a database system architecture which supports the heterogeneous system software, thereby eliminating the need for various computer hardware and support personnel. The class of heterogeneous databases and database systems expounded herein is termed federated, meaning that they are joined in order to meet certain organizational requirements and because they require their respective application specificities, integrity constraints, and security requirements to be upheld. Federated databases and systems are new. While there are no technological solutions, there has been considerable research towards their development. This tutorial is aimed at exposing the need for such solutions. A taxonomy is introduced in our review of existing research undertakings and exploratory developments. With this taxonomy, we contrast and compare various approaches to federating databases and systems.
ID:141
CLASS:2
Title: A toolkit for the incremental implementation of heterogeneous database management systems
Abstract: The integration of heterogeneous database environments is a difficult and complex task. The A la carte Framework addresses this complexity by providing a reusable and extensible architecture in which a set of heterogeneous database management systems can be integrated. The goal is to support incremental integration of existing database facilities into heterogeneous, interoperative, distributed systems. The Framework addresses the three main issues in heterogeneous systems integration. First, it identifies the problems in integrating heterogeneous systems. Second, it identifies the key interfaces and parameters required for autonomous systems to interoperate correctly. Third, it demonstrates an approach to integrating these interfaces in an extensible and incremental way. The A la carte Framework provides a set of reusable, integrating components which integrate the major functional domains, such as transaction management, that could or should be integrated in heterogeneous systems. It also provides a mechanism for capturing key characteristics of the components and constraints which describe how the components can be mixed and interchanged, thereby helping to reduce the complexity of the integration process. Using this framework, we have implemented an experimental, heterogeneous configuration as part of the object management work in the software engineering research consortium, Arcadia.
ID:142
CLASS:2
Title: Making smalltalk a database system
Abstract: To overcome limitations in the modeling power of existing database systems and provide a better tool for database application programming, Servio Logic Corporation is developing a computer system to support a set-theoretic data model in an object-oriented programming environment We recount the problems with existing models and database systems We then show how features of Smalltalk, such such as operational semantics, its type hierarchy, entity identity and the merging of programming and data language, solve many of those problems Nest we consider what Smalltalk lacks as a database system secondary storage management, a declarative semantics, concurrency, past states To address these shortcomings, we needed a formal data model We introduce the GemStone data model, and show how it helps to define path expressions, a declarative semantics and object history in the OPAL language We summarize similar approaches, and give a brief overview of the GemStone system implementation
ID:143
CLASS:2
Title: Simplifying distributed database systems design by using a broadcast network
Abstract: Atomic broadcast and failure detection are powerful primitives for distributed database systems In the distributed database system LAMBDA, they are provided as network primitives In this paper, we show how atomic broadcast and failure detection simplify transaction commitment, concurrency control, and crash recovery Specifically, we give a simple two-phase non-blocking commit protocol, whereas three phases are required in a point-to-point network We also give a simplified read-one/write-all update algorithm for replicated data and an easily implemented log-based recovery algorithm providing uninterrupted transaction processingThe benefits of performing the atomic broadcast and failure detection at the network level are also discussed Performing these functions at the network level not only simplifies database protocols but also better utilizes the broadcast network fewer messages are transmitted Comparisons between LAMBDA and existing distributed database systems are also made
ID:144
CLASS:2
Title: A methodology for database system performance evaluation
Abstract: This paper presents a methodology for evaluating the performance of database management systems and database machines in a multiuser environment. Three main factors that affect transaction throughput in a multiuser environment are identified: multiprogramming level, degree of data sharing among simultaneously executing transactions, and transaction mix. We demonstrate that only four basic query types are needed to construct a benchmark that will evaluate the performance of a system under a wide variety of workloads. Finally, we present the results of applying our techniques to the Britton-Lee IDM 500 database machine
ID:145
CLASS:2
Title: Reliable scheduling in a TMR database system
Abstract: A Triple Modular Redundant (TMR) system achieves high reliability by replicating data and all processing at three independent nodes. When TMR is used for database processing all nonfaulty computers must execute the same sequence of transactions, and this is ensured by a collection of processes known as schedulers. In this paper we study the implementation of efficient schedulers through analysis of various enhancements such as null transactions and message batching. The schedulers have been implemented in an experimental TMR system and the evaluation results are presented here.
ID:146
CLASS:2
Title: On type systems for object-oriented database programming languages
Abstract: The concept of an object-oriented database programming language (OODBPL) is appealing because it has the potential of combining the advantages of object orientation and database programming to yield a powerful and universal programming language design. A uniform and consistent combination of object orientation and database programming, however, is not straightforward. Since one of the main components of an object-oriented programming language is its type system, one of the first problems that arises during an OODBPL design is related to the development of a uniform, consistent, and theoretically sound type system that is sufficiently expressive to satisfy the combined needs of object orientation and database programming.The purpose of this article is to answer two questions: "What are the requirements that a modern type system for an object-oriented database programming language should satisfy?" and "Are there any type systems developed to-date that satisfy these requirements?". In order to answer the first question, we compile the set of requirements that an OODBPL type system should satisfy. We then use this set of requirements to evaluate more than 30 existing type systems. The result of this extensive analysis shows that while each of the requirements is satisfied by at least one type system, no type system satisfies all of them. It also enables identification of the mechanisms that lie behind the strengths and weaknesses of the current type systems.
ID:147
CLASS:2
Title: Queueing network models for concurrent transaction processing in a database system
Abstract: This paper presents two queueing network models which correspond to different implementations of the lock management algorithm for concurrent transaction processing in a database system. These models are developed to investigate the effects of varying the granularity of locks and the degree of multiprogramming on the performance of a database system. A numerical example is presented for a set of apparently realistic parameters and its results are discussed. In addition to other conclusions, these results also confirm the result of Ries and Stonebraker, using a simulation model [9], that a relatively coarse granularity is sufficient to allow enough parallelism for efficient resource utilization. In contrast with simulation models, the queueing network models presented in this paper allow us to examine more closely the cause-effect relationships of concurrent transaction processing in a database system at less cost.
ID:148
CLASS:2
Title: Coordinating backup/recovery and data consistency between database and file systems
Abstract: Managing a combined store consisting of database data and file data in a robust and consistent manner is a challenge for database systems and content management systems. In such a hybrid system, images, videos, engineering drawings, etc. are stored as files on a file server while meta-data referencing/indexing such files is created and stored in a relational database to take advantage of efficient search. In this paper we describe solutions for two potentially problematic aspects of such a data management system: backup/recovery and data consistency. We present algorithms for performing backup and recovery of the DBMS data in a coordinated fashion with the files on the file servers. Our algorithms for coordinated backup and recovery have been implemented in the IBM DB2/DataLinks product [1]. We also propose an efficient solution to the problem of maintaining consistency between the content of a file and the associated meta-data stored in the DBMS from a reader's point of view without holding long duration locks on meta-data tables. In the model, an object is directly accessed and edited in-place through normal file system APIs using a reference obtained via an SQL Query on the database. To relate file modifications to meta-data updates, the user issues an update through the DBMS, and commits both file and meta-data updates together.
ID:149
CLASS:2
Title: Design of partially replicated distributed database systems: an integrated methodology
Abstract: The objective of this research is to develop and integrate tools for the design of partially replicated distributed database systems. Many existing tools are inappropriate for designing large-scale distributed databases due to their large computational requirements. Our goal is to develop tools that solve the design problems reasonably quickly, typically by using heuristic algorithms that provide approximate or near-optimal solutions.In developing this design methodology, we assume that information regarding the types of user requests and their rates of arrival into the system is known a priori. The methodology assumes a general model for transaction execution. In this paper we discuss three aspects of the design methodology: the data allocation problem, the use of a static load-balancing scheme in coordination with the allocation scheme, and the design evaluation and review step. Our methodology employs iterative design techniques using performance evaluation as a means to iterate.
ID:150
CLASS:2
Title: Modeling the storage architectures of commercial database systems
Abstract: Modeling the storage structures of a DBMS is a prerequisite to understanding and optimizing database performance. Previously, such modeling was very difficult because the fundamental role of conceptual-to-internal mappings in DBMS implementations went unrecognized.In this paper we present a model of physical databases, called the transformation model, that makes conceptual-to-internal mappings explicit. By exposing such mappings, we show that it is possible to model the storage architectures (i.e., the storage structures and mappings) of many commercial DBMSs in a precise, systematic, and comprehendible way. Models of the INQUIRE, ADABAS, and SYSTEM 2000 storage architectures are presented as examples of the model's utility.We believe the transformation model helps bridge the gap between physical database theory and practice. It also reveals the possibility of a technology to automate the development of physical database software.
ID:151
CLASS:2
Title: A functional approach to integrating database and expert systems
Abstract: A new system architecture shares certain characteristics with database systems, expert systems, functional programming languages, and spreadsheet systems, but is very different from any of these.
ID:152
CLASS:2
Title: Applications of Byzantine agreement in database systems
Abstract: In this paper we study when and how B Byzantine agreement protocol can he used in general-purpose database management systems. We present an overview of the failure model used for Byzantine agreement, and of the protocol itself. We then present correctness criteria for database processing in this failure environment and discuss strategies for satisfying them. In doing this, we present new failure models for input/output nodes and study ways to distribute input transactions to processing nodes under these models. Finally, we investigate applications of Byzantine agreement protocols in the more common failure environment where processors are assumed to halt after a failure.
ID:153
CLASS:2
Title: Optimization of join operations in horizontally partitioned database systems
Abstract: This paper analyzes the problem of joining two horizontally partitioned relations in a distributed database system. Two types of semijoin strategies are introduced, local and remote. Local semijoins are performed at the site of the restricted relation (or fragment), and remote semijoins can be performed at an arbitrary site. A mathematical model of a semijoin strategy for the case of remote semijoins is developed, and lower bounding and heuristic procedures are proposed. The results of computational experiments are reported. The experiments include an analysis of the heuristics' performance relative to the lower bounds, sensitivity analysis, and error analysis. These results reveal a good performance of the heuristic procedures, and demonstrate the benefit of using semijoin operations to reduce the size of fragments prior to their transmission. The algorithms for the case of remote semijoins were found to be superior to the algorithms for the case of local semijoins. In addition, we found that the estimation accuracy of the selectivity factors has a significant effect on the incurred communication cost.
ID:154
CLASS:2
Title: A natural language interface to a multiple databased office information system
Abstract: As an organization grows through mergers and acquisitions and through the integration of independently designed information sub-systems, it could find itself faced with a collection of databases implemented on heterogeneous database management systems. This paper presents a system to provide integrated access to multiple, independently designed databases through a portable natural language front-end and a multiple database access system back-end.The overall system is divided into a natural language interface (NLI) front-end and a multiple database access system (MDAS) back-end. The MDAS integrates the various conceptual views into a single global external schema. In a prototype of the overall system, the NLI will accept a natural language query and produce a relational calculus query expression over a global external schema. The MDAS will construct the response to the query by accessing the separate constituent databases through the local database management systems (DBMS's).
ID:155
CLASS:2
Title: Requirements for XML document database systems
Abstract: The shift from SGML to XML has created new demands for managing structured documents. Many XML documents will be transient representations for the purpose of data exchange between different types of applications, but there will also be a need for effective means to manage persistent XML data as a database. In this paper we explore requirements for an XML database management system. The purpose of the paper is not to suggest a single type of system covering all necessary features. Instead the purpose is to initiate discussion of the requirements arising from document collections, to offer a context in which to evaluate current and future solutions, and to encourage the development of proper models and systems for XML database management. Our discussion addresses issues arising from data modelling, data definition, and data manipulation.
ID:156
CLASS:2
Title: "Yes, but does it scale?": practical considerations for database-driven information systems
Abstract: This paper explores the process of designing and implementing a database-driven system of online documentation, and putting it live on the web for customers to use. Using real-life examples, it discusses practical considerations for balancing performance, scalability, and reliability.
ID:157
CLASS:2
Title: Data allocation in distributed database systems
Abstract: The problem of allocating the data of a database to the sites of a communication network is investigated. This problem deviates from the well-known file allocation problem in several aspects. First, the objects to be allocated are not known a priori; second, these objects are accessed by schedules that contain transmissions between objects to produce the result. A model that makes it possible to compare the cost of allocations is presented; the cost can be computed for different cost functions and for processing schedules produced by arbitrary query processing algorithms.For minimizing the total transmission cost, a method is proposed to determine the fragments to be allocated from the relations in the conceptual schema and the queries and updates executed by the users.For the same cost function, the complexity of the data allocation problem is investigated. Methods for obtaining optimal and heuristic solutions under various ways of computing the cost of an allocation are presented and compared.Two different approaches to the allocation management problem are presented and their merits are discussed.
ID:158
CLASS:2
Title: Issues and approaches to design of real-time database systems
Abstract: Real-time database systems support applications which have severe performance constraints such as fast response time and continued operation in the face of catastrophic failures. Real-time database systems are still in the state of infancy, and issues and alternatives in their design are not very well explored. In this paper, we discuss issues in the design of real-time database systems and discuss different alternatives for resolving these issues. We discuss the aspects in which requirements and design issues of real-time database systems differ from those of conventional database systems. We discuss two approaches to design real-time database systems, viz., main memory resident databases and design by trading a feature (like serializability).We also discuss requirements in the design of real-time distributed database systems, and specifically discuss issues in the design of concurrency control and crash recovery. It is felt that long communication delays may be a factor in limiting the performance of real-time distributed database systems. We present a concurrency control algorithm for real-time distributed database systems whose performance is not limited by communication delays.
ID:159
CLASS:2
Title: Fuzzy functional dependencies and lossless join decomposition of fuzzy relational database systems
Abstract: This paper deals with the application of fuzzy logic in a relational database environment with the objective of capturing more meaning of the data. It is shown that with suitable interpretations for the fuzzy membership functions, a fuzzy relational data model can be used to represent ambiguities in data values as well as impreciseness in the association among them. Relational operators for fuzzy relations have been studied, and applicability of fuzzy logic in capturing integrity constraints has been investigated. By introducing a fuzzy resemblance measure EQUAL for comparing domain values, the definition of classical functional dependency has been generalized to fuzzy functional dependency (ffd). The implication problem of ffds has been examined and a set of sound and complete inference axioms has been proposed. Next, the problem of lossless join decomposition of fuzzy relations for a given set of fuzzy functional dependencies is investigated. It is proved that with a suitable restriction on EQUAL, the design theory of a classical relational database with functional dependencies can be extended to fuzzy relations satisfying fuzzy functional dependencies.
ID:160
CLASS:2
Title: A theory of reliability in database systems
Abstract: Reliable concurrent processing of transactions in a database system is examined. Since serializability, the conventional concurrency control correctness criterion, is not adequate in the presence of common failures, another theory of correctness is proposed, involving the concepts of commit serializability, recoverability, and resiliency.
ID:161
CLASS:2
Title: Office-by-example: an integrated office system and database manager
Abstract: Office-by-Example (OBE) is an integrated office information system that has been under development at IBM Research. OBE, an extension of Query-by-Example, supports various office features such as database tables, word processing, electronic mail, graphics, images, and so forth. These seemingly heterogeneous features are integrated through a language feature called example elements. Applications involving example elements are processed by the database manager, an integrated part of the OBE system. In this paper we describe the facilities and architecture of the OBE system and discuss the techniques for integrating heterogeneous objects.
ID:162
CLASS:2
Title: Intermedia: A case study of the differences between relational and object-oriented database systems
Abstract: This paper compares two approaches to meeting the data handling requirements of Intermedia, a hypermedia system developed at the Institute for Research in Information and Scholarship at Brown University. Intermedia, though written using an object-oriented programming language, relies on a traditional relational database management system for data storage and retrieval. We examine the ramifications of replacing the relational database with an object-oriented database. We begin by describing the important characteristics each database system. We then describe Intermedia and give an overview of its architecture and its data handling requirements. We explain why and how we used a relational database management system and the problems that we encountered with this system. We then present the design of an object-oriented database schema for Intermedia and compare the relational and object-oriented database management system approaches.
ID:163
CLASS:2
Title: Programming constructs for database system implementation in EXODUS
Abstract: The goal of the EXODUS extensible DBMS project is to enable the rapid development of a wide spectrum of high-performance, application-specific database systems EXODUS provides certain kernel facilities for use by all applications and a set of tools to aid the database implementor (DBI) in generating new database system software. Some of the DBI's work is supported by EXODUS tools which generate database components from a specification. However, components such as new abstract data types, access methods, and database operations must be explicitly coded by the DBI. This paper analyzes the major programming problems faced by the DBI, describing the collection of programming language constructs that EXODUS provides for simplifying the DBI's task. These constructs have been embedded in the E programming language, an extension of C++ designed specifically for implementing DBMS software.
ID:164
CLASS:2
Title: Architecture and implementation of the Darmstadt database kernel system
Abstract: The multi-layered architecture of the DArmStadt Data Base System (DASDBS) for advanced applications is introduced DASDBS is conceived as a family of application-specific database systems on top of a common database kernel system. The main design problem considered here is, What features are common enough to be integrated into the kernel and what features are rather application-specific? Kernel features must be simple enough to be efficiently implemented and to serve a broad class of clients, yet powerful enough to form a convenient basis for application-oriented layers. Our kernel provides mechanisms to efficiently store hierarchically structured complex objects, and offers operations which are set-oriented and can be processed in a single scan through the objects. To achieve high concurrency in a layered system, a multi-level transaction methodology is applied. First experiences with our current implementation and some lessons we have learned from it are reported.
ID:165
CLASS:2
Title: A recovery algorithm for a high-performance memory-resident database system
Abstract: With memory prices dropping and memory sizes increasing accordingly, a number of researchers are addressing the problem of designing high-performance database systems for managing memory-resident data. In this paper we address the recovery problem in the context of such a system. We argue that existing database recovery schemes fall short of meeting the requirements of such a system, and we present a new recovery mechanism which is designed to overcome their shortcomings. The proposed mechanism takes advantage of a few megabytes of reliable memory in order to organize recovery information on a per &ldquo;object&rdquo; basis. As a result, it is able to amortize the cost of checkpoints over a controllable number of updates, and it is also able to separate post-crash recovery into two phases&mdash;high-speed recovery of data which is needed immediately by transactions, and background recovery of the remaining portions of the database. A simple performance analysis is undertaken, and the results suggest our mechanism should perform well in a high-performance, memory-resident database environment.
ID:166
CLASS:2
Title: The datacycle architecture for very high throughput database systems
Abstract: The evolutionary trend toward a database-driven public communications network has motivated research into database architectures capable of executing thousands of transactions per second. In this paper we introduce the Datacycle architecture, an attempt to exploit the enormous transmission bandwidth of optical systems to permit the implementation of high throughput multiprocessor database systems. The architecture has the potential for unlimited query throughput, simplified data management, rapid execution of complex queries, and efficient concurrency control. We describe the logical operation of the architecture and discuss implementation issues in the context of a prototype system currently under construction.
ID:167
CLASS:2
Title: A database design methodology and tool for information systems
Abstract: A model and methodology for describing the information objects in an office information system and how such objects flow among the components of such a system are presented. The model and methodology support the specification of information objects at multiple levels of abstraction. An interactive prototype design tool based on the methodology and model has been designed and experimentally implemented.
ID:168
CLASS:2
Title: GESTALT: an expressive database programming system
Abstract: Many new database applications require computational and data modelling power simply not present in conventional database management systems. Developers are forced to design complex encodings of complex data into a limited set of database types, and to embed DML commands into a host programming language, a notoriously tricky and error-prone enterprise.In this paper, we describe the design and implementation of GESTALT, a system and methodology for organizing and interfacing to multiple heterogeneous, existing database systems. Application programs are written in a supported programming language (currently C and Lisp) using high-level data and control abstractions native to the language. The system is flexible in that the underlying database systems can easily be replaced/upgraded/augmented without affecting existing application programs.We also describe our experience with the system: GESTALT has been in daily operational use at MIT for over a year, supporting an information system for CAF, a research facility for the automation of semiconductor fabrication.
ID:169
CLASS:2
Title: A stochastic evaluation model for database organizations in data retrieval systems
Abstract: Experimental work in the valuation of large scale data retrieval systems has been scarce due to its difficulty and prohibitive cost. This paper discusses a simulation model of a data retrieval system which has the effect of significantly reducing the cost of experimentation and enabling research never attempted before. The model is designed to estimate the retrieval workload of alternative data retrieval systems. These data retrieval systems can be organized under several database organizations, including inverted list, threaded list, and cellular list organizations and hybrid combinations of these systems. Effectiveness of the methodology is demonstrated by using the model to study the effect of database organizations in data retrieval systems. In particular, the impact of query complexity is analyzed.
ID:170
CLASS:2
Title: An English language question answering system for a large relational database
Abstract: By typing requests in English, casual users will be able to obtain explicit answers from a large relational database of aircraft flight and maintenance data using a system called PLANES. The design and implementation of this system is described and illustrated with detailed examples of the operation of system components and examples of overall system operation. The language processing portion of the system uses a number of augmented transition networks, each of which matches phrases with a specific meaning, along with context registers (history keepers) and concept case frames; these are used for judging meaningfulness of questions, generating dialogue for clarifying partially understood questions, and resolving ellipsis and pronoun reference problems. Other system components construct a formal query for the relational database, and optimize the order of searching relations. Methods are discussed for handling vague or complex questions and for providing browsing ability. Also included are discussions of important issues in programming natural language systems for limited domains, and the relationship of this system to others.
ID:171
CLASS:2
Title: A Formal System for Reasoning about Programs Accessing a Relational Database
Abstract: A formal system for proving properties of programs accessing a database is introduced. Proving that a program preserves consistency of the database is one of the possible applications of the system. The formal system is a variant of dynamic logic and incorporates a data definition language (DDL) for describing relational databases and a data manipulation language (DML) whose programs access data in a database. The DDL is a many-sorted first-order language that accounts for data aggregations. The DML features a many-sorted assignment in place of the usual data manipulation statements, in addition to the normal programming language constructs.
ID:172
CLASS:2
Title: PYTHIA-II: a knowledge/database system for managing performance data and recommending scientific software
Abstract: Often scientists need to locate appropriate software for their problems and then select from among many alternatives. We have previously proposed an approach for dealing with this task by processing performance data of the targeted software. This approach has been tested using a customized implementation referred to as PYTHIA. This experience made us realize the complexity of the algorithmic discovery of knowledge from performance data and of the management of these data together with the discovered knowledge. To address this issue, we created PYTHIA-II&mdash;a modular framework and system which combines a general knowledge discovery in databases (KDD) methodology and recommender system technologies to provide advice about scientific software/hardware  artifacts. The functionality and effectiveness of the system is demonstrated for two existing performance studies using sets of software for solving partial differential equations. From the end-user perspective, PYTHIA-II allows users to specify the problem to be solved and their computational objectives. In turn, PYTHIA-II (i) selects the software available for the user's problem (ii) suggests parameter values, and (iii) assesses the recommendation provided. PYTHIA-II provides all the necessary facilities to set up database schemas for testing suites and associated performance data in order to test sets of software. Moreover, it allows easy interfacing of alternative data mining and recommendation facilities. PYTHIA-II is an open-ended system implemented on public domain software and has  been used for performance evaluation in several different problem domains.
ID:173
CLASS:2
Title: Logical, internal, and physical reference behavior in CODASYL database systems
Abstract: This work investigates one aspect of the performance of CODASYL database systems: the data reference behavior. We introduce a model of database traversals at three levels: the logical, internal, and physical levels. The mapping between the logical and internal levels is defined by the internal schema, whereas the mapping between the internal and the physical levels depends on cluster properties of the database. Our model explains the physical reference behavior for a given sequence of DML statements at the logical level.Software has been implemented to monitor references in two selected CODASYL DBMS applications. In a series of experiments the physical reference behavior was observed for varying internal schemas and cluster properties of the database. The measurements were limited to retrieval transactions, so that a variety of queries could be analyzed for the same well-known state of the database. Also, all databases were relatively small in order to allow fast reloading with varying internal schema parameters. In all cases, the database transactions showed less locality of reference than do programs under virtual memory operating systems; some databases showed no locality at all. No evidence of physical sequentiality was found. This suggests that standard page replacement strategies are not optimal for CODASYL database buffer management; instead, replacement decisions in a database buffer should be based on specific knowledge available from higher system layers.
ID:174
CLASS:2
Title: The integrity subsystem of a distributed database system for workstations
Abstract: Database Management Systems represent important tools for professional applications of inter-connected workstations. This paper presents the integrity subsystem of such a DBMS. Private and shared data are distinguished, as well as an original and possibly several duplicates. It is shown which kinds of integrity constraints across the borders of private and shared data are possible, and a data definition system is described which guides the user towards a complete definition of integrity constraints. For this purpose, the design methodology for an extended relational data model is adapted. The implementation of the concepts presented within a prototype federative database server system is described.
ID:175
CLASS:2
Title: Reliability mechanisms for SDD-1: a system for distributed databases
Abstract: This paper presents the reliability mechanisms of SDD-1, a prototype distributed database system being developed by the Computer Corporation of America. Reliability algorithms in SDD-1 center around the concept of the Reliable Network (RelNet). The RelNet is a communications medium incorporating facilities for site status monitoring, event timestamping, multiply buffered message delivery, and the atomic control of distributed transactions.This paper is one of a series of companion papers on SDD-1 [3, 4, 6, 13].
ID:176
CLASS:2
Title: Effects of locking granularity in a database management system
Abstract: Many database systems guarantee some form of integrity control upon multiple concurrent updates by some form of locking. Some &ldquo;granule&rdquo; of the database is chosen as the unit which is individually locked, and a lock management algorithm is used to ensure integrity. Using a simulation model, this paper explores the desired size of a granule. Under a wide variety of seemingly realistic conditions, surprisingly coarse granularity is called for. The paper concludes with some implications of these results concerning the viability of so-called &ldquo;predicate locking&rdquo;.
ID:177
CLASS:2
Title: Performance of a database manager in a virtual memory system
Abstract: Buffer space is created and managed in database systems in order to reduce accesses to the I/O devices for database information. In systems using virtual memory any increase in the buffer space may be accompanied by an increase in paging. The effects of these factors on system performance are quantified where system performance is a function of page faults and database accesses to I/O devices. This phenomenon is examined through the analysis of empirical data gathered in a multifactor experiment. The factors considered are memory size, size of buffer space, memory replacement algorithm, and buffer management algorithm. The improvement of system performance through an increase in the size of the buffer space is demonstrated. It is also shown that for certain values of the other factors an increase in the size of the buffer space can cause performance to deteriorate.
ID:178
CLASS:2
Title: An authorization mechanism for a relational database system
Abstract: A multiuser database system must selectively permit users to share data, while retaining the ability to restrict data access. There must be a mechanism to provide protection and security, permitting information to be accessed only by properly authorized users. Further, when tables or restricted views of tables are created and destroyed dynamically, the granting, authentication, and revocation of authorization to use them must also be dynamic. Each of these issues and their solutions in the context of the relational database management system System R are discussed.When a database user creates a table, he is fully and solely authorized to perform upon it actions such as read, insert, update, and delete. He may explicitly grant to any other user any or all of his privileges on the table. In addition he may specify that that user is authorized to further grant these privileges to still other users. The result is a directed graph of granted privileges originating from the table creator.At some later time a user A may revoke some or all of the privileges which he previously granted to another user B. This action usually revokes the entire subgraph of the grants originating from A's grant to B. It may be, however, that B will still possess the revoked privileges by means of a grant from another user C, and therefore some or all of B's grants should not be revoked. This problem is discussed in detail, and an algorithm for detecting exactly which of B's grants should be revoked is presented.
ID:179
CLASS:2
Title: System R: relational approach to database management
Abstract: System R is a database management system which provides a high level relational data interface. The systems provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment.This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product.
ID:180
CLASS:2
Title: Process synchronization in database systems
Abstract: The problem of process synchronization in database systems is analyzed in a strictly systematic way, on a rather abstract level; the abstraction is chosen such that the essential characteristics of the problem can be distinctly modeled and investigated. Using a small set of concepts, a consistent description of the whole problem is developed; many widely used, but only vaguely defined, notions are defined exactly within this framework. The abstract treatment of the problem immediately leads to practically useful insights with respect to possible solutions, although implementational aspects are not discussed in detail.
ID:181
CLASS:2
Title: Sequentiality and prefetching in database systems
Abstract: Sequentiality of access is an inherent characteristic of many database systems. We use this observation to develop an algorithm which selectively prefetches data blocks ahead of the point of reference. The number of blocks prefetched is chosen by using the empirical run length distribution and conditioning on the observed number of sequential block references immediately preceding reference to the current block. The optimal number of blocks to prefetch is estimated as a function of a number of &ldquo;costs,&rdquo; including the cost of accessing a block not resident in the buffer (a miss), the cost of fetching additional data blocks at fault times, and the cost of fetching blocks that are never referenced. We estimate this latter cost, described as memory pollution, in two ways. We consider the treatment (in the replacement algorithm) of prefetched blocks, whether they are treated as referenced or not, and find that it makes very little difference. Trace data taken from an operational IMS database system is analyzed and the results are presented. We show how to determine optimal block sizes. We find that anticipatory fetching of data can lead to significant improvements in system operation.
ID:182
CLASS:2
Title: System level concurrency control for distributed database systems
Abstract: A distributed database system is one in which the database is spread among several sites and application programs &ldquo;move&rdquo; from site to site to access and update the data they need. The concurrency control is that portion of the system that responds to the read and write requests of the application programs. Its job is to maintain the global consistency of the distributed database while ensuring that the termination of the application programs is not prevented by phenomena such as deadlock. We assume each individual site has its own local concurrency control which responds to requests at that site and can only communicate with concurrency controls at other sites when an application program moves from site to site, terminates, or aborts.This paper presents designs for several distributed concurrency controls and demonstrates that they work correctly. It also investigates some of the implications of global consistency of a distributed database and discusses phenomena that can prevent termination of application programs.
ID:183
CLASS:2
Title: Retrospection on a database system
Abstract: This paper describes the implementation history of the INGRES database system. It focuses on mistakes that were made in progress rather than on eventual corrections. Some attention is also given to the role of structured design in a database system implementation and to the problem of supporting nontrivial users. Lastly, miscellaneous impressions of UNIX, the PDP-11, and data models are given.
ID:184
CLASS:2
Title: Parallelism and recovery in database systems
Abstract: In this paper a new method to increase parallelism in database systems is described. Use is made of the fact that for recovery reasons, we often have two values for one object in the database&mdash;the new one and the old one. Introduced and discussed in detail is a certain scheme by which readers and writers may work simultaneously on the same object. It is proved that transactions executed according to this scheme have the correct effect; i.e., consistency is preserved. Several variations of the basic scheme which are suitable depending on the degree of parallelism required, are described.
ID:185
CLASS:2
Title: Concurrency control in a system for distributed databases (SDD-1)
Abstract: This paper presents the concurrency control strategy of SDD-1. SDD-1, a System for Distributed Databases, is a prototype distributed database system being developed by Computer Corporation of America. In SDD-1, portions of data distributed throughout a network may be replicated at multiple sites. The SDD-1 concurrency control guarantees database consistency in the face of such distribution and replication.This paper is one of a series of companion papers on SDD-1 [4, 10, 12, 21].
ID:186
CLASS:2
Title: Introduction to a system for distributed databases (SDD-1)
Abstract: The declining cost of computer hardware and the increasing data processing needs of geographically dispersed organizations have led to substantial interest in distributed data management. SDD-1 is a distributed database management system currently being developed by Computer Corporation of America. Users interact with SDD-1 precisely as if it were a nondistributed database system because SDD-1 handles all issues arising from the distribution of data. These issues include distributed concurrency control, distributed query processing, resiliency to component failure, and distributed directory management. This paper presents an overview of the SDD-1 design and its solutions to the above problems.This paper is the first of a series of companion papers on SDD-1 (Bernstein and Shipman [2], Bernstein et al. [4], and Hammer and Shipman [14]).
ID:187
CLASS:2
Title: Data abstractions for database systems
Abstract: Data abstractions were originally conceived as a specification tool in programming. They also appear to be useful for exploring and explaining the capabilities and shortcomings of the data definition and manipulation facilities of present-day database systems. Moreover they may lead to new approaches to the design of these facilities. In the first section the paper introduces an axiomatic method for specifying data abstractions and, on that basis, gives precise meaning to familiar notions such as data model, data type, and database schema. In a second step the various possibilities for specifying data types within a given data model are examined and illustrated. It is shown that data types prescribe the individual operations that are allowed within a database. Finally, some additions to the method are discussed which permit the formulation of interrelationships between arbitrary operations.
ID:188
CLASS:2
Title: Achieving robustness in distributed database systems
Abstract: The problem of concurrency control in distributed database systems in which site and communication link failures may occur is considered. The possible range of failures is not restricted; in particular, failures may induce an arbitrary network partitioning. It is desirable to attain a high &ldquo;level of robustness&rdquo; in such a system; that is, these failures should have only a small impact on system operation.A level of robustness termed maximal partial operability is identified. Under our models of concurrency control and robustness, this robustness level is the highest level attainable without significantly degrading performance.A basis for the implementation of maximal partial operability is presented. To illustrate its use, it is applied to a distributed locking concurrency control method and to a method that utilizes timestamps. When no failures are present, the robustness modifications for these methods induce no significant additional overhead.
ID:189
CLASS:2
Title: Performance enhancements to a relational database system
Abstract: In this paper we examine four performance enhancements to a database management system: dynamic compilation, microcoded routines, a special-purpose file system, and a special-purpose operating system. All were examined in the context of the INGRES database management system. Benchmark timings that are included suggest the attractiveness of dynamic compilation and a special-purpose file system. Microcode and a special-purpose operating system are analyzed and appear to be of more limited utility in the INGRES context.
ID:190
CLASS:2
Title: On database systems development through logic
Abstract: The use of logic as a single tool for formalizing and implementing different aspects of database systems in a uniform manner is discussed. The discussion focuses on relational databases with deductive capabilities and very high-level querying and defining features. The computational interpretation of logic is briefly reviewed, and then several pros and cons concerning the description of data, programs, queries, and language parser in terms of logic programs are examined. The inadequacies are discussed, and it is shown that they can be overcome by the introduction of convenient extensions into logic programming. Finally, an experimental database query system with a natural language front end, implemented in PROLOG, is presented as an illustration of these concepts. A description of the latter from the user's point of view and a sample consultation session in Spanish are included.
ID:191
CLASS:2
Title: A formal approach to the definition and the design of conceptual schemata for databased systems
Abstract: A formal approach is proposed to the definition and the design of conceptual database diagrams to be used as conceptual schemata in a system featuring a multilevel schema architecture, and as an aid for the design of other forms of schemata. We consider E-R (entity-relationship) diagrams, and we introduce a new representation called CAZ-graphs. A rigorous connection is established between these diagrams and some formal constraints used to describe relationships in the framework of the relational data model. These include functional and multivalued dependencies of database relations. The basis for our schemata is a combined representation for two fundamental structures underlying every relation: the first defined by its minimal atomic decompositions, the second by its elementary functional dependencies.The interaction between these two structures is explored, and we show that, jointly, they can represent a wide spectrum of database relationships, of which the well-known one-to-one, one-to-many, and many-to-many associations constitute only a small subset. It is suggested that a main objective in conceptual schema design is to ensure a complete representation of these two structures. A procedure is presented to design schemata which obtain this objective while eliminating redundancy. A simple correspondence between the topological properties of these schemata and the structure of multivalued dependencies of the original relation is established. Various applications are discussed and a number of illustrative examples are given.
ID:192
CLASS:2
Title: Query processing in a system for distributed databases (SDD-1)
Abstract: This paper describes the techniques used to optimize relational queries in the SDD-1 distributed database system. Queries are submitted to SDD-1 in a high-level procedural language called Datalanguage. Optimization begins by translating each Datalanguage query into a relational calculus form called an envelope, which is essentially an aggregate-free QUEL query. This paper is primarily concerned with the optimization of envelopes.Envelopes are processed in two phases. The first phase executes relational operations at various sites of the distributed database in order to delimit a subset of the database that contains all data relevant to the envelope. This subset is called a reduction of the database. The second phase transmits the reduction to one designated site, and the query is executed locally at that site.The critical optimization problem is to perform the reduction phase efficiently. Success depends on designing a good repertoire of operators to use during this phase, and an effective algorithm for deciding which of these operators to use in processing a given envelope against a given database. The principal reduction operator that we employ is called a semijoin. In this paper we define the semijoin operator, explain why semijoin is an effective reduction operator, and present an algorithm that constructs a cost-effective program of semijoins, given an envelope and a database.
ID:193
CLASS:2
Title: An object-oriented approach to database system implementation
Abstract: This paper examines object-oriented programming as an implementation technique for database systems. The object-oriented approach encapsulates the representations of database entities and relationships with the procedures that manipulate them. To achieve this, we first define abstractions of the modeling constructs of the data model that describe their common properties and behavior. Then we represent the entity types and relationship types in the conceptual schema and the internal schema by objects that are instances of these abstractions. The generic procedures (data manipulation routines) that comprise the user interface can now be implemented as calls to the procedures associated with these objects.A generic procedure model of database implementation techniques is presented and discussed. Several current database system implementation techniques are illustrated as examples of this model, followed by a critical analysis of our implementation technique based on the use of objects. We demonstrate that the object-oriented approach has advantages of data independence, run-time efficiency due to eliminating access to system descriptors, and support for low-level views.
ID:194
CLASS:2
Title: High-level programming features for improving the efficiency of a relational database system
Abstract: This paper discusses some high-level language programming constructs that can be used to manipulate the relations of a relational database system efficiently. Three different constructs are described: (1) tuple identifiers that directly reference tuples of a relation; (2) cursors that may iterate over the tuples of a relation; and (3) markings, a form of temporary relation consisting of a set of tuple identifiers. In each case, attention is given to syntactic, semantic, and implementation considerations.The use of these features is first presented within the context of the programming language PLAIN, and it is then shown how these features could be used more generally to provide database manipulation capabilities in a high-level programming language. Consideration is also given to issues of programming methodology, with an important goal being the achievement of a balance between the enforcement of good programming practices and the ability to write efficient programs.
ID:195
CLASS:2
Title: A database encryption system with subkeys
Abstract: A new cryptosystem that is suitable for database encryption is presented. The system has the important property of having subkeys that allow the encryption and decryption of fields within a record. The system is based on the Chinese Remainder Theorem.
ID:196
CLASS:2
Title: Persistent memory: a storage architecture for object-oriented database systems
Abstract: Object-oriented databases are needed to support database objects with a wide variety of types and structures. A persistent memory system provides a storage architecture for long-term, reliable retention of objects with rich types and structures in the virtual memory itself. It is based on a uniform memory abstraction, which eliminates the distinction between transient objects (data structures) and persistent objects (files and databases), and therefore, allows the same set of powerful and flexible operations to be applied with equal efficiency on both transient and persistent objects from a programming language such as Lisp or Prolog. Because no separate file system is assumed for long-term, reliable storage of objects, the system requires a crash recovery scheme at the level of the virtual memory, which is a major contribution of the paper. It is expected that the persistent memory system will lead to significant simplifications in implementing applications such as object-oriented databases.
ID:197
CLASS:2
Title: Extensibility in the Starburst database system
Abstract: The goal of the Starburst project is to examine how traditional relational database systems must be adapted for new applications and technologies. We believe that extensibility is an important requirement for future database systems, and have identified some of the problems of building an extensible relational database system. We focus on five potential areas for extensibility: external data storage, storage management, access methods, abstract data types, and complex objects. Our approach emphasizes extensions written by knowledgeable programmers that are invoked as needed from within the primary database system. We show by an example how this method could be used to implement support for complex objects. Although many questions remain unanswered, extensible relational database systems are a promising area of research.
ID:198
CLASS:2
Title: A Database Management System for the Federal Courts
Abstract: A judicial systems laboratory has been established and several large-scale information management systems projects have been undertaken within the Federal Judicial Center in Washington, D.C. The newness of the court application area, together with the experimental nature of the initial prototypes, required that the system building tools be as flexible and efficient as possible for effective software design and development. The size of the databases, the expected transaction volumes, and the long-term value of the court records required a data manipulation system capable of providing high performance and integrity. The resulting design criteria, the programming capabilities developed, and their use in system construction are described herein. This database programming facility has been especially designed as a technical management tool for the database administrator, while providing the applications programmer with a flexible database software interface for high productivity.Specifically, a network-type database management system using SAIL as the data manipulation host language is described. Generic data manipulation verb formats using SAIL's macro facilities and dynamic data structuring facilities allowing in-core database representations have been developed to achieve a level of flexibility not usually attained in conventional database systems.
ID:199
CLASS:2
Title: Designing a Portable Natural Language Database Query System
Abstract: One barrier to the acceptance of natural language database query systems is the substantial installation effort required for each new database. Much of this effort involves the encoding of semantic knowledge for the domain of discourse, necessary to correctly interpret and respond to natural language questions. For such systems to be practical, techniques must be developed to increase their portability to new domains.This paper discusses several issues involving the portability of natural language interfaces to database systems, and presents the approach taken in CO-OP &mdash; a natural language database query system that provides cooperative responses to English questions and operates with a typical CODA-SYL database system. CO-OP derives its domain-specific knowledge from a lexicon (the list of words known to the system) and the information already present in the structure and content of the underlying database. Experience with the implementation suggests that strategies that are not directly derivative of cognitive or linguistic models may nonetheless play an important role in the development of practical natural language systems.
ID:200
CLASS:3
Title: Human-Computer Interaction in the Control of Dynamic Systems
Abstract: Modes of human-computer interaction in the control of dynamicsystems are discussed, and the problem of allocating tasks betweenhuman and computer considered. Models of human performance in avariety of tasks associated with the control of dynamic systems arereviewed. These models are evaluated in the context of a designexample involving human-computer interaction in aircraftoperations. Other examples include power plants, chemical plants,and ships.
ID:201
CLASS:3
Title: Incorporating Human-Computer Interaction into the undergraduate computer science curriculum
Abstract: This special session presents issues, approaches and experiences related to incorporating Human-Computer Interaction (HCI) into the undergraduate CS curriculum. Since the publication of the ACM SIGCHI Curricula for Human-Computer Interaction in 1992 [1], CS educators have tried various implementations of these guidelines. These implementations have been mainly offered as elective courses or modules within other courses, partly because the CS mainstream has been slow to recognize the importance of user interface design and HCI issues in software development.Today at least 50% of the code written for software applications is specific to the user interface. Average end-user expertise is continually dropping due to the proliferation of inexpensive computers available to the general public. Accordingly, user interface effectiveness has become increasingly important in software development, as the user interface has the power to "make or break" a software product. Through its emphasis on user-centered design, task analysis, and usability evaluation among other topics, an HCI course addresses more than 1/3 of the guidelines and imperatives of the ACM Code of Ethics [2]. This includes issues related to professional review, evaluations of computer systems, social responsibility and quality of life, and non-discrimination and dignity of end-users.CS educators have begun realizing the importance of incorporating HCI into the educational experience of students. Approximately 40% of the CAC-accredited degree programs include an HCI course at the upper level; however, this course is offered mostly as an elective (37% of degree programs) as opposed to a required course (3% of degree programs) [3]. Also, HCI is included as a core knowledge area in Computing Curricula 2001 [4, 5, 6]; unfortunately, only six hours have been devoted to it, as opposed to, say, 36 hours for architecture.
ID:202
CLASS:3
Title: The development of a Human Computer Interaction course at a senior synthesis course
Abstract: HCI has been taught in universities and colleges as an elective for computer science majors. An inter-disciplinary synthesis course was developed in the Department of Computer Science at Indiana University of Pennsylvania. The course was first offered in the fall of 2004. Objectives, content, and pedagogy of this course are outlined. The paper is concluded by citing challenges and issues regarding the development and launching of such a course.
ID:203
CLASS:3
Title: Exploratory search interfaces: categorization, clustering and beyond: report on the XSI 2005 workshop at the Human-Computer Interaction Laboratory, University of Maryland
Abstract: The development and testing of systems to support users engaged in exploratory search activities (i.e., searches where the target may be undefined) is an challenge for the online search community. In this article we report on a workshop on exploratory search issues organized in conjunction with the University of Maryland Human-Computer Interaction Laboratory's Annual Symposium and Open House in June 2005. This workshop brought together researchers from the fields of Information Seeking (IS), Information Retrieval (IR), Human-Computer Interaction (HCI) and Information Visualization (IV) for a cross-disciplinary exploration of these and other issues. Although originally intended to focus on interfaces to support exploratory search the workshop blossomed into a rich discussion of not only interface issues, but also evaluation, the cognitive processes that underlie information exploration and research methods.
ID:204
CLASS:3
Title: Mapping the discourse of HCI researchers with citation analysis
Abstract: This paper observes the development of human-computer interaction as a research discipline from 1991 to 1993. From a citation analysis of three volumes of three journals, the field of human computer interaction is identified as emerging from a supporting base of four fields: computer science, information systems, psychology, and human factors/ ergonomics. Results of this analysis support the proposition that human-computer interaction is emerging as a distinct field of study.
ID:205
CLASS:3
Title: Bridging the gaps between software engineering and Human-Computer Interaction
Abstract: The First International Workshop on the Relationships between Software Engineering and Human-Computer Interaction was held on May 3--4, 2003 as part of the 2003 International Conference on Software Engineering, in Portland, OR, U.S.A. This workshop was motivated by a perception among researchers, practitioners, and educators that the fields of Human-Computer Interaction and Software Engineering were largely ignoring each other and that they needed to work together more closely and to understand each other better. This paper describes the motivation, goals, organization, and outputs of the workshop.
ID:206
CLASS:3
Title: Bridging the Gaps II: Bridging the Gaps between Software Engineering and Human-Computer Interaction
Abstract: The Second International Workshop on the Relationshipsbetween Software Engineering and Human-Computer Interaction was held on May 24-25, 2004 as part of the 2004 International Conference on SoftwareEngineering, in Edinburgh, Scotland. This workshop wasthe second at ICSE and the fourth in a series held at internationalconferences in the past two years. It was motivatedby a perception among researchers, practitioners,and educators that the fields of Human-Computer Interactionand Software Engineering were largely ignoringeach other and that they needed to work together moreclosely and to understand each other better. This reportdescribes the motivation, goals, organization, and outputsof the workshop.
ID:207
CLASS:3
Title: Towards a semio-cognitive theory of Human-Computer Interaction
Abstract: The research here presented is theoretical and introduces a critical analysis of instrumental approaches in Human-Computer Interaction (HCI). From a semiotic point of view interfaces are not "natural" or "neutral" instruments, but rather complex sense production devices. Interaction, in other words, is far from being a "transparent" process.In this abstract we present the fundaments of a theoretical model that combines Semiotics with Cognitive Science approaches.
ID:208
CLASS:3
Title: A collaborative learning trial between New Zealand and Sweden-using Lotus Notes Domino in teaching the concepts of Human Computer Interaction
Abstract: This paper reports the results of a collaborative learning exercise between students at Auckland Institute of Technology and Uppsala University. The exercise was conducted using both a Lotus Notes Domino&amp;trade; collaborative database and electronic mail to support students working in remote groups to perform a common task. Issues concerning the logistics of such an exercise, student participation and evaluations of the process, ethical considerations and the quality of the learning process are discussed. Some conclusions are drawn concerning the value of Group Ware technology to support this form of collaborative learning, and suggestions are made for future developments.
ID:209
CLASS:3
Title: Designing StoryRooms: interactive storytelling spaces for children
Abstract: Costly props, complicated authoring technologies, and limited access to space are among the many reasons why children can rarely enjoy the experience of authoring room-sized interactive stories.  Typically in these kinds of environments, children are restricted to being story participants, rather than story authors.  Therefore, we have begun the development of StoryRooms, room-sized immersive storytelling urrent technology implementation and example StoryRooms.
ID:210
CLASS:3
Title: How do design and evaluation interrelate in HCI research?
Abstract: Human-Computer Interaction (HCI) is defined by the Association for Computing Machinery (ACM) Special Interest Group on Computer-Human Interaction (SIGCHI) as "a discipline concerned with the design, evaluation, and implementation of interactive computing systems for human use and with the study of the major phenomenon surrounding them" [18]. In HCI there are authors that focus more on designing for usability and there are authors that focus more on evaluating usability. The relationship between these communities is not really clear. We use author cocitation analysis, multivariate techniques, and visualization tools to explore the relationships between these communities. The results of the analysis revealed seven clusters that could be identified as Design Theory and Complexity, Design Rationale, Cognitive Theories and Models, Cognitive Engineering, Computer-Supported Cooperative Work (CSCW), Participatory Design, and User-Centered Design.
ID:211
CLASS:3
Title: Systems, interactions, and macrotheory
Abstract: A significant proportion of early HCI research was guided by one very clear vision: that the existing theory base in psychology and cognitive science could be developed to yield engineering tools for use in the interdisciplinary context of HCI design. While interface technologies and heuristic methods for behavioral evaluation have rapidly advanced in both capability and breadth of application, progress toward deeper theory has been modest, and some now believe it to be unnecessary. A case is presented for developing new forms of theory, based around generic &ldquo;systems of interactors.&rdquo; An overlapping, layered structure of macro- and microtheories could then serve an explanatory role, and could also bind together contributions from the different disciplines. Novel routes to  formalizing and applying such theories provide a host of interesting and tractable problems for future basic research in HCI.
ID:212
CLASS:3
Title: EWHCI '93 (East-West international conference on Human-Computer Interaction): conference report, Moscow, Russia, August 3\&ndash;7
Abstract: The third annual East-West Human-Computer Interaction conference was held in Moscow in early August, 1993. The first meeting of this kind was a single-track symposium held just before the collapse of the USSR in 1991; the second used the East-West name and was held in St. Petersburg in 1992. The conference took place at the International Centre for Scientific and Technical Information (ICSTI), ICSTI, led by Juri Gornostaev, served as the Russian organizers of the conference. The western organization was chaired by Austin Henderson, with Len Bass and Claus Unger serving as the technical program chairs.
ID:213
CLASS:3
Title: Review of "The Psychology of Human-Computer Interaction by Stuart K. Card, Thomas P. Moran and Alan Newell
Abstract: This is a book that is intended not just to be read but to be used-through discussion, study, and prolonged reflection. The major concern of the book is, as the title implies, how humans interact with computers. The authors construct an empirically based cognitive theory of a skilled human-computer interaction and apply it to the specific problem of text editing. They do not simply inform readers of the problems, they provide solutions to the problems and describe a methodology for unifying psychology and computer science. A few years ago Ben Shneiderman [1] listed the computer science topics relevant to human-factors research. &lt;u&gt;The Psychology of Human-Computer Interaction&lt;/u&gt; tells the student how to actually perform human-factors research. It discusses the processes involved, the techniques to use, and the methods to follow.
ID:214
CLASS:3
Title: Human-computer interface development: concepts and systems for its management
Abstract: Human-computer interface management, from a computer science viewpoint, focuses on the process of developing quality human-computer interfaces, including their representation, design, implementation, execution, evaluation, and maintenance. This survey presents important concepts of interface management: dialogue independence, structural modeling, representation, interactive tools, rapid prototyping, development methodologies, and control structures. Dialogue independence is the keystone concept upon which all the other concepts depend. It is a characteristic that separates design of the interface from design of the computational component of an application system so that modifications in either tend not to cause changes in the other. The role of a dialogue developer, whose main purpose is to create quality interfaces, is a direct result of the dialogue independence concept. Structural models of the human-computer interface serve as frameworks for understanding the elements of interfaces and for guiding the dialogue developer in their construction. Representation of the human-computer interface is accomplished by a variety of notational schemes for describing the interface. Numerous kinds of interactive tools for human-computer interface development free the dialogue developer from much of the tedium of "coding" dialogue. The early ability to observe behavior of the interface&mdash;and indeed that of the whole application system&mdash;provided by rapid prototyping increases communication among system designers, implementers, evaluators, and end-users. Methodologies for interactive system development consider interface management to be an integral part of the overall development process and give emphasis to evaluation in the development life cycle. Finally, several types of control structures govern how sequencing among dialogue and computational components is designed and executed. Numerous systems for human-computer interface management are presented to illustrate these concepts.
ID:215
CLASS:3
Title: The state of the art in automating usability evaluation of user interfaces
Abstract: Usability evaluation is an increasingly important part of the user interface design process. However, usability evaluation can be expensive in terms of time and human resources, and automation is therefore a promising way to augment existing approaches. This article presents an extensive survey of usability evaluation methods, organized according to a new taxonomy that emphasizes the role of automation. The survey analyzes existing techniques, identifies which aspects of usability evaluation automation are likely to be of use in future research, and suggests new ways to expand existing approaches to better support usability evaluation.
ID:216
CLASS:3
Title: Supporting concurrency, communication, and synchronization in human-computer interaction\&mdash;the Sassafras UIMS
Abstract: Sassafras is a prototype User Interface Management System (UIMS) specifically designed to support a wide range of user interface styles. In particular, it supports the implementation of user interfaces where the user is free to manipulate multiple input devices and perform several (possibly related) tasks concurrently. These interfaces can be compactly represented and efficiently implemented without violating any of the rules of well-structured programming. Sassafras also supports elaborate run-time communication and synchronization among the modules that make up the user interface. This is needed to implement user interfaces that have context-sensitive defaults, and it simplifies recovery from semantic errors.Sassafras is based on a new language for specifying the syntax of human-computer dialogues known as Event-Response Language (ERL) and a new run-time structure and communication mechanism for UIMSs known as the Local Event Broadcast Method (LEBM). Both ERL and LEBM are described in detail, and implementation techniques are presented. The effectiveness of Sassafras is demonstrated by describing two interfaces that have been implemented with Sassafras.
ID:217
CLASS:3
Title: Human computer interaction: an operational definition
Abstract: This paper poses an operational definition for the term Human-Computer Interaction and distinguishes it from the term User Interface. Human-Computer Interaction includes several separate communication paths between the human and the computer's natural environment, the computer's own ergonomics, an operating environment, and an application environment. Each of these paths occur in parallel and have both a dialog and an interface component. A distinction is made between those dialog stages which allow multiple interpretations of symbols (soft-dialog) and those which allow only one interpretation for each symbol (hard-dialog). The term User Dialog includes only the soft-dialog stages, and the User Interface consists of the mechanisms for communicating the User Dialog.
ID:218
CLASS:3
Title: An HCI continuing education curriculum for industry
Abstract: A field becomes a valid science by having a body of knowledge which its members recognize as basic to the field. This knowledge includes a set of research questions which the field addresses, a set of methodologies applicable to the research questions, the small number of basic theories that guide the research and a large amount of detail on techniques, phenomenon and field specific definitions. The methodologies and the theories they embody are what constitutes the paradigm of the field. As research and serendipitous discoveries take place close to and within the membership of the field, the body of knowledge is expanded, remolded, revised and restructured.
ID:219
CLASS:3
Title: A tool for creating predictive performance models from user interface demonstrations
Abstract: A central goal of many user interface development tools has been to make the construction of high quality interfaces easy enough that iterative design approaches could be a practical reality. In the last 15 years significant advances in this regard have been achieved. However, the evaluation portion of the iterative design process has received relatively little support from tools. Even though advances have also been made in usability evaluation methods, nearly all evaluation is still done &ldquo;by hand,&rdquo; making it more expensive and difficult than it might be. This paper considers a partial implementation of the CRITIQUE usability evaluation tool that is being developed to help remedy this situation by automating a number of evaluation tasks. This paper will consider techniques used by the system to produce predictive models (keystroke level models and simplified GOMS models) from demonstrations of sample tasks in a fraction of the time needed by conventional handcrafting methods. A preliminary comparison of automatically generated models with models created by an expert modeler show them to produce very similar predictions (within 2%). Further, because they are automated, these models promise to be less subject to human error and less affected by the skill of the modeler.
ID:220
CLASS:3
Title: Designing PETS: a personal electronic teller of stories
Abstract: We have begun the development of a new robotic pet that cansupport children in the storytelling process. Children can buildtheir own pet by snapping together the modular animal parts of thePETS robot. After their pet is built, children can tellstories using the My Pets software. These stories canthen be acted out by their robotic pet. This video paper describesthe motivation for this research and the design process of ourintergenerational design team in building the first PETSprototypes. We will discuss our progress to date and our focus forthe future.
ID:221
CLASS:3
Title: On the effective use and reuse of HCI knowledge
Abstract: The article argues that new approaches for delivering HCI knowledge from theory to designers will be necessary in the new millennium. First the role of theory in HCI design to date is reviewed, including the progress made in cognitive theories of interaction and their impact on the design pr ocess. The role of bridging models that build on models of interaction is described, but it is argued that direct application of cognitive theory to design is limited by scalability problems. The alternative of representing HCI knowledge as claims and the role of the task-artefact approach to theory-based design are introduced. Claims are proposed as a possible bridging representation that may enable theories to frame appropriate recommendations for designers and, vice versa, enable designers to ask appropriate questions for theoretical research. However, claims provide design advice grounded in  specific scenarios and examples, which limits their generality. The prospects for reuse becoming an important mode of development and the possible directions in generalizing claims for reuse are discussed, including generalizing claims beyond their original context, providing a context for reuse of claims by linking them to generic task and domain models. It is argued that generic models provide a way forward for developing reusable libraries of interactive components. The approach is illustrated from a case study of extracting claims from one information ret rieval application, generalizing claims for future reuse in information-searching tasks, and reapplying claims in the Web-based Multimedia Broker application. The article concludes by proposing that HCI knowledge should be theory-grounded, and development of reusable &ldquo;designer-digestible&rdquo; packets  will be an important contribution in the future.
ID:222
CLASS:3
Title: MSIS 2006: model curriculum and guidelines for graduate degree programs in information systems
Abstract: This article presents the MSIS 2006 Model Curriculum and Guidelines for Graduate Degree Programs in Information Systems. As with MSIS 2000 and its predecessors, the objective is to create a model for schools designing or revising an MS curriculum in Information Systems. The curriculum was designed by a joint committee of the Association for Information Systems and the Association for Computing Machinery.MSIS2006 is a major update of MSIS 2000. Features include increasing the number of required courses from 10 to 12 while revising prerequisites, introducing new courses and revising existing courses to modernize the curriculum, and alternatives for phased upgrading from MSIS2000 to MSIS 2006.As with the previous curriculum, it is the product of detailed consultation with the IS community. The curriculum received the endorsement of 8 major IS professional groups.
ID:223
CLASS:3
Title: The use of anti-patterns in human computer interaction: wise or III-advised?
Abstract: In this paper the tenability of anti-patterns in Human-Computer Interaction is explored. Patterns have been accepted as being useful in software development and more recently also in Human-Computer Interaction. A concerted effort is being made in Software Engineering to identify and document anti-patterns. Patterns and anti-patterns are essentially about transferring captured expert knowledge, therefore compatibility between the nature of anti-patterns and the nature of the learner's internal knowledge representation and processing is crucial. This paper addresses the differences and similarities between patterns and anti-patterns and how this impacts on the mental models and cognitive processing of patterns and anti-patterns. We present evidence from theories of mental modelling and reasoning that highlight possible significant dangers in the use of anti-patterns to teach novices human-computer interaction principles. If the notion that the current representation of anti-patterns is not supporting cognitive processing is correct, a new approach to structuring anti-patterns is needed. Recommendations are made towards a new specification technique for HCI antipatterns.
ID:224
CLASS:3
Title: Testing a walkthrough methodology for theory-based design of walk-up-and-use interfaces
Abstract: The value of theoretical analyses in user interface design has been hotly debated. All sides agree that it is difficult to apply current theoretical models within the constraints of real-world development projects. We attack this problem in the context of bringing the theoretical ideas within a model of exploratory learning [19] to bear on the evaluation of alternative interfaces for walk-up-and-use systems. We derived a &ldquo;cognitive walkthrough&rdquo; procedure for systematically evaluating features of an interface in the context of the theory. Four people independently applied this procedure to four alternative interfaces for which we have empirical usability data. Consideration of the walkthrough sheds light on the consistency with which such a procedure can be applied as well as the accuracy of the results.
ID:225
CLASS:3
Title: Tools for children to create physical interactive storyrooms
Abstract: Over the past few years, researchers have been exploringpossibilities for ways in which embedded technologies can enrichchildrens storytelling experiences. In this article we present ourresearch on physical interactive storytelling environments from achilds perspective. We present the system architecture as well as aformative study of the technologys use with 18 children, ages 5-6.We discuss the challenges and opportunities for kindergartenchildren to become creators of their own physical storytellinginteractions.
ID:226
CLASS:3
Title: A model for notification systems evaluation\&mdash;assessing user goals for multitasking activity
Abstract: Addressing the need to tailor usability evaluation methods (UEMs) and promote effective reuse of HCI knowledge for computing activities undertaken in divided-attention situations, we present the foundations of a unifying model that can guide evaluation efforts for notification systems. Often implemented as ubiquitous systems or within a small portion of the traditional desktop, notification systems typically deliver information of interest in a parallel, multitasking approach, extraneous or supplemental to a user's attention priority. Such systems represent a difficult challenge to evaluate meaningfully. We introduce a design model of user goals based on blends of three critical parameters---interruption, reaction, and comprehension. Categorization possibilities form a logical, descriptive design space for notification systems, rooted in human information processing theory. This model allows conceptualization of distinct action models for at least eight classes of notification systems, which we describe and analyze with a human information processing model. System classification regions immediately suggest useful empirical and analytical evaluation metrics from related literature. We present a case study that demonstrates how these techniques can assist an evaluator in adapting traditional UEMs for notification and other multitasking systems. We explain why using the design model categorization scheme enabled us to generate evaluation results that are more relevant for the system redesign than the results of the original exploration done by the system's designers.
ID:227
CLASS:3
Title: The effects of culture on performance achieved through the use of human computer interaction
Abstract: The user interface development process focuses on understanding users and their individual differences. These differences result from, inter alia, differences in culture. The primary goal of this research project was to determine whether Hofstede's [1991] cultural dimensions affect the performance achieved through the use of human-computer interaction. In order to achieve this goal, it was necessary to (1) identify the characteristics of the cultural dimensions; (2) identify test subjects and test interfaces displaying appropriate cultural dimensions and (3) assess the impact of these cultural dimensions on the speed, accuracy and satisfaction levels achieved by test subjects using the test interfaces to perform data collection tasks. Test subjects and website interfaces were identified in terms of the cultural dimension characteristics. The test subjects were selected based not only on their cultural dimensions, but also by controlling for user profile variables. The data resulting from the experiment was then analyzed to establish whether these dimensions had any impact on the performance achieved when using these websites. The results of the experiment did not provide sufficient evidence to conclude that any of the tested cultural dimensions affected human performance. However, the performance levels attained suggest that the usability of the interfaces was increased for all users, as a result of accommodating high uncertainty avoidance, masculinity, collectivism and high power distance characteristics into the design of the interfaces. In addition, two main categories of further research have arisen as a result of this research. The first category comprises new research questions. The second focuses on the changes that should be made to the research design used for this research effort.
ID:228
CLASS:3
Title: Concepts of cognition and consciousness: four voices
Abstract: This paper considers theories of cognition and consciousness in four traditions: neuroscience, cognitive science, activity theory and the distributed cognition approach. It is most concerned with social theories of consciousness---activity theory and distributed cognition---but briefly considers biological and computational models as a foil or backdrop against which the social theories stand out more clearly.
ID:229
CLASS:3
Title: Computer-assisted evaluation of interface designs
Abstract: Research in human-computer interaction has resulted in the development of task-theoretic analytical models that allow interface designers to evaluate the interface design on paper before building a prototype. Despite the potential of such models to reduce interface development costs and improve design quality, they are not widely used. We believe that the complexity of these models is the main impediment and propose a computer system to remove this obstacle. The proposed system enables interface designers to describe an interface design formally and then assess it in terms of usability dimensions such as ease of learning and ease of use. It facilitates and structures task analysis and spares designers the burden of learning the complex syntax of analytical models. This paper describes the proposed system and discusses an empirical study designed to evaluate it. The empirical results show that the proposed system improves the accuracy and speed of evaluation of interface designs and contributes to more favorable attitudes towards analytical models.
ID:230
CLASS:3
Title: Interface and data architecture for query preview in networked information systems
Abstract: There are numerous problems associated with formulating queries on networked information systems. These include increased data volume and complexity, accompanied by slow network access. This article proposes a new approach to a network query user interfaces that consists of two phases: query preview and query refinement. This new approach is based on the concepts of dynamic queries and query previews, which guides users in rapidly and dynamically eliminating undesired records, reducing the data volume to a manageable size, and refining queries locally before submission over a network. Examples of two applications are given: a Restaurant Finder and a prototype for NASA's Earth Observing Systems Data Information Systems (EOSDIS). Data architecture is discussed, and user feedback is  presented.
ID:231
CLASS:3
Title: The GOMS family of user interface analysis techniques: comparison and contrast
Abstract: Sine the publication of The Psychology of Human-Computer Interaction, the GOMS model has been one of the most widely known theoretical concepts in HCI. This concept has produced severval GOMS analysis techniques that differ in appearance and form, underlying architectural assumptions, and predictive power. This article compares and contrasts four popular variantsof the GOMS family (the Keystroke-Level Model, the original GOMS formulation, NGOMSL, and CPM-GOMS) by applying them to a single task example.
ID:232
CLASS:3
Title: Using GOMS for user interface design and evaluation: which technique?
Abstract: Since the seminal book, The Psychology of Human-Computer Interaction, the GOMS model has been one of the few widely known theoretical concepts in human-computer interaction. This concept has spawned much research to verify and extend the original work and has been used in real-world design and evaluation situations. This article synthesizes the previous work on GOMS to provide an integrated view of GOMS models and how they can be used in design. We briefly describe the major variants of GOMS that have matured sufficiently to be used in actual design. We then provide guidance to practitioners about which GOMS variant to use for different design situations. Finally, we present examples of the application of GOMS to practical design problems and then summarize the lessons learned.
ID:233
CLASS:3
Title: HCI International '93: 5th International Conference on Human-Computer Interaction
Abstract: This book explores the use of text and how people react to it. Many of the same issues that confront graphic designers in the print medium are similar to problems confronted by interface designers. Discussions of colour, typeface, and graphic layout are all transferable to the screen even though they are discussed in terms of the printed page. By using the same techniques as graphic artists, the interface designer can direct the user's attention to the most appropriate information and reduce confusion. Also, by looking at what designers in the graphic arts are doing, designers of interfaces can expand their scope and add more visceral appeal to their products.
ID:234
CLASS:3
Title: The role and position of graphics in computer science education
Abstract: The SIGGRAPH Education Committee has been considering recommendations for inclusion of graphics in various curricula for higher education. Several issues of computer graphics in computer science or computer science/engineering curricula are identified here. In particular the course content and support facilities necessary and the status of graphics courses within programs are discussed. A basic premise is stated that Curriculum '78[1] and other guidelines[2, 3, 4, 5, 6] for educational programs are flawed in their lack of computer graphics content. The reasons for this are not immediately apparent, but might in part be attributed to the lower profile of graphics during the time of their initial development. Since that time the significant technical advancements and standardization of terms and concepts have not been incorporated as changes in recommended curricula designs.
ID:235
CLASS:3
Title: Human-computer interaction research at the University of Illinois (lab review)
Abstract: The Engineering Psychology program at the University of Illinois is interdisciplinary in nature and involves the participation of faculty members, undergraduate, graduate and post-doctoral students from the Department of Psychology, Department of Industrial Engineering, the Institute of Aviation, and the Beckman Institute. While the interests of the faculty members and the students run the gamut from nuts and bolts ergonomics to the design of intelligent systems there are two topics, the design of displays and the acquisition of perceptual-motor and cognitive skills, which serve as a unifying focus of research in Human-Computer Interaction.The examination of issues in display design and skill acquisition ranges from basic research on visual psychophysics and automatization to applied research and design projects on aviation displays and computer-based tutorials. Three laboratories within the Engineering Psychology program contribute to these research efforts: the Visual Information Processing laboratory, the Human-Computer Interaction laboratory, and the Aviation Research laboratory. In the description that follows we will indicate the mission of each of these laboratories and the manner in which they contribute to the interdisciplinary research programs on displays and skill acquisition.The core faculty members conducting Human-Computer Interaction research within the interdisciplinary Engineering Psychology program include:John Andersen (Psychology)John Flach (Engineering)Arthur Kramer (Psychology)Gavan Lintern (Aviation)George McConkie (Psychology)Neville Moray (Engineering)Penelope Sanderson (Engineering)Alan Stokes (Aviation)Brian Ross (Psychology)Christopher Wickens (Aviation)
ID:236
CLASS:3
Title: Classroom BRIDGE: using collaborative public and desktop timelines to support activity awareness
Abstract: Classroom BRIDGE supports activity awareness by facilitating planning and goal revision in collaborative, project-based middle school science. It integrates large-screen and desktop views of project times to support incidental creation of awareness information through routine document transactions, integrated presentation of awareness information as part of workspace views, and public access to subgroup activity. It demonstrates and develops an object replication approach to integrating synchronous and asynchronous distributed work for a platform incorporating both desktop and large-screen devices. This paper describes an implementation of these concepts with preliminary evaluation data, using timeline-based user interfaces.
ID:237
CLASS:3
Title: Automating CPM-GOMS
Abstract: CPM-GOMS is a modeling method that combines the task decomposition of a GOMS analysis with a model of human resource usage at the level of cognitive, perceptual, and motor operations. CPM-GOMS models have made accurate predictions about skilled user behavior in routine tasks, but developing such models is tedious and error-prone. We describe a process for automatically generating CPM-GOMS models from a hierarchical task decomposition expressed in a cognitive modeling tool called Apex. Resource scheduling in Apex automates the difficult task of interleaving the cognitive, perceptual, and motor resources underlying common task operators (e.g. mouse move-and-click). Apex's UI automatically generates PERT charts, which allow modelers to visualize a model's complex parallel behavior. Because interleaving and visualization is now automated, it is feasible to construct arbitrarily long sequences of behavior. To demonstrate the process, we present a model of automated teller interactions in Apex and discuss implications for user modeling
ID:238
CLASS:3
Title: A multi-view intelligent editor for digital video libraries
Abstract: Silver is an authoring tool that aims to allow novice users to edit di gital video. The goal is to make editing of digital video as easy as text editing. Silver provides multiple coordinated views, including project, source, outline, subject, storyboard, textual transcript and timeline views. Selections and edits in any view are synchronized with all other views. A variety of recognition algorithms are applied to the video and audio content and then are used to aid in the editing tasks. The Informedia Digital Library supplies the recognition algorithms and metadata used to support intelligent editing, and Informedia also provides search and a repository. The metadata includes shot boundaries and a time-synchronized transcript, which are used to support intelligent selection and intelligent cut/copy/paste.
ID:239
CLASS:3
Title: Reading on human factors in computer systems: the 1989 list
Abstract: For the last six years I have taught a course on Human Factors and Computer Systems at the University of Michigan (Industrial and Operations Engineering 436). The course is described in an article that appeared in IEEE Computer Graphics and Applications five years ago (Green, 1984). The course topics and assignments have not changed since then (though they have been refined), so the article is still accurate. I also wrote an article two years ago for this bulletin (Green, 1987) describing the course readings. Since then, there have been requests for the latest version of the reading list, both from practitioners and academicians. The reading list follows with along with a commentary.
ID:240
CLASS:3
Title: An introductory algorithm teacher
Abstract: A non-machine specific design of an algorithm teacher is proposed. It is a programmed environment to help students in a beginning computer science course learn problem solving skills. This paper provides an overview of the problem, a motivation and justification, followed by a brief description of what the program should provide the student.
ID:241
CLASS:3
Title: Multi-faceted evaluation for complex, distributed activities
Abstract: Computer-supported cooperative learning presents challenges for evaluation methodology: Learning events and learning outcomes are dispersed in time and space, making causal relationships difficult to identify. We are developing techniques to address these challenges including systematic sampling, collation of multiple evaluation methods and data, and the use of collaborative critical incidents. In this paper we overview and discuss this emerging methodology.
ID:242
CLASS:3
Title: Update on the HCI Education Survey
Abstract: The HCI Education Survey was originally designed to collect and maintain a databse of information on courses, faculty and programs that focus on Human Computer Interaction in higher education. The database was intended to provide prospective students (particularly graduate students) information about educational opportunities, and secondarily, to provide HCI educators information about other HCI educators. The survey also was designed to be accessed and updated in electronic form. It was based upon a review of other surveys (Ferguson, 1989; Human Factors Society, 1991; Mantei &amp;amp; Smelcer, 1984; Software Engineering Institute, 1991) and field-testing.
ID:243
CLASS:3
Title: Value-centred HCI
Abstract: HCI is misdefined. We need to redefine it. HCI is misfocused. We need to refocus it. HCI has a window of opportunity to recreate itself as a design discipline. It must focus on the intention of gifted design, which is to improve the world by delivering new sources of value. A focus on value creates a paradoxical discipline that fuses subjectivity and objectivity in a single process. HCI must be objectively systematic and reliable in the pursuit of subjective value. Traditional disciplines have delivered truth. The goal of HCI is to deliver value. In my invited presentation, I will outline why we can and must change within HCI, where we are now (and how we got there), what I believe we should change to. I close with a research agenda for value-centred HCI.
ID:244
CLASS:3
Title: A Tracking Framework for Collaborative Human Computer Interaction
Abstract: The ability to track multiple people and their body parts (i.e., face and hands) in a complex environment is crucial for designing a collaborative natural human computer interaction (HCI). One of the challenging issues in tracking body parts of people is the data association uncertainty while assigning measurements to the proper tracks in the case of occlusion and close interaction of body parts of different people. This paper describes a framework for tracking body parts of people in 2D/3D using multiple hypothesis tracking (MHT) algorithm. A path coherence function has been incorporated along with MHT to reduce the negative effects of closely spaced measurements that produce unconvincing tracks and needless computations. The performance of the framework has been validated using experiments on real sequence of images.
ID:245
CLASS:3
Title: Book reivew: Handbook of Human-Computer Interaction ed. by Martin Helander (North-Holland 1988)
Abstract: My conclusions are: If money grew on trees you should rush to the nearest bookstore to buy this book after stopping to pick a good handful of the green stuff. Unfortunately, money does not grow on trees where I come from, so unless you are in more fortunate circumstances you will probably have to settle for looking at the handbook in some library. I do recommend the purchase of the handbook by libraries that cater to larger communities of usability professionals.
ID:246
CLASS:3
Title: Interdisciplinary application tracks in an undergraduate computer science curriculum
Abstract: The Computer Science Department at Winona State University revised its curriculum to include an interdisciplinary approach adapted to the study of computer science. The new curriculum consists of a traditional Computer Science option and an Applied Computer Science option consisting of four separate tracks, namely: bioinformatics, computer information systems, geographic information technology, and human computer interaction. This paper describes the design strategy and implementation plan as well as the content of our multi-track Applied Computer Science curriculum.
ID:247
CLASS:3
Title: It's worth the hassle!: the added value of evaluating the usability of mobile systems in the field
Abstract: The distinction between field and laboratory is classical in research methodology. In human-computer interaction, and in usability evaluation in particular, it has been a controversial topic for several years. The advent of mobile devices has revived this topic. Empirical studies that compare evaluations in the two settings are beginning to appear, but they provide very different results. This paper presents results from an experimental comparison of a field-based and a lab-based usability evaluation of a mobile system. The two evaluations were conducted in exactly the same way. The conclusion is that it is definitely worth the hassle to conduct usability evaluations in the field. In the field-based evaluation we identified significantly more usability problems and this setting revealed problems with interaction style and cognitive load that were not identified in the laboratory.
ID:248
CLASS:3
Title: Effects of feedback on eye typing with a short dwell time
Abstract: Eye typing provides means of communication especially for people with severe disabilities. Recent research indicates that the type of feedback impacts typing speed, error rate, and the user's need to switch her gaze between the on-screen keyboard and the typed text field. The current study focuses on the issues of feedback when a short dwell time (450 ms vs. 900 ms in a previous study) is used. Results show that the findings obtained using longer dwell times only partly apply for shorter dwell times. For example, with a short dwell time, spoken feedback results in slower text entry speed and double entry errors. A short dwell time requires sharp and clear feedback that supports the typing rhythm.
ID:249
CLASS:3
Title: Advanced visualization for OLAP
Abstract: Data visualization is one of the big issues of database research. OLAP as a decision support technology is highly related to the developments of data visualization area. In this paper we demonstrate how the Cube Presentation Model (CPM), a novel presentational model for OLAP screens, can be naturally mapped on the Table Lens, which is an advanced visualization technique from the Human-Computer Interaction area, particularly tailored for cross-tab reports. We consider how the user interacts with an OLAP screen and based on the particularities of Table Lens, we propose an automated proactive users support. Finally, we discuss the necessity and the applicability of advanced visualization techniques in the presence of recent technological developments.
ID:250
CLASS:3
Title: Adding human computer interaction studies into the informatics and computing engineering bachelor degrees in Latin America
Abstract: The fast paced evolution of computing disciplines forces educators to a constant reviewing and actualisation of both content and syllabus of professional studies. However, the studies within the area of Human Computer Interaction (HCI) had been traditionally left aside, being the interface design a primordial necessity in the job market. This proposal offers a vision, along with advantages, which would come with the incorporation of HCI studies to the syllabus of careers like the Bachelor in Informatics and Computing Engineering.
ID:251
CLASS:3
Title: Designing a digital library for young children
Abstract: As more information resources become accessible using computers,our digital interfaces to those resources need to be appropriate for all people.  However when it comes to digital libraries, the interfaces have typically been designed for older children or adults.  Therefore, we have begun to develop a digital library interface developmentally appropriate for young children (ages 5-10 years old).  Our prototype system we now call SearchKids offers a graphical interface for querying, browsing and reviewing search results.  This paper describes our motivation for the research, the design partnership we established between children and adults, our design process, the technology outcomes of our current work, and the lessons we have learned.
ID:252
CLASS:3
Title: Enter the usability engineer: integrating HCI and software engineering
Abstract: This paper examines the role of Human Computer Interaction in the context of the Computer Science and Software Engineering curricula. We suggest there needs to be much more integration between Computer Science and HCI. We believe this can be brought about by adopting HCI as the underlying principle to the development of systems. Usability engineering would provide the necessary framework for the development of usable systems.
ID:253
CLASS:3
Title: Live from the stacks: user feedback on mobile computers and wireless tools for library patrons
Abstract: Digital library research is made more robust and effective when        end-user opinions and viewpoints inform the research, design and            development process.  A rich understanding of user tasks and contexts is    especially necessary when investigating the use of mobile computers in      traditional and digital library environments, since the nature and scope of the research questions at hand remain relatively undefined.  This paper     outlines findings from a library technologies user survey and on-site       mobile library access prototype testing, and presents future research       directions that can be derived from the results of these two studies.
ID:254
CLASS:3
Title: Using a large projection screen as an alternative to head-mounted displays for virtual environments
Abstract: Head-mounted displays for virtual environments facilitate an immersive experience that seems more real than an experience provided by a desk-top monitor [18]; however, the cost of head-mounted displays can prohibit their use. An empirical study was conducted investigating differences in spatial knowledge learned for a virtual environment presented in three viewing conditions: head-mounted display, large projection screen, and desk-top monitor. Participants in each condition were asked to reproduce their cognitive map of a virtual environment, which had been developed during individual exploration of the environment along a predetermined course. Error scores were calculated, indicating the degree to which each participant's map differed from the actual layout of the virtual environment. No statistically significant difference was found between the head-mounted display and large projection screen conditions. An implication of this result is that a large projection screen may be an effective, inexpensive substitute for a head-mounted display.
ID:255
CLASS:3
Title: Teaching Software Psychology: expanding the perspective
Abstract: This paper describes the curriculum development and teaching experience of a junior core course entitled Software Psychology, offered in the undergraduate Software Engineering program at the author's affiliated university. In particular, the pedagogy of problem-based learning is introduced, together with the evolution of the course content. It will also address issues such as resources and facilities needed for the course, and the students' perceived learning as well as the author's lessons learned therein.
ID:256
CLASS:3
Title: An information systems perspective of the SIGCHI curricula
Abstract: The SIGCHI curricula report published in 1992 [ACM 1992] describes four proposed courses in HCI and two model curricula for HCI. This paper critiques the curricula report's recommendations as they pertain to an undergraduate information systems course and to the structure of a recently revised curriculum in Computer Information Systems. The evaluation found that the curricula report is useful as an initial set of curriculum guidelines, but that customization of its recommendations is needed to satisfy individual program, college, and university objectives. Suggestions for improvements to the curricula report and advice to information systems curriculum developers are listed.
ID:257
CLASS:3
Title: Integrating status and event phenomena in formal specifications of interactive systems
Abstract: In this paper we investigate the appropriateness of formal specification languages for the description of user interface phenomena. Specifically, we are concerned with the distinction between continuously available information, which we call status, and atomic, non-persistent information, which we call events. We propose a hybrid model and notation to address status and event phenomena symmetrically. We demonstrate the effectiveness of this model for designing and understanding mixed control interaction, an especially important topic in the design of multi-user systems.
ID:258
CLASS:3
Title: HILITES: the information service for the world HCI community
Abstract: The Hci Information and LITerature Enquiry Service (HILITES) has been developed from two initiatives over the years, one being a series of research projects and the other a service development on a commercial basis. In this paper the user's needs for information services are discussed, and the developments leading to HILITES are reviewed. The facilities provided by HILITES are outlined, including the on-line database containing over 25,000 items of literature available for on-line search, the weekly accessions list and request service, the copy and/or loan service of hard copies to subscribers, and related special services. The latest development (November 1991) has been the release of the HILITES database on CD-ROM.
ID:259
CLASS:3
Title: The HCI Bibliography project
Abstract: The HCI Bibliography project has just released its first collections of a free-access online extended bibliography on Human-Computer Interaction. The basic goal of the project is to put an online bibliography for most of HCI on the screens of all researchers and developers in the field through anonymous ftp access, mail servers, and Mac and DOS floppy disks. Through the efforts of volunteers, the bibliography is approaching 1000 entries, with abstracts and/or tables of contents; eventually, citation information and hypertext access will be added. The first release contains the complete contents of all the ACM CHI conference, the complete journal Human-Computer Interaction, and several other important sources. Eventually, all of HCI will be online and freely accessible around the world.
ID:260
CLASS:3
Title: A paradigm for community-based human computer interaction education
Abstract: With the pervasiveness of computers throughout our environment, there is a growing demand for diligent Human Computer Interaction (HCI) education of graduate and undergraduate students to satisfy the growing needs of a multiplicity of organizations. This paper describes one approach to teaching HCI while requiring students to develop systems for various city, school, and university organizations. The benefits derived by the students and the clients receiving their services are many. There is wide range of opportunities for community-based HCI education. The various client applications provide a plethora of learning scenarios that may be explored in a classroom setting. The process, experience, and results are presented in this paper.
ID:261
CLASS:3
Title: Theory adapters as discipline coordinators
Abstract: A successful discipline of software engineering will, over time, incorporate within its own borders those theories and techniques from other disciplines which are relevant in and helpful to software development. Since both software engineering and its cognate disciplines will change over time, it must not only incorporate external theories and techniques, but establish active coordination with other disciplines. Having explicit models and plans for achieving this coordination is preferable to leaving it to chance. This paper outlines a model for coordinating software engineering and cognitive support research through theory transfer by applied theoreticians. Ongoing work on incorporating cognitive support theories into software engineering processes and education are cast as an example effort falling under this discipline coordination model. The model is conservative in that it does not suggest a radical transformation of software engineering, but our application to cognitive support does highlight a need for more directed theory application, and generates proposals for non-trivial additions to the accepted body of knowledge.
ID:262
CLASS:3
Title: Touchscreen field specification for public access database queries: let your fingers do the walking
Abstract: Database query is becoming a common task in public access systems; touchscreens can provide an appealing interface for such a system. This paper explores three interfaces for constructing queries on alphabetic field values with a touchscreen interface; including a QWERTY keyboard, an Alphabetic keyboard, and a Reduced Input Data Entry (RIDE) interface. The RIDE interface allows field values to be entered with fewer &ldquo;keystrokes&rdquo; (touches) than either keyboard while eliminating certain errors. In one test database, the RIDE interface required 69% fewer keystrokes than either keyboard interface.
ID:263
CLASS:3
Title: Adapting GOMS to model human-robot interaction
Abstract: A formal interaction modeling technique known as Goals, Operators, Methods, and Selection rules (GOMS) is well-established in human-computer interaction as a cost-effective way of evaluating designs without the participation of end users. This paper explores the use of GOMS for evaluating human-robot interaction. We provide a case study in the urban search-and-rescue domain and raise issues for developing GOMS models that have not been previously addressed. Further, we provide rationale for selecting different types of GOMS modeling techniques to help the analyst model human-robot interfaces.
ID:264
CLASS:3
Title: MAIL: a framework for critical technical practice
Abstract: This paper proposes a new framework for applying Critical Technical Practice (CTP) to the area of Human Computer Interaction (HCI). Through this paper the framework is developed, justified and explained. The framework is then demonstrated using three cases. Since the conception of CTP in the late 1990s it has attracted interest from various areas, and through research is helping to yield new ways forward for problem areas, design and products in HCI. However the application of CTP has always been up to individuals and their own interpretation. The proposed framework tries to add structure through a series of easy to follow steps that the CTP adopter can use to apply CTP to their problem area, design or product. The framework is motivated by the need to provide the adopter with a straight forward way to critically think about his/her HCI arena. Ways to enable this are provided and areas for future work are discussed. The use cases presented help to demonstrate how the framework can be applied to each of these different areas.
ID:265
CLASS:3
Title: The UAN: a user-oriented representation for direct manipulation interface designs
Abstract: Many existing interface representation techniques, especially those associated with UIMS, are constructional and focused on interface implementation, and therefore do not adequately support a user-centered focus. But it is in the behavioral domain of the user that interface designers and evaluators do their work. We are seeking to complement constructional methods by providing a tool-supported technique capable of specifying the behavioral aspects of an interactive system&ndash;the tasks and the actions a user performs to accomplish those tasks. In particular, this paper is a practical introduction to use of the User Action Notation (UAN), a task- and user-oriented notation for behavioral representation of asynchronous, direct manipulation interface    designs. Interfaces are specified in UAN as a quasihierarchy of asynchronous tasks. At the lower levels, user actions are associated with feedback and system state changes. The notation makes use of visually onomatopoeic symbols and is simple enough to read with little instruction. UAN is being used by growing numbers of interface developers and researchers. In addition to its design role, current research is investigating how UAN can support production and maintenance of code and documentation.
ID:266
CLASS:3
Title: Visual display, pointing, and natural language: the power of multimodal interaction
Abstract: This paper examines user behavior during multimodal human-computer interaction (HCI). It discusses how pointing, natural language, and graphical layout should be integrated to enhance the usability of multimodal systems. Two experiments were run to study simulated systems capable of understanding written natural language and mouse-supported pointing gestures. Results allowed to: (a) develop a taxonomy of communication acts aimed at identifying targets; (b) determine the conditions under which specific referent identification strategies are likely to be produced; (c) suggest guidelines for designing effective multimodal interfaces; (d) show that performance is strongly influenced by interface graphical layout and by user expertise. Our study confirms the value of simulation as a tool for building HCI models and supports the basic idea that linguistic, visual, and motor cues can be integrated to favor effective multimodal communication.
ID:267
CLASS:3
Title: A framework for user-interfaces to databases
Abstract: A framework for user-interfaces to databases (IDSs) is proposed which draws from existing research on human computer interaction (HCI) and database systems. The framework is described in terms of a classification of the characteristic components of an IDS. These components, when progressively refined, may be mapped to a conceptual object-oriented language for the precise specification of the IDS. A prototype system is presented, showing the potential for automated mapping of a language specification to a fully functional implementation. As well as providing general support to any database interface developer, we believe that the framework will prove useful for researching a number of IDS issues.
ID:268
CLASS:3
Title: A survey of research on context-aware homes
Abstract: The seamless integration of people, devices and computation will soon become part of our daily life. Sensors, actuators, wireless networks and ubiquitous devices powered by intelligent computation will blend into future environments in which people will live. Despite showing great promise, research into future computing technologies is often far removed from the needs of users. The nature of such future systems is often too obtrusive, seemingly denying their purpose. Furthermore, most research on context-aware environments and ubiquitous computing conducted so far has concentrated on supporting people while at work. This paper presents research issues that need to be addressed to enhance the quality of life for people living in context-aware homes. We survey current research and present strategies that facilitate the diffusion of information technology into homes in order to inspire positive emotions, encourage effortless exploration of content and help occupants to achieve tasks at hand.
ID:269
CLASS:3
Title: Task-oriented representation of asynchronous user interfaces
Abstract: A simple, task-oriented notation for describing user actions in asynchronous user interfaces is introduced. This User Action Notation (UAN) allows the easy association of actions with feedback and system state changes as part of a set of asynchronous interface design techniques, by avoiding the verbosity and potential vagueness of prose. Use within an actual design and implementation project showed the UAN to be expressive, concise, and highly readable because of its simplicity. The task- and user-oriented techniques are naturally asynchronous and a good match for object-oriented implementation. Levels of abstraction are readily applied to allow definition of primitive tasks for sharing and reusability and to allow hiding of details for chunking. The UAN provides a critical articulation point, bridging the gap between the task viewpoint of the behavioral domain and the event-driven nature of the object-oriented implementational domain. The potential for UAN task description analysis may address some of the difficulties in developing asynchronous interfaces.
ID:270
CLASS:3
Title: Complementarity and convergence of heuristic evaluation and usability test: a case study of universal brokerage platform
Abstract: The aim of this paper is twofold (i) comparing the effectiveness of two evaluation methods, namely heuristic evaluation and usability testing, as applied to an experimental version of the UNIVERSAL Brokerage Platform (UBP), and (ii) inferring implications from the empirical findings of the usability test. Eight claims derived from previous research works are reviewed with the data of the current study. While the complementarity and convergence of the results yielded by the two methods can be confirmed to a certain extent, no conclusive explication about their divergence can be obtained, especially the issue whether usability problems reported lead to failures in real use. One of the significant implications thus drawn is to conduct meta-analysis on a sufficient number of well-designed and professionally performed empirical works on usability evaluation methods.
ID:271
CLASS:3
Title: Open syntax: improving access for all users
Abstract: Trends in new multi-modal user interfaces and pervasive mobile computing are raising technical problems for building flexible interfaces that can adapt to different communication modes. I hope to show how some aspects of the technical solutions that will be needed for these problems will also help to solve problems of access for elderly users.
ID:272
CLASS:3
Title: Cross-cultural usability of the library metaphor
Abstract: Computing metaphors have become an integral part of information systems design, yet they are deeply rooted in cultural practices. This paper presents an investigation of the cross-cultural use and usability of such metaphors by studying the library metaphor of digital libraries in the cultural context of the Maori, the indigenous population of New Zealand. The ethnographic study examines relevant features of the Maori culture, their form of knowledge transfer and their use of physical and digital libraries. On this basis, the paper points out why and when the library metaphor fails Maori and other indigenous users, and indicates how this knowledge can contribute to the improvement of future designs.
ID:273
CLASS:3
Title: History, state and future of user interface management systems
Abstract: This paper is an attempt to survey the topic of User Interface Management Systems (UIMSs). We give a short account of the historical development of UIMSs, try to capture what is regarded as state of the art in the area, and examine the role of a UIMS in the process of software development. We also summarize several future research directions commonly recognized as important.
ID:274
CLASS:3
Title: Role of interface manipulation style and scaffolding on cognition and concept learning in learnware
Abstract: This research investigates the role of interface manipulation style on reflective cognition and concept learning through a comparison of the effectiveness of three verisons of a software application for learning two-dimensional transformation geometry. The three versions respectively utilize a Direct Object Manipulation (DOM) interface in which the user manipulates the visual representation of objects being transformed; a Direct Concept Manipulation (DCM) interface in which the user manipulates the visual representation of the transformation being applied to the object; and a Reflective Direct Concept Manipulation (RDCM) interface in which the DCM approach is extended with scaffolding. Empirical results of a study showed that grade-6 students using the RDCM version learned significantly more than those using the DCM version, who is turn learned significantly more than those using the DOM version. Students using the RDCM version had to process information consciously and think harder than those using the DCM and DOM versions. Despite the relative difficulty when using the RDCM interface style, all three groups expressed a similar (positive) level of liking for the software. This research suggests that some of the educational deficiencies of Direct Manipulation (DM) interfaces are not necessarily caused by their &ldquo;directness,&rdquo; but by what they are directed at&mdash;in this case directness toward objects rather than embedded educational concepts being learned. This paper furthers our understanding of how the DM metaphor can be used in learning- and knowledge-centered software (i.e., learnware) by proposing a new DM metaphor (i.e., DCM), and the incorporation of scaffolding to enhance the DCM approach to promote reflective cognition and deep learning.
ID:275
CLASS:3
Title: Human factors in computer systems:  some useful readings
Abstract: Four years ago I began teaching a course on Human Factors and Computer Systems at the University of Michigan (formerly Industrial and Operations Engineering (IOE) 491, now IOE 436). An overview of that course is given in a paper that appeared in IEEE Computer Graphics and Applications (Green, 1984). That paper sparked the interest of many people outside the University and, as a consequence, I have received requests for additional information. While there have been many improvements in the class exercises and software used for the course since that article, a significant change was the addition of a collection of readings two years ago. This article identifies those materials and comments on them. That information should be useful to educators developing their own courses and to practitioners interested in current developments in user interface design.
ID:276
CLASS:3
Title: HCI for Web-based development of interactive medical mulitmedia courseware - lessons learned
Abstract: This is an industrial placement project, which aimed at implementing principles from Human Computer Interaction (HCI) to develop a usable interactive Web Site for the Central Manchester Healthcare NHS Trust. Some of the lessons learned from this experience are reported here in the article.This report starts with an appraisal of how learning develops to outline the components required in any learning package. The development life cycle has been outlined, starting with defining problem specification, into analysis, design, development and ending with evaluation Some relevant sources, technical and others, with a brief bibliography are included.
ID:277
CLASS:3
Title: Visualizing digital library search results with categorical and hierarchical axes
Abstract: Digital library search results are usually shown as a textual list,    with 10-20 items per page. Viewing several thousand search results at once on a two-dimensional display with continuous variables is a promising       alternative. Since these displays can overwhelm some users, we created a    simplified two-dimensional display that uses categorical and hierarchical   axes, called hieraxes. Users appreciate the meaningful and limited number   of terms on each hieraxis. At each grid point of the display we show a      cluster of color-coded dots or a bar chart. Users see the entire result set and can then click on labels to move down a level in the hierarchy.         Handling broad hierarchies and arranging for imposed hierarchies led to     additional design innovations. We applied hieraxes to a digital video   library of science topics used by middle school teachers, a legal   information system, and a technical library using the ACM Computing Classification System. Feedback from usability testing with 32 subjects revealed strengths and weaknesses.
ID:278
CLASS:3
Title: Two-handed input using a PDA and a mouse
Abstract: We performed several experiments using a Personal Digital Assistant (PDA) as an input device in the non-dominant hand along with a mouse in the dominant hand. A PDA is a small hand-held palm-size computer like a 3Com Palm Pilot or a Windows CE device. These are becoming widely available and are easily connected to a PC. Results of our experiments indicate that people can accurately and quickly select among a small numbers of buttons on the PDA using the left hand without looking, and that, as predicted, performance does decrease as the number of buttons increases. Homing times to move both hands between the keyboard and devices are only about 10% to 15% slower than times to move a single hand to the mouse, suggesting that acquiring two devices does not cause a large penalty. In an application task, we found that scrolling web pages using buttons or a scroller on the PDA matched the speed of using a mouse with a conventional scroll bar, and beat the best two-handed times reported in an earlier experiment. These results will help make two-handed interactions with computers more widely available and more effective.
ID:279
CLASS:3
Title: Implementing interface attachments based on surface representations
Abstract: This paper describes an architecture for supporting interfaceattuchments - small interactive programs which are designed toaugment the functionality of other applications. This architectureis designed to work with a diverse set of conventionalapplications, but require only a minimal set of hooks into thoseapplications. In order to achieve this, the work described hereconcentrates on what we will call observational attachments, asubclass of attachments that operate primarily by observing andmanipulating the surface representations of applications - that isthe visual information that applications would normally display onthe screen or print. These attachments can be thought of as lookingover the shoulder of the user to assist with various tasks. Byrequiring very little modification to, or help from, theapplications they augment, this approach supports the creation of aset of uniform services that can be applied across a more diverseset of applications than traditional approaches.
ID:280
CLASS:3
Title: The use of scenarios in human-computer interaction research: turbocharging the tortoise of cumulative science
Abstract: A scenario is an idealised but detailed description of a specific instance of human-computer interaction (HCI). A set of scenarios can be used as a &ldquo;filter bank&rdquo; to weed out theories whose scope is too narrow for them to apply to many real HCI situations. By helping redress the balance between generality and accuracy in theories derived from cognitive psychology, this use of scenarios (1) allows the researcher to build on empirical findings already established while avoiding the tar-pits associated with the experimental methodology, (2) enables the researcher to consider a range of phenomena in a single study, thereby directly addressing the question of the scope of the theory, and (3) ensures that the resulting theory will be applicable in HCI contexts.
ID:281
CLASS:3
Title: Teaching the empirical approach to designing human-computer interaction via an experiential group project
Abstract: Empirical research plays an important role in the design of user-interfaces and is frequently included in university courses on human-computer interaction. For instance, the ACM SIGCHI guidelines refer to the importance of empirical research, although they do not specify how this approach to user-interface design should be taught. In an Honours (fourth-year) course at the University of Natal, Pietermaritzburg, the theoretical foundation of empirical research is augmented with a real experience of running a simple experiment. This experiment is planned, executed and analysed by the class as a whole. This paper describes the type of empirical studies carried out and discusses the benefits and limitations of such studies in this educational context.
ID:282
CLASS:3
Title: Towards task models for embedded information retrieval
Abstract: This paper investigates to what extent task-oriented user support based on plan recognition is feasible in a highly situation-driven domain like information retrieval (IR) and discusses requirements for appropriate task models. It argues that information seeking tasks which are embedded in some higher-level external task context (e.g. travel planning) often exhibit procedural dependences; that these dependences are mainly due to external task; and that they can be exploited for inferring the users' goals and plans. While there is a clear need for task models in IR to account for situational determinants of user behaviour, what is required are hybrid models that take account of both is &ldquo;planned&rdquo; and &ldquo;situated&rdquo; aspects. Empirical evidence for the points made is reported from a probabilistic analysis of retrieval sessions with a fact database and from experience with plan-based and state-based methods for user support in an experimental travel planning system
ID:283
CLASS:3
Title: Can we do without GUIs? Gesture and speech interaction with a patient information system
Abstract: We have developed a gesture input system that provides a common interaction technique across mobile, wearable and ubiquitous computing devices of diverse form factors. In this paper, we combine our gestural input technique with speech output and test whether or not the absence of a visual display impairs usability in this kind of multimodal interaction. This is of particular relevance to mobile, wearable and ubiquitous systems where visual displays may be restricted or unavailable. We conducted the evaluation using a prototype for a system combining gesture input and speech output to provide information to patients in a hospital Accident and Emergency Department. A group of participants was instructed to access various services using gestural inputs. The services were delivered by automated speech output. Throughout their tasks, these participants could see a visual display on which a GUI presented the available services and their corresponding gestures. Another group of participants performed the same tasks but without this visual display. It was predicted that the participants without the visual display would make more incorrect gestures and take longer to perform correct gestures than the participants with the visual display. We found no significant difference in the number of incorrect gestures made. We also found that participants with the visual display took longer than participants without it. It was suggested that for a small set of semantically distinct services with memorable and distinct gestures, the absence of a GUI visual display does not impair the usability of a system with gesture input and speech output.
ID:284
CLASS:3
Title: Maintaining concentration to achieve task completion
Abstract: When faced with a challenging goal, knowledge workers need to concentrate on their tasks so that they move forward toward completion. Since frustrations, distractions, and interruptions can interfere with their smooth progress, design strategies should enable users to maintain concentration. This paper promotes awareness of this issue, reviews related work, and suggests three initial strategies: Reduce short-term and working memory load, provide information abundant interfaces, and increase automaticity.
ID:285
CLASS:3
Title: End-user training methods: what we know, need to know
Abstract: End-User Training (EUT) has enjoyed a rich tradition of research in Information Systems. However, with the growing pace of change in technology and the dynamic nature of business, organizations are spending an increasing amount of money on end-user training. Training methods are also changing with little research to support new approaches. Thus, extensive research is required in the future. To be credible, end-user training research should preserve and build upon the significant literature that exists, both in IS and Education. This paper provides a review of EUT literature focusing on training methods. It summarizes the findings, while pointing out key future research issues.
ID:286
CLASS:3
Title: Task modeling for ambient intelligent environments: design support for situated task executions
Abstract: The design of interactive systems for an ambient intelligent environment poses many challenges because of the great diversity in devices the user has control of and the user's situation imposed by the environment. Although task-centered interface design is an established approach for traditional form-based and even for multi-device user interfaces, this design approach is, in its current form, not ready for the design of user interfaces for ambient intelligent environments. In this paper we propose a task-centered approach to design interaction mechanisms for ambient intelligent environments by means of visualization and simulation. We focus on three different concepts that are important to support this approach: situated task allocations, user interface distributions and visualization of context influences. Because the execution of a task depends strongly on the situation or context of use, the consequences of a context change on the execution of a task specification should be communicated with the task designer during the design process. The designer should be able to define the possibilities to execute a task while taking into account constraints imposed by the environment of the user. A tool to support this approach using visualization of the environment and simulation of the interface configurations is introduced.
ID:287
CLASS:3
Title: Practical usability evaluation
Abstract: Practical Usability Evaluation is an introduction to cost-effective, low-skill, low-investment methods of usability assessment. The methods include (1) Inspection Methods (e.g., heuristic evaluation), (2) Observational Skills and Video (including user testing with think-aloud protocols), (3) Program Instrumentation, and (4) Questionnaires. The tutorial features many step-by-step procedures to aid in evaluation plan design.
ID:288
CLASS:3
Title: Understanding design as a social creative process
Abstract: The Human-Computer Interaction community has long been concerned with design. Terms such as 'creativity' and 'innovation' are frequently used when referring to the design process and in this paper we examine what creativity is with respect to design. Design is often a collaborative and, therefore, a social activity. We review the evolution of definitions of creativity, leading to our proposal of a unified definition, we present a theoretical account of why social creativity should in principle be more productive than individual creativity. We explain findings to the contrary in terms of three social influences on creativity and suggest that research in supporting design should focus on mitigating the effects of these social influences on the creativity of design teams.
ID:289
CLASS:3
Title: Comparing usability problems and redesign proposals as input to practical systems development
Abstract: Usability problems predicted by evaluation techniques are useful input to systems development; it is uncertain whether redesign proposals aimed at alleviating those problems are likewise useful. We present a study of how developers of a large web application assess usability problems and redesign proposals as input to their systems development. Problems and redesign proposals were generated by 43 evaluators using an inspection technique and think aloud testing. Developers assessed redesign proposals to have higher utility in their work than usability problems. In interviews they explained how redesign proposals gave them new ideas for tackling well known problems. Redesign proposals were also seen as constructive and concrete input. Few usability problems were new to developers, but the problems supported prioritizing ongoing development of the application and taking design decisions. No developers, however, wanted to receive only problems or redesigns. We suggest developing and using redesign proposals as an integral part of usability evaluation.
ID:290
CLASS:3
Title: Tool for accurately predicting website navigation problems, non-problems, problem severity, and effectiveness of repairs
Abstract: The Cognitive Walkthrough for the Web (CWW) is a partially automated usability evaluation method for identifying and repairing website navigation problems. Building on five earlier experiments [3,4], we first conducted two new experiments to create a sufficiently large dataset for multiple regression analysis. Then we devised automatable problem-identification rules and used multiple regression analysis on that large dataset to develop a new CWW formula for accurately predicting problem severity. We then conducted a third experiment to test the prediction formula and refined CWW against an independent dataset, resulting in full cross-validation of the formula. We conclude that CWW has high psychological validity, because CWW gives us (a) accurate measures of problem severity, (b) high success rates for repairs of identified problems (c) high hit rates and low false alarms for identifying problems, and (d) high rates of correct rejections and low rates of misses for identifying non-problems.
ID:291
CLASS:3
Title: Differences in pointing task performance between preschool children and adults using mice
Abstract: Several experiments by psychologists and human factors researchers have shown that when young children execute pointing tasks, they perform at levels below older children and adults. However, these experiments have not provided user interface designers with an understanding of the severity or the nature of the difficulties young children have when using input devices. To address this need, we conducted a study to gain a better understanding of 4 and 5 year-old children's use of mice. We compared the performance of thirteen 4 year-olds, thirteen 5 year-olds and thirteen young adults in point-and-click tasks. Plots of the paths taken by the participants show severe differences between adults' and preschool children's ability to control the mouse. We were not surprised then to find age had a significant effect on accuracy, target reentry, and efficiency. We also found that target size had a significant effect on accuracy and target reentry. Measuring movement time at four different times (first entering target, last entering target, pressing button, releasing button) yielded the result that Fitts' law models children well only up to the time they first enter the target. Overall, we found that the difference between the performance of children and adults was large enough to warrant user interface interactions designed specifically for preschool children. The results additionally suggest that children need the most help once they get close to targets.
ID:292
CLASS:3
Title: Unpacking critical parameters for interface design: evaluating notification systems with the IRC framework
Abstract: We elaborate a proposal for capturing, extending, and reusing design knowledge gleaned through usability testing. The proposal is specifically targeted to address interface design for notification systems, but its themes can be generalized to any constrained and well-defined genre of interactive system design. We reiterate arguments for and against using critical parameters to characterize user goals and usability artifacts. Responding to residual arguments, we suggest that clear advantages for research cohesion, design knowledge reuse, and HCI education are possible if several challenges are overcome. As a first step, we recommend a slight variation to the concept of a critical parameter, which would allow both abstract and concrete knowledge representation. With this concept, we demonstrate a feasible approach by introducing equations that elaborate and allow evolution of notification system critical parameters, which is made operational with a variety of usability evaluation instruments. A case study illustrates how one general instrument allowed system designs to be meaningfully compared and resulted in valuable inferences for interface reengineering. Broad implications and conclusions about this approach will be of interest to others concerned with using critical parameters in interface design, development of notification systems interfaces, or approaches to design rationale and knowledge reuse.
ID:293
CLASS:3
Title: User interface design with matrix algebra
Abstract: It is usually very hard, both for designers and users, to reason reliably about user interfaces. This article shows that 'push button' and 'point and click' user interfaces are algebraic structures. Users effectively do algebra when they interact, and therefore we can be precise about some important design issues and issues of usability. Matrix algebra, in particular, is useful for explicit calculation and for proof of various user interface properties.With matrix algebra, we are able to undertake with ease unusally thorough reviews of real user interfaces: this article examines a mobile phone, a handheld calculator and a digital multimeter as case studies, and draws general conclusions about the approach and its relevance to design.
ID:294
CLASS:3
Title: Using peer-evaluation in a website design course
Abstract: Peer-evaluation is used to assess students' levels of comprehension of primary Human Computer Interaction (HCI) principles as implemented in a specific learning product - a website. Specifically, students demonstrate their knowledge and application of sixteen individual HCI components summarized into five scales by evaluating the design and implementation of their peers' website. Validation of student learning outcomes is achieved through an analysis of agreement between student peer and instructor evaluations of these websites.
ID:295
CLASS:3
Title: The future of human-computer interaction
Abstract: Is an HCI revolution just around the corner?
ID:296
CLASS:3
Title: Aligning learning objectives with service-learning outcomes in a mobile computing application
Abstract: We propose the development of a mobile, location-aware tour of the Bonsai Exhibition Garden of the North Carolina Arboretum. The tour will be a web-based, customizable, multimedia presentation on handheld Personal Digital Assistants. The complete tour, including all presentation materials and system installation, will be developed via a series of three classes at the University of North Carolina at Asheville. These classes, Database Management Systems, Human Computer Interface, and Systems Integration, will occur over a period of two semesters. The objective of this work is to create relevant and effective coursework that empowers students. Students produce state-of-the-art technology that serves their community thereby demonstrating the value of both the technology and their understanding of that technology for the betterment of others.
ID:297
CLASS:3
Title: The reification of metaphor as a design tool
Abstract: Despite causing many debates in human-computer interaction (HCI), the term &ldquo;metaphor&rdquo; remains a central element of design practice. This article investigates the history of ideas behind user-interface (UI) metaphor, not only technical developments, but also less familiar perspectives from education, philosophy, and the sociology of science. The historical analysis is complemented by a study of attitudes toward metaphor among HCI researchers 30 years later. Working from these two streams of evidence, we find new insights into the way that theories in HCI are related to interface design, and offer recommendations regarding approaches to future UI design research.
ID:298
CLASS:3
Title: Designing a portal for older users: A case study of an industrial/academic collaboration
Abstract: A multidisciplinary team from industry, government, and academia developed prototype email, Web search, and navigation systems for users over 60 years old who were inexperienced in using computers and had never used the Internet. The academics encountered problems in persuading other team members of the specific challenges of designing for and working with older people. A number of ways of overcoming such challenges were implemented, and the final &ldquo;radically simple&rdquo; systems evaluated by a team of older people. The collaboration highlighted the conflicting pressures of the commercial world and the time and patience needed to design for older users.
ID:299
CLASS:3
Title: A new role for anthropology?: rewriting "context" and "analysis" in HCI research
Abstract: In this paper we want to reconsider the role anthropology (both its theory and methods) can play within HCI research. One of the areas anthropologists can contribute to here is to rethink the notion of social context where technology is used. Context is usually equated with the immediate activities such as work tasks, when and by whom the task is performed. This tends to under represent some fundamental aspects of social life, like culture and history. In this paper, we want to open up a discussion about what context means in HCI and to emphasize socio-structural and historical aspects of the term. We will suggest a more inclusive analytic way that able the HCI community to make "better" sense of use situation. An example of technology use in a workplace will be given to demonstrate the yields this kind of theoretical framework can bring into HCI.
ID:300
CLASS:4
Title: Machine learning in automated text categorization
Abstract: The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.
ID:301
CLASS:4
Title: Explanation-based learning: a survey of programs and perspectives
Abstract: Explanation-based learning (EBL) is a technique by which an intelligent system can learn by observing examples. EBL systems are characterized by the ability to create justified generalizations from single training instances. They are also distinguished by their reliance on background knowledge of the domain under study. Although EBL is usually viewed as a method for performing generalization, it can be viewed in other ways as well. In particular, EBL can be seen as a method that performs four different learning tasks: generalization, chunking, operationalization, and analogy.This paper provides a general introduction to the field of explanation-based learning. Considerable emphasis is placed on showing how EBL combines the four learning tasks mentioned above. The paper begins with a presentation of an intuitive example of the EBL technique. Subsequently EBL is placed in its historical context and the relation between EBL and other areas of machine learning is described. The major part of this paper is a survey of selected EBL programs, which have been chosen to show how EBL manifests each of the four learning tasks. Attempts to formalize the EBL technique are also briefly discussed. The paper concludes with a discussion of the limitations of EBL and the major open questions in the field.
ID:302
CLASS:4
Title: Improving accuracy in word class tagging through the combination of machine learning systems
Abstract: We examine how differences in language models, learned by different data-driven systems performing the same NLP task, can be exploited to yield a higher accuracy than the best individual system. We do this by means of experiments involving the task of morphosyntactic word class tagging, on the basis of three different tagged corpora. Four well-known tagger generators (hidden Markov model, memory-based, transformation rules, and maximum entropy) are trained on the same corpus data. After comparison, their outputs are combined using several voting strategies and second-stage classifiers. All combination taggers outperform their best component. The reduction in error rate varies with the material in question, but can be as high as 24.3% with the LOB corpus.
ID:303
CLASS:4
Title: Combining Information Extraction Systems Using Voting and Stacked Generalization
Abstract: This article investigates the effectiveness of voting and stacked generalization -also known as stacking- in the context of information extraction (IE). A new stacking framework is proposed that accommodates well-known approaches for IE. The key idea is to perform cross-validation on the base-level data set, which consists of text documents annotated with relevant information, in order to create a meta-level data set that consists of feature vectors. A classifier is then trained using the new vectors. Therefore, base-level IE systems are combined with a common classifier at the meta-level. Various voting schemes are presented for comparing against stacking in various IE domains. Well known IE systems are employed at the base-level, together with a variety of classifiers at the meta-level. Results show that both voting and stacking work better when relying on probabilistic estimates by the base-level systems. Voting proved to be effective in most domains in the experiments. Stacking, on the other hand, proved to be consistently effective over all domains, doing comparably or better than voting and always better than the best base-level systems. Particular emphasis is also given to explaining the results obtained by voting and stacking at the meta-level, with respect to the varying degree of similarity in the output of the base-level systems.
ID:304
CLASS:4
Title: Finding Latent Code Errors via Machine Learning over Program Executions
Abstract: This paper proposes a technique for identifying programproperties that indicate errors. The technique generates machinelearning models of program properties known to resultfrom errors, and applies these models to program propertiesof user-written code to classify and rank propertiesthat may lead the user to errors. Given a set of propertiesproduced by the program analysis, the technique selectssubset of properties that are most likely to reveal an error.An implementation, the Fault Invariant Classifier,demonstrates the efficacy of the technique. The implementationuses dynamic invariant detection to generate programproperties. It uses support vector machine and decision treelearning tools to classify those properties. In our experimentalevaluation, the technique increases the relevance(the concentration of fault-revealing properties) by a factorof 50 on average for the C programs, and 4.8 for the Javaprograms. Preliminary experience suggests that most of thefault-revealing properties do lead a programmer to an error.
ID:305
CLASS:4
Title: Review of "Genetic Algorithms for Machine Learning by John J. Greffenstette
Abstract: This book is a reprint from a special issue of the journal Machine Learning on Genetic Algorithms (GAs). The fact that it was the third special issue of the journal on GAs shows the continuous increase of interest of people in the field. The five selected articles were presented at the Fourth International Conference on Genetic Algorithms, in June 1991 (San Diego), and at a Special Workshop for Machine Learning at the same conference. The intended book audience is both researchers and practitioners in the field.
ID:306
CLASS:4
Title: Round robin classification
Abstract: In this paper, we discuss round robin classification (aka pairwise classification), a technique for handling multi-class problems with binary classifiers by learning one classifier for each pair of classes. We present an empirical evaluation of the method, implemented as a wrapper around the Ripper rule learning algorithm, on 20 multi-class datasets from the UCI database repository. Our results show that the technique is very likely to improve Ripper's classification accuracy without having a high risk of decreasing it. More importantly, we give a general theoretical analysis of the complexity of the approach and show that its run-time complexity is below that of the commonly used one-against-all technique. These theoretical results are not restricted to rule learning but are also of interest to other communities where pairwise classification has recently received some attention. Furthermore, we investigate its properties as a general ensemble technique and show that round robin classification with C5.0 may improve C5.0's performance on multi-class problems. However, this improvement does not reach the performance increase of boosting, and a combination of boosting and round robin classification does not produce any gain over conventional boosting. Finally, we show that the performance of round robin classification can be further improved by a straight-forward integration with bagging.
ID:307
CLASS:4
Title: Adaptive information extraction
Abstract: The growing availability of online textual sources and the potential number of applications of knowledge acquisition from textual data has lead to an increase in Information Extraction (IE) research. Some examples of these applications are the generation of data bases from documents, as well as the acquisition of knowledge useful for emerging technologies like question answering, information integration, and others related to text mining. However, one of the main drawbacks of the application of IE refers to its intrinsic domain dependence. For the sake of reducing the high cost of manually adapting IE applications to new domains, experiments with different Machine Learning (ML) techniques have been carried out by the research community. This survey describes and compares the main approaches to IE and the different ML techniques used to achieve Adaptive IE technology.
ID:308
CLASS:4
Title: Mining with rarity: a unifying framework
Abstract: Rare objects are often of great interest and great value. Until recently, however, rarity has not received much attention in the context of data mining. Now, as increasingly complex real-world problems are addressed, rarity, and the related problem of imbalanced data, are taking center stage. This article discusses the role that rare classes and rare cases play in data mining. The problems that can result from these two forms of rarity are described in detail, as are methods for addressing these problems. These descriptions utilize examples from existing research. So that this article provides a good survey of the literature on rarity in data mining. This article also demonstrates that rare classes and rare cases are very similar phenomena---both forms of rarity are shown to cause similar problems during data mining and benefit from the same remediation methods.
ID:309
CLASS:4
Title: Efficient Feature Selection via Analysis of Relevance and Redundancy
Abstract: Feature selection is applied to reduce the number of features in many applications where data has hundreds or thousands of features. Existing feature selection methods mainly focus on finding relevant features. In this paper, we show that feature relevance alone is insufficient for efficient feature selection of high-dimensional data. We define feature redundancy and propose to perform explicit redundancy analysis in feature selection. A new framework is introduced that decouples relevance analysis and redundancy analysis. We develop a correlation-based method for relevance and redundancy analysis, and conduct an empirical study of its efficiency and effectiveness comparing with representative methods.
ID:310
CLASS:4
Title: Bias-Variance Analysis of Support Vector Machines for the Development of SVM-Based Ensemble Methods
Abstract: Bias-variance analysis provides a tool to study learning algorithms and can be used to properly design ensemble methods well tuned to the properties of a specific base learner. Indeed the effectiveness of ensemble methods critically depends on accuracy, diversity and learning characteristics of base learners. We present an extended experimental analysis of bias-variance decomposition of the error in Support Vector Machines (SVMs), considering Gaussian, polynomial and dot product kernels. A characterization of the error decomposition is provided, by means of the analysis of the relationships between bias, variance, kernel type and its parameters, offering insights into the way SVMs learn. The results show that the expected trade-off between bias and variance is sometimes observed, but more complex relationships can be detected, especially in Gaussian and polynomial kernels. We show that the bias-variance decomposition offers a rationale to develop ensemble methods using SVMs as base learners, and we outline two directions for developing SVM ensembles, exploiting the SVM bias characteristics and the bias-variance dependence on the kernel param
ID:311
CLASS:4
Title: Integrating ILP and EBL
Abstract: This paper presents a review of recent work that integrates methods from Inductive Logic Programming (ILP) and Explanation-Based Learning (EBL). ILP and EBL methods have complementary strengths and weaknesses and a number of recent projects have effectively combined them into systems with better performance than either of the individual approaches. In particular, integrated systems have been developed for guiding induction with prior knowledge (ML-Smart, FOCL, GRENDEL) refining imperfect domain theories (FORTE, AUDREY, Rx), and learning effective search-control knowledge (AxA-EBL, DOLPHIN).
ID:312
CLASS:4
Title: Tree induction vs. logistic regression: a learning-curve analysis
Abstract: Tree induction and logistic regression are two standard, off-the-shelf methods for building models for classification. We present a large-scale experimental comparison of logistic regression and tree induction, assessing classification accuracy and the quality of rankings based on class-membership probabilities. We use a learning-curve analysis to examine the relationship of these measures to the size of the training set. The results of the study show several things. (1) Contrary to some prior observations, logistic regression does not generally outperform tree induction. (2) More specifically, and not surprisingly, logistic regression is better for smaller training sets and tree induction for larger data sets. Importantly, this often holds for training sets drawn from the same domain (that is, the learning curves cross), so conclusions about induction-algorithm superiority on a given domain must be based on an analysis of the learning curves. (3) Contrary to conventional wisdom, tree induction is effective at producing probability-based rankings, although apparently comparatively less so for a given training-set size than at making classifications. Finally, (4) the domains on which tree induction and logistic regression are ultimately preferable can be characterized surprisingly well by a simple measure of the separability of signal from noise.
ID:313
CLASS:4
Title: Selective Rademacher Penalization and Reduced Error Pruning of Decision Trees
Abstract: Rademacher penalization is a modern technique for obtaining data-dependent bounds on the generalization error of classifiers. It appears to be limited to relatively simple hypothesis classes because of computational complexity issues. In this paper we, nevertheless, apply Rademacher penalization to the in practice important hypothesis class of unrestricted decision trees by considering the prunings of a given decision tree rather than the tree growing phase. This study constitutes the first application of Rademacher penalization to hypothesis classes that have practical significance. We present two variations of the approach, one in which the hypothesis class consists of all prunings of the initial tree and another in which only the prunings that are accurate on growing data are taken into account. Moreover, we generalize the error-bounding approach from binary classification to multi-class situations. Our empirical experiments indicate that the proposed new bounds outperform distribution-independent bounds for decision tree prunings and provide non-trivial error estimates on real-world data sets.
ID:314
CLASS:4
Title: Feature Selection for Unsupervised Learning
Abstract: In this paper, we identify two issues involved in developing an automated feature subset selection algorithm for unlabeled data: the need for finding the number of clusters in conjunction with feature selection, and the need for normalizing the bias of feature selection criteria with respect to dimension. We explore the feature selection problem and these issues through FSSEM (Feature Subset Selection using Expectation-Maximization (EM) clustering) and through two different performance criteria for evaluating candidate feature subsets: scatter separability and maximum likelihood. We present proofs on the dimensionality biases of these feature criteria, and present a cross-projection normalization scheme that can be applied to any criterion to ameliorate these biases. Our experiments show the need for feature selection, the need for addressing these two issues, and the effectiveness of our proposed solutions.
ID:315
CLASS:4
Title: Subgroup Discovery with CN2-SD
Abstract: This paper investigates how to adapt standard classification rule learning approaches to subgroup discovery. The goal of subgroup discovery is to find rules describing subsets of the population that are sufficiently large and statistically unusual. The paper presents a subgroup discovery algorithm, CN2-SD, developed by modifying parts of the CN2 classification rule learner: its covering algorithm, search heuristic, probabilistic classification of instances, and evaluation measures. Experimental evaluation of CN2-SD on 23 UCI data sets shows substantial reduction of the number of induced rules, increased rule coverage and rule significance, as well as slight improvements in terms of the area under ROC curve, when compared with the CN2 algorithm. Application of CN2-SD to a large traffic accident data set confirms these findings.
ID:316
CLASS:4
Title: An extended transformation approach to inductive logic programming
Abstract: Inductive logic programming (ILP) is concerned with learning relational descriptions that typically have the form of logic programs. In a transformation approach, an ILP task is transformed into an equivalent learning task in a different representation formalism. Propositionalization is a particular transformation method, in which the ILP task is compiled to an attribute-value learning task. The main restriction of propositionalization methods such as LINUS is that they are unable to deal with nondeterminate local variables in the body of hypothesis clauses. In this paper we show how this limitation can be overcome., by systematic first-order feature construction using a particular individual-centered feature bias. The approach can be applied in any domain where there is a clear notion of individual. We also show how to improve upon exhaustive first-order feature construction by using a relevancy filter. The proposed approach is illustrated on the &ldquo;trains&rdquo; and &ldquo;mutagenesis&rdquo; ILP domains.
ID:317
CLASS:4
Title: Bottom-up relational learning of pattern matching rules for information extraction
Abstract: Information extraction is a form of shallow text processing that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present an algorithm, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template. RAPIER is a bottom-up learning algorithm that incorporates techniques from several inductive logic programming systems. We have implemented the algorithm in a system that allows patterns to have constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text. We present encouraging experimental results on two domains.
ID:318
CLASS:4
Title: Learning to predict train wheel failures
Abstract: This paper describes a successful but challenging application of data mining in the railway industry. The objective is to optimize maintenance and operation of trains through prognostics of wheel failures. In addition to reducing maintenance costs, the proposed technology will help improve railway safety and augment throughput. Building on established techniques from data mining and machine learning, we present a methodology to learn models to predict train wheel failures from readily available operational and maintenance data. This methodology addresses various data mining tasks such as automatic labeling, feature extraction, model building, model fusion, and evaluation. After a detailed description of the methodology, we report results from large-scale experiments. These results clearly show the great potential of this innovative application of data mining in the railway industry.
ID:319
CLASS:4
Title: Online Choice of Active Learning Algorithms
Abstract: This work is concerned with the question of how to combine online an ensemble of active learners so as to expedite the learning progress in pool-based active learning. We develop an active-learning master algorithm, based on a known competitive algorithm for the multi-armed bandit problem. A major challenge in successfully choosing top performing active learners online is to reliably estimate their progress during the learning session. To this end we propose a simple maximum entropy criterion that provides effective estimates in realistic settings. We study the performance of the proposed master algorithm using an ensemble containing two of the best known active-learning algorithms as well as a new algorithm. The resulting active-learning master algorithm is empirically shown to consistently perform almost as well as and sometimes outperform the best algorithm in the ensemble on a range of classification problems.
ID:320
CLASS:4
Title: In Defense of One-Vs-All Classification
Abstract: We consider the problem of multiclass classification. Our main thesis is that a simple "one-vs-all" scheme is as accurate as any other approach, assuming that the underlying binary classifiers are well-tuned regularized classifiers such as support vector machines. This thesis is interesting in that it disagrees with a large body of recent published work on multiclass classification. We support our position by means of a critical review of the existing literature, a substantial collection of carefully controlled experimental work, and theoretical arguments.
ID:321
CLASS:4
Title: Reviews of "Machine Learning by Ryszard S. Michalski, Jaime G. Carbonell, and Tom M. Mitchell
Abstract: Machine Learning is in the enviable yet difficult position of being the first published collection of papers on the subject. By "machine learning" the editors refer to three distinct areas: the engineering of computer systems to learn in particular application areas, the theoretical analysis of learning algorithms, and the simulation of human learning processes. However, there is very little material on the third topic, and almost no mention of neurological models or biological theories of adaptation. The word "machine" is as important as "learning."
ID:322
CLASS:4
Title: Relational learning as search in a critical region
Abstract: Machine learning strongly relies on the covering test to assess whether a candidate hypothesis covers training examples. The present paper investigates learning relational concepts from examples, termed &lt;em&gt;relational learning&lt;/em&gt; or &lt;em&gt;inductive logic programming&lt;/em&gt;. In particular, it investigates the chances of success and the computational cost of relational learning, which appears to be severely affected by the presence of a phase transition in the covering test. To this aim, three up-to-date relational learners have been applied to a wide range of artificial, fully relational learning problems. A first experimental observation is that the phase transition behaves as an attractor for relational learning; no matter which region the learning problem belongs to, all three learners produce hypotheses lying within or close to the phase transition region. Second, a &lt;em&gt;failure region&lt;/em&gt; appears. All three learners fail to learn any accurate hypothesis in this region. Quite surprisingly, the probability of failure does not systematically increase with the size of the underlying target concept: under some circumstances, longer concepts may be easier to accurately approximate than shorter ones. Some interpretations for these findings are proposed and discussed.
ID:323
CLASS:4
Title: Efficient algorithms for decision tree cross-validation
Abstract: Cross-validation is a useful and generally applicable technique often employed in machine learning, including decision tree induction. An important disadvantage of straightforward implementation of the technique is its computational overhead. In this paper we show that, for decision trees, the computational overhead of cross-validation can be reduced significantly by integrating the cross-validation with the normal decision tree induction process. We discuss how existing decision tree algorithms can be adapted to this aim, and provide an analysis of the speedups these adaptations may yield. We identify a number of parameters that influence the obtainable speedups, and validate and refine our analysis with experiments on a variety of data sets with two different implementations. Besides cross-validation, we also briefly explore the usefulness of these techniques for bagging. We conclude with some guidelines concerning when these optimizations should be considered.
ID:324
CLASS:4
Title: APE: learning user's habits to automate repetitive tasks
Abstract: The APE (Adaptive Programming Environment) project focuses on applying Machine Learning techniques to embed a software assistant into the VisualWorks Smalltalk interactive programming environment. The assistant is able to learn user's habits and to automatically suggest to perform repetitive tasks on his behalf. This paper describes our assistant and focuses more particularly on the learning issue. It explains why state-of-the-art Machine Learning algorithms fail to provide an efficient solution for learning user's habits, and shows, through experiments on real data that a new algorithm we have designed for this learning task, achieves better results than related algorithms.
ID:325
CLASS:4
Title: Machine learning comprehension grammars for ten languages
Abstract: Comprehension grammars for a sample of ten languages (English, Dutch, German, French, Spanish, Catalan, Russian, Chinese, Korean, and Japanese) were derived by machine learning from corpora of about 400 sentences. Key concepts in our learning theory are: probabilistic association of words and meanings, grammatical and semantical form generalization, grammar computations, congruence of meaning, and dynamical assignment of denotational value to a word.
ID:326
CLASS:4
Title: Inductive logic programming: derivations, successes and shortcomings
Abstract: Inductive Logic Programming (ILP) is a research area which investigates the construction of first-order definite clause theories from examples and background knowledge. ILP systems have been applied successfully in a number of real-world domains. These include the learning of structure-activity rules for drug design, finite-element mesh design rules, rules for primary-secondary prediction of protein structure and fault diagnosis rules for satellites. There is a well established tradition of learning-in-the-limit results in ILP. Recently some results within Valiant's PAC-learning framework have also been demonstrated for ILP systems. In this paper it is argued that algorithms can be directly derived from the formal specifications of ILP. This provides a common basis for Inverse Resolution, Explanation-Based Learning, Abduction and Relative Least General Generalisation. A new general-purpose, efficient approach to predicate invention is demonstrated. ILP is underconstrained by its logical specification. Therefore a brief overview of extra-logical constraints used in ILP systems is given. Some present limitations and research directions for the field are identified.
ID:327
CLASS:4
Title: Content based SMS spam filtering
Abstract: In the recent years, we have witnessed a dramatic increment in the volume of spam email. Other related forms of spam are increasingly revealing as a problem of importance, specially the spam on Instant Messaging services (the so called SPIM), and Short Message Service (SMS) or mobile spam.Like email spam, the SMS spam problem can be approached with legal, economic or technical measures. Among the wide range of technical measures, Bayesian filters are playing a key role in stopping email spam. In this paper, we analyze to what extent Bayesian filtering techniques used to block email spam, can be applied to the problem of detecting and stopping mobile spam. In particular, we have built two SMS spam test collections of significant size, in English and Spanish. We have tested on them a number of messages representation techniques and Machine Learning algorithms, in terms of effectiveness. Our results demonstrate that Bayesian filtering techniques can be effectively transferred from email to SMS spam.
ID:328
CLASS:4
Title: Sampling-based sequential subgroup mining
Abstract: Subgroup discovery is a learning task that aims at finding interesting rules from classified examples. The search is guided by a utility function, trading off the coverage of rules against their statistical unusualness. One shortcoming of existing approaches is that they do not incorporate prior knowledge. To this end a novel generic sampling strategy is proposed. It allows to turn pattern mining into an iterative process. In each iteration the focus of subgroup discovery lies on those patterns that are unexpected with respect to prior knowledge and previously discovered patterns. The result of this technique is a small diverse set of understandable rules that characterise a specified property of interest. As another contribution this article derives a simple connection between subgroup discovery and classifier induction. For a popular utility function this connection allows to apply any standard rule induction algorithm to the task of subgroup discovery after a step of stratified resampling. The proposed techniques are empirically compared to state of the art subgroup discovery algorithms.
ID:329
CLASS:4
Title: Fast Kernel Classifiers with Online and Active Learning
Abstract: Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efficient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention?This contribution proposes an empirical answer. We first present an online SVM algorithm based on this premise. LASVM yields competitive misclassification rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels.
ID:330
CLASS:4
Title: Distributional word clusters vs. words for text categorization
Abstract: We study an approach to text categorization that combines distributional clustering of words and a Support Vector Machine (SVM) classifier. This word-cluster representation is computed using the recently introduced Information Bottleneck method, which generates a compact and efficient representation of documents. When combined with the classification power of the SVM, this method yields high performance in text categorization. This novel combination of SVM with word-cluster representation is compared with SVM-based categorization using the simpler bag-of-words (BOW) representation. The comparison is performed over three known datasets. On one of these datasets (the 20 Newsgroups) the method based on word clusters significantly outperforms the word-based representation in terms of categorization accuracy or representation efficiency. On the two other sets (Reuters-21578 and WebKB) the word-based representation slightly outperforms the word-cluster representation. We investigate the potential reasons for this behavior and relate it to structural differences between the datasets.
ID:331
CLASS:4
Title: Introduction to special issue on machine learning approaches to shallow parsing
Abstract: This article introduces the problem of partial or shallow parsing (assigning partial syntactic structure to sentences) and explains why it is an important natural language processing (NLP) task. The complexity of the task makes Machine Learning an attractive option in comparison to the handcrafting of rules. On the other hand, because of the same task complexity, shallow parsing makes an excellent benchmark problem for evaluating machine learning algorithms. We sketch the origins of shallow parsing as a specific task for machine learning of language, and introduce the articles accepted for this special issue, a representative sample of current research in this area. Finally, future directions for machine learning of shallow parsing are suggested.
ID:332
CLASS:4
Title: A new approximate maximal margin classification algorithm
Abstract: A new incremental learning algorithm is described which approximates the maximal margin hyperplane w.r.t. norm p &ge; 2 for a set of linearly separable data. Our algorithm, called ALMA_p (Approximate Large Margin algorithm w.r.t. norm p), takes O( (p-1) / (&alpha;2 &gamma;2 ) ) corrections to separate the data with p-norm margin larger than (1-&alpha;)&gamma;, where g is the (normalized) p-norm margin of the data. ALMA_p avoids quadratic (or higher-order) programming methods. It is very easy to implement and is as fast as on-line algorithms, such as Rosenblatt's Perceptron algorithm. We performed extensive experiments on both real-world and artificial datasets. We compared ALMA_2 (i.e., ALMA_p with p = 2) to standard Support vector Machines (SVM) and to two incremental algorithms: the Perceptron algorithm and Li and Long's ROMMA. The accuracy levels achieved by ALMA_2 are superior to those achieved by the Perceptron algorithm and ROMMA, but slightly inferior to SVM's. On the other hand, ALMA_2 is quite faster and easier to implement than standard SVM training algorithms. When learning sparse target vectors, ALMA_p with p &gt; 2 largely outperforms Perceptron-like algorithms, such as ALMA_2.
ID:333
CLASS:4
Title: Machine learning in the liberal arts curriculum
Abstract: Machine learning is typically considered a graduate-level course with an artificial intelligence course as a prerequisite. However, it does not need to be positioned this way, and in the liberal arts curriculum in particular, there are advantages to offering this course to undergraduate students. An undergraduate course in machine learning is easily and naturally structured to introduce research concepts and to work within a research paradigm. It also introduces the use of statistics, reflected both in the machine learning systems studied and in the experimental methodology. Furthermore, it allows for an interdisciplinary perspective, as students can be encouraged to work on problems from other departments in the college. This paper describes the benefits of offering such a course and outlines a course structure and resources for doing so.
ID:334
CLASS:4
Title: Extracting key-substring-group features for text classification
Abstract: In many text classification applications, it is appealing to take every document as a string of characters rather than a bag of words. Previous research studies in this area mostly focused on different variants of generative Markov chain models. Although discriminative machine learning methods like Support Vector Machine (SVM) have been quite successful in text classification with word features, it is neither effective nor efficient to apply them straightforwardly taking all substrings in the corpus as features. In this paper, we propose to partition all substrings into statistical equivalence groups, and then pick those groups which are important (in the statistical sense) as features (named key-substring-group features) for text classification. In particular, we propose a suffix tree based algorithm that can extract such features in linear time (with respect to the total number of characters in the corpus). Our experiments on English, Chinese and Greek datasets show that SVM with key-substring-group features can achieve outstanding performance for various text classification tasks.
ID:335
CLASS:4
Title: Learning Ensembles from Bites: A Scalable and Accurate Approach
Abstract: Bagging and boosting are two popular ensemble methods that typically achieve better accuracy than a single classifier. These techniques have limitations on massive data sets, because the size of the data set can be a bottleneck. Voting many classifiers built on small subsets of data ("pasting small votes") is a promising approach for learning from massive data sets, one that can utilize the power of boosting and bagging. We propose a framework for building hundreds or thousands of such classifiers on small subsets of data in a distributed environment. Experiments show this approach is fast, accurate, and scalable.
ID:336
CLASS:4
Title: Learning apprentice system for turbine modelling
Abstract: A learning apprentice system is presented, which learns from examples extracted from user dialogues. The system provides an interface between the user and the turbine modeller. While a dialogue is carried out between the user and the turbine modelling software, the system observes the dialogues and whenever a new example is observed which performs a task completely, the system tries to learn it. The learning methodology used by the system is described and various drawbacks are pointed out. A new learning methodology is proposed which easily overcomes the problems faced by the earlier methodology.
ID:337
CLASS:4
Title: Finknn: a fuzzy interval number k-nearest neighbor classifier for prediction of sugar production from populations of samples
Abstract: This work introduces FINkNN, a k-nearest-neighbor classifier operating over the metric lattice of conventional interval-supported convex fuzzy sets. We show that for problems involving populations of measurements, data can be represented by fuzzy interval numbers (FINs) and we present an algorithm for constructing FINs from such populations. We then present a lattice-theoretic metric distance between FINs with arbitrary-shaped membership functions, which forms the basis for FINkNN's similarity measurements. We apply FINkNN to the task of predicting annual sugar production based on populations of measurements supplied by Hellenic Sugar Industry. We show that FINkNN improves prediction accuracy on this task, and discuss the broader scope and potential utility of these techniques.
ID:338
CLASS:4
Title: Ultraconservative online algorithms for multiclass problems
Abstract: In this paper we study a paradigm to generalize online classification algorithms for binary classification problems to multiclass problems. The particular hypotheses we investigate maintain one prototype vector per class. Given an input instance, a multiclass hypothesis computes a similarity-score between each prototype and the input instance and sets the predicted label to be the index of the prototype achieving the highest similarity. To design and analyze the learning algorithms in this paper we introduce the notion of ultraconservativeness. Ultraconservative algorithms are algorithms that update only the prototypes attaining similarity-scores which are higher than the score of the correct label's prototype. We start by describing a family of additive ultraconservative algorithms where each algorithm in the family updates its prototypes by finding a feasible solution for a set of linear constraints that depend on the instantaneous similarity-scores. We then discuss a specific online algorithm that seeks a set of prototypes which have a small norm. The resulting algorithm, which we term MIRA (for Margin Infused Relaxed Algorithm) is ultraconservative as well. We derive mistake bounds for all the algorithms and provide further analysis of MIRA using a generalized notion of the margin for multiclass problems. We discuss the form the algorithms take in the binary case and show that all the algorithms from the first family reduce to the Perceptron algorithm while MIRA provides a new Perceptron-like algorithm with a margin-dependent learning rate. We then return to multiclass problems and describe an analogous multiplicative family of algorithms with corresponding mistake bounds. We end the formal part by deriving and analyzing a multiclass version of Li and Long's ROMMA algorithm. We conclude with a discussion of experimental results that demonstrate the merits of our algorithms.
ID:339
CLASS:4
Title: Support vector machine active learning with applications to text classification
Abstract: Support vector machines have met with significant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classified in advance. In many settings, we also have the option of using &lt;em&gt;pool-based active learning&lt;/em&gt;. Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce a new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm using the notion of a &lt;em&gt;version space&lt;/em&gt;. We present experimental results showing that employing our active learning method can significantly reduce the need for labeled training instances in both the standard inductive and transductive settings.
ID:340
CLASS:4
Title: A refinement approach to handling model misfit in text categorization
Abstract: Text categorization or classification is the automated assigning of text documents to pre-defined classes based on their contents. This problem has been studied in information retrieval, machine learning and data mining. So far, many effective techniques have been proposed. However, most techniques are based on some underlying models and/or assumptions. When the data fits the model well, the classification accuracy will be high. However, when the data does not fit the model well, the classification accuracy can be very low. In this paper, we propose a refinement approach to dealing with this problem of model misfit. We show that we do not need to change the classification technique itself (or its underlying model) to make it more flexible. Instead, we propose to use successive refinements of classification on the training data to correct the model misfit. We apply the proposed technique to improve the classification performance of two simple and efficient text classifiers, the Rocchio classifier and the na&iuml;ve Bayesian classifier. These techniques are suitable for very large text collections because they allow the data to reside on disk and need only one scan of the data to build a text classifier. Extensive experiments on two benchmark document corpora show that the proposed technique is able to improve text categorization accuracy of the two techniques dramatically. In particular, our refined model is able to improve the na&iuml;ve Bayesian or Rocchio classifier's prediction performance by 45% on average.
ID:341
CLASS:4
Title: Classification and regression: money *can* grow on trees
Abstract: With over 800 million pages covering most areas of human endeavor, the World-wide Web is a fertile ground for data mining research to make a difference to the effectiveness of information search. Today, Web surfers access the Web through two dominant interfaces clicking on hyperlinks and searching via keyword queries This process is often tentative and unsatisfactory Better support is needed for expressing one's information need and dealing with a search result in more structured ways than available now. Data mining and machine learning have significant roles to play towards this end.In this paper we will survey recent advances in learning and mining problems related to hypertext in general and the Web in particular. We will review the continuum of supervised to semi-supervised to unsupervised learning problems, highlight the specific challenges which distinguish data mining in the hypertext domain from data mining in the context of data warehouses, and summarize the key areas of recent and ongoing research.
ID:342
CLASS:4
Title: An evaluation of statistical spam filtering techniques
Abstract: This paper evaluates five supervised learning methods in the context of statistical spam filtering. We study the impact of different feature pruning methods and feature set sizes on each learner's performance using cost-sensitive measures. It is observed that the significance of feature selection varies greatly from classifier to classifier. In particular, we found support vector machine, AdaBoost, and maximum entropy model are top performers in this evaluation, sharing similar characteristics: not sensitive to feature selection strategy, easily scalable to very high feature dimension, and good performances across different datasets. In contrast, naive Bayes, a commonly used classifier in spam filtering, is found to be sensitive to feature selection methods on small feature set, and fails to function well in scenarios where false positives are penalized heavily. The experiments also suggest that aggressive feature pruning should be avoided when building filters to be used in applications where legitimate mails are assigned a cost much higher than spams (such as &lambda; = 999), so as to maintain a better-than-baseline performance. An interesting finding is the effect of mail headers on spam filtering, which is often ignored in previous studies. Experiments show that classifiers using features from message header alone can achieve comparable or better performance than filters utilizing body features only. This implies that message headers can be reliable and powerfully discriminative feature sources for spam filtering.
ID:343
CLASS:4
Title: Co-EM support vector learning
Abstract: Multi-view algorithms, such as co-training and co-EM, utilize unlabeled data when the available attributes can be split into independent and compatible subsets. Co-EM outperforms co-training for many problems, but it requires the underlying learner to estimate class probabilities, and to learn from probabilistically labeled data. Therefore, co-EM has so far only been studied with naive Bayesian learners. We cast linear classifiers into a probabilistic framework and develop a co-EM version of the Support Vector Machine. We conduct experiments on text classification problems and compare the family of semi-supervised support vector algorithms under different conditions, including violations of the assumptions underlying multi-view learning. For some problems, such as course web page classification, we observe the most accurate results reported so far.
ID:344
CLASS:4
Title: A multistrategy approach for digital text categorization from imbalanced documents
Abstract: The goal of the research described here is to develop a multistrategy classifier system that can be used for document categorization. The system automatically discovers classification patterns by applying several empirical learning methods to different representations for preclassified documents belonging to an imbalanced sample. The learners work in a parallel manner, where each learner carries out its own feature selection based on evolutionary techniques and then obtains a classification model. In classifying documents, the system combines the predictions of the learners by applying evolutionary techniques as well. The system relies on a modular, flexible architecture that makes no assumptions about the design of learners or the number of learners available and guarantees the independence of the thematic domain.
ID:345
CLASS:4
Title: Optimizing algorithms for pronoun resolution
Abstract: The paper aims at a deeper understanding of several well-known algorithms and proposes ways to optimize them. It describes and discusses factors and strategies of factor interaction used in the algorithms. The factors used in the algorithms and the algorithms themselves are evaluated on a German corpus annotated with syntactic and coreference information (Negra) (Skut et al., 1997). A common format for pronoun resolution algorithms with several open parameters is proposed, and the parameter settings optimal on the evaluation data are given.
ID:346
CLASS:4
Title: Learning methods to combine linguistic indicators: improving aspectual classification and revealing linguistic insights
Abstract: Aspectual classification maps verbs to a small set of primitive categories in order to reason about time. This classification is necessary for interpreting temporal modifiers and assessing temporal relationships, and is therefore a required component for many natural language applications.A verb's aspectual category can be predicted by co-occurrence frequencies between the verb and certain linguistic modifiers. These frequency measures, called linguistic indicators, are chosen by linguistic insights. However, linguistic indicators used in isolation are predictively incomplete, and are therefore insufficient when used individually.In this article, we compare three supervised machine learning methods for combining multiple linguistic indicators for aspectual classification: decision trees, genetic programming, and logistic regression. A set of 14 indicators are combined for classification according to two aspectual distinctions. This approach improves the classification performance for both distinctions, as evaluated over unrestricted sets of verbs occurring across two corpora. This demonstrates the effectiveness of the linguistic indicators and provides a much-needed full-scale method for automatic aspectual classification. Moreover, the models resulting from learning reveal several linguistic insights that are relevant to aspectual classification. We also compare supervised learning methods with an unsupervised method for this task.
ID:347
CLASS:4
Title: Generalization error bounds for Bayesian mixture algorithms
Abstract: Bayesian approaches to learning and estimation have played a significant role in the Statistics literature over many years. While they are often provably optimal in a frequentist setting, and lead to excellent performance in practical applications, there have not been many precise characterizations of their performance for finite sample sizes under general conditions. In this paper we consider the class of Bayesian mixture algorithms, where an estimator is formed by constructing a data-dependent mixture over some hypothesis space. Similarly to what is observed in practice, our results demonstrate that mixture approaches are particularly robust, and allow for the construction of highly complex estimators, while avoiding undesirable overfitting effects. Our results, while being data-dependent in nature, are insensitive to the underlying model assumptions, and apply whether or not these hold. At a technical level, the approach applies to unbounded functions, constrained only by certain moment conditions. Finally, the bounds derived can be directly applied to non-Bayesian mixture approaches such as Boosting and Bagging.
ID:348
CLASS:4
Title: Greedy algorithms for classification\&mdash;consistency, convergence rates, and adaptivity
Abstract: Many regression and classification algorithms proposed over the years can be described as greedy procedures for the stagewise minimization of an appropriate cost function. Some examples include additive models, matching pursuit, and boosting. In this work we focus on the classification problem, for which many recent algorithms have been proposed and applied successfully. For a specific regularized form of greedy stagewise optimization, we prove consistency of the approach under rather general conditions. Focusing on specific classes of problems we provide conditions under which our greedy procedure achieves the (nearly) minimax rate of convergence, implying that the procedure cannot be improved in a worst case setting. We also construct a fully adaptive procedure, which, without knowing the smoothness parameter of the decision boundary, converges at the same rate as if the smoothness parameter were known.
ID:349
CLASS:4
Title: Efficiently handling feature redundancy in high-dimensional data
Abstract: High-dimensional data poses a severe challenge for data mining. Feature selection is a frequently used technique in pre-processing high-dimensional data for successful data mining. Traditionally, feature selection is focused on removing irrelevant features. However, for high-dimensional data, removing redundant features is equally critical. In this paper, we provide a study of feature redundancy in high-dimensional data and propose a novel correlation-based approach to feature selection within the filter model. The extensive empirical study using real-world data shows that the proposed approach is efficient and effective in removing redundant and irrelevant features.
ID:350
CLASS:4
Title: A divisive information theoretic feature clustering algorithm for text classification
Abstract: High dimensionality of text can be a deterrent in applying complex learners such as Support Vector Machines to the task of text classification. Feature clustering is a powerful alternative to feature selection for reducing the dimensionality of text data. In this paper we propose a new information-theoretic divisive algorithm for feature/word clustering and apply it to text classification. Existing techniques for such "distributional clustering" of words are agglomerative in nature and result in (i) sub-optimal word clusters and (ii) high computational cost. In order to explicitly capture the optimality of word clusters in an information theoretic framework, we first derive a global criterion for feature clustering. We then present a fast, divisive algorithm that monotonically decreases this objective function value. We show that our algorithm minimizes the "within-cluster Jensen-Shannon divergence" while simultaneously maximizing the "between-cluster Jensen-Shannon divergence". In comparison to the previously proposed agglomerative strategies our divisive algorithm is much faster and achieves comparable or higher classification accuracies. We further show that feature clustering is an effective technique for building smaller class models in hierarchical classification. We present detailed experimental results using Naive Bayes and Support Vector Machines on the 20Newsgroups data set and a 3-level hierarchy of HTML documents collected from the Open Directory project (www.dmoz.org).
ID:351
CLASS:4
Title: Kernel methods for relation extraction
Abstract: We present an application of kernel methods to extracting relations from unstructured natural language sources. We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels. We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting &lt;tt&gt;person-affiliation&lt;/tt&gt; and &lt;tt&gt;organization-location&lt;/tt&gt; relations from text. We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results.
ID:352
CLASS:4
Title: Content-based book recommending using learning for text categorization
Abstract: Recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user's likes and dislikes.  Most existing recommender systems use collaborative filtering methods that base recommendations on other users' preferences. By contrast,content-based methods use information about an item itself to make suggestions.This approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations. We describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization.  Initial experimental results demonstrate that this approach can produce accurate recommendations.
ID:353
CLASS:4
Title: Bayesian inductive logic programming
Abstract: Inductive Logic Programming (ILP) involves the construction of first-order definite clause theories from examples and background knowledge. Unlike both traditional Machine Learning and Computational Learning Theory, ILP is based on lock-step development of Theory, Implementations and Applications. ILP systems have successful applications in the learning of structure-activity rules for drug design, semantic grammars rules, finite element mesh design rules and rules for prediction of protein structure and mutagenic molecules. The strong applications in ILP can be contrasted with relatively weak PAC-learning results (even highly restricted forms of logic programs are known to be prediction-hard). It has been recently argued that the mismatch is due to distributional assumptions made in  application domains. These assumptions can be modelled as a Bayesian prior probability representing subjective degrees of belief. Other authors have argued for the use of Bayesian prior distributions for reasons different to those here, though this has not lead to a new model of polynomial-time learnability. Incorporation of Bayesian prior distributions over time-bounded hypotheses in PAC leads to a new model called U-learnability. It is argued that U-learnability is more appropriate than PAC for Universal (Turing computable) languages. Time-bounded logic programs have been shown to be polynomially U-learnable under certain distributions. The use of time bounded hypotheses enforces decidability and allows a unified characterization of speed-up learning and inductive learning. U-learnability has as special cases PAC and Natarajan's model of speed-up learning.
ID:354
CLASS:4
Title: The LEM3 implementation of learnable evolution model and its testing on complex function optimization problems
Abstract: Learnable Evolution Model (LEM) is a form of non-Darwinian evolutionary computation that employs machine learning to guide evolutionary processes. Its main novelty are new type of operators for creating new individuals, specifically, hypothesis generation, which learns rules indicating subareas in the search space that likely contain the optimum, and hypothesis instantiation, which populates these subspaces with new individuals. This paper briefly describes the newest and most advanced implementation of learnable evolution, LEM3, its novel features, and results from its comparison with a conventional, Darwinian-type evolutionary computation program (EA), a cultural evolution algorithm (CA), and the estimation of distribution algorithm (EDA) on selected function optimization problems (with the number of variables varying up to 1000). In every experiment, LEM3 outperformed the compared programs in terms of the evolution length (the number of fitness evaluations needed to achieved a desired solution), sometimes more than by one order of magnitude.
ID:355
CLASS:4
Title: Large-scale text categorization by batch mode active learning
Abstract: Large-scale text categorization is an important research topic for Web data mining. One of the challenges in large-scale text categorization is how to reduce the human efforts in labeling text documents for building reliable classification models. In the past, there have been many studies on applying active learning methods to automatic text categorization, which try to select the most informative documents for labeling manually. Most of these studies focused on selecting a single unlabeled document in each iteration. As a result, the text categorization model has to be retrained after each labeled document is solicited. In this paper, we present a novel active learning algorithm that selects a batch of text documents for labeling manually in each iteration. The key of the batch mode active learning is how to reduce the redundancy among the selected examples such that each example provides unique information for model updating. To this end, we use the Fisher information matrix as the measurement of model uncertainty and choose the set of documents to effectively maximize the Fisher information of a classification model. Extensive experiments with three different datasets have shown that our algorithm is more effective than the state-of-the-art active learning techniques for text categorization and can be a promising tool toward large-scale text categorization for World Wide Web documents.
ID:356
CLASS:4
Title: Randomized Variable Elimination
Abstract: Variable selection, the process of identifying input variables that are relevant to a particular learning problem, has received much attention in the learning community. Methods that employ a learning algorithm as a part of the selection process (wrappers) have been shown to outperform methods that select variables independently from the learning algorithm (filters), but only at great computational expense. We present a randomized wrapper algorithm whose computational requirements are within a constant factor of simply learning in the presence of all input variables, provided that the number of relevant variables is small and known in advance. We then show how to remove the latter assumption, and demonstrate performance on several problems.
ID:357
CLASS:4
Title: Redundant feature elimination for multi-class problems
Abstract: We consider the problem of eliminating redundant Boolean features for a given data set, where a feature is redundant if it separates the classes less well than another feature or set of features. Lavra&ccaron; et al. proposed the algorithm REDUCE that works by pairwise comparison of features, i.e., it eliminates a feature if it is redundant with respect to another feature. Their algorithm operates in an ILP setting and is restricted to two-class problems. In this paper we improve their method and extend it to multiple classes. Central to our approach is the notion of a neighbourhood of examples: a set of examples of the same class where the number of different features between examples is relatively small. Redundant features are eliminated by applying a revised version of the REDUCE method to each pair of neighbourhoods of different class. We analyse the performance of our method on a range of data sets.
ID:358
CLASS:4
Title: Learning from imbalanced data sets with boosting and data generation: the DataBoost-IM approach
Abstract: Learning from imbalanced data sets, where the number of examples of one (majority) class is much higher than the others, presents an important challenge to the machine learning community. Traditional machine learning algorithms may be biased towards the majority class, thus producing poor predictive accuracy over the minority class. In this paper, we describe a new approach that combines boosting, an ensemble-based learning algorithm, with data generation to improve the predictive power of classifiers against imbalanced data sets consisting of two classes. In the DataBoost-IM method, hard examples from both the majority and minority classes are identified during execution of the boosting algorithm. Subsequently, the hard examples are used to separately generate synthetic examples for the majority and minority classes. The synthetic data are then added to the original training set, and the class distribution and the total weights of the different classes in the new training set are rebalanced. The DataBoost-IM method was evaluated, in terms of the F-measures, G-mean and overall accuracy, against seventeen highly and moderately imbalanced data sets using decision trees as base classifiers. Our results are promising and show that the DataBoost-IM method compares well in comparison with a base classifier, a standard benchmarking boosting algorithm and three advanced boosting-based algorithms for imbalanced data set. Results indicate that our approach does not sacrifice one class in favor of the other, but produces high predictions against both minority and majority classes.
ID:359
CLASS:4
Title: Netman: a learning network traffic controller
Abstract: One of the goals of Machine Learning is the production of software that can improve itself. Such software can learn from experience and adapt to changing situations and requirements. In addition, such software can refine its knowledge-base, perhaps leading to a level of expertise beyond that of human experts.This paper describes NETMAN, a knowledge-based program that uses a machine learning technique, Knowledge-based Learning, in the domain of Network Traffic Control. NETMAN's task is to maximize call completion in a circuit-switched telecommunications network. NETMAN learns from its own experiences and by observing the actions of other agents.NETMAN is one of the components of ILS (Integrated Learning System), which contains implementations of several learning paradigms working together to improve problem-solving performance. NETMAN combines two machine learning paradigms: Explanation-Based Learning and Empirical Learning.
ID:360
CLASS:4
Title: Speedup learning for repair-based search by identifying redundant steps
Abstract: Repair-based search algorithms start with an initial solution and attempt to improve it by iteratively applying repair operators. Such algorithms can often handle large-scale problems that may be difficult for systematic search algorithms. Nevertheless, the computational cost of solving such problems is still very high. We observed that many of the repair steps applied by such algorithms are redundant in the sense that they do not eventually contribute to finding a solution. Such redundant steps are particularly harmful in repair-based search, where each step carries high cost due to the very high branching factor typically associated with it. Accurately identifying and avoiding such redundant steps would result in faster local search without harming the algorithm's problem-solving ability. In this paper we propose a speedup learning methodology for attaining this goal. It consists of the following steps: defining the concept of a &lt;em&gt;redundant step&lt;/em&gt;; acquiring this concept during off-line learning by analyzing solution paths for training problems, tagging all the steps along the paths according to the redundancy definition and using an induction algorithm to infer a classifier based on the tagged examples; and using the acquired classifier to filter out redundant steps while solving unseen problems. Our algorithm was empirically tested on instances of real-world employee timetabling problems (ETP). The problem solver to be improved is based on one of the best methods for solving some large ETP instances. Our results show a significant improvement in speed for test problems that are similar to the given example problems.
ID:361
CLASS:4
Title: Scalability and efficiency in multi-relational data mining
Abstract: Efficiency and Scalability have always been important concerns in the field of data mining, and are even more so in the multi-relational context, which is inherently more complex. The issue has been receiving an increasing amount of attention during the last few years, and quite a number of theoretical results, algorithms and implementations have been presented that explicitly aim at improving the efficiency and Scalability of multi-relational data mining approaches. With this article we attempt to present a structured overview.
ID:362
CLASS:4
Title: An introduction to variable and feature selection
Abstract: Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.
ID:363
CLASS:4
Title: Stopping criterion for boosting based data reduction techniques: from binary to multiclass problem
Abstract: So far, boosting has been used to improve the quality of moderately accurate learning algorithms, by weighting and combining many of their weak hypotheses into a final classifier with theoretically high accuracy. In a recent work (Sebban, Nock and Lallich, 2001), we have attempted to adapt boosting properties to data reduction techniques. In this particular context, the objective was not only to improve the success rate, but also to reduce the time and space complexities due to the storage requirements of some costly learning algorithms, such as nearest-neighbor classifiers. In that framework, each weak hypothesis, which is usually built and weighted from the learning set, is replaced by a single learning instance. The weight given by boosting defines in that case the relevance of the instance, and a statistical test allows one to decide whether it can be discarded without damaging further classification tasks. In Sebban, Nock and Lallich (2001), we addressed problems with two classes. It is the aim of the present paper to relax the class constraint, and extend our contribution to multiclass problems. Beyond data reduction, experimental results are also provided on twenty-three datasets, showing the benefits that our boosting-derived weighting rule brings to weighted nearest neighbor classifiers.
ID:364
CLASS:4
Title: Tracking the best linear predictor
Abstract: In most on-line learning research the total on-line loss of the algorithm is compared to the total loss of the best off-line predictor u from a comparison class of predictors. We call such bounds static bounds. The interesting feature of these bounds is that they hold for an arbitrary sequence of examples. Recently some work has been done where the predictor ut at each trial t is allowed to change with time, and the total on-line loss of the algorithm is compared to the sum of the losses of ut at each trial plus the total "cost" for shifting to successive predictors. This is to model situations in which the examples change over time, and different predictors from the comparison class are best for different segments of the sequence of examples. We call such bounds shifting bounds. They hold for arbitrary sequences of examples and arbitrary sequences of predictors.Naturally shifting bounds are much harder to prove. The only known bounds are for the case when the comparison class consists of a sequences of experts or boolean disjunctions. In this paper we develop the methodology for lifting known static bounds to the shifting case. In particular we obtain bounds when the comparison class consists of linear neurons (linear combinations of experts). Our essential technique is to project the hypothesis of the static algorithm at the end of each trial into a suitably chosen convex region. This keeps the hypothesis of the algorithm well-behaved and the static bounds can be converted to shifting bounds.
ID:365
CLASS:4
Title: Email classification with co-training
Abstract: The main problems in text classification are lack of labeled data, as well as the cost of labeling the unlabeled data. We address these problems by exploring co-training - an algorithm that uses unlabeled data along with a few labeled examples to boost the performance of a classifier. We experiment with co-training on the email domain. Our results show that the performance of co-training depends on the learning algorithm it uses. In particular, Support Vector Machines significantly outperforms Naive Bayes on email classification.
ID:366
CLASS:4
Title: Integration issue in knowledge acquisition systems
Abstract: Ever since knowledge acquisition systems have been applied to real world problems, the issue of integration has become important (Gaines 88). Most often, the following types of integration are considered:&amp;bull; Systems are to be integrated: the knowledge acquisition system is more closely linked with an expert system shell (Eshelman et al. 87), or a database system is linked to the knowledge acquisition system in order to read data of a domain.&amp;bull; Various sources of knowledge are to be integrated: text files, data files, statistics, rules and facts all contain knowledge about a domain and should be handled by the same system (Gaines 88).&amp;bull; The represented knowledge of various experts is to be integrated either into one consistent domain model or into a model which shows the conflicting views of the domain (Shaw 88).&amp;bull; Diverse knowledge sources with their respective representations are to be integrated, e.g. a taxonomy of domain concepts, possible values of attributes, well-formedness conditions of facts and rules.&amp;bull; Diverse tasks of knowledge acquisition are to be integrated: declaring epistemic primitives, defining concepts, adding facts and rules, deducing new facts from rules, dealing with inconsistencies, changing the terminology, changing facts and rules, grouping facts or rules together, presenting possible operations, showing views of the represented domain model, indicating consequences of an operation to the user. A particular topic of task integration is the integration of machine learning into a knowledge acquisition system.In this paper, we want to discuss only the last two integration problems and present the solutions we implemented in the BLIP system, currently under development at the Technical University of Berlin1. First, we give a short overview of knowledge acquisition tasks and indicate which tasks some prototype systems can handle. As we will see, the integration of machine learning into knowledge acquisition is still not yet well achieved. Second, we discuss the integration of tasks. In particular, the integration of machine learning into knowledge acquisition is discussed in some detail. The architecture of BLIP illustrates the paradigm of cooperative balanced modeling of both system and user. Third, we investigate the integration of knowledge sources. The integrity between diverse knowledge sources, the propagation of consequences of the operation to all relevant knowledge sources, and the interpretability of a component's results by other components are the 3 issues there. In BLIP, the knowledge needed for the learning task is also integrated into the domain knowledge. This gives BLIP the power of closed-loop learning (Michalski 87). Fourth, we describe the integration of BLIP's learning into knowledge revision in more detail.
ID:367
CLASS:4
Title: Boosting to correct inductive bias in text classification
Abstract: This paper studies the effects of boosting in the context of different classification methods for text categorization, including Decision Trees, Naive Bayes, Support Vector Machines (SVMs) and a Rocchio-style classifier. We identify the inductive biases of each classifier and explore how boosting, as an error-driven resampling mechanism, reacts to those biases. Our experiments on the Reuters-21578 benchmark show that boosting is not effective in improving the performance of the base classifiers on common categories. However, the effect of boosting for rare categories varies across classifiers: for SVMs and Decision Trees, we achieved a 13-17% performance improvement in macro-averaged F1 measure, but did not obtain substantial improvement for the other two classifiers. This interesting finding of boosting on rare categories has not been reported before.
ID:368
CLASS:4
Title: Learning and making decisions when costs and probabilities are both unknown
Abstract: In many data mining domains, misclassification costs are different for different examples, in the same way that class membership probabilities are example-dependent. In these domains, both costs and probabilities are unknown for test examples, so both cost estimators and probability estimators must be learned. After discussing how to make optimal decisions given cost and probability estimates, we present decision tree and naive Bayesian learning methods for obtaining well-calibrated probability estimates. We then explain how to obtain unbiased estimators for example-dependent costs, taking into account the difficulty that in general, probabilities and costs are not independent random variables, and the training examples for which costs are known are not representative of all examples. The latter problem is called sample selection bias in econometrics. Our solution to it is based on Nobel prize-winning work due to the economist James Heckman. We show that the methods we propose perform better than MetaCost and all other known methods, in a comprehensive experimental comparison that uses the well-known, large, and challenging dataset from the KDD'98 data mining contest.
ID:369
CLASS:4
Title: Intelligent information triage
Abstract: In many applications, large volumes of time-sensitive textual information require triage: rapid, approximate prioritization for subsequent action. In this paper, we explore the use of prospective indications of the importance of a time-sensitive document, for the purpose of producing better document filtering or ranking. By prospective, we mean importance that could be assessed by actions that occur in the future. For example, a news story may be assessed (retrospectively) as being important, based on events that occurred after the story appeared, such as a stock price plummeting or the issuance of many follow-up stories. If a system could anticipate (prospectively) such occurrences, it could provide a timely indication of importance. Clearly, perfect prescience is impossible. However, sometimes there is sufficient correlation between the content of an information item and the events that occur subsequently. We describe a process for creating and evaluating approximate information-triage procedures that are based on prospective indications. Unlike many information-retrieval applications for which document labeling is a laborious, manual process, for many prospective criteria it is possible to build very large, labeled, training corpora automatically. Such corpora can be used to train text classification procedures that will predict the (prospective) importance of each document. This paper illustrates the process with two case studies, demonstrating the ability to predict whether a news story will be followed by many, very similar news stories, and also whether the stock price of one or more companies associated with a news story will move significantly following the appearance of that story. We conclude by discussing how the comprehensibility of the learned classifiers can be critical to success.}
ID:370
CLASS:4
Title: Applications of inductive logic programming
Abstract: Some applications of Inductive Logic Programming (ILP) are presented. Those applications are chosen that specifically benefit from relational descriptions generated by ILP programs, and from ILP's ability to accommodate background knowledge. Applications included are: drug design, predicting the secondary structure of proteins, and design of finite-element meshes. Some other applications are briefly described. The practical advantages and disadvantages of ILP learning are discussed.
ID:371
CLASS:4
Title: An intrinsic reward mechanism for efficient exploration
Abstract: How should a reinforcement learning agent act if its sole purpose is to efficiently learn an optimal policy for later use? In other words, how should it explore, to be able to exploit later? We formulate this problem as a Markov Decision Process by explicitly modeling the internal state of the agent and propose a principled heuristic for its solution. We present experimental results in a number of domains, also exploring the algorithm's use for learning a policy for a skill given its reward function---an important but neglected component of skill discovery.
ID:372
CLASS:4
Title: MISSL: multiple-instance semi-supervised learning
Abstract: There has been much work on applying multiple-instance (MI) learning to content-based image retrieval (CBIR) where the goal is to rank all images in a known repository using a small labeled data set. Most existing MI learning algorithms are non-transductive in that the images in the repository serve only as test data and are not used in the learning process. We present MISSL (Multiple-Instance Semi-Supervised Learning) that transforms any MI problem into an input for a graph-based single-instance semi-supervised learning method that encodes the MI aspects of the problem simultaneously working at both the bag and point levels. Unlike most prior MI learning algorithms, MISSL makes use of the unlabeled data.
ID:373
CLASS:4
Title: OLLIE: on-line learning for information extraction
Abstract: This paper reports work aimed at developing an open, distributed learning environment, OLLIE, where researchers can experiment with different Machine Learning (ML) methods for Information Extraction. Once the required level of performance is reached, the ML algorithms can be used to speed up the manual annotation process. OLLIE uses a browser client while data storage and ML training is performed on servers. The different ML algorithms use a unified programming interface; the integration of new ones is straightforward.
ID:374
CLASS:4
Title: Constructive induction and genetic algorithms for learning concepts with complex interaction
Abstract: Constructive Induction is the process of transforming the original representation of hard concepts with complex interaction into a representation that highlights regularities. Most Constructive Induction methods apply a greedy strategy to find interacting attributes and then construct functions over them. This approach fails when complex interaction exists among attributes and the search space has high variation. In this paper, we illustrate the importance of applying Genetic Algorithms as a global search strategy for these methods and present MFE2/GA1, while comparing it with other GA-based Constructive Induction methods. We empirically analyze our Genetic Algorithm's operators and compare MFE2/GA with greedy-based methods. We also performed experiments to evaluate the presented method when concept has attributes participating in more than one complex interaction. In experiments that are conducted, MFE2/GA successfully finds interacting attributes and constructs functions to represent interactions. Results show the advantage of using Genetic Algorithms for Constructive Induction when compared with greedy-based methods.
ID:375
CLASS:4
Title: Incorporating contextual information in recommender systems using a multidimensional approach
Abstract: The article presents a multidimensional (MD) approach to recommender systems that can provide recommendations based on additional contextual information besides the typical information on users and items used in most of the current recommender systems. This approach supports multiple dimensions, profiling information, and hierarchical aggregation of recommendations. The article also presents a multidimensional rating estimation method capable of selecting two-dimensional segments of ratings pertinent to the recommendation context and applying standard collaborative filtering or other traditional two-dimensional rating estimation techniques to these segments. A comparison of the multidimensional and two-dimensional rating estimation approaches is made, and the tradeoffs between the two are studied. Moreover, the article introduces a combined rating estimation method, which identifies the situations where the MD approach outperforms the standard two-dimensional approach and uses the MD approach in those situations and the standard two-dimensional approach elsewhere. Finally, the article presents a pilot empirical study of the combined approach, using a multidimensional movie recommender system that was developed for implementing this approach and testing its performance.
ID:376
CLASS:4
Title: Model Averaging for Prediction with Discrete Bayesian Networks
Abstract: In this paper we consider the problem of performing Bayesian model-averaging over a class of discrete Bayesian network structures consistent with a partial ordering and with bounded in-degree k. We show that for N nodes this class contains in the worst-case at least &lt;img align=middle src=dash04a-omega.jpeg alt="omega eq"&gt; distinct network structures, and yet model averaging over these structures can be performed using &lt;img align=middle src=dash04a-bigo.jpeg alt="bigo eq"&gt; operations. Furthermore we show that there exists a single Bayesian network that defines a joint distribution over the variables that is equivalent to model averaging over these structures. Although constructing this network is computationally prohibitive, we show that it can be approximated by a tractable network, allowing approximate model-averaged probability calculations to be performed in O(N) time. Our result also leads to an exact and linear-time solution to the problem of averaging over the 2N possible feature sets in a naive Bayes model, providing an exact Bayesian solution to the troublesome feature-selection problem for naive Bayes classifiers. We demonstrate the utility of these techniques in the context of supervised classification, showing empirically that model averaging consistently beats other generative Bayesian-network-based models, even when the generating model is not guaranteed to be a member of the class being averaged over. We characterize the performance over several parameters on simulated and real-world data.
ID:377
CLASS:4
Title: Learning to detect malicious executables in the wild
Abstract: In this paper, we describe the development of a fielded application for detecting malicious executables in the wild. We gathered 1971 benign and 1651 malicious executables and encoded each as a training example using n-grams of byte codes as features. Such processing resulted in more than 255 million distinct n-grams. After selecting the most relevant n-grams for prediction, we evaluated a variety of inductive methods, including naive Bayes, decision trees, support vector machines, and boosting. Ultimately, boosted decision trees outperformed other methods with an area under the roc curve of 0.996. Results also suggest that our methodology will scale to larger collections of executables. To the best of our knowledge, ours is the only fielded application for this task developed using techniques from machine learning and data mining.
ID:378
CLASS:4
Title: Exploiting dictionaries in named entity extraction: combining semi-Markov extraction processes and data integration methods
Abstract: We consider the problem of improving named entity recognition (NER) systems by using external dictionaries---more specifically, the problem of extending state-of-the-art NER systems by incorporating information about the similarity of extracted entities to entities in an external dictionary. This is difficult because most high-performance named entity recognition systems operate by sequentially classifying words as to whether or not they participate in an entity name; however, the most useful similarity measures score entire candidate names. To correct this mismatch we formalize a semi-Markov extraction process, which is based on sequentially classifying segments of several adjacent words, rather than single words. In addition to allowing a natural way of coupling high-performance NER methods and high-performance similarity functions, this formalism also allows the direct use of other useful entity-level features, and provides a more natural formulation of the NER problem than sequential word classification. Experiments in multiple domains show that the new model can substantially improve extraction performance over previous methods for using external dictionaries in NER.
ID:379
CLASS:4
Title: Machine Learning Methods for Predicting Failures in Hard Drives: A Multiple-Instance Application
Abstract: We compare machine learning methods applied to a difficult real-world problem: predicting computer hard-drive failure using attributes monitored internally by individual drives. The problem is one of detecting rare events in a time series of noisy and nonparametrically-distributed data. We develop a new algorithm based on the multiple-instance learning framework and the naive Bayesian classifier (mi-NB) which is specifically designed for the low false-alarm case, and is shown to have promising performance. Other methods compared are support vector machines (SVMs), unsupervised clustering, and non-parametric statistical tests (rank-sum and reverse arrangements). The failure-prediction performance of the SVM, rank-sum and mi-NB algorithm is considerably better than the threshold method currently implemented in drives, while maintaining low false alarm rates. Our results suggest that nonparametric statistical tests should be considered for learning problems involving detecting rare events in time series data. An appendix details the calculation of rank-sum significance probabilities in the case of discrete, tied observations, and we give new recommendations about when the exact calculation should be used instead of the commonly-used normal approximation. These normal approximations may be particularly inaccurate for rare event problems like hard drive failures.
ID:380
CLASS:4
Title: Research of Machine Learning Method for Specific Information Recognition on the Internet
Abstract: With the available resources on the Internet becoming plentiful, a large amount of harmfulinformation is permeating in and has been influencing people's normal work and living seriously. Therefore, some harmful data stream must be recognized and filtered out effectively.After analyzing some harmful contents in Internet information stream, we present a new method, which recognizes specific information by Machine Learning (ML). We extracted key information from a number of corpuses through ML method to obtain the part of speech (POS) Transfer-Form for key information by learning from corpuses, which is based on the same pronunciation matching of key information. Further more, the testing value of key information will be obtained in real corpus to examine the likelihood between matching rules from information streams and those learnt from corpuses through the average value of POS transfer probability of key information. Therefore, the testing value for the whole real data stream will be obtained. The experiment proved that the method was efficient for recognizing certainInternet harmful information.
ID:381
CLASS:4
Title: Hierarchical rule generalisation for speaker identification in fiction books
Abstract: This paper presents a hierarchical pattern matching and generalisation technique which is applied to the problem of locating the correct speaker of quoted speech found in fiction books. Patterns from a training set are generalised to create a small number of rules, which can be used to identify items of interest within the text. The pattern matching technique is applied to finding the Speech-Verb, Actor and Speaker of quotes found in fiction books. The technique performs well over the training data, resulting in rule-sets many times smaller than the training set, but providing very high accuracy. While the rule-set generalised from one book is less effective when applied to different books than an approach based on hand coded heuristics, performance is comparable when testing on data closely related to the training set.
ID:382
CLASS:4
Title: New Horn Revision Algorithms
Abstract: A revision algorithm is a learning algorithm that identifies the target concept, starting from an initial concept. Such an algorithm is considered efficient if its complexity (in terms of the measured resource) is polynomial in the syntactic distance between the initial and the target concept, but only polylogarithmic in the number of variables in the universe. We give efficient revision algorithms in the model of learning with equivalence and membership queries. The algorithms work in a general revision model where both deletion and addition revision operators are allowed. In this model one of the main open problems is the efficient revision of Horn formulas. Two revision algorithms are presented for special cases of this problem: for depth-1 acyclic Horn formulas, and for definite Horn formulas with unique heads.
ID:383
CLASS:4
Title: Tracking linear-threshold concepts with Winnow
Abstract: In this paper, we give a mistake-bound for learning arbitrary linear-threshold concepts that are allowed to change over time in the on-line model of learning. We use a variation of the Winnow algorithm and show that the bounds for learning shifting linear-threshold functions have many of the same advantages that the traditional Winnow algorithm has on fixed concepts. These benefits include a weak dependence on the number of irrelevant attributes, inexpensive runtime, and robust behavior against noise. In fact, we show that the bound for tracking Winnow has even better performance with respect to irrelevant attributes. Let X&isin;[0,1]n be an instance of the learning problem. In the previous bounds, the number of mistakes depends on lnn. In this paper, the shifting concept bound depends on max ln(||X||1). We show that this behavior is a result of certain parameter choices in the tracking version of Winnow, and we show how to use related parameters to get a similar mistake bound for the traditional fixed concept version of Winnow.
ID:384
CLASS:4
Title: A unified framework for model-based clustering
Abstract: Model-based clustering techniques have been widely used and have shown promising results in many applications involving complex data. This paper presents a unified framework for probabilistic model-based clustering based on a bipartite graph view of data and models that highlights the commonalities and differences among existing model-based clustering algorithms. In this view, clusters are represented as probabilistic models in a model space that is conceptually separate from the data space. For partitional clustering, the view is conceptually similar to the Expectation-Maximization (EM) algorithm. For hierarchical clustering, the graph-based view helps to visualize critical/important distinctions between similarity-based approaches and model-based approaches. The framework also suggests several useful variations of existing clustering algorithms. Two new variations---balanced model-based clustering and hybrid model-based clustering---are discussed and empirically evaluated on a variety of data types.
ID:385
CLASS:4
Title: Fusion of domain knowledge with data for structural learning in object oriented domains
Abstract: When constructing a Bayesian network, it can be advantageous to employ structural learning algorithms to combine knowledge captured in databases with prior information provided by domain experts. Unfortunately, conventional learning algorithms do not easily incorporate prior information, if this information is too vague to be encoded as properties that are local to families of variables. For instance, conventional algorithms do not exploit prior information about repetitive structures, which are often found in object oriented domains such as computer networks, large pedigrees and genetic analysis. In this paper we propose a method for doing structural learning in object oriented domains. It is demonstrated that this method is more efficient than conventional algorithms in such domains, and it is argued that the method supports a natural approach for expressing and incorporating prior information provided by domain experts.
ID:386
CLASS:4
Title: Use of the zero norm with linear models and kernel methods
Abstract: We explore the use of the so-called zero-norm of the parameters of linear models in learning. Minimization of such a quantity has many uses in a machine learning context: for variable or feature selection, minimizing training error and ensuring sparsity in solutions. We derive a simple but practical method for achieving these goals and discuss its relationship to existing techniques of minimizing the zero-norm. The method boils down to implementing a simple modification of vanilla SVM, namely via an iterative multiplicative rescaling of the training data. Applications we investigate which aid our discussion include variable and feature selection on biological microarray data, and multicategory classification.
ID:387
CLASS:4
Title: Cluster ensembles --- a knowledge reuse framework for combining multiple partitions
Abstract: This paper introduces the problem of combining multiple partitionings of a set of objects into a single consolidated clustering without accessing the features or algorithms that determined these partitionings. We first identify several application scenarios for the resultant 'knowledge reuse' framework that we call cluster ensembles. The cluster ensemble problem is then formalized as a combinatorial optimization problem in terms of shared mutual information. In addition to a direct maximization approach, we propose three effective and efficient techniques for obtaining high-quality combiners (consensus functions). The first combiner induces a similarity measure from the partitionings and then reclusters the objects. The second combiner is based on hypergraph partitioning. The third one collapses groups of clusters into meta-clusters which then compete for each object to determine the combined clustering. Due to the low computational costs of our techniques, it is quite feasible to use a supra-consensus function that evaluates all three approaches against the objective function and picks the best solution for a given situation. We evaluate the effectiveness of cluster ensembles in three qualitatively different application scenarios: (i) where the original clusters were formed based on non-identical sets of features, (ii) where the original clustering algorithms worked on non-identical sets of objects, and (iii) where a common data-set is used and the main purpose of combining multiple clusterings is to improve the quality and robustness of the solution. Promising results are obtained in all three situations for synthetic as well as real data-sets.
ID:388
CLASS:4
Title: \&epsilon;-mdps: learning in varying environments
Abstract: In this paper &epsilon;-MDP-models are introduced and convergence theorems are proven using the generalized MDP framework of Szepesvari and Littman. Using this model family, we show that Q-learning is capable of finding near-optimal policies in varying environments. The potential of this new family of MDP models is illustrated via a reinforcement learning algorithm called event-learning which separates the optimization of decision making from the controller. We show that event-learning augmented by a particular controller, which gives rise to an &epsilon;-MDP, enables near optimal performance even if considerable and sudden changes may occur in the environment. Illustrations are provided on the two-segment pendulum problem.
ID:389
CLASS:4
Title: Learning rules and their exceptions
Abstract: We present in this article a top-down inductive system, ALLiS, for learning linguistic structures. Two difficulties came up during the development of the system: the presence of a significant amount of noise in the data and the presence of exceptions linguistically motivated. It is then a challenge for an inductive system to learn rules from this kind of data. This leads us to add a specific mechanism, refinement, which enables learning rules and their exceptions. In the first part of this article we evaluate the usefulness of this device and show that it improves results when learning linguistic structures.In the second part, we explore how to improve the efficiency of the system by using prior knowledge. Since Natural Language is a strongly structured object, it may be important to investigate whether linguistic knowledge can help to make natural language learning more efficiently and accurately. This article presents some experiments demonstrating that linguistic knowledge improves learning. The system has been applied to the shared task of the CoNLL'00 workshop.
ID:390
CLASS:4
Title: Classes of kernels for machine learning: a statistics perspective
Abstract: In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.
ID:391
CLASS:4
Title: On the algorithmic implementation of multiclass kernel-based vector machines
Abstract: In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy.
ID:392
CLASS:4
Title: Learning to form dynamic committees
Abstract: Learning agents may improve performance when they cooperate with other agents. Specifically, learning agents forming a committee may outperform individual agents. This "ensemble effect" is well known for multi-classifier systems in Machine Learning. However, multi-classifier systems assume all data is known to all classifiers while we focus on agents that learn from cases (examples) that are owned and stored individually. In this article we focus on the selection of the agents that join a committee for solving a problem. Our approach is to frame committee membership as a learning task for the convener agent. The committee convener agent learns to form a committee in a dynamic way: at each point in time the convener agent decides whether it is better to invite a new member to join the committee (and which agent to invite) or to close the membership. The convener agent performs learning in the space of voting situations, i.e. learns when the current committee voting situation is likely to solve correctly (or not) a problem. The learning process allows an agent to decide when to solve a problem individually, when to convene a committee is better, and which individual agents to be invited to join the committee. Our experiments show that learning to form dynamic committees results in smaller committees while maintaining (and sometimes improving) the problem solving accuracy.
ID:393
CLASS:4
Title: Interactive deduplication using active learning
Abstract: Deduplication is a key operation in integrating data from multiple sources. The main challenge in this task is designing a function that can resolve when a pair of records refer to the same entity in spite of various data inconsistencies. Most existing systems use hand-coded functions. One way to overcome the tedium of hand-coding is to train a classifier to distinguish between duplicates and non-duplicates. The success of this method critically hinges on being able to provide a covering and challenging set of training pairs that bring out the subtlety of deduplication function. This is non-trivial because it requires manually searching for various data inconsistencies between any two records spread apart in large lists.We present our design of a learning-based deduplication system that uses a novel method of interactively discovering challenging training pairs using active learning. Our experiments on real-life datasets show that active learning significantly reduces the number of instances needed to achieve high accuracy. We investigate various design issues that arise in building a system to provide interactive response, fast convergence, and interpretable output.
ID:394
CLASS:4
Title: A flexible learning system for wrapping tables and lists in HTML documents
Abstract: A program that makes an existing website look like a database is called a wrapper. Wrapper learning is the problem of learning website wrappers from examples. We present a wrapper-learning system called WL2 that can exploit several different representations of a document. Examples of such different representations include DOM-level and token-level representations, as well as two-dimensional geometric views of the rendered page (for tabular data) and representations of the visual appearance of text asm it will be rendered. Additionally, the learning system is modular, and can be easily adapted to new domains and tasks. The learning system described is part of an "industrial-strength" wrapper management system that is in active use at WhizBang Labs. Controlled experiments show that the learner has broader coverage and a faster learning rate than earlier wrapper-learning systems.
ID:395
CLASS:4
Title: Mining time-changing data streams
Abstract: Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a stationary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying processes generating them changed during this time, sometimes radically. Although a number of algorithms have been proposed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.
ID:396
CLASS:5
Title: Developing multiagent systems: The Gaia methodology
Abstract: Systems composed of interacting autonomous agents offer a promising software engineering approach for developing applications in complex domains. However, this multiagent system paradigm introduces a number of new abstractions and design/development issues when compared with more traditional approaches to software development. Accordingly, new analysis and design methodologies, as well as new tools, are needed to effectively engineer such systems. Against this background, the contribution of this article is twofold. First, we synthesize and clarify the key abstractions of agent-based computing as they pertain to agent-oriented software engineering. In particular, we argue that a multiagent system can naturally be viewed and architected as a computational organization, and we identify the appropriate organizational abstractions that are central to the analysis and design of such systems. Second, we detail and extend the Gaia methodology for the analysis and design of multiagent systems. Gaia exploits the aforementioned organizational abstractions to provide clear guidelines for the analysis and design of complex and open software systems. Two representative case studies are introduced to exemplify Gaia's concepts and to show its use and effectiveness in different types of multiagent system.
ID:397
CLASS:5
Title: Multiagent Systems for resource allocation in Peer-to-Peer systems
Abstract: This paper introduces Multiagent Systems (MAS) storage resource allocation algorithms and methods on a Peer-to-Peer (P2P) system of computer storage resources. MAS Complex Adaptive Systems (CAS) based on squirrels behaviors are proposed and evaluated to produce emergent global behaviors that can solve the storage resource allocation problem in a distributed system of peers. Experimental results support the initial hypothesis that hoarding mechanisms found on squirrels behaviors efficiently allocate resources in distributed systems.
ID:398
CLASS:5
Title: Knowledge in multiagent systems: initial configurations and broadcast
Abstract: The semantic framework for the modal logic of knowledge due to Halpern and Moses provides a way to ascribe knowlegde to agents in distributed and multiagent systems. In this paper we study two special cases of this framework: full systems and hypercubes. Both model static situtations in which no agents has any information about another agent's state. Full systems and hypercubes are an appropriate model for the initial configurations of many systems of interest. We establish a correspondence between full systems and hypercube systems and certain classes of Kripke frames. We show that these classes of systems correspond to the same logic. Moreover, this logic is also the same as that generated by the larger class of weakly directed frames. We provide a sound and complete axiomatization, S5WDn of this logic, and study its computational complexity. Finally, we show that under certain natural assumptions, in a model where knowledge evolves over time, S5WDn characteristics the properties of knowledge not just at the initial configuration, but also at all later configurations. In this particular, this holds for homogeneous broadcast systems, which capture settings in which agents are intially ignorant of each others local states, operate synchronously, have perfect recall, and can communicate only by broadcasting.
ID:399
CLASS:5
Title: A Sociological Framework for Multiagent Systems
Abstract: Individual rationality limits the ability of decison makers to accommodate group behavior where coordination is required. By replacing the asocial policy of doing the best thing possible for the individual with a mathematically precise notion of being "good enough," satisficing game theory provides a framework with which to synthesize artificial multiagent societies that are designed to cooperate.
ID:400
CLASS:5
Title: Verification of Multiagent Systems via Ordered Binary Decision Diagrams: An Algorithm and Its Implementation
Abstract: We investigate the problem of the verification of epistemic properties of multiagent systems via model checking. Specifically, we extend and adapt methods based on ordered binary decision diagrams, a mainstream verification technique in reactive systems. We provide an algorithm, and present a software package that implements it. We discuss the software and benchmark it by means of a standard example in the literature, the dining cryptographers.
ID:401
CLASS:5
Title: Multiagent systems engineering of organization-based multiagent systems
Abstract: In this paper, we examine the Multiagent Systems Engineering (MaSE) methodology and its applicability to developing organization-based multiagent systems, which are especially relevant to context aware systems. We discuss the inherent shortcomings of MaSE and then present our approach to modeling the concepts required for organizations including goals, roles, agents, capabilities, and the assignment of agents to roles. Finally, we extend MaSE to allow it to overcome its inherent shortcomings and capture the organizational concepts defined in our organizational model.
ID:402
CLASS:5
Title: Model Sharing in Multiagent Systems
Abstract: Agent modelling is made complex by the number of models that the agents need to track. This paper discusses the use of intra-agent and inter-agent model sharing as an approach to reducing the number of models to be tracked by an agent. The paper also describes the GossipWorld simulation environment we have created for studying agent modelling and model sharing issues. Finally, the paper discusses ways of coping with the spread of delusion (with experimental results) that can occur as a result of inter-agent model sharing.
ID:403
CLASS:5
Title: Negotiating Gestalt: Artistic Expression and Coalition Formation in Multiagent Systems
Abstract: We present a system using semi-autonomous agents to help artists express ideas. The agents control their representation on a canvas via interactions in an agent space by themselves and in coalitions. The artist can take direct control if the resulting image is not satisfactory. We have implemented the Surreal engine that realises these ideas and we present a case study of its usage in producing Mondriaan-like paintings.
ID:404
CLASS:5
Title: Verification of Multiagent Systems via Unbounded Model Checking
Abstract: We present an approach to the problem of verification of epistemic properties of multi-agent systems by means of symbolic model checking. In particular, it is shown how to extend the technique of unbounded model checking from a purely temporal setting to a temporal-epistemic one. In order to achieve this, we base our discussion on interpreted systems semantics, a popular semantics used in multi-agent systems literature.We give details of the technique and show how it can be applied to the well-known train, gate and controller problem.
ID:405
CLASS:5
Title: Ontological Feedback in Multiagent Systems
Abstract: In this paper, we present a computational framework for the detection of ontological discrepancies in multiagent systems. The framework is developed as a basis for the generation of feedback utterances at the ontological level. In our method, presuppositions are extracted from the senders message, expressed in a common vocabulary, and compared with the recipients ontology, which is expressed in type theory. Discrepancies are detected by the receiving agent if it notices type conflicts, particular inconsistencies or ontological gaps. Depending on the kind of discrepancy, the agent generates a particular feedback message in order to establish alignment of its private ontology with the ontology of the sender.
ID:406
CLASS:5
Title: Protocol/Mechanism Design for Cooperation/Competition
Abstract: Developing interaction rules/protocols among multiple agents is one of the central research topics in multi-agent systems. For cooperative agents, we need to develop protocols so that agents can achieve some common goal if they follow the protocol. Also, for competitive/selfish agents, we need to design mechanisms/protocols so that some socially desirable outcome can be achieved, even if agents act selfishly. This article presents a brief overview of the authors works on this topic over the last five years.
ID:407
CLASS:5
Title: Multiagent systems as software architecture: another perspective on software engineering with multiagent systems
Abstract: The trend in agent-oriented software engineering is to consider multiagent systems (MASs) as a radically new way of engineering software. This position isolates agent-oriented software engineering from mainstream software engineering and could be one important reason why MASs are not widely adopted in industry yet.In this paper, we present another perspective on software engineering with MASs. We put forward MASs as software architecture. We give an overview of a reference architecture for situated MAS. This reference architecture extracts and generalizes common functions and structures from various applications we have studied and built. The reference architecture provides a blueprint for architectural design of MAS applications that share the come base of the systems it is derived from. Considering MASs essentially as software architecture paves the way to integration with mainstream software engineering.
ID:408
CLASS:5
Title: Acquiring and adapting probabilistic models of agent conversation
Abstract: Communication in multiagent systems (MASs) is usually governed by agent communication languages (ACLs) and communication protocols carrying a clear cut semantics. With an increasing degree of openness, however, the need arises for more flexible models of communication that can handle the uncertainty associated with the fact that adherence to a supposedly agreed specification of possible conversations cannot be ensured on the side of other agents.As one example for such a model, interaction frames follow an empirical semantics view of communication, where meaning is defined in terms of expected consequences, and allow for a combination of existing expectations with empirical observation of how communication is used in practice.In this paper, we use methods from the fields of case-based reasoning, inductive logic programming and cluster analysis to devise a formal scheme for the acquisition and adaptation of interaction frames from actual conversations, enabling agents to autonomously (i.e. independent of users and system designers) create and maintain a concise model of the different classes of conversation in a MAS on the basis of an initial set of ACL and protocol specifications. This resembles the first rigorous attempt to solve this problem that is decisive for building truly autonomous agents.
ID:409
CLASS:5
Title: Using game days to teach a multiagent system class
Abstract: Multiagent systems is an attractive problem solving approach that is becoming ever more feasible and popular in today's world. It combines artificial intelligence (AI) and distributed problem solving to allow designers (programmers and engineers alike) to solve problems otherwise deemed awkward in traditional approaches that are less flexible and centralized. In the Fall semester of 2002, I introduced a new game-based technique to my Multiagent Systems class. The class was aimed for seniors (with special permission) and graduate students in Computer Science, covering some breadth and depth of issues in multiagent systems. One of the requirements was participation in four Game Days. On each Game Day, student teams competed against each other in games related to issues such as auction, task allocation, coalition formation, and negotiation. This article documents my designs of and lessons learned from these Game Days. The Game Days were very successful. Through role-playing, the students were motivated and learned about multiagent systems.
ID:410
CLASS:5
Title: Learning trust strategies in reputation exchange networks
Abstract: An agent's trust decision strategy consists of the agent's policies for making trust-related decisions, such as who to trust, how trustworthy to be, what reputations to believe, and when to tell truthful reputations. In reputation exchange networks, learning trust decision strategies is complex, compared to non-reputation-communicating systems. When potential partners may exchange reputation information about an agent, the agent's interactions with one partner are no longer independent from interactions with another; partners may tell each other about their experiences with the agent, influencing future behavior. This research enumerates the types of decisions an agent faces in reputation exchange networks, explains the interdependencies between these decisions, and correlates rewards to each decision. Experimental results using the Agent Reputation and Trust (ART) Testbed demonstrate the success of strategy-learning agents over agents employing naive strategies. The variation in performance of reputation-based learning vs. experience-based learning over different opponents illustrates the need to dynamically determine when to utilize reputations vs. experience in making trust decisions.
ID:411
CLASS:5
Title: An integrated token-based algorithm for scalable coordination
Abstract: Efficient coordination among large numbers of heterogeneous agents promises to revolutionize the way in which some complex tasks, such as responding to urban disasters can be performed. However, state of the art coordination algorithms are not capable of achieving efficient and effective coordination when a team is very large. Building on recent successful token-based algorithms for task allocation and information sharing, we have developed an integrated and efficient approach to effective coordination of large scale teams. We use tokens to encapsulate anything that needs to be shared by the team, including information, tasks and resources. The tokens are efficiently routed through the team via the use of local decision theoretic models. Each token is used to improve the routing of other tokens leading to a dramatic performance improvement when the algorithms work together. We present results from an implementation of this approach which demonstrates its ability to coordinate large teams.
ID:412
CLASS:5
Title: Monopolizing markets by exploiting trust
Abstract: We investigate trust-based relationships in electronic supply chains where trust is a measure of consistency in meeting negotiated contract deadlines. We consider scenarios where contractors assign contracts to contractees who are significantly more successful in meeting deadlines. The task deadlines are drawn from a known distribution. We present a probabilistic analysis that enables contractees to strategically bid on only certain tasks to earn the trust of their contractors. Once a contractee achieves a high level of trust, it can then exploit that trust to increasingly corner a larger portion of the market share of all tasks. We present a trust exploitation scheme that monopolizes the market against greedy contractees who bid on all announced tasks. We also show that such market monopolization is not possible in the presence of a trusted, but non-exploiting, contractee. The exploiter can, however, effectively "starve" the non-exploiting trusted agent.
ID:413
CLASS:5
Title: Gradient field-based task assignment in an AGV transportation system
Abstract: Assigning tasks to agents is complex, especially in highly dynamic environments. Typical protocol-based approaches for task assignment such as Contract Net have proven their value, however, they may not be flexible enough to cope with continuously changing circumstances. In this paper we study and validate the feasibility of a field-based approach for task assignment in a complex problem domain.In particular, we apply the field-based approach for task assignment in an AGV transportation system. In this approach, transports emit fields into the environment that attract idle AGVs. To avoid multiple AGVs driving towards the same transport, AGVs emit repulsive fields. AGVs combine received fields and follow the gradient of the combined fields, that guide them towards pick locations of transports. The AGVs continuously reconsider the situation of the environment and task assignment is delayed until the load is picked, improves the flexibility of the system.Extensive experiments indicate that the field-based approach outperforms the standard Contract Net approach on various performance measures, such as the average wait time of transports and throughput. Limitations of the field-based approach are an unequal distribution of wait times across different transports and a small increase of bandwidth occupation.
ID:414
CLASS:5
Title: Modeling exceptions via commitment protocols
Abstract: This paper develops a model for exceptions and an approach for incorporating them in commitment protocols among autonomous agents. Modeling and handling exceptions is critical for successful applications of multiagent systems. Protocols help build multiagent systems, but traditional representations (such as finite state machines or Petri nets) inadequately model complex interactions and exceptions therein. Emerging commitment-based representations are promising, because they declaratively reflect the semantics of an interaction. However, current approaches lack a strong treatment of exceptions.This paper treats both expected and unexpected exceptions. A commitment protocol is modeled as a set of computations, each representing an allowed interaction and showing the evolving commitments of the participants. Exceptions are modeled via preference structures induced on these sets of computations. The preference structures statically show how expected exceptions are handled whereas the structures must be enhanced dynamically to handle unexpected exceptions. Our approach includes operators for composing protocols and exception handlers, whereby appropriate exception handlers can be dynamically introduced into a protocol as needed.The main contributions of this paper are (1) a framework for modeling and handling exceptions intelligently in commitment protocols and (2) a demonstration of the benefits of commitment protocols over traditional formalisms in handling exceptions.
ID:415
CLASS:5
Title: A Multi-Agent Approach for Peer-to-Peer Based Information Retrieval System
Abstract: This paper develops and analyzes distributed search techniques for use in a peer-to-peer (P2P) network-based Information Retrieval (IR) system. In the absence of a centralized mediator with global knowledge that directs requests to appropriate agents, agents must cooperate to forward the queries among themselves so as to find appropriate agents, and return and merge the results in order to fulfill the information retrieval task in a distributed environment. In our approach, the agent society is connected through an agent-view structure maintained by each agent. Initially, the agent-view structures are formed by agents connecting to each other randomly. However, we show that such an approach can be significantly enhanced by dynamically reorganizing the underlying agent-view topology and deploying contextsensitive distributed search algorithms. Experimental results indicate that appropriate organizational structures and distributed search mechanisms can have a positive influence on system performance.
ID:416
CLASS:5
Title: Detecting deception in reputation management
Abstract: We previously developed a social mechanism for distributed reputation management, in which an agent combines testimonies from several witnesses to determine its ratings of another agent. However, that approach does not fully protect against spurious ratings generated by malicious agents. This paper focuses on the problem of deception in testimony propagation and aggregation. We introduce some models of deception and study how to efficiently detect deceptive agents following those models. Our approach involves a novel application of the well-known weighted majority technique to belief function and their aggregation. We describe simulation experiments to study the number of apparently accurate witnesses found in different settings, the number of witnesses on prediction accuracy, and the evolution of trust networks.
ID:417
CLASS:5
Title: Towards robust teams with many agents
Abstract: Agents in deployed multi-agent systems monitor other agents to coordinate and collaborate. However, as the number of agents monitored is scaled up, two key challenges arise: (i) the number of monitoring hypotheses to be considered can grow exponentially in the number of agents; and (ii) agents become physically and logically unconnected (unobservable) to their peers. This paper examines these challenges in teams of cooperating agents, focusing on a monitoring task that is of particular importance to robust teamwork: detecting disagreements among team-members. We present YOYO, a highly scalable disagreement-detection algorithm which guarantees sound detection in time linear in the number of agents despite the exponential number of hypotheses. In addition,we present new upper bounds for the number of agents that must be monitored in a team to guarantee disagreement detection. Both YOYO and the new bounds are explored analytically and empirically in thousands of monitoring problems, scaled to thousands of agents.
ID:418
CLASS:5
Title: Specifying agent behavior as concurrent tasks
Abstract: Software agents are currently the subject of much research in many interrelated fields. Unfortunately, there has not been enough emphasis on defining the techniques required to build practical agent systems. While many agent researchers refer to tasks, few really define what they mean. Tasks only define the internal processing an agent must perform, but also how interactions with other agents relate to internal processes.
ID:419
CLASS:5
Title: Contextualizing commitment protocol
Abstract: Commitment protocols are modularized specifications of interactions understood in terms of commitments. Purchase is a classic example of a protocol. Although a typical protocol would capture the essence of the interactions desired, in practice, it should be adapted depending on the circumstances or context and the agents' preferences based on that context. For example, when applying purchase in different contexts, it may help to allow sending reminders for payments or returning goods to obtain a refund. We contextualize a protocol by adapting it via different transformations.Our contributions are the following: (1) a protocol is transformed by composing its specification with a transformer specification; (2) contextualization is characterized operationally by relating the original and transformed protocols; and (3) contextualization is related to protocol compliance.
ID:420
CLASS:5
Title: Modular BDI architecture
Abstract: One of the main challenges in agent-oriented programming is the design of specialized programming languages for single agent development. They should provide transparent interfaces to existing mainstream programming languages for easy integration with external code and legacy software. The underlying architecture of such programming languages has to be robust enough to support various approaches to knowledge representation and agent reasoning models.In this paper we propose a modular BDI agent programming architecture, which is independent of the internal structure of its components and agent reasoning model. The connections between the components of such a BDI system are provided by interaction rules. Using this separation, we are able to draw a clear distinction between knowledge representation issues of a BDI agent system components and its dynamics.
ID:421
CLASS:5
Title: Software Engineering for Large-Scale Multi-Agent Systems - SELMAS 2005: workshop report
Abstract: This paper is intended to sum up the results of the 4th International Workshop on Software Engineering for Large-Scale Multi-Agent Systems (SELMAS 2005) held in St. Louis, Missouri, USA, May 15--16, 2005, as part of the 27th International Conference on Software Engineering (ICSE'05). The main purpose of this workshop was to share and pool together the collective experience of people, both academics and practitioners, who are actively working on software engineering for large-scale multi-agent systems. A selected set of expanded workshop papers and invited papers will appear in the 4th edition of the book Software Engineering for Multi-Agent Systems (LNCS, Springer, 2006). The theme of this workshop edition was "Software Everywhere - Context-Aware Agents". The workshop consisted of an opening presentation, several paper presentations organized into three technical sessions, two keynotes, one panel, and two discussion groups. During the workshop we informally reviewed ongoing and previous work and debated a number of important issues. The SELMAS 2005 Web site, including the electronic version of this report, can be found at &lt;www.teccomm.les.inf.pucrio.br/selmas2005&gt;. We begin by presenting an overview of our goals and the workshop structure, and then focus on the workshop technical program and results.
ID:422
CLASS:5
Title: Predicting agent strategy mix of evolving populations
Abstract: We study agent societies where self-interested agents interact repeatedly over extended time periods. In particular, we are interested in environments where agents can form mutually beneficial relationships by exchanging help but an agent would rather receive help than give it. Evolutionary tournaments with competing help-giving strategies can model scenarios where agents periodically adopt strategies that are outperforming others in the population. Such experiments, however, can be computationally costly and hence it is difficult to prescribe a rational strategy choice given environmental conditions like task mix, strategy distribution in the population, etc. A preferred approach, pursued in this paper, is to analytically capture the dynamics of the strategy mix in the population under an evolutionary tournament. Such an analytical model can be used to predict the evolutionarily dominant strategy, the rational strategy choice.
ID:423
CLASS:5
Title: An efficient algorithm for multiagent plan coordination
Abstract: The multiagent plan coordination problem arises whenever multiple agents plan to achieve their individual goals independently, but might mutually benefit by coordinating their plans to avoid working at cross purposes or duplicating effort. Although variations of this problem have been studied in the literature, there is as yet no agreement over a general characterization of the problem. In this paper, we describe a general framework that extends the partial-order, causal-link plan representation to the multiagent case, and that treats coordination as a form of iterative repair of plan flaws that cross agents. We show, analytically and empirically, that this algorithmic formulation can scale to the multiagent case better than can a straightforward application of the most advanced single-agent plan coordination technique, highlighting fundamental differences between single-agent and multiagent planning.
ID:424
CLASS:5
Title: A secure architectural description language for agent systems
Abstract: Multi-agent systems are now being considered a promising architectural approach for building Internet-based applications. One of the most critical and important aspects of software deployed on the web has always been the security of their architectures. However, despite considerable work in software architecture during the last decade, few research efforts have aimed at truly defining languages for designing and formalizing agent architectures and more specifically secure ones. This paper identifies the foundations for an architectural description language (ADL) to specify secure multi-agent systems. We propose a set of system design primitives and conceptualize it with the Z specification language to capture a "core" architectural model to build secure MAS architectures. We apply it on an e-commerce example to illustrate our proposal.
ID:425
CLASS:5
Title: On Safe Kernel Stable Coalition Forming among Agents
Abstract: We investigate and discuss safety and privacy preserving properties of a game-theortic based coalition algorithm KCA for forming kernel stable coalitions among information agents in face of imperfect information on actual coalition values, and changing agent society. In addition, we analyze the chances of deceiving information agents to succeed in coalition negotiations using the KCA protocol. We show that a certain type of fraud which leads to an increase of individual profit can neither be prevented nor detected, but this comes at the expense of exponentially high computation costs for the deceiving agent.
ID:426
CLASS:5
Title: An HLA-based multiagent system for optimized resource allocation after strong earthquakes
Abstract: In this paper the author presents a distributed simulation system for disaster response activities based on the High Level Architecture (HLA). This simulation system focuses on resource management issues including the allocation of scarce response resources to operational areas and it consists of three major components: (1) simulators for the disaster environment, e.g., simulators for damages, casualties and fire spread, (2) simulators for the operations of personnel and technical equipment and (3) some auxiliary simulators. A Multiagent System which models resource allocation tasks within an Emergency Operation Center (EOC) is linked to this simulation. This paper describes the overall architecture of the system and presents some results based on a prototype implementation.
ID:427
CLASS:5
Title: Agent-based cooperative learning: a proof-of-concept experiment
Abstract: This paper presents an innovative multiagent system to support cooperative learning among students both in the real classrooms and in distance education. The system, called I-MINDS, consists of a group of intelligent agents. A teacher agent monitors the student activities and helps the teacher manage and better adapt to the class. A student agent, on the other hand, interacts with the teacher agent and other student agents to support cooperative learning activities behind-the-scene for a student. Two I-MINDS innovations are (a) agent-federated "buddy group" formation and (b) automated ranking of questions and responses. We have tested our I-MINDS prototype with experiment and control groups to evaluate the impact of I-MINDS in learning. The results are encouraging.
ID:428
CLASS:5
Title: Optimal utterances in dialogue protocols
Abstract: Dialogue protocols have been the subject of considerable attention with respect to their potential applications in multiagent system environments. Formalisations of such protocols define classes of dialogue locutions, concepts of a dialogue state, and rules under which a dialogue proceeds. One important consideration in implementing a protocol concerns the criteria an agent should apply in choosing which utterance will constitute its next contribution to a discussion in progress: ideally, an agent should select a locution that (by some measure) "optimises" the outcome. The precise interpretation of 'optimise' is, however, something that may vary greatly depending on the nature and intent of a dialogue area. If we consider 'persuasion' protocols, where one agent's intention is to convince others of the validity or invalidity of a specific proposition, then optimality might be regarded in the sense of "choice of locution that results in a 'minimal length' debate": thus the agent defending a hypothesis tries to select utterances that will convince other participants of the validity of this hypothesis after 'as few locutions as possible'. We present a formal setting for considering the problem of deciding if a particular utterance in the context of a persuasion dialogue is optimal in this sense. We show that, in general, this decision problem is both NP--hard and CO-NP--hard.
ID:429
CLASS:5
Title: Interaction is meaning: a new model for communication in open systems
Abstract: We propose a new model for agent communication in open systems that is based on the principle that the meaning of communicative acts lies in their experienced consequences. A formal framework for analysing such evolving semantics is defined. An extensive analysis of example interaction processes shows that our framework allows for an assessment of several properties of the communicative conventions governing a multiagent system. Among other advantages, our framework is capable of providing a very straightforward definition of communicative conflict. Also, it allows agents to reason about the effects of their communicative behaviour on the structure of communicative expectations as a whole when making decisions.
ID:430
CLASS:5
Title: A multi-agent system for the quantitative simulation of biological networks
Abstract: We apply the multi-agent system (MAS) platform to the task of biological network simulation. In this paper, we describe the simulation of signal transduction (ST) networks using the DECAF [9] MAS architecture. Unlike previous approaches that relied on systems of differential equations (DE), the distributed framework of MAS scales well and allows us to model large, highly interconnected ST pathways. This scalability is achieved by adopting a hybrid strategy that factors macro-level measures, such as reaction rateconstants, to calculate the stochastic kinetics at the level of individual molecules. Thus, by capturing the ST domain at an intermediate level of abstraction, we are able to retain much of the granularity afforded by a purely individual-based approach. The task distribution within a MAS enables us to model certain physical properies, such as diffusion and subcellular compartmentalization, which have proven to be difficult for DE systems. We demonstrate that large-grained agents are well suited to maintaining interal state representations and efficient in computing reactant concentration, both of which are vital considerations in modeling the ST domain. In our system, a molecular species is modeled as an individual agent with hierarchical task network structures to represent self- and externally-initiated reactions. An agent's identity is determined by a rule file (one for every participating molecular species) that specifies the reactions it may participate in, as well as its initial concentration. Reactions within the system are actuated by inter-agent communication. We present results from modeling the well-studied epidermal growth factor (EGF) pathway, demonstrating the viability of MAS technologies as a simulation platform for biological networks.
ID:431
CLASS:5
Title: The eager bidder problem: a fundamental problem of DAI and selected solutions
Abstract: The contract net protocol is a widely used protocol in DAI, as it proved to be a flexible and low communication interaction protocol for task assignment. It is however not clear in which manner agents participating in a contract net should allocate their resources if a large number of contract net protocols is performed concurrently. If the agent allocates too many resources too early, e.g. when making a bid, it may not get any bid accepted and resources have been allocated while other negotiations have come to an end and it is no longer able to make bids for them. If it allocates resources too late, e.g. after being awarded the contract, it may have made bids for more tasks than its resources allow for, possibly all being accepted and resulting in commitments that cannot be kept. We call this dilemma the Eager Bidder Problem. Apart from resource allocation this problem is of further importance as it constitutes the "dual" problem to engaging in multiple simultaneous first-price sealed-bid auctions.We present an ad hoc solution and two more complex strategies for solving this problem. Furthermore, we introduce a new method based on a statistical approach. We describe these mechanisms and how they deal with the concept of commitment at different levels. We conclude with criteria for the decision which of these mechanisms is best selected for a given problem domain.
ID:432
CLASS:5
Title: On the response of EMT-based control to interacting targets and models
Abstract: A novel control mechanism was recently introduced based on Extended Markov Tracking (EMT) [9, 10]. In this paper, we present a study of its response to multiple interacting control goals. We show a simple extension that can be integrated into EMT-based control, and which provides it with the ability to handle several behavioral targets. Experimental support for the validity of this extension is provided. We also describe an experiment with a simulated robot, where EMT-based controllers interact and interfere indirectly via the environment. Experiments support the resilience of multiagent EMT-based team control to potential conflicts that may appear within a team.
ID:433
CLASS:5
Title: Modeling mental states in the analysis of multiagent systems requirements
Abstract: This paper presents an agent-oriented requirements engineering approach that combines informal i* models with formal specifications in the CASL language through the use of Intentional Annotated Strategic Rationale diagrams. In the resulting framework, agent goals and knowledge are represented as their mental states, which allows for the formal analysis of, among other things, agent interactions and incomplete knowledge. CASL models can also serve as high-level specifications for multiagent systems.
ID:434
CLASS:5
Title: Unit testing in multi-agent systems using mock agents and aspects
Abstract: In this paper, we present a unit testing approach for MASs based on the use of Mock Agents. Each Mock Agent is responsible for testing a single role of an agent under successful and exceptional scenarios. Aspect-oriented techniques are used, in our testing approach, to monitor and control the execution of asynchronous test cases. We present an implementation of our approach on top of JADE platform, and show how we extended JUnit test framework in order to execute JADE test cases.
ID:435
CLASS:5
Title: On fault tolerance in law-governed multi-agent systems
Abstract: There has been much research about frameworks and tools to build multi-agent systems in different domains in recent years. These systems have particular features such as autonomy, distribution, sociability, cooperation and others implemented in another software entity, known as an agent. In order to achieve some previously defined goals, the agents interact between themselves to complete their tasks. One issue that arises from this kind of software is how can we ensure their dependability, considering the reliability of critical applications and the availability of those agents that play important roles with their responsibilities; i.e., how to dynamically and automatically identify the most critical agents and increase their availability and reliability? To this end, over the past few years there has been work on this problem proposing different approaches, each one solving a restricted problem involving dependability and leaving the global problem to be solved afterwards. This paper describes a solution to increase the availability of such systems through a technique of fault tolerance known as agent replication, and to increase its reliability through a mechanism of agent interaction regulation called law enforcement mechanism. The main contribution of this work is to improve the capability of calculating how critical an agent is to the system through its interactions with other agents and to provide a framework that uses this information to ensure availability and reliability.
ID:436
CLASS:5
Title: Making personalized recommendations to customers in a service-oriented economy: a quantitative model based on reputation and risk attitude
Abstract: In the current service-oriented economy, professional workforce and service personnel have to make not only reasonable but also personalized recommendations in response to individual customer's query. This affects not only the likelihood that the customer takes the recommendations as a short-term benefit but also the service providers' reputation in a long run. However, as different customers have different risk attitudes, they have different trade-off between the service providers' reputation and the recommendations' utilities. Therefore, the classical decision model considering only the utility and success rate is inadequate. We reconsider the problem of making recommendations from multiple perspectives, including reputation and risk attitude. We explain how this model can facilitate service providers to make effective decisions at strategic, tactical, and operations level regarding service recommendations.
ID:437
CLASS:5
Title: An integrated framework for adaptive reasoning about conversation patterns
Abstract: We present an integrated approach for reasoning about and learning conversation patterns in multiagent communication. The approach is based on the assumption that information about the communication language and protocols available in a multiagent system is provided in the form of dialogue sequence patterns, possibly tagged with logical conditions and instance information. We describe an integrated social reasoning architecture m2InFFrA that is capable of (i) processing such patterns, (ii) making communication decisions in a boundedly rational way, and (iii) learning patterns and their strategic application from observation.
ID:438
CLASS:5
Title: Impact of problem centralization in distributed constraint optimization algorithms
Abstract: Recent progress in Distributed Constraint Optimization Problems (DCOP) has led to a range of algorithms now available which differ in their amount of problem centralization. Problem centralization can have a significant impact on the amount of computation required by an agent but unfortunately the dominant evaluation metric of "number of cycles" fails to account for this cost. We analyze the relative performance of two recent algorithms for DCOP: OptAPO, which performs partial centralization, and Adopt, which maintains distribution of the DCOP. Previous comparison of Adopt and OptAPO has found that OptAPO requires fewer cycles than Adopt. We extend the cycles metric to define "Cycle-Based Runtime (CBR)" to account for both the amount of computation required in each cycle and the communication latency between cycles. Using the CBR metric, we show that Adopt outperforms OptAPO under a range of communication latencies. We also ask: What level of centralization is most suitable for a given communication latency? We use CBR to create performance curves for three algorithms that vary in degree of centralization, namely Adopt, OptAPO, and centralized Branch and Bound search.
ID:439
CLASS:5
Title: A multi-agent approach for solving optimization problems involving expensive resources
Abstract: In this paper, we propose a multi-agent approach for solving a class of optimization problems involving expensive resources, where monolithic local search schemes perform miserably. More specifically, we study the class of bin-packing problems. Under our proposed Fine-Grained Agent System scheme, rational agents work both collaboratively and selfishly based on local search and mimic physics-motivated systems. We apply our approach to a generalization of bin-packing - the Inventory Routing Problem with Time Windows - which is an important logistics problem, and demonstrate the efficiency and effectiveness of our approach.
ID:440
CLASS:5
Title: Role-Based Approaches for Agent Development
Abstract: Roles seem to be a suitable concept for the development of agent-based systems. We have compared different approaches based on roles for agent development. This paper illustrates the used evaluation criteria and reports the lesson learned.
ID:441
CLASS:5
Title: A Multiagent Approach for Logistics Performance Prediction Using Historical and Context Information
Abstract: This paper presents a multiagent architecture and methods for intelligent decision support in logistics processes. It extends current advanced prediction systems by providing the ability to combine history and situated reasoning. The contribution of the paper is threefold: first, a multi-agent architecture and learning algorithms are developed that enables us to combine background models learned from history data with context-related knowledge about the current situation; second, using a large real data set we show that adding situated knowledge actually improves the performance of a supply chain decision support system; and third, for our settings we evaluate the degree to which agent-assisted decision support is actually usable/sufficient to improve human decision-making and to support automated decision-making in dynamic supply network management scenarios.
ID:442
CLASS:5
Title: A Framework to Control Emergent Survivability of Multi Agent Systems
Abstract: As the science of multi-agent systems matures, many developers are looking to deploy mission critical applications on distributed multi-agent systems (DMAS). Due to their distributed nature, designing survivable resource constrained DMAS is a serious challenge. Fortunately, the intrinsic flexibility of DMAS allows them to shift resources at runtime between dimensions of functionality such as security, robustness, and the primary application. In this paper we present an algebra for computing overall survivability from these dimensions of success, and a control infrastructure that leverages these degrees of freedom to make run-time adaptations at multiple hierarchical levels to maximize overall system survivability. We have implemented this survivability control infrastructure on the Cougaar agent architecture, and built a military logistics application that can survive in chaotic environments. Finally, we present results from assessing the performance of this application, and discuss the implications for future deployed DMAS.
ID:443
CLASS:5
Title: Empirical-Rational Semantics of Agent Communication
Abstract: The missing of an appropriate semantics of agent communication languages is one of the most challenging issues of contemporary AI. Although several approaches to this problem exist, none of them is really suitable for dealing with agent autonomy, which is a decisive property of artificial agents. This paper introduces an observation-based approach to the semantics of agent communication, which combines benefits of the two most influential traditional approaches to agent communication semantics, namely the mentalistic (agent-centric) and the objectivist (i.e., commitment- or protocol-oriented) approach. Our approach makes use of the fact that the most general meaning of agent utterances lays in their expectable consequences in terms of agent actions, and that communications result from hidden but nevertheless rational and to some extent reliable agent intentions. In this work, we present a formal framework which enables the empirical derivation of communication meanings from the observation of rational agent utterances, and introduce thereby a probabilistic and utility-oriented perspective of social commitments.
ID:444
CLASS:5
Title: Agent-Oriented software engineering report on the 4<sup>th</sup> AOSE workshop (AOSE 2003)
Abstract: Agent-Orientation is emerging as a powerful new paradigm in computing. Concepts, methodologies and tools from the agents paradigm are one of the best candidates for the foundations of the next generation of mainstream software systems. The Agent-Oriented Software Engineering (AOSE) workshop is an international event that brings together researchers and groups active in the area of agent-based software development. Here we briefly report on the fourth edition of the AOSE workshop.
ID:445
CLASS:5
Title: Inter-organizational networks as patterns for self-organizing multiagent systems
Abstract: Market-based approaches for task-assignment multiagent systems consist of customer agents with jobs to assign, and provider agents that have the resources to perform these jobs. Jobs can be complex in the sense that they require the collaboration of several provider agents. We present a set of sociological forms of inter-organizational networks that have the potential to increase performance through the structure they impose on collaboration.This gain of structure is especially valuable in settings where communication is limited, which is an appropriate assumption in large-scale applications. We empirically evaluate these organizational forms according to the amount of communication required and the rate of failed task-assignments, and compare them to a system without organizational forms. Furthermore,we investigate the effect of letting agents choose at runtime in which kind of organizational form to engage and which other agents to choose for this collaboration.Our evaluation shows that the proposed organizational forms and mechanisms for self-organization have the ability to improve the efficiency of a market-based multiagent system.
ID:446
CLASS:5
Title: Modelling secure multiagent systems
Abstract: Security plays an important role in the development of multiagent systems. However, a careful analysis of software development processes shows that the definition of security requirements is, usually, considered after the design of the system. This is, mainly, due to the fact that agent oriented software engineering methodologies have not integrated security concerns throughout their developing stages. The integration of security concerns during the whole range of the development stages could help towards the development of more secure multiagent systems. In this paper we introduce extensions to the Tropos methodology to enable it to model security concerns throughout the whole development process. A description of the new concepts is given along with an explanation of how these concepts are integrated to the current stages of Tropos. An example from the health care sector is used to illustrate the above.
ID:447
CLASS:5
Title: A false-name-proof double auction protocol for arbitrary evaluation values
Abstract: We develop a new false-name-proof double auction protocol called the Generalized Threshold Price Double auction (GTPD) protocol. False-name-proofness generalizes strategy-proofness by incorporating the possibility of false-name bids, e.g., bids submitted using multiple e-mail addresses. An existing protocol called TPD protocol is false-name-proof but can handle only the cases where marginal utilities of each agent always decrease, while our new GTPD protocol can handle arbitrary evaluation values. When marginal utilities can increase, some bids cannot be divided into a single unit (e.g., an all-or-nothing bid). Due to the existence of such indivisible bids, meeting supply/demand becomes difficult. Furthermore, a seller/buyer can submit a false-name-bid by pretending to be a potential buyer/seller to manipulate allocations and payments.In the GTPD protocol, the auctioneer is required to absorb the supply-demand imbalance up to a given upper-bound. Also, the GTPD incorporate a new false-name-proof one-sided auction protocol that is guaranteed to sell/buy a certain number of units. Simulation results show that when the threshold price is set appropriately, this protocol can obtain a good social surplus, and the number of absorbed units is much smaller than the given upper-bound.
ID:448
CLASS:5
Title: Using RoboCup to teach multiagent systems and the distributed mindset
Abstract: We present our experiences using the RoboCup soccerserver simulator and Biter, our own agent platform, for the teaching of a graduate multiagent systems' class. The RoboCup simulator and Biter are both described. We argue that the combination of RoboCup and Biter forms an effective platform for the teaching of multiagent systems and the distributed mindset. Results from two semesters using these tools are presented. These results confirm our claims. Finally, we characterize this work within the framework provided by the STEELMAN Draft of the Computing Curricula 2001 initiative.
ID:449
CLASS:5
Title: Flexible protocol specification and execution: applying event calculus planning using commitments
Abstract: Protocols represent the allowed interactions among communicating agents. Protocols are essential in applications such as electronic commerce where it is necessary to constrain the behaviors of autonomous agents. Traditional approaches, which model protocols in terms of action sequences, limit the flexibility of the agents in executing the protocols. By contrast, we develop an approach for specifying protocols in which we capture the content of the actions through agents' commitments to one another. We formalize commitments in a variant of the event calculus. We provide operations and reasoning rules to capture the evolution of commitments through the agents' actions. Using these rules in addition to the basic event calculus axioms enables agents to reason about their actions explicitly to flexibly accommodate the exceptions and opportunities that arise at run time. This reasoning is implemented using an event calculus planner that helps us determine flexible execution paths that respect the protocol specifications.
ID:450
CLASS:5
Title: IAGO project and development of compound agents
Abstract: The IAGO Project explores the question of whether a software model, in the form of a computational model of cognitive behavior, can contribute to better anticipation of asymmetrical threats. The computational model used in IAGO is based on Cognitive Blending, a theoretical model proposed in the Cognitive Sciences to explain fundamental or backstage cognitive operations in the brain. This model was implemented with the use of multiagent systems that coordinated their activity with a bio-inspired operator called a Connector. This operator and several others used in the IAGO project have been incorporated into a programming library, called the CMAS Library. CMAS stands for Compound Multiagent System. Compound refers to multiagent systems, in which at least some of the agents contain embedded multiagent systems. In the case of IAGO these embedded systems implement Cognitive Blending.
ID:451
CLASS:5
Title: Implantable medical devices as agents and part of multiagent systems
Abstract: The consideration of medical implants as an increasingly important population of isolated agents is a valuable perspective that should not be ignored by the agent community. Implanted Medical Device (IMD) applications are complex, naturally distributed, and could benefit from such attention. This paper explores implantable medical devices and their attributes in an agent context and terminology. It submits that an increasing body of IMDs should be considered agents and that there are opportunities for incorporating these implantable agents into multiagent systems (MAS). This will include: (i) Discussion of several IMDs in traditional agent terms. (ii) Discussion of trends and issues in IMDs related to their potential role in MAS. (iii) Experimental exploration of some potential MAS applications in the problem of medical monitoring. (iv) Broader discussion of the value of framing IMDs and applications involving them in the agent paradigm.
ID:452
CLASS:5
Title: How equitable is rational negotiation?
Abstract: Notions of fairness have recently received increased attention in the context of resource allocation problems, pushed by diverse applications where not only pure utilitarian efficiency is sought. In this paper, we study a framework where allocations of goods result from distributed negotiation conducted by autonomous agents implementing very simple deals. Assuming that these agents are strictly self-interested, we investigate how equitable the outcomes of such negotiation processes are. We first discuss a number of methodological issues raised by this study, pertaining in particular to the design of suitable payment functions as a means of distributing the social surplus generated by a deal amongst the participating agents. By running different experiments, we finally identify conditions favouring equitable outcomes.
ID:453
CLASS:5
Title: Monotonic concession protocols for multilateral negotiation
Abstract: The most natural way of thinking about negotiation is probably a situation whereby each of the parties involved initially make a proposal that is particularly beneficial to themselves and then incrementally revise their earlier proposals in order to come to an agreement. This idea has been formalised in the so-called monotonic concession protocol, a set of rules defining the range of acceptable moves during a negotiation process intended to follow this general scheme. In the case of negotiation between just two agents, the monotonic concession protocol has become a textbook example and its formal properties are well-understood. In the case of multilateral negotiation, where more than two agents need to come to an agreement, on the other hand, it is not at all clear how to set up a monotonic concession protocol. As it turns out, the design of such a protocol boils down to the question of what constitutes a multilateral concession. In this paper, we make several proposals as to what might be an appropriate definition and analyse the properties of the proposed concession criteria.
ID:454
CLASS:5
Title: Resource allocation among agents with preferences induced by factored MDPs
Abstract: Distributing scarce resources among agents in a way that maximizes the social welfare of the group is a computationally hard problem when the value of a resource bundle is not linearly decomposable. Furthermore, the problem of determining the value of a resource bundle can be a significant computational challenge in itself, such as for an agent operating in a stochastic environment, where the value of a resource bundle is the expected payoff of the optimal policy realizable given these resources. Recent work has shown that the structure in agents' preferences induced by stochastic policy-optimization problems (modeled as MDPs) can be exploited to solve the resource-allocation and the policy-optimization problems simultaneously, leading to drastic (often exponential) improvements in computational efficiency. However, previous work used a flat MDP model that scales very poorly. In this work, we present and empirically evaluate a resource-allocation mechanism that achieves much better scaling by using factored MDP models, thus exploiting both the structure in agents' MDP-induced preferences, as well as the structure within agents' MDPs.
ID:455
CLASS:5
Title: Improving resource utilisation in market oriented grid management and scheduling
Abstract: Service providers of the future could dynamically negotiate for, and create their infrastructure on Grid based utility computing and communication providers. Such commercialisation of large scale gridsystems requires the provision of mechanisms to share the wide pool of Grid brokered resources such as computers, software, licences and peripherals amongst many users and organisations. Quickly and efficiently servicing resource requests is critical to the efficiency of such Grid based utility computing and communication providers. However, distributed resource negotiation is itself a contributor to lower system utilisation, as the negotiation process introduces latency and reservation uncertainty in the system. The CORA architecture is a market based resource negotiation system that utilises a Vickrey auction to make allocations of resource requests to resource providers. The architecture utilises a novel combination techniques to improve utilisation, including oversubscription, coallocation, just-in-time reallocation and a novel exible contract structure. This paper introduces two significant improvements to the CORA architecture. Firstly, redundant contracts are generated to resolve the problem of post bid unavailability of bidders. Secondly, this paper utilises a new auction architecture that does not require the auctioneer to be trusted. The advantage is that any entity (untrusted or otherwise) can conduct a verifiable and privacy preserving Vickrey auction, removing the need for a trusted and privileged auctioneer within the system
ID:456
CLASS:5
Title: Task planning for human-robot interaction
Abstract: Human-robot interaction requires explicit reasoning on the human environment and on the robot capacities to achieve its tasks in a collaborative way with a human partner.This paper focuses on organization of the robot decisional abilities and more particularly on the management of human interaction as an integral part of the robot control architecture. Such an architecture should be the framework that will allow the robot to accomplish its tasks but also produce behaviors that support its engagement vis-a-vis its human partner and interpret similar behaviors from him.Together and in coherence with this framework, we intend to develop and experiment various task planners and interaction schemes, that will allow the robot to select and perform its tasks while taking into account explicitly the constraints imposed by the presence of humans, their needs and preferences.We have considered a scheme where the robot plans for itself and for the human in order not only (1) to assess the feasibility of the task (at a certain level) before performing it, but also (2) to share the load between the robot and the human and (3) to explain/illustrate a possible course of action.
ID:457
CLASS:5
Title: Model generation for PRS-like agents
Abstract: We develop a sound foundation for model checking algorithms for the class of PRS-style BDI agents, by showing how a reachability graph for any given PRS-type agent can be constructed from the agent program, thus addressing a long-standing issue in the verification of BDI agents.
ID:458
CLASS:5
Title: Trusted kernel-based coalition formation
Abstract: We define Trusted Kernel-based Coalition Formation as a novel extension to the traditional kernel-based coalition formation process which ensures agents choose the most reliable coalition partners and are guaranteed to obtain the payment they deserve. To this end, we develop an encryption-based communication protocol and a payment scheme which ensure that agents cannot manipulate the mechanism to their own benefit. Moreover, we integrate a generic trust model in the coalition formation process that permits the selection of the most reliable agents over repeated coalition games. We empirically evaluate our mechanism when iterated and show that, in the long run, it always chooses the coalition structure that has the maximum expected value and determines the payoffs that match their level of reliability.
ID:459
CLASS:5
Title: A distributed framework for solving the Multiagent Plan Coordination Problem
Abstract: We examine whether and how the Multiagent Plan Coordination Problem, the problem of resolving interactions between the plans of multiple agents, can be cast as a Distributed Constraint Optimization Problem (DCOP). We use ADOPT, a state-of-the-art DCOP solver that can solve DCOPs in an asynchronous, parallel manner using local communication between individual computational agents. We then demonstrate how we can take advantage of novel flaw-assignment strategies and plan coordination algorithms to significantly improve the performance of ADOPT on representative coordination problems. We close with a consideration of possible advances in framing our DCOP representation of the Multiagent Plan Coordination Problem.
ID:460
CLASS:5
Title: IOM/T: an interaction description language for multi-agent systems
Abstract: A multi-agent system is a useful approach for the complex systems. One of the important concepts of multi-agent systems is cooperativeness, or interactions. However, existing languages for implementing interactions lack expressiveness. This causes gaps between design and implementation. This paper analyzes language functionalities for implementing interactions. Furthermore, a new interaction description language IOM/T is proposed based on the findings. Interaction would become easy to implement based on design using IOM/T.
ID:461
CLASS:5
Title: Foundations of organizational structures in multiagent systems
Abstract: We analyze the notion of organizational structure in multiagent systems and explain the precise added value and the effects of such organizational structure on the involved agents. To pursue this aim, contributions from social and organization theory are considered which provide a solid theoretical foundation to this analysis. We argue that organizational structures should be seen along at least three dimensions, instead of just one: power, coordination, and control. In order to systematize the approach, formal tools are used to describe the organizational structure as well as the effect of such structures on the activities in multiagent systems. We specify the properties and the consequences of organizational structures for the actions of the involved agents.
ID:462
CLASS:5
Title: Using cooperative mediation to coordinate traffic lights: a case study
Abstract: Several approaches tackle the problem of reducing traffic jams. A class of these approaches deals with coordination of traffic lights in order to allow vehicles traveling in a given direction to pass an arterial without stopping at junctions. In short, classical approaches, which are mostly based on offline and centralized determination of the prioritized direction, are quite inflexible since they cannot cope with dynamic changes in the traffic volume. More flexible approaches have been proposed based on implicit coordination and implicit communication (e.g. derived from game theory and swarm intelligence). These have advantages as well as shortcomings. The present paper presents an approach based on cooperative mediation which is a compromise between totally autonomous coordination with implicit communication and the classical centralized solution. We use a distributed constraint optimization algorithm in a dynamic scenario, showing that the mediation is able to reduce the frequency of miscoordination.
ID:463
CLASS:5
Title: Effect of referrals on convergence to satisficing distributions
Abstract: We investigate a framework where agents locate high-quality service providers by using referrals from peer agents. The performance of providers is measured by the satisfaction obtained by agents from using their services. Provider performance depends upon its intrinsic capability and upon its current load. We present an algorithm for selecting a service provider for a given task which includes mechanisms for deciding when and who to ask for a referral. This mechanism requires learning, over interactions, both the performance levels of different service providers, as well as the quality of referrals provided by other agents. We use a satisficing rather than an optimizing framework, where agents are content to receive service quality above a threshold. Agents have to learn the quality of others' referrals and the quality of providers to find satisficing providers. We compare the effectiveness of referral systems with or without deception with systems without referrals. We identify zones, based on an observed entropy metric, where using referrals is helpful in promoting fast convergence to satisficing distributions.
ID:464
CLASS:5
Title: Agent-organized networks for dynamic team formation
Abstract: Many multi-agent systems consist of a complex network of autonomous yet interdependent agents. Examples of such networked multi-agent systems include supply chains and sensor networks. In these systems, agents have a select set of other agents with whom they interact based on environmental knowledge, cognitive capabilities, resource limitations, and communications constraints. Previous findings have demonstrated that the structure of the artificial social network governing the agent interactions is strongly correlated with organizational performance. As multi-agent systems are typically embedded in dynamic environments, we wish to develop distributed, on-line network adaptation mechanisms for discovering effective network structures. Therefore, within the context of dynamic team formation, we propose several strategies for agent-organized networks (AONs) and evaluate their effectiveness for increasing organizational performance.
ID:465
CLASS:5
Title: Efficient learning of multi-step best response
Abstract: We provide a uniform framework for learning against a recent history adversary in arbitrary repeated bimatrix games, by modeling such an agent as a Markov Decision Process. We focus on learning an optimal non-stationary policy in such an MDP over a finite horizon and adapt an existing efficient Monte Carlo based algorithm for learning optimal policies in such MDPs. We show that this new efficient algorithm can obtain higher average rewards than a previously known efficient algorithm against some opponents in the contract game. Though this improvement comes at the cost of increased domain knowledge, a simple experiment in the Prisoner's Dilemma game shows that even when no extra domain knowledge (besides that the opponent's memory size is known) is assumed, the error can still be small.
ID:466
CLASS:5
Title: Using ECA rules to implement mobile query agents for fast-evolving pure P2P database systems
Abstract: A challenging issue in fast-evolving pure P2P networks is the design of an appropriate mechanism for processing queries. Since both the data content of the peers as well as their acquaintances, change rapidly the typical P2P querying techniques become inappropriate. We are interested in P2P networks where peers are mobile and own a database. In this dynamic context the usage of a Mobile Agent framework appears very promising. The paper investigates the issues related to the above problem and proposes a P2P and Mobile Agent architecture based on Active Database technology. We argue that, the employment of ECA rules both for answering queries and deploying agents leads to an efficient as well as simple query processing technique. Furthermore, the proposed mobile agent system architecture offers a number of advantages due to the performance and scalability that can be achieved using Active Databases.
ID:467
CLASS:5
Title: Multi-Agent Patrolling with Reinforcement Learning
Abstract: Patrolling tasks can be encountered in a variety of real-world domains, ranging from computer network administration and surveillance to computer wargame simulations. It is a complex multi-agent task, which usually requires agents to coordinate their decision-making in order to achieve optimal performance of the group as a whole. In this paper, we show how the patrolling task can be modeled as a reinforcement learning (RL) problem, allowing continuous and automatic adaptation of the agents strategies to their environment. We demonstrate that an efficient cooperative behavior can be achieved by using RL methods, such as Q-Learning, to train individual agents. The proposed approach is totally distributed, which makes it computationally efficient. The empirical evaluation proves the effectiveness of our approach, as the results obtained are substantially better than the results available so far on this domain.
ID:468
CLASS:5
Title: Groups as Agents with Mental Attitudes
Abstract: We discuss a model of cooperation among autonomous agents, based on the attribution of mental attitudes to groups: these attitudes represent the shared beliefs and objectives and the wish to reduce the costs for the members. When agents take a decision they have to recursively model what their partners are expected to do under the assumption that they are cooperative, and they have to adopt the goals and desires attributed to the group: otherwise, the other members consider them uncooperative and thus liable.
ID:469
CLASS:5
Title: Issues in Multiagent System Development
Abstract: Methodologies for multiagent system development should assist the developer in making decisions about those aspects of the analysis, design and implementation, that are crucial for multiagent systems, namely, social and cognitive concepts (e.g. norms and goals). In this paper, we review existing agent-oriented methodologies. We conclude that there is a big gap between the analysis and design models and the implementation. We identify some open issues for multiagent system development. We introduce our vision of a development methodology for multiagent systems, based on the OperA analysis models and the agent-oriented programming language 3APL.
ID:470
CLASS:5
Title: A Cooperative Negotiation Protocol for Physiological Model Combination
Abstract: The global model of a complex phenomenon can emerge from the cooperative negotiation of agents embedding local partial models of the phenomenon. We adopted this approach to model complex physiological phenomena, such as those related to the metabolism of glucose-insulin and to the determination of the heart rate (pacing). In this paper we formally describe and analyze the properties of a cooperative negotiation protocol we developed to allow the agents to produce a global coherent model of a physiological phenomenon. We concentrate on the study of the conditions under which an agreement is guaranteed to be reached. We also show details of an application concerning the pacing problem.
ID:471
CLASS:5
Title: Brain Meets Brawn: Why Grid and Agents Need Each Other
Abstract: The Grid and agent communities both develop concepts and mechanisms for open distributed systems, albeit from different perspectives. The Grid community has historically focused on "brawn": infrastructure, tools, and applications for reliable and secure resource sharing within dynamic and geographically distributed virtual organizations. In contrast, the agents community has focused on "brain": autonomous problem solvers that can act flexibly in uncertain and dynamic environments. Yet as the scale and ambition of both Grid and agent deployments increase, we see a convergence of interests, with agent systems requiring robust infrastructure and Grid systems requiring autonomous, flexible behaviors. Motivated by this convergence of interests, we review the current state of the art in both areas, review the challenges that concern the two communities, and propose research and technology development activities that can allow for mutually supportive efforts.
ID:472
CLASS:5
Title: Exploration of Unknown Environments with Motivational Agents
Abstract: This paper addresses the problem of exploring unknown, dynamic environments with motivational agents. The goal is the acquisition of a model of the environment including models of the entities that populate the environment. We describe the exploration strategy of both single and multiple agents. Each agent performs directed exploration using an action selection method based on the maximization of the intensity of positive feelings and minimization of negative ones. The exploration strategy for multiple agents relies on considering a team leader that integrates the maps and coordinates the actions of the members of the team. We present and discuss the results of an experiment conducted in simulated environments.
ID:473
CLASS:5
Title: Towards Automated Procurement via Agent-Aware Negotiation Support
Abstract: Negotiation events in industrial procurement involving multiple, highly customisable goods pose serious challenges to buying agents when trying to determine the best set of providing agents offers. Typically, a buying agents decision involves a large variety of constraints that may involve attributes of a very same item as well as attributes of multiple items. In this paper we describe iBundler, an agent-aware negotiation service to solve the winner determination problem considering buyers and providers constraints and preferences.
ID:474
CLASS:5
Title: Coalition calculation in a dynamic agent environment
Abstract: We consider a dynamic market-place of self-interested agents with differing capabilities. A task to be completed is proposed to the agent population. An agent attempts to form a coalition of agents to perform the task. Before proposing a coalition, the agent must determine the optimal set of agents with whom to enter into a coalition for this task; we refer to this activity as coalition calculation. To determine the optimal coalition, the agent must have a means of calculating the value of any given coalition. Multiple metrics (cost, time, quality etc.) determine the true value of a coalition. However, because of conflicting metrics, differing metric importance and the tendency of metric importance to vary over time, it is difficult to obtain a true valuation of a given coalition. Previous work has not addressed these issues. We present a solution based on the adaptation of a multi-objective optimization evolutionary algorithm. In order to obtain a true valuation of any coalition, we use the concept of Pareto dominance coupled with a distance weighting algorithm. We determine the Pareto optimal set of coalitions and then use an instance-based learning algorithm to select the optimal coalition. We show through empirical evaluation that the proposed technique is capable of eliciting metric importance and adapting to metric variation over time.
ID:475
CLASS:5
Title: Playing the e-business game in 3D virtual worlds
Abstract: In this paper we present an integrated, game-like e-Business environment that follows the role model of Massively Multi-User Online Role-Playing Games (MMORPGs). The interface is realized as a 3D virtual world using affordable game engine technology. Our environment provides a platform for conducting business and it is supposed to be a community facilitator to create and establish a lively and sustainable online community involving both, providers and consumers. It is information-rich and multimedia-based offering transparent access to disparate information sources.
ID:476
CLASS:5
Title: A survey of autonomic communications
Abstract: Autonomic communications seek to improve the ability of network and services to cope with unpredicted change, including changes in topology, load, task, the physical and logical characteristics of the networks that can be accessed, and so forth. Broad-ranging autonomic solutions require designers to account for a range of end-to-end issues affecting programming models, network and contextual modeling and reasoning, decentralised algorithms, trust acquisition and maintenance---issues whose solutions may draw on approaches and results from a surprisingly broad range of disciplines. We survey the current state of autonomic communications research and identify significant emerging trends and techniques.
ID:477
CLASS:5
Title: A reactive agent-based problem-solving model: Application to localization and tracking
Abstract: For two decades, multi-agent systems have been an attractive approach for problem solving and have been applied to a wide range of applications. Despite the lack of generic methodology, the reactive approach is interesting considering the properties it provides. This article presents a problem-solving model based on a swarm approach where agents interact using physics-inspired mechanisms. The initial problem and its constraints are represented through agents' environment, the dynamics of which is part of the problem-solving process. This model is then applied to localization and target tracking. Experiments assess our approach and compare it to widely-used classical algorithms.
ID:478
CLASS:5
Title: Teaching undergraduate software design in a liberal arts environment using RoboCup
Abstract: Most large research universities include a software design or software development course as a required or elective component of an undergraduate computer science major. For various reasons, some institutions, including many liberal arts colleges and primarily undergraduate institutions, do not. In this paper, we present a software design course, tailored to undergraduate computer science students within a liberal arts environment, based on the RoboCup soccer simulation platform. We describe the course curriculum and outline its goals, which student evaluations suggest it achieved. We also outline the features of our "NewKrislet" soccer player, which provides an elementary but sufficiently functional entry point to Robocup client design.
ID:479
CLASS:5
Title: Automated negotiation for order transaction of injection mold manufacturer
Abstract: Today, there are several markets in cyber space where companies trade electronically due to the development of Information Technology. On the other hand, the most important thing in trades is negotiation. So, in order to support current business practices as well as new ones on the Internet, electronic commerce systems need an ability to negotiate. In this paper, proposed is a method by which a seller can be supported by an agent which plays a role in negotiation process among small and medium companies, especially injection mold manufacturer. If the manufacturing capacity cannot afford to produce all orders, the manufacturer may want to extend due dates and the buyers may want to discount prices. The negotiation agent discussed in this paper cooperates with the schedule agent to get due-date information, and performs a role in one (seller)-to-many (buyer) negotiation processes.
ID:480
CLASS:5
Title: Design considerations for multiagent systems on very small platforms
Abstract: The large and growing number of PDAs, tablet computers, JavaPhones, and other small footprint devices are a tempting target for deployment of multiagent systems (MAS). In this paper we describe previous and ongoing efforts to develop Cougaar MicroEdition (CougaarME), a scaled down version of the DARPA Cougaar MAS. We describe the challenges of small devices (including hardware choices and constraints) and the challenges of architecting the various elements (communications, service advertisement, policies).
ID:481
CLASS:5
Title: A decision making procedure for collaborative planning
Abstract: A team of agents planning to perform a complex task make a number of interrelated decisions as they determine precisely how that complex task will be performed. The decision set includes the choice of recipes and parameters as well as the determination of responsibilities for each agent. This work formally defines a search problem with search operators that correspond to the team planning decisions. It defines rationality for a collaborative team and ensures that a team will abandon performance of a complex task if it is ultimately in its economic best interest to do so. The model respects the constraints on mental states specified by the SharedPlans theory of collaboration.
ID:482
CLASS:5
Title: Advice-exchange in heterogeneous groups of learning agents
Abstract: This research aims at studying the effects of exchanging information during the learning process in Multiagent Systems. The concept of advice-exchange, introduced in [3], consists in requesting extra feedback, in the form of episodic advice, from other agents that are solving similar problems. This work is concerned with the exchange of information in heterogeneous groups of learning-agents that either share the same environment or are solving problems with similar structure. Concepts, such as self confidence, trust and advisor preference, were introduced in the experiments that led to the results discussed in this paper.
ID:483
CLASS:5
Title: Distributed coordination based on temporal planning for tactical aircraft simulation
Abstract: This paper deals with the modeling of agents plans thanks to hybrid automata which support the evolution of time and resources. The main advantage of the formalism we chose is the control and the validation of both individual and multi-agent plans. The paper presents significant operators e.g. merging, reallocation, etc.) to manage the multi-agent plans and to avoid replanning the multi-agent plan from scratch.
ID:484
CLASS:5
Title: Distribution of goals addressed to a group of agents
Abstract: The problem investigated in this paper is the distribution of goals addressed to a group of rational agents. Those agents are characterized by their ability (i.e. what they can do), their knowledge about the world and their commitments.The goals of the group are represented by conditional preferences. In order to deduce the actual goals of the group, we determine its ability using each agent's ability and we suppose that the agents share a common knowledge about the world. The individual goals of an agent are deduced using its ability, the knowledge it has about the world, its own commitments and the commitments of the other agents of the group.
ID:485
CLASS:5
Title: Commitments and causality for multiagent design
Abstract: This paper unifies two recent strands of research in multiagent system design. One, commitments are widely recognized as capturing important aspects of interactions among agents, but current approaches tend to emphasize individual commitments and typically restrict themselves to interactions between pairs of agents. Two, methodologies for multiagent system design consider protocols and coordination requirements, but do not seriously accommodate commitments. This paper proposes a methodology to infer commitments from an example conversation among several parties. Based on the conversation, we build a commitment causality diagram indicating the causal relations among the commitments. Using this diagram, we generate behavior models for each role. We show that the models produced successfully capture commitment-level protocols and allow flexible implementation of non-commitment communications provided the causal relations are preserved.
ID:486
CLASS:5
Title: Introducing human-like hearing perception in intelligent virtual agents
Abstract: Although, at this moment, there are not many researches on perceptual models for Intelligent Virtual Agents, the few that exist are more focused on visual than on hearing perception. This paper describes the work that the Universidad Politicnica de Madrid, in collaboration with the University of Nottingham, is carrying on, with the aim of endowing Intelligent Virtual Agents with a synthetic model of human-like hearing perception. We have based the design and development of this model on Endsley studies about situational awareness [10,11], where perception can be understood as the first level of awareness. This perceptual model also extends and reinterprets the key concepts introduced by a CSCW model of awareness known as the Spatial Model of Interaction [4]. In this paper, we give an overview of those studies related to our work and we describe how we have achieved our purposes: selecting an agent architecture; re-defining and reinterpreting the set of key concepts introduced by the Spatial Model of Interaction; and analysing those factors that make the auditory perceptual model more realistic. We also expose how a mathematical function can describe the agents clarity of perception and how this model of perception has been implemented and evaluated.
ID:487
CLASS:5
Title: A multiagent approach for musical interactive systems
Abstract: The present work proposes a model for a Multiagent System capable to deal with music. Through such a Musical Multiagent System many Computer Music problems may appear. This proposal is based on a community of agents that interact through musical events (MIDI), simulating the behavior of a musical group. The result is the implementation of an intelligent accompaniment system, where agents "listen" and interact with each other. Agents with musical cognition were able to play their instruments with synchronism and satisfy their internal goals.
ID:488
CLASS:5
Title: LOTTO: group formation by overhearing in large teams
Abstract: We present in this paper an extension to our overhearing-based group formation framework \otto (Organizing Teams Through Overhearing). This framework allows a team to dynamically re-organize itself in accordance to the evolution of the communication possibilities between agents. In \otto, each agent overhears some of the messages exchanged within the team and uses them to incrementally update a map of the organization of the team. One of the key points of \otto is that only few resources are used - a small data structure is used to track each team member and there is no need to store the overheard messages. With \lotto (\otto for Large numbers of agents) we address the problem of large teams in which using \otto "as is" would be costly in terms of memory and therefore contrary to \otto's philosophy of low resource usage. Therefore, we have implemented a strategy that allows an agent with a limited memory to track only a part of the team. We have run a series of experiments in order to evaluate the impact of this limitation and present some results.
ID:489
CLASS:5
Title: Performance models for large scale multiagent systems: using distributed POMDP building blocks
Abstract: Given a large group of cooperative agents, selecting the right coordination or conflict resolution strategy can have a significant impact on their performance (e.g., speed of convergence). While performance models of such coordination or conflict resolution strategies could aid in selecting the right strategy for a given domain, such models remain largely uninvestigated in the multiagent literature. This paper takes a step towards applying the recently emerging distributed POMDP (partially observable Markov decision process) frameworks, such as MTDP (Markov team decision process), in service of creating such performance models. To address issues of scale-up, we use small-scale models, called building blocks that represent the local interaction among a small group of agents. We discuss several ways to combine building blocks for performance prediction of a larger-scale multiagent system.We present our approach in the context of DCSPs (distributed constraint satisfaction problems), where we first show that there is a large bank of conflict resolution strategies and no strategy dominates all others across different domains. By modeling and combining building blocks, we are able to predict the performance of five different DCSP strategies for four different domain settings, for a large-scale multiagent system. Our approach thus points the way to new tools for strategy analysis and performance modeling in multiagent systems in general.
ID:490
CLASS:5
Title: Helping based on future expectations
Abstract: Autonomous agents interacting in an open world can be considered to be primarily driven by self interests. Previous work in this area has prescribed a strategy of reciprocal behavior, based on past interactions, for promoting and sustaining cooperation among such self-interested agents. Here we present a new mechanism where agents base their decisions both on historical data as well as on future interaction expectations. A decision mechanism is presented that compares current helping cost with expected future savings from interaction with the agent requesting help. We experiment with heterogeneous agents that have varying expertise for different job types. We evaluate the effect of both change of agent expertise and distribution of task types on subsequent agent relationships. The reciprocity mechanism based on future expectations is found to be robust and flexible in adjusting to the environmental dynamics.
ID:491
CLASS:5
Title: Concurrent layered learning
Abstract: Hierarchies are powerful tools for decomposing complex control tasks into manageable subtasks. Several hierarchical approaches have been proposed for creating agents that can execute these tasks. Layered learning is such a hierarchical paradigm that relies on learning the various subtasks necessary for achieving the complete high-level goal. Layered learning prescribes training low-level behaviors (those closer to the environmental inputs) prior to high-level behaviors. In past implementations these lower-level behaviors were always frozen before advancing to the next layer. In this paper, we hypothesize that there are situations where layered learning would work better were the lower layers allowed to keep learning concurrently with the training of subsequent layers, an approach we call concurrent layered learning. We identify a situation where concurrent layered learning is beneficial and present detailed empirical results verifying our hypothesis. In particular, we use neuro-evolution to concurrently learn two layers of a layered learning approach to a simulated robotic soccer keepaway task. The main contribution of this paper is evidence that there exist situations where concurrent layered learning outperforms traditional layered learning. Thus, we establish that, when using layered learning, the concurrent training of layers can be an effective option.
ID:492
CLASS:5
Title: Capturing agent autonomy in roles and XML
Abstract: A key question in the field of agent-oriented software engineering is how the kind and extent of autonomy owned by computational agents can be appropriately captured. As long as this question is not answered convincingly, it is very unlikely that agent-oriented software (having "autonomy" as a real property rather than just a catchy label) gets broadly accepted in industry and commerce. In particular, in order to be of practical value an answer to this question has to come in form of concrete techniques which enable developers of agent-oriented software to precisely capture the scope of behavioral freedom and self-control they want to concede to a computational agent. This paper describes two such techniques. First, a formal schema called RNS for specifying the boundaries of autonomous agent behavior. This schema is conceptually grounded in sociological role theory, and employs the concepts of role, norm and sanction to capture agent autonomy. What makes RNS particularly valuable and distinct from related autonomy specification approaches is, among other things, its strong expressiveness and high precision. Second, a software tool called XRNS which enables developers to easily generate RNS-based autonomy specifications in XML format. Encoded in XML, these specifications are easily accessible to all stakeholders in an agent-oriented software under development, and can be even processed directly by XML enabled computational agents.
ID:493
CLASS:6
Title: Training neural network language models on very large corpora
Abstract: During the last years there has been growing interest in using neural networks for language modeling. In contrast to the well known back-off n-gram language models, the neural network approach attempts to overcome the data sparseness problem by performing the estimation in a continuous space. This type of language model was mostly used for tasks for which only a very limited amount of in-domain training data is available.In this paper we present new algorithms to train a neural network language model on very large text corpora. This makes possible the use of the approach in domains where several hundreds of millions words of texts are available. The neural network language model is evaluated in a state-of-the-art real-time continuous speech recognizer for French Broadcast News. Word error reductions of 0.5% absolute are reported using only a very limited amount of additional processing time.
ID:494
CLASS:6
Title: Residual speech signal compression: an experiment in the practical application of neural network technology
Abstract: Neural networks are a popular area of research today. However, neural network algorithms have only recently proven valuable to application problems. This paper seeks to aid in the process of transferring neural network technology from research to a development environment by describing our experience in applying this technology.The application studied here is Speaker Identity Verification (SIV), which is the task of verifying a speaker's identity by comparing the speaker's voice pattern to a stored template.In this paper, we describe the application of the back-propagation neural network algorithm to one aspect of the SIV problem, called Residual Compression (RC). The RC problem is to extract useful features from a part of the speech signal that was not utilized by previous SIV systems. Here, we describe a neural network architecture, pre-processing algorithm, training methodology, and empirical results for this problem. We also present a few guidelines for the use of neural networks in applied settings.
ID:495
CLASS:6
Title: Real time application of artificial neural network for incipient fault detection of induction machines
Abstract: This paper describes several artificial neural network architectures for real time application in incipient fault detection of induction machines. The artificial neural networks perform the fault detection in real time, based on direct measurements from the motor, and no rigorous mathematical model of the motor is needed. Different approaches used to develop a reliable fault detector are presented and compared in this paper. The designed networks vary in complexity and accuracy. A high-order fault detector neural network is discussed first. Then noise considerations are included in more complex fault detector models, since noise is an important factor in the design and analysis of real time fault detector neural networks. Simulation results show that with appropriate designs, artificial neural networks perform satisfactorily in real time incipient fault detection of induction machines.
ID:496
CLASS:6
Title: Continuous learning: a design methodology for fault-tolerant neural networks
Abstract: Fault tolerance in artificial neural networks is an important feature, in particular when the application is critical or when maintenance is difficult. This paper presents a general design methodology for designing fault-tolerant architectures, starting from the behavioral description of the nominal network and from the nominal algorithm. The behavioral level is considered to detect errors due to hardware faults, while system survival is guaranteed by the reactivation of learning mechanisms of the nominal network. An example of the use of this methodology is presented and evaluated.
ID:497
CLASS:6
Title: 3D object reconstruction and representation using neural networks
Abstract: 3D object reconstruction is frequent used in various fields such as product design, engineering, medical and artistic applications. Numerous reconstruction techniques and software were introduced and developed. However, the purpose of this paper is to fully integrate an adaptive artificial neural network (ANN) based method in reconstructing and representing 3D objects. This study explores the ability of neural networks in learning through experience when reconstructing an object by estimating it's z-coordinate. Neural networks' capability in representing most classes of 3D objects used in computer graphics is also proven. Simple affined transformation is applied on different objects using this approach and compared with the real objects. The results show that neural network is a promising approach for reconstruction and representation of 3D objects.
ID:498
CLASS:6
Title: Artificial neural networks: a science in trouble
Abstract: This article points out some very serious misconceptions about the brain in connectionism and artificial neural networks. Some of the connectionist ideas have been shown to have logical flaws, while others are inconsistent with some commonly observed human learning processes and behavior. For example, the connectionist ideas have absolutely no provision for learning from stored information, something that humans do all the time. The article also argues that there is definitely a need for some new ideas about the internal mechanisms of the brain. It points out that a very convincing argument can be made for a "control theoretic" approach to understanding the brain. A "control theoretic" approach is actually used in all connectionist and neural network algorithms and it can also be justified from recent neurobiological evidence. A control theoretic approach proposes that there are subsystems within the brain that control other subsystems. Hence a similar approach can be taken in constructing learning algorithms and other intelligent systems.
ID:499
CLASS:6
Title: Mining sales data using a neural network model of market response
Abstract: Modeling aggregate market response is a core issue in marketing research. In this research, we extend previous forecasting comparative research by comparing the forecasting accuracy of feed-forward neural network models to the premier market modeling technique, Multiplicative Competitive Interaction (MCI) models. Forecasts are compared in two separate studies: (1) the Information Resources Inc. (IRI) coffee dataset from Marion, IN and (2) the A. C. Nielsen catsup dataset from Sioux Falls, SD. Our results suggest neural networks are a useful substitute for MCI models when there are too few observations available to estimate a fully-extended MCI model. Implications are discussed.
ID:500
CLASS:6
Title: Inspection effectiveness in software development: a neural network approach
Abstract: In this paper we discuss an analysis of inspection effectiveness based on defect escapes. We present a neural network approach to inspection based on the back propagation model for identifying inspections with defect escapes. Our analysis shows several findings that provide new insights on defect escapes and inspection effectiveness. Our approach is quite novel, not only because of its focus on defect escapes, but also because of its application of neural network techniques to the analysis of software inspection effectiveness. We believe that other software development organizations may benefit from this work as well.
ID:501
CLASS:6
Title: Extracting decision trees from trained neural networks
Abstract: Neural Networks are successful in acquiring hidden knowledge in datasets. Their biggest weakness is that the knowledge they acquire is represented in a form not understandable to humans. Researchers tried to address this problem by extracting rules from trained Neural Networks. Most of the proposed rule extraction methods required specialized type of Neural Networks; some required binary inputs and some were computationally expensive. Craven proposed extracting MofN type Decision Trees from Neural Networks. We believe MofN type Decision Trees are only good for MofN type problems and trees created for regular high dimensional real world problems may be very complex. In this paper, we introduced a new method for extracting regular C4.5 like Decision Trees from trained Neural Networks. We showed that the new method (DecText) is effective in extracting high fidelity trees from trained networks. We also introduced a new discretization technique to make DecText be able to handle continuous features and a new pruning technique for finding simplest tree with the highest fidelity.
ID:502
CLASS:6
Title: Neural network models in simulation: a comparison with traditional modeling approaches
Abstract: Neural models are enjoying a resurgence in systems research primarily due to a general interest in the connectionist approach to modeling in artificial intelligence and to the availability of faster and cheaper hardware on which neural net simulations can be executed. We have experimented with using a multi-layer neural network model as a simulation model for a basic ballistics model. In an effort to evaluate the efficiency of the neural net implementation for simulation modeling, we have compared its performance with traditional methods for geometric data fitting such as linear regression and surface response methods. Both of the latter approaches are standard features in many statistical software packages. We have found that the neural net model appears to be inadequate in most respects and we hypothesize that accuracy problems arise, primarily, because the neural network model does not capture the system structure characteristic of all physical models. We discuss the experimental procedure, issues and problems, and finally consider possible future research directions.
ID:503
CLASS:6
Title: Artificial neural network technology
Abstract: The Defense Advanced Research Projects Agency (DARPA) has initiated a major new program in Artificial Neural Network Technology. This technology may lead to solution of complex information processing and autonomous control problems (including problems that require real-time processing and response) that have persistently evaded solution by conventional techniques. If its promise can be realized, artificial neural network technology will provide powerful tools for a broad range of military applications, including sophisticated systems for target recognition and tracking and for real-time guidance.
ID:504
CLASS:6
Title: Neural network simulation on shared-memory vector multiprocessors
Abstract: We simulate three neural networks on a vector multiprocrssor. The training time can be reduced significantly especially when the training data size is large. These three neural networks are: 1) the feedforward network, 2) the recurrent network and 3) the Hopfield network. The training algorithms are programmed in such a way to best utilize 1) the inherent parallelism in neural computing, and 2) the vector and concurrent operations available on the parallel machine. To prove the correctness of parallelized training algorithms, each neural network is trained to perform a specific function. The feedforward network is trained to perform the Fourier transform, the recurrent network is trained to predict the solution of a delay differential equation, the Hopfield network is trained to solve the traveling salesman problem. The machine we experiment with is the Alliant FX/80.
ID:505
CLASS:6
Title: Selection of a neural network system for visual inspection
Abstract: This paper provides a background to the field of neural networks with a particular focus on vision applications. It includes an extensive literature review and reports on preliminary work towards the implementation of a visual inspection application using neural network simulators.
ID:506
CLASS:6
Title: Neural networks and artificial intelligence
Abstract: Neural networks have been called &ldquo;more important than the atomic bomb&rdquo; and have received a major funding commitment from DARPA. Nevertheless, it is difficult to find even a mention of neural network concepts and applications in many computer science or information systems curricula. In fact, few computer science or information systems faculty are aware of the profound implications of neurocomputing on the future of their field. This paper contends that neural networks must be a significant part of any artificial intelligence course. It illustrates how neural network concepts can be integrated into traditional artificial intelligence course material. Two programming packages for simulating neural networks on personal computers are recommended.
ID:507
CLASS:6
Title: Critical issues in mapping neural networks on message-passing multicomputers
Abstract: Connectionist models such as artificial neural systems, offer an intrinsically concurrent computational paradigm. We investigate the architectural requirements for efficiently simulating large neural networks on a multicomputer system with thousands of fine-grained processors and distributed memory. First, models for characterizing the structure of a neural network and the function of individual cells are developed. These models provide guidelines for efficiently mapping the network onto multicomputer topologies such as the hypercube, hypernet and torus. They are further used to estimate the amount of interprocessor communication bandwidth required, and the number of processors needed to meet a particular cost/performance goal. Design issues such as memory organization and the effect of VLSI technology are also considered.
ID:508
CLASS:6
Title: Pattern-based fault diagnosis using neural networks
Abstract: The detection and diagnosis of faults in real time are active areas of research in knowledge-based expert systems. Several methods of diagnosis have been applied to a variety of physical systems. Rule-based approaches have been applied successfully to some domains. However, encoding knowledge in rule bases raises many difficult knowledge acquisition issues; in addition, rule-based systems are often too slow to be effectively applied in a real-time environment. More advanced diagnostic systems may incorporate a simulation of the physical system in the knowledge base. Although simulation-based expert systems can exhibit powerful capabilities, simulating the domain properly may be difficult and too computationally intensive for real-time diagnosis.An effort is underway at The University of Tennessee Space Institute to develop diagnostic expert system methodologies based on the analysis of patterns of behavior of physical mechanisms. In this approach, fault diagnosis is conceptualized as the mapping or association of patterns of input data (e.g., from instrumentation) to patterns representing one or more fault conditions. Associative memories and neural networks are being investigated as a means of storing and retrieving fault scenarios, as they offer several powerful and useful features, including 1) general mapping capabilities, 2) resistance to noisy input data, 3) the ability to be trained in a supervised learning mode, and 4) the capability of operation with incomplete input.Pattern-based fault diagnosis and detection methodologies are currently being applied to jet and rocket engines. These domains are characterized by failure scenarios which may be catastrophic, and may occur over very short time periods. A requirement of the present study is that diagnoses be performed in real time, in order to allow time for effective action to be taken prior to possible engine destruction.              This paper 1) outlines an architecture for a real-time pattern-based diagnostic expert system capable of accommodating noisy, incomplete, and possibly erroneous input data, and 2) presents results from prototype systems applied to jet and rocket engine fault diagnosis.
ID:509
CLASS:6
Title: Software for neural networks
Abstract: Neural networks "compute" though not in the way that traditional computers do. It is necessary to accept their weaknesses to use their strengths. We discuss some of the assumptions and constraints that govern operation of neural nets, describe one particular non-linear network---the BSB model---in a little detail, and present two applications of neural network computations to illustrate some of the peculiarities inherent in this architecture. We show how a network can be trained to estimate answers to simple multiplication problems and how a network can be used to disambiguate lexical items by context. In both examples, the way information is represented in the pattern of system unit activities is at least as important as the details of the learning and retrieval algorithms used.
ID:510
CLASS:6
Title: An intelligent neural network programming system (NNPS)
Abstract: A neural network programming system based on parallel neural information processing has been presented. With the neural network programming system built upon a 100M local computer network, the system has thus provided users high speed, general purpose and large scale neural network application development platforms.
ID:511
CLASS:6
Title: Neural networks and dynamic complex systems
Abstract: We describe the use of neural networks for optimization and inference associated with a variety of complex systems. We show how a string formalism can be used for parallel computer decomposition, message routing and sequential optimizing compilers. We extend these ideas to a general treatment of spatial assessment and distributed artificial intelligence.
ID:512
CLASS:6
Title: Constructing deterministic finite-state automata in recurrent neural networks
Abstract: Recurrent neural networks that are trained to behave like deterministic finite-state automata (DFAs) can show deteriorating performance when tested on long strings. This deteriorating performance can be attributed to the instability of the internal representation of the learned DFA states. The use of a sigmoidel discriminant function together with the recurrent structure contribute to this instability. We prove that a simple algorithm can construct second-order recurrent neural networks with a sparse interconnection topology and sigmoidal discriminant function such that the internal DFA state representations are stable, that is, the constructed network correctly classifies strings of arbitrary length. The algorithm is based on encoding strengths of weights directly into the neural network. We derive a relationship between the weight strength and the number of DFA states for robust string classification. For a DFA with n state and minput alphabet symbols, the constructive algorithm generates a &ldquo;programmed&rdquo; neural network with O(n) neurons and O(mn) weights. We compare our algorithm to other methods proposed in the literature.
ID:513
CLASS:6
Title: Time series forecasting using neural networks
Abstract: Artificial neural networks are suitable for many tasks in pattern recognition and machine learning. In this paper we present an APL system for forecasting univariate time series with artificial neural networks. Unlike conventional techniques for time series analysis, an artificial neural network needs little information about the time series data and can be applied to a broad range of problems. However, the problem of network &ldquo;tuning&rdquo; remains: parameters of the backpropagation algorithm as well as the network topology need to be adjusted for optimal performances. For our application, we conducted experiments to find the right parameters for a forecasting network. The artificial neural networks that were found delivered a better forecasting performance than results  obtained by the well known ARIMA technique.
ID:514
CLASS:6
Title: The time dimension of neural network models
Abstract: This review attempts to provide an insightful perspective on the role of time within neural network models and the use of neural networks for problems involving time. The most commonly used neural network models are defined and explained giving mention to important technical issues but avoiding great detail. The relationship between recurrent and feedforward networks is emphasised, along with the distinctions in their practical and theoretical abilities. Some practical examples are discussed to illustrate the major issues concerning the application of neural networks to data with various types of temporal structure, and finally some highlights of current research on the more difficult types of problems are presented.
ID:515
CLASS:6
Title: Is designing a neural network application an art or a science?
Abstract: Both the danger and the attraction of pattern recognition problems lie in their simplicity. When investigators first approach such a problem, they may, with an easiness which borders on carelessness, utilize the methods most familiar to them; and... they sometimes receive satisfactory results. More often, they are haunted by troubles. Only a good understanding of the problem will permit them to solve it. The quality of problem solving depends on the investigator's abilities, experience, knowledge and very often on... fertile imagination. In this paper we will discuss the problems connected with practical neural network application design.
ID:516
CLASS:6
Title: A cascading neural-net for traffic management of computer networks
Abstract: An effective method to execute the traffic management of computer networks using a cascading neural-net (CNN) is proposed in this paper. The proposed CNN consists of a two-level neural model. The lower level, the back-propagation neural model, will detect whether the tested network is overloaded or not. The higher level, the counter-propagation neural model, will classify and exclude the status of congestion derived from the overload of tested network. Therefore, if the diagnostic effect gained from the lower level neural model is positive, the higher level neural model will be triggered to map one of flow exemplars to reroute the traffic of computer networks. These two-level neural model is iteratively and interchangeably executed to achieve the objective of traffic management. To validate the feasibility, the proposed CNN has been applied to a LAN environment. The experimental results demonstrate that the developed CNN can efficiently and effectively provide substantial assistance for decision making in network traffic management.
ID:517
CLASS:6
Title: Identification of parallelism in neural networks by simulation with language J.
Abstract: Neural networks, trained by backpropagation, are designed and described in the language J, an APL derivative with powerful function encapsulation features. Both the languages J [4,6,7] and APL [5] help to identify and isolate the parallelism that is inherent in network training algorithms. Non-critical details of data input and derived output processes are de-emphasized by relegating those functions to callable stand-alone modules. Such input and output modules can be isolated and customized individually for managing communication with arbitrary, external storage systems. The central objective of this research is the design and precise description of a neural network training kernel. Such kernel designs are valuable for producing efficient reusable computer codes and facilitating the transfer of neural network technology from developers to users.
ID:518
CLASS:6
Title: A concurrent object-oriented framework for the simulation of neural networks
Abstract: This paper discusses the issues in simulating neural networks using an object-oriented concurrent programming framework, based on our experience in developing two generations of the NSL (Neural Simulation Language) simulation system. The second generation simulation system, NSL 2.0, was designed and implemented utilizing object-oriented programming concepts. We close with future design and implementation directions.
ID:519
CLASS:6
Title: Facilitating neural dynamics for delay compensation and prediction in evolutionary neural networks
Abstract: Delay in the nervous system is a serious issue for an organism that needs to act in real time. For example, during the time a signal travels from a peripheral sensor to the central nervous system, a moving object in the environment can cover a significant distance which can lead to critical errors in the effect of the corresponding motor output. This paper proposes that facilitating synapses which show a dynamic sensitivity to the changing input may play an important role in compensating for neural delays, through extrapolation. The idea was tested in a modified 2D pole-balancing problem which included sensory delays. Within this domain, we tested the behavior of recurrent neural networks with facilitatory neural dynamics trained via neuroevolution. Analysis of the performance and the evolved network parameters showed that, under various forms of delay, networks utilizing extrapolatory dynamics are at a significant competitive advantage compared to networks without such dynamics. In sum, facilitatory (or extrapolatory) dynamics can be used to compensate for delay at a single-neuron level, thus allowing a developing nervous system to stay in touch with the present environmental state.
ID:520
CLASS:6
Title: Detection of electromyographic signals from facial muscles with neural networks
Abstract: The goal of this research was to investigate neural network-based methods to be applied in the processing of biomedical signals. We developed a neural network-based method for the detection of voluntarily produced changes in facial muscle action potentials. Electromyographic signals were recorded from the corrugator supercilii and zygomaticus major facial muscles. The facial muscle action potentials of thirty subjects were measured while they performed a series of voluntary contractions of these muscles. Wavelet denoising or digital bandpass filtering was applied to the preprocessing of the signals. A neural network was exploited for an offline classification of various phases of these signals. The results show that the neural network-based technique developed functioned very well, producing a reliable recognition accuracy of 96 to 99&percnt;. Because of these promising results, we will proceed in the development of this method for real-time applications that benefit from the analysis of electromyographic signals.
ID:521
CLASS:6
Title: An intelligent agent approach for teaching neural networks using LEGO\&\#174; handy board robots
Abstract: In this article we describe a project for an undergraduate artificial intelligence class. The project teaches neural networks using LEGO&reg; handy board robots. Students construct robots with two motors and two photosensors. Photosensors provide readings that act as inputs for the neural network. Output values power the motors and maintain the robot along the designated path. In doing this project, students come to realize the difference between training a neural network and the trained neural network. The fun factor associated with this project has encouraged students to elect artificial intelligence as part of their course of study.
ID:522
CLASS:6
Title: The Applicability of Recurrent Neural Networks for Biological Sequence Analysis
Abstract: Selection of machine learning techniques requires a certain sensitivity to the requirements of the problem. In particular, the problem can be made more tractable by deliberately using algorithms that are biased toward solutions of the requisite kind. In this paper, we argue that recurrent neural networks have a natural bias toward a problem domain of which biological sequence analysis tasks are a subset. We use experiments with synthetic data to illustrate this bias. We then demonstrate that this bias can be exploitable using a data set of protein sequences containing several classes of subcellular localization targeting peptides. The results show that, compared with feed forward, recurrent neural networks will generally perform better on sequence analysis tasks. Furthermore, as the patterns within the sequence become more ambiguous, the choice of specific recurrent architecture becomes more critical.
ID:523
CLASS:6
Title: Discovering Gene Networks with a Neural-Genetic Hybrid
Abstract: Recent advances in biology (namely, DNA arrays) allow an unprecedented view of the biochemical mechanisms contained within a cell. However, this technology raises new challenges for computer scientists and biologists alike, as the data created by these arrays is often highly complex. One of the challenges is the elucidation of the regulatory connections and interactions between genes, proteins and other gene products. In this paper, a novel method is described for determining gene interactions in temporal gene expression data using genetic algorithms combined with a neural network component. Experiments conducted on real-world temporal gene expression data sets confirm that the approach is capable of finding gene networks that fit the data. A further repeated approach shows that those genes significantly involved in interaction with other genes can be highlighted and hypothetical gene networks and circuits proposed for further laboratory testing.
ID:524
CLASS:6
Title: A speech synthesizer for Persian text using a neural network with a smooth ergodic HMM
Abstract: The feasibility of converting text into speech using an inexpensive computer with minimal memory is of great interest. Speech synthesizers have been developed for many popular languages (e.g., English, Chinese, Spanish, French, etc.), but designing a speech synthesizer for a language is largely dependant on the language structure. In this article, we develop a Persian synthesizer that includes an innovative text analyzer module. In the synthesizer, the text is segmented into words and after preprocessing, a neural network is passed over each word. In addition to preprocessing, a new model (SEHMM) is used as a postprocessor to compensate for errors generated by the neural network. The performance of the proposed model is verified and the intelligibility of the synthetic speech is assessed via listening tests.
ID:525
CLASS:6
Title: The principled design of large-scale recursive neural network architectures--dag-rnns and the protein structure prediction problem
Abstract: We describe a general methodology for the design of large-scale recursive neural network architectures (DAG-RNNs) which comprises three fundamental steps: (1) representation of a given domain using suitable directed acyclic graphs (DAGs) to connect visible and hidden node variables; (2) parameterization of the relationship between each variable and its parent variables by feedforward neural networks; and (3) application of weight-sharing within appropriate subsets of DAG connections to capture stationarity and control model complexity. Here we use these principles to derive several &lt;em&gt;specific&lt;/em&gt; classes of DAG-RNN architectures based on lattices, trees, and other structured graphs. These architectures can process a wide range of data structures with variable sizes and dimensions. While the overall resulting models remain probabilistic, the internal deterministic dynamics allows efficient propagation of information, as well as training by gradient descent, in order to tackle large-scale problems. These methods are used here to derive state-of-the-art predictors for protein structural features such as secondary structure (1D) and both fine- and coarse-grained contact maps (2D). Extensions, relationships to graphical models, and implications for the design of neural architectures are briefly discussed. The protein prediction servers are available over the Web at: &lt;a target=_new href="http://www.igb.uci.edu/tools.htm"&gt;www.igb.uci.edu/tools.htm&lt;/a&gt;.
ID:526
CLASS:6
Title: Mining heterogeneous gene expression data with time lagged recurrent neural networks
Abstract: Heterogeneous types of gene expressions may provide a better insight into the biological role of gene interaction with the environment, disease development and drug effect at the molecular level. In this paper for both exploring and prediction purposes a Time Lagged Recurrent Neural Network with trajectory learning is proposed for identifying and classifying the gene functional patterns from the heterogeneous nonlinear time series microarray experiments. The proposed procedures identify gene functional patterns from the dynamics of a state-trajectory learned in the heterogeneous time series and the gradient information over time. Also, the trajectory learning with Back-propagation through time algorithm can recognize gene expression patterns vary over time. This may reveal much more information about the regulatory network underlying gene expressions. The analyzed data were extracted from spotted DNA microarrays in the budding yeast expression measurements, produced by Eisen et al. The gene matrix contained 79 experiments over a variety of heterogeneous experiment conditions. The number of recognized gene patterns in our study ranged from two to ten and were divided into three cases. Optimal network architectures with different memory structures were selected based on Akaike and Bayesian information statistical criteria using two-way factorial design. The optimal model performance was compared to other popular gene classification algorithms such as Nearest Neighbor, Support Vector Machine, and Self-Organized Map. The reliability of the performance was verified with multiple iterated runs.
ID:527
CLASS:6
Title: Load balancing loosely synchronous problems with a neural network
Abstract: Hopfield and Tank have introduced the use of neural networks for the solution of optimization problems such as the traveling salesman problem. Here we show how to generalize this method to decompose loosely synchronous problems onto parallel machines and in particular the hypercube. In this case, decomposition or load balancing can be formulated graph theoretically in terms of optimal partitioning of the computational graph into N = 2d subgraphs. The algorithm has a suggestive spin system interpretation, with the ferromagnetic interaction minimizing the communication and the long range paramagnetic force balancing the load. The optimal fixed point of the network is in the Higgs phase of the magnet, with the domains of constant spontaneous magnetization representing the optimal decomposition map.The method is fast, reliable and admits various simple implementations: sequential, concurrent on the hypercube, analog on the neural network with adaptive weights (&ldquo;learning&rdquo;).We analyze the sequential performance of various mean field based network algorithms and we compare the network approach with the statistical Monte Carlo technique of simulated annealing.
ID:528
CLASS:6
Title: Ariel: a scalable multiprocessor for the simulation of neural networks
Abstract: Ariel is a multiprocessor architecture that we are developing to simulate neural networks and other models of distributed computation. The design is based upon a hierarchical network of coarse-grained processing modules. The module hardware uses fast digital signal processors and very large semiconductor memories to provide the throughput and storage capacity required to simulate large networks. Our objective is to provide a system that can be scaled up to simulate neural networks composed of millions of nodes and 10s of billions of interconnections at rates exceeding 100 billion connection operations per second. This paper discusses the technical challenges in neural network simulation and describes Ariel's major components.
ID:529
CLASS:6
Title: High speed neural network chip for trigger purposes in high energy physics
Abstract: A novel neural chip SAND (Simple Applicable Neural Device) is described. It is highly usable for hardware triggers in particle physics. The chip is optimized for a high input data rate (50 MHz, 16 bit data) at a very low cost basis. The performance of a single SAND chip is 200 MOPS due to four parallel 16 bit multipliers and 40 bit adders working in one clock cycle. The chip is able to implement feedforward neural networks with a maximum of 512 input neurons and three hidden layers. Kohonen feature maps and radial basis function networks may be also calculated. Four chips will be implemented on a PCI-board for simulation and on a VME board for trigger and on- and off-line analysis.
ID:530
CLASS:6
Title: A constructive algorithm for neural networks that generalize
Abstract: APL functions were designed to describe a constructive algorithm that synthesizes a neural network while optimizing its ability to generalize. Algorithms are implemented in programs to discover networks of binary weights that assign unfamiliar, high-dimension binary patterns to their most similar classes. Constructive algorithms that create networks are important for the design of classifiers based on array-processors made from fast two-level circuits. APL is an effective tool for the exposition of a constructive algorithm that can discover a minimal neural network. APL experts can benefit from being introduced to this interesting application and demonstration of the language's potential for describing array-based software or hardware. For constructing networks, algorithms for target switching and minimizing the overlap in the separation of training patterns have been used. Typically, prototypes of constructive algorithms as commonly implemented with scalar-based procedural languages require hundreds of program statements. Array-based formulations of similar constructive algorithms with functional style programming languages like APL and J are completed with a few brief functions.
ID:531
CLASS:6
Title: Mapping a complex temporal problem into a combination of static and dynamic neural networks
Abstract: Until recently, time related artificial intelligence problems were considered difficult to tackle, and the element time was often eliminated from the core problem. Only during the last decades, researchers (Decortis &amp;amp; Cacciabue, [4]; Klopf &amp;amp; Morgan, [7]) started to explore the importance of time dependencies in artificial intelligence systems. Two different methods - 'time windows' or 'time buffers' and 'dynamic systems' - were experimented and improved in classic artificial intelligence problems like expert systems (Malkoff, [9]).The next step was to apply these two methods to artificial neural network algorithms. Initially, these algorithms (like back-propagation) used a 'time window' approach (Levin et al., [8]; Chakraborty et al., [3]) but recently dynamic network algorithms were developed (Hirsch, [6]; Reiss &amp;amp; Taylor, [10]; Schmidhuber, [13]; Williams &amp;amp; Zipser, [14]).We explain the advantages of these algorithms and the problems that occur due to their computational requirements. We introduce a method for lowering this requirement by splitting a temporal task into a (smaller) temporal part and a static or non-temporal part. By doing so we obtain the advantages of both methods: the inherent implementation of unknown time dependencies with the dynamic neural network and the low computational effort of the static neural network. We demonstrate this approach on a simple diagnosis problem.
ID:532
CLASS:6
Title: Time in neural networks
Abstract: After the revival of interest in connectionism in the eighties and its successful application to pattern recognition problems, the time has come to consider its role in the field of temporal processing. We present here a general overview of the field of temporal neural networks. In order to give a broad framework to this presentation, we first present general properties of time that are used by AI models. This sets out the properties of time: - on its own, - with respect to a problem, - with respect to a model. We then present a short summary of time processing in symbolic AI. The main part of this article, a classification of temporal neural models, is introduced by a short presentation of basic connectionist models. This classification is then made and several relevant examples are presented. We conclude the article with underlining the difference between temporal reasoning and neural temporal processing, and give an introduction to the following papers of this Sigart special section.
ID:533
CLASS:6
Title: On a learnability question associated to neural networks with continuous activations (extended abstract)
Abstract: This paper deals with learnability of concept classes defined by neural networks, showing the hardness of PAC-learning (in the complexity, not merely information-theoretic sense) for networks with a particular class of activation. The obstruction lies not with the VC dimension, which is known to grow slowly; instead, the result follows the fact that the loading problem is NP-complete. (The complexity scales badly with input dimension; the loading problem is polynomial-time if the input dimension is constant.) Similar and well-known theorems had already been proved by Megiddo and by Blum and Rivest, for binary-threshold networks. It turns out the general problem for continuous sigmoidal-type functions, as used in practical applications involving steepest descent, is not NP-hard&mdash;there are &ldquo;sigmoidals&rdquo; for which the problem is in fact trivial&mdash;so it is an open question to determine what properties of the activation function cause difficulties. Ours is the first result on the hardness of loading networks which do not consist of binary neurons; we employ a piecewise-linear activation function that has been used in the neural network literature. Our theoretical results lend further justification to the use of incremental (architecture-changing) techniques for training networks.
ID:534
CLASS:6
Title: Parallel construction of minimal perfect hashing functions with neural networks
Abstract: The seeking of minimal perfect hashing functions (MPHF) has a long history and conventional construction methods are sequential algorithms. To parallelize the MPHF construction, a new method using neural networks is proposed in this paper. It constructs a MPHF by training a massive array of neural nets, and the training tasks can be carried out simultaneously. As the total MPHF construction time is proportional to the key set size, the new method can be applied to build MPHFs for large key sets. In one experiment, a MPHF for a dictionary of 24,464 English words is constructed by training an array of 764 multilayered feedforward neural nets. Network training time is reduced by employing an incremental training procedure. Implementation issues concerning persistent object storage and retrieval are also discussed.
ID:535
CLASS:6
Title: Neural networks: a new dimension in expert systems applications
Abstract: Despite significant advances in expert systems, efforts to build truly intelligent systems that approach reasoning and sensory ability of humans have not been rewarding. A new AI approach, neural networks, utilizes brain-like processing to emulate human learning. This approach will be the focus of commercial applications of AI in the 90s. This article will discuss major features of neural networks and address the impact of this approach on expert systems, as well as implications for research in this area.
ID:536
CLASS:6
Title: Adaptive wavelet neural network for prediction of hourly NOX and NO2 concentrations
Abstract: Adaptive neural network is a powerful tool for prediction of air pollution abatement scenarios. But it is often difficult to avoid overfit during the training of adaptive neural network. In this paper, based on the wavelet theory, a new algorithm is proposed to improve the generalization of adaptive neural network during on-line learning. The new algorithm trains adaptive wavelet neural network to model hourly NOx and NO2 concentrations of variance of emission sources. Results show that the new algorithm improves the generalization and the convergence velocity of adaptive wavelet neural network during on-line learning. The simulations also illustrate that adaptive wavelet neural network is capable of resolving variance of emission sources.
ID:537
CLASS:6
Title: Microphone arrays and neural networks for robust speech recognition
Abstract: This paper explores use of synergistically-integrated systems of microphone arrays and neural networks for robust speech recognition in variable acoustic environments, where the user must not be encumbered by microphone equipment. Existing speech recognizers work best for "high-quality close-talking speech." Performance of these recognizers is typically degraded by environmental interference and mismatch in training conditions and testing conditions. It is found that use of microphone arrays and neural network processors can elevate the recognition performance of existing speech recognizers in an adverse acoustic environment, thus avoiding the need to retrain the recognizer, a complex and tedious task. We also present results showing that a system of microphone arrays and neural networks can achieve a higher word recognition accuracy in an unmatched training/testing condition than that obtained with a retrained speech recognizer using array speech for both training and testing, i.e., a matched training/testing condition.
ID:538
CLASS:6
Title: Breeding swarms: a new approach to recurrent neural network training
Abstract: This paper shows that a novel hybrid algorithm, Breeding Swarms, performs equal to, or better than, Genetic Algorithms and Particle Swarm Optimizers when training recurrent neural networks. The algorithm was found to be robust and scale well to very large networks, ultimately outperforming Genetic Algorithms and Particle Swarm Optimization in 79 of 80 tested networks. This research shows that the Breeding Swarm algorithm is a viable option when choosing an algorithm to train recurrent neural networks.
ID:539
CLASS:6
Title: Parallel Event-Driven Neural Network Simulations Using the Hodgkin-Huxley Neuron Model
Abstract: Neural systems are composed of a large number of highly-connected neurons and are widely simulated within the neurological community. In this paper, we examine the application of parallel discrete event simulation techniques to networks of a complex model called the Hodgkin-Huxley neuron[1]. We describe the conversion of this model into an event-driven simulation, a technique that offers the potential of much greater performance in parallel and distributed simulations compared to time-stepped techniques. We report results of an initial set of experiments conducted to determine the feasibility of this parallel event-driven Hodgkin-Huxley model and analyze its viability for large-scale neural simulations.
ID:540
CLASS:6
Title: Neural networks in APL
Abstract: Neural networks are fairly straightforward to program in a matrix oriented language such as APL. The only general improvement that would benefit them would be the implementation of sparse matrics. Small networks can be trained quite easily using the standard procedures (back propagation, etc).
ID:541
CLASS:6
Title: Multiprocessor simulation of neural networks with NERV
Abstract: A general-purpose simulation system for neural networks is computationally very demanding. This paper presents some estimations of the computing power required, the necessary interconnection bandwidth, and the requisite memory size. Next, the hardware architecture of the NERV multiprocessor system is derived that fulfills these requirements. Up to 320 processors 68020 are used in a single VME crate together with a Macintosh II as a host computer. This set-up provides a computing power of 1300 MIPS together with a friendly graphical user interface. To support the simulation of arbitrarily interconnected networks and of asynchronous update models, the VME bus has been extended by a broadcast feature and a global max finder. The software architecture is outlined. It consists of system software, utilities, and application software.
ID:542
CLASS:6
Title: 98\&cent;/Mflops/s ultra-large-scale neural-network training on a pIII cluster
Abstract: Artificial neural networks with millions of adjustable parameters and a similar number of training examples are a potential solution for difficult, large-scale pattern recognition problems in areas such as speech and face recognition, classification of large volumes of web data and finance. The bottleneck is that neural network training involves iterative gradient descent and is extremely computationally intensive. In this paper we present a technique for distributed training of Ultra Large Scale Neural Networks (ULSNN) on Bunyip, a Linux-based cluster of 196 Pentium III processors. To illustrate ULSNN training we describe an experiment in which a neural network with 1.73 million adjustable parameters was trained to recognize machine-printed Japanese characters from a database   containing 9  million training patterns. The training runs with a average performance of 163.3 Gflops/s (single precision). With a machine cost of $150,913, this yields a price/performance ratio of 92.4&cent; /Mflops/s (single precision).
ID:543
CLASS:6
Title: Ada design of a neural network
Abstract: A neural network is a computer program structured as a simplified model of a brain. It contains nodes (analogous to neurons) and connections between nodes (analogous to synapses). Neural networks can solve difficult pattern-matching problems. A node sums the inputs it receives from other nodes and passes the result through a transfer function to produce its output. A modifiable weight is associated with each connection. A network is trained on a given training set of inputs. During training, the weights are successively adjusted to produce the desired output.Classical design and implementation of neural networks are based on arrays that hold the node values and connection weights. The control structure consists of nested loops through these arrays. This paper suggests instead an object-based design where the nodes are modeled as objects to be operated on. This design models the conceptual network more closely and makes the software more understandable and maintainable. A generic Ada package representing a neural network is presented in some detail.
ID:544
CLASS:6
Title: Neural Networks in the Undergraduate Curriculum
Abstract: Neural networks have been and will continue to be major researchareas in artificial intelligence. Such models show promise inachieving human-like performance, particularly in areas such asspeech and pattern recognition. Recently, neural networks havebegun to find their way out of the research labs and into the realmof practical applications. Unfortunately, however, the study ofsuch networks has been largely overlooked in the computer scienceundergraduate curriculum. Tomorrow's marketplace demands thatcomputer science students be familiar with neural networks and withtheir problem solving abilities. This paper presents a proposal fora neural network module to be integrated into an Introduction toArtificial Intelligence course. Such a module proved to be a verypopular and successful part of such a course given by the author.Sample projects assigned in the course will be presented. In anumber of these projects, students either used microcomputerapplication software which simulates several neural network modelsor programmed their own simulations on microcomputers.
ID:545
CLASS:6
Title: Solutions to the module orientation and rotation problems by neural computation networks
Abstract: In this paper we study two strategies for modifying a given placement of modules in order to improve the quality of the routing results in the next stage of design. We assume that the modules have already been placed. The first strategy seeks to minimize the total wire length by flipping each module about its vertical and/or horizontal axes of symmetry. The second strategy seeks to minimize the total wire length by rotating each module by a multiple of 90 degrees. We introduce a new algorithm based on the Hopfield-Tank neural-net model to solve these problems. Our algorithm performs better than the best algorithms known for these problems. Both problems are shown to be NP-Complete.
ID:546
CLASS:6
Title: Accurate CMOS bridge fault modeling with neural network-based VHDL saboteurs
Abstract: This paper presents a new bridge fault model that is based on a multiple layer feedforward neural network and implemented within the framework of a VHDL saboteur cell. Empirical evidence and experimental results show that it satisfies a prescribed set of bridge fault model criteria better than existing approaches. The new model computes exact bridged node voltages and propagation delay times with due attention to surrounding circuit elements. This is significant since, with the exception of full analog simulation, no other technique attempts to model the delay effects of bridge defects. Yet, compared to these analog simulations, the new approach is orders of magnitude faster and achieves reasonable accuracy; computing bridged node voltages with an average error near 0.006 volts and propagation delay times with an average error near 14 ps.
ID:547
CLASS:6
Title: Toward V\&amp;V of neural network based controllers
Abstract: Online adaptation is a powerful means to handle unexpected slow or catastrophic changes of the system's behavior (e.g., a stuck or broken rudder of an aircraft). Therefore, adaptation is one way for realizing a self-healing system. Substantial research and development has been made to use neural networks (NN) for such tasks (e.g., integrated in various unmanned helicopters and test-flown on a modified F-15 aircraft). Despite the advantages of adaptive neural network based systems, the lack of methods to perform certification, verification, and validation (V&V) of such systems severely restricts their applicability.In this paper, we report on ongoing work to develop V&V techniques and processes for NN-based safety-critical control systems, in our case an aircraft flight control system. Although the project ultimately aims at V&V of online adaptive systems, this paper focuses on the first part of this project dealing with so-called pre-trained neural networks (PTNN). V&V techniques developed here are important pre-requisites for handling the online adaptive case. In particular, we describe highlights of a process guide which has been developed within this project and discuss important V&V issues which need to be addressed during certification.
ID:548
CLASS:6
Title: Using simulation and neural networks to develop a scheduling advisor
Abstract: The research using artificial intelligence and computer simulation introduces a new approach for solving the job shop-scheduling problem. The new approach is based on the development of a neural network-scheduling advisor, which is trained using optimal scheduling decisions. The data set, which is used to train the neural network, is obtained from simulation experiments with small-scale job shop scheduling problems. The paper formulates the problem and after a review of the current solution methods it describes the steps of a new methodology for developing the neural network-scheduling advisor and collecting the data required for its training. The paper concludes by mentioning the expected findings that can be used to evaluate the degree of success of the new methodology.
ID:549
CLASS:6
Title: Performance evaluation of a partial retraining scheme for defective multi-layer neural networks
Abstract: This paper addresses an efficient stuck-defect compensation scheme for multi-layer artificial neural networks implemented in hardware devices. To compensate for stuck defects, we have proposed a two-stage partial retraining scheme that adjusts weights belonging to a neuron affected by defects based on back-propagation(BP) algorithm between two layers. For input neurons, the partial retraining scheme is applied two times; first-stage between the input layer and the hidden layer, second-stage between the hidden layer and the output layer. The partial retraining scheme does not need any additional circuits if the hardware neural network has circuits for learning. In this paper we discuss the performance of the partial retraining scheme, retraining time, network yield and generalization ability. As a result, the partial retraining scheme could compensate the neuron stuck defects about 10 times faster than the whole network retraining by BP algorithm. In addition, yields of networks are also improved. The partial retraining scheme achieved more than 80% recognition ratio for noisy input patterns when 16% neurons of the network have 0-stuck or 1-stuck defects.
ID:550
CLASS:6
Title: Neural networks, financial trading and the efficient markets hypothesis
Abstract: The efficient markets hypothesis asserts that the price of an asset reflects all of the information that can be obtained from past prices of the asset. A direct corollary of this hypothesis is that stock prices follow a random walk, and that any profits derived from timing the market are due entirely to chance. In the absence of any ability to predict the market, the most appropriate strategy---according to proponents of the efficient markets hypothesis---is to buy and hold. In this paper we describe a methodology by which neural networks can be trained indirectly, using a genetic algorithm based weight optimisation procedure, to determine buy and sell points for financial commodities traded on a stock exchange. In order to test the significance of the returns achieved using this methodology, we compare the returns on four financial price series with returns achieved on random walk data derived from each of these series using a bootstrapping procedure. These bootstrapped samples contain exactly the same distribution of daily returns as the original series, but lack any serial dependence present in the original. Our results indicate that on some price series the return achieved is significantly greater than that which can be achieved on the bootstrapped samples. This lends support to the claim that some financial time series are not entirely random, and that---contrary to the predictions of the efficient markets hypothesis---a trading strategy based solely on historical price data can be used to achieve returns better than those achieved using a buy-and-hold strategy.
ID:551
CLASS:6
Title: Model abstraction for discrete event systems using neural networks and sensitivity information
Abstract: Simulation is one of the most powerful tools for modeling and evaluating the performance of complex systems, however, it is computationally slow. One approach to overcome this limitation is to develop a "metamodel". In other words, generate a "surrogate" model of the original system that accurately captures the relationships between input and output, yet it is computationally more efficient than simulation. Neural networks (NN) are known to be good function approximators and thus make good metamodel candidates. During training, a NN is presented with several input/output pairs, and is expected to learn the functional relationship between inputs and outputs of the simulation model. So, a trained net can predict the output for inputs other than the ones presented during training. This ability of NNs to generalize depends on the number of training pairs used. In general, a large number of such pairs is required and, since they are obtained through simulation, the metamodel development is slow. In DES simulation it is often possible to use perturbation analysis to also obtain sensitivity information with respect to various input parameters. In this paper, we investigate the use of sensitivity information to reduce the simulation effort required for training a NN metamodel.
ID:552
CLASS:6
Title: Combining artificial neural networks and statistics for stock-market forecasting
Abstract: We have developed a stock-market forecasting system based on artificial neural networks. The system has been trained with the Standard &amp; Poor 500 composite indexes of past twenty years. Meanwhile, the system produces the forecasts and adjusts itself by comparing its forecasts with the actual indexes. Since most of stock-market forecasting systems are based on some kind of statistical models, we have also implemented a statistical system based on Box-Jenkins ARIMA(p,d,q) model of time series. We compare the performance of the these systems. It shows that the artificial neural network's forecasting is generally superior to time series but it occasionally produces some very wild forecasting values. We then developed a transfer function model to forecast based on the indexes and the forecasts by the artificial neural networks.
ID:553
CLASS:6
Title: Automating judgmental decisions using neural networks: a model for processing business loan applications
Abstract: This paper presents a neural network model that simulates a business loan officer. The network is trained by showing financial ratios, past credit ratings, and loan records of a mixed sample of defaulted and non-defaulted companies. Once it is trained, it recommends to grant or deny a loan. The model uses human judgment of an expert as well as mathematical analysis of financial ratios. It includes into consideration the relative importance of different inputs, and the degree of belief in human judgments. An approach is shown, which allows an &ldquo;explanation&rdquo; for the decisions made. The results show that a neural network can be a valuable tool in simulating human decision-making.
ID:554
CLASS:6
Title: Word sense disambiguation in a Korean-to-Japanese MT system using neural networks
Abstract: This paper presents a method to resolve word sense ambiguity in a Korean-to-Japanese machine translation system using neural networks. The execution of our neural network model is based on the concept codes of a thesaurus. Most previous word sense disambiguation approaches based on neural networks have limitations due to their huge feature set size. By contrast, we reduce the number of features of the network to a practical size by using concept codes as features rather than the lexical words themselves.
ID:555
CLASS:6
Title: Sensitivity analysis of fuzzy and neural network models
Abstract: It is well known that soft computing techniques can be very well deployed for software engineering applications. Among these fuzzy and neural models are widely used to estimate lines of codes, effort, software maintainability, software understandability etc. This paper proposes to carry out a sensitivity analysis of the two models and shows which one is better. This is done with the help of a case study where the two models are used to measure software maintainability.
ID:556
CLASS:6
Title: A network intrusion detection system based on the artificial neural networks
Abstract: To address the problem of high false alarm rate confronted by the traditional intrusion detection systems, this paper presents a new method of applying the artificial neural networks to the network intrusion detection system. We designed and implemented a network intrusion detection system based on the artificial neural networks, and then several experiments have been carried out. For the known intrusions, the false alarm rate is less than 3 percent, and, for the unknown intrusions, the false alarm rate is less than 13 percent. All of these experimental results indicate that this method is advantageous over the traditional intrusion detection methods and some other new methods suggested.
ID:557
CLASS:6
Title: A neural network design for circuit partitioning
Abstract: This paper proposes a neural network model for circuit bipartitioning. The massive parallelism of neural nets has been successfully exploited to balance the partitions of a circuit and to reduce the external wiring between the partitions. The experimental results obtained by neural nets are found to be comparable with that achieved by Fiduccia and Mattheyses algorithm.
ID:558
CLASS:6
Title: On optimization of polling policy represented by neural network
Abstract: This paper deals with the problem of scheduling a server in a polling system with multiple queues and complete information. We represent the polling policy by a neural network; namely, given the number of waiting customers in each queue, the server determines next queue he should visit according to the output of the neural network. By using the simulated annealing method, we improve the neural polling policy in such a way that the mean delay of customers is minimized. Numerical results show that the present approach is especially valid for asymmetric polling systems whose analytical optimization is considered intractable.
ID:559
CLASS:6
Title: Predicting the quality of service of wireless LANs using neural networks
Abstract: Wireless Local Area Networks (WLANs) are particularly difficult to manage due to the highly dynamic nature of the traffic, caused by variations on the number of users, their locations and the type applications they use. In this paper, we propose a new modeling approach, based on neural networks, that is able to predict the Quality of Service (QoS) of WLANs based on the characterization of an operational scenario. From measurements of the inbound and outbound traffic at each Access Point (AP) and of the QoS perceived at each Cell, the model estimates the QoS when the number of users grows. The model does not require the knowledge of the exact network characteristics, since it is only based on measurements carried out at the APs. This modeling approach can be of great help in the planning and management of WLANs. Several realistic network scenarios were defined in order to test the validity of the model. The results show that the model can achieve excellent performance, since the QoS prediction is accurate even when there are significant changes in the number of users.
ID:560
CLASS:6
Title: Root finding and approximation approaches through neural networks
Abstract: In this paper, we propose two approaches to approximate high order multivariate polynomials and to estimate the number of roots of high order univariate polynomials. We employ high order neural networks such as Ridge Polynomial Networks and Pi -- Sigma Networks, respectively. To train the networks efficiently and effectively, we recommend the application of stochastic global optimization techniques. Finally, we propose a two step neural network based technique, to estimate the number of roots of a high order univariate polynomial.
ID:561
CLASS:6
Title: Object oriented software quality prediction using general regression neural networks
Abstract: This paper discusses the application of General Regression Neural Network (GRNN) for predicting the software quality attribute -- fault ratio. This study is carried out using static Object-Oriented (OO) measures (64 in total) as the independent variables and fault ratio as the dependent variable. Software metrics used include those concerning inheritance, size, cohesion and coupling. Prediction models are designed using 15 possible combinations of the four categories of the measures. We also tested the goodness of fit of the neural network model with the standard parameters. Our study is conducted in an academic institution with the software developed by students of Undergraduate/Graduate courses.
ID:562
CLASS:6
Title: Plagiarism detection using feature-based neural networks
Abstract: This paper focuses on the use of code features for automatic plagiarism detection. Instead of the text-based analyses employed by current plagiarism detectors, we propose a system that is based on properties of assignments that course instructors use to judge the similarity of two submissions. This system uses neural network techniques to create a feature-based plagiarism detector and to measure the relevance of each feature in the assessment. The system was trained and tested on assignments from an introductory computer science course, and produced results that are comparable to the most popular plagiarism detectors.
ID:563
CLASS:6
Title: A simulation of final stop consonants in speech perception using the bicameral neural network model
Abstract: This paper demonstrates the integration of contextual information in a neural network for speech perception. Neural networks have been unable to integrate such information successfully because they cannot implement conditional rule structures. The Bicameral neural network employs an asynchronous controller which allows conditional rules to choose neurons for update rather than updating them randomly. The Bicameral model is applied to the perception of word-final plosives, an ongoing problem for machine recognition of speech.
ID:564
CLASS:6
Title: A neural network for speedy trials
Abstract: In recent years, the case loads of judges have increased, while speedy trial laws place a time limit between the defendant's arrest and trial dates. Because of this time constraint, it seems that for minor cases, judges pass sentences based on a set of certain factors (patterns) not based on the individual merits of each case. Patterns may be learned by a neural network. In this paper, we investigate the credibility of the neural network approach as a viable tool in the sentencing process and we show its superiority over the ID3 approach.
ID:565
CLASS:6
Title: Hopfield networks, neural data structures and the nine flies problem: neural network programming projects for undergraduates
Abstract: This paper describes two neural network programming projects suitable for undergraduate students who have already completed introductory courses in Programming and Data Structures. It briefly outlines the structure and operation of Hopfield Networks from a data structure stand-point and demonstrates how these type of neural networks may be used to solve interesting problems like Perelman's Nine Flies Problem. Although the Hopfield model is well defined mathematically, students do not have to be very familiar with the mathematics of the model in order to use it to solve problems. Students are actively encouraged to design modifications to their implementations in order to obtain faster or more accurate solutions. Additionally, students are also expected to compare the neural network's performance with traditional approaches, in order that they may appreciate the subtleties of both approaches. Sample results are provided from projects which have been completed during the last three-year period.
ID:566
CLASS:6
Title: Risk assessment of drilling and completion operations in petroleum wells using a monte carlo and a neural network approach
Abstract: This paper intends to show how two different methodologies, a Monte Carlo simulation method and a connectionist approach can be used to estimate the total time assessment in drilling and completion operations of oil wells in deep waters. The former approach performs a Monte Carlo simulation based on data from field operations. In the later one, correlations and regularities in parameters selected from a petroleum company database were detected using a competitive neural network, and then, a feedforward neural network was trained to estimate the average, standard deviation and total time wasted in the accomplishment of the well. At the end, the results obtained by both models are compared. The analyst could evaluate the precision of the estimated total-time based on geometric and technological parameters provided by the neural network tool, with those supplied by the traditional Monte Carlo method based on data of the drilling and completion operations.
ID:567
CLASS:6
Title: Utility based data mining for time series analysis: cost-sensitive learning for neural network predictors
Abstract: In corporate data mining applications, cost-sensitive learning is firmly established for predictive classification algorithms. Conversely, data mining methods for regression and time series analysis generally disregard economic utility and apply simple accuracy measures. Methods from statistics and computational intelligence alike minimise a symmetric statistical error, such as the sum of squared errors, to model ordinary least squares predictors. However, applications in business elucidate that real forecasting problems contain non-symmetric errors. The costs arising from over- versus underprediction are dissimilar for errors of identical magnitude, requiring an ex-post correction of the prediction to derive valid decisions. To reflect this, an asymmetric cost function is developed and employed as the objective function for neural network training, deriving superior forecasts and a cost efficient decision. Experimental results for a business scenario of inventory-levels are computed using a multilayer perceptron trained with different objective functions, evaluating the performance in competition to statistical forecasting methods.
ID:568
CLASS:6
Title: Evaluation of various training algorithms in a neural network model for software engineering applications
Abstract: Software Engineering as a discipline emerged in response to the software crisis perceived by the industry. It is a well known fact that at the beginning of any project, the software industry needs to know how much will it cost to develop and what would be the time required. Resource estimation in software engineering is more challenging than resource estimation in any other industry. A number of resource estimation methods are currently available and the neural network model is one of them. This paper proposes to evaluate various training algorithms in a neural network model and shows which is the best suited for software engineering applications.
ID:569
CLASS:6
Title: A novel method for protein subcellular localization based on boosting and probabilistic neural network
Abstract: Subcellular localization is a key functional characteristic of proteins. An automatic, reliable and efficient prediction system for protein subcellular localization is needed for large-scale genome analysis. In this paper, we introduce a novel subcellular prediction method combining boosting algorithm with probabilistic neural network algorithm. This new approach provided superior prediction performance compared with existing methods. The total prediction accuracy on Reinhardt and Hubbard's dataset reached up to 92.8% for prokaryotic protein sequences and 81.4% for eukaryotic protein sequences under 5-fold cross validation. On our new dataset, the total accuracy achieved 83.2%. This novel method provides superior prediction performance compared with existing algorithms based on amino acid composition and can be a complementing method to other existing methods based on sorting singals.
ID:570
CLASS:6
Title: NeuroFPGA -- Implementing Artificial Neural Networks on Programmable Logic Devices
Abstract: An FPGA implementation of a multilayer perceptron neural network is presented. The system is parameterized both in network related aspects (e.g.: number of layers and number of neurons in each layer) and implementation parameters (e.g.: word width, pre-scaling factors and number of available multipliers). This allows to use the design for different network realizations, or to try different area-speed trade-offs simply by recompiling the design. Fixed point arithmetic with pre-scaling configurable in a per layer basis was used. The system was tested on an ARC-PCI board from AlteraTM. Several examples from different application domains were implemented showing the flexibility and ease of use of the obtained circuit. Even with the rather old board used, an appreciable speed-up was obtained compared with a software-only implementation based on Matlab neural network toolbox.
ID:571
CLASS:6
Title: Document classification and recurrent neural networks
Abstract: The paper describes an automatic document classification system called NeuroClass, developed for the Air Transportation Field of Transport Canada. NeuroClass is a working classification tool for natural language text, based on recurrent neural network technology. In laboratory tests, it outperformed prototypes developed with other neural network paradigms.
ID:572
CLASS:6
Title: Neural network for partitionable variational inequalities
Abstract: Often in recent times industries have asked mathematicians to determine their "true" utility functions directly from the available data about used resources and about the corresponding profits in order to optimize the latter with respect to the former. The possibility of determining the utility function directly from the data is very important because in this way the exact situation of the company is described. Moreover, the biggest companies divide their investments into several activities. The optimization of their utility function can lead to problems that involve separable or partitionable functions. Two-layered feed-forward neural networks are able to approximate any separable function while fitting the data mantaining the separable structure with the desired approximation error. Thus the theory of partitionable variational inequalities can be used in order to find the optimum of the utility function, subject to some constraints. The presence of the partitionable structure is important because it simplifies the resolution algorithms and makes them more efficient. Moreover, with stronger assumptions about the function, the above results can be generalized to a larger class of utility functions: one problem of dimension n can be split into n problems of dimension one.
ID:573
CLASS:6
Title: An hybridization of an ant-based clustering algorithm with growing neural gas networks for classification tasks
Abstract: Conventional ant-based clustering algorithms and growing neural gas networks are combined to produce an unsupervised classification algorithm that exploits the strengths of both techiques. The ant-based clustering algorithm detects existing classes on a training data set, and at the same time, trains several growing neural gas networks. On a second stage, these networks are used to classify previously unseen input vectors into the classes detected by the ant-based algorithm. The proposed algorithm eliminates the need of changing the number of agents and the dimensions of the environment when dealing with large databases.
ID:574
CLASS:6
Title: Accurate software performance estimation using domain classification and neural networks
Abstract: For the design of an embedded system, there is a variety of available processors, each one offering a different trade-off concerning factors such as performance and power consumption. High-level performance estimation of the embedded software implemented in a particular architecture is essential for a fast design space exploration, including the choice of the most appropriate processor. However, advanced architectures present many features, such as deep pipelines, branch prediction mechanisms and cache sizes, that have a non-linear impact on the execution time, which becomes hard to evaluate. In order to cope with this problem, this paper presents a neural network based approach for high-level performance estimation, which easily adapts to the non-linear behavior of the execution time in such advanced architectures. A method for automatic classification of applications is proposed, based on topological information extracted from the control flow graph of the application, enabling the utilization of domain-specific estimators and thus resulting in more accurate estimates. Practical experiments on a variety of benchmarks show estimation results with a mean error of 6.41% and a maximum error of 32%, which is more precise than previous work based on linear and non-linear approaches.
ID:575
CLASS:6
Title: Optimal path analysis using a predator-prey neural network model
Abstract: A neural network research effort is currently underway at Rome Air Development Center, the Intelligence and Reconnaissance Division (RADC/IR). Griffiss Air Force Base. The purpose of this research is to solve computationally difficult intelligence exploitation problems that have eluded conventional techniques, e.g., target recognition, battlefield multi-sensor correlation and fusion, and intelligence situation assessment.This paper describes the use of a predator-prey neural network paradigm for path analysis. A proof-of-concept simulation is developed and successfully utilized to map optimal/near-optimal paths from given starting points to given destinations through a field of obstacles.The worst-case computational complexity for this algorithm, when implemented on a parallel architecture, is in order of &Ogr;(n), where n is equal to the number of nodes in the network. Serial implementations are in order of &Ogr;(n1.5). This is noteworthy because fast and reasonable solutions to complex problems are often preferable to an ideal optimal solution that typically requires specialized hardware and/or too much time and money to generate.Potential applications for this model include trafficability analysis and route prediction. This model could also serve as a pre-search tool to set search bounds for heuristic search algorithms such as A*. Application of this paradigm to the Enhanced Terrain Perspective Viewer is also discussed.
ID:576
CLASS:6
Title: Teaching neural networks using LEGO handy board robots in an artificial intelligence course
Abstract: In this paper we propose a novel method for teaching neural networks with back propagation in an undergraduate Artificial Intelligence course. We use an agent based approach in the course, as outlined in the textbook Artificial Intelligence A Modern Approach by Stuart Russell and Peter Norvig [7]. The students build a robot agent whose task is to learn path-following behavior with a neural network. Robot agents are constructed from standard LEGO pieces and use the MIT Handy Board as a controller.
ID:577
CLASS:6
Title: Early bankruptcy detection using neural networks
Abstract: In 1993, Austria had the highest number of bankruptcies since 1945. The total liabilities came to approximately US&dollar;3 billion.Powerful tools for the early detection of company risks are very important to avoid high economic losses. Artificial neural networks (ANN) are suitable for many tasks in pattern recognition and machine learning. In this paper we present an ANN for early detection of company failures using balance sheet ratios. The neural network has been successfully tested with real data of Austrian private limited companies. The research activities included the design of an APL application with a graphical user interface to find out the relevant input data and tune the ANN.The developed APL workspace takes advantage of modern windowing features  running on IBM compatible computers.
ID:578
CLASS:6
Title: Evolving neural network ensembles for control problems
Abstract: In neuroevolution, a genetic algorithm is used to evolve a neural network to perform a particular task. The standard approach is to evolve a population over a number of generations, and then select the final generation's champion as the end result. However, it is possible that there is valuable information present in the population that is not captured by the champion. The standard approach ignores all such information. One possible solution to this problem is to combine multiple individuals from the final population into an ensemble. This approach has been successful in supervised classification tasks, and in this paper, it is extended to evolutionary reinforcement learning in control problems. The method is evaluated on a challenging extension of the classic pole balancing task, demonstrating that an ensemble can achieve significantly better performance than the champion alone.
ID:579
CLASS:6
Title: Artificial neural networks as cognitive tools for professional writing
Abstract: Computers are cognitive tools &mdash; they extend the capabilities of the human mind. Paper and pencil are also cognitive tools &mdash; they enhance human memory by acting as a permanent record, and they mediate the formation of thought by serving as a scratchpad or rehearsal device. However, there is a qualitative difference between these cognitive tools: the computer as a writing environment can become an active participant in the process while paper and pencil must remain passive instruments. Because this fundamental difference seems obvious to me, I'm surprised that most computer-aided writing (CAW) software available today is based on a model derived from writing with traditional tools.Such computer tools as spelling checkers, &ldquo;style&rdquo; checkers (actually, they check usage) and outliners, have been around in one form or another for a long time now. Yet they have not really had much of an impact. Most of these packages share three major drawbacks: their analysis is based on statistical measurements of simple surface features of writing; they provide &ldquo;after-the-fact&rdquo; profiling; and, in general, they treat all text as equal.As Shoshana Zuboff points out (In the Age of the Smart Machine), the computer &mdash; unlike the tools of the Industrial Revolution &mdash; not only automates, it also informates. When it comes to text technology, we've done a great deal to automate the process &mdash; as illustrated by word processing and desktop publishing. But we're only starting to use the computer's capacity to informate the process.A quick review of four categories of CAW tools indicates the state-of-the-art.Statistical Text Analysis: I'll use Bell Lab's Writer's Workbenchtm to cover a broad category of software which makes use of patterns to analyze prose. The Workbench is a collection of small programs to measure surface features of writing. For example, the user can find out readability level, average sentence length, word length, percentage of sentence types, percentage of passive-voice verbs, jargon, wordiness, sexist language, spelling errors, and improper usage. Other programs are intended to analyze rhetoric structures. For example, a program displays only the first and last sentence in each paragraph, with the idea that this representation will allow the user to check for logical transitions.Though the package has been around for some time now, it never really caught on. Its UNIX requirement and relatively high cost lessened the likelihood of widespread use. Additionally, the forty-or-more programs are all discrete &mdash; meaning that a truly cumulative analysis of a passage, taking into account the interaction of stylistic features, isn't possible. And, as a third limitation, because the analysis works at such a low level in the composing process, writers sometimes become obsessed with the accidents (surface errors) of their prose rather than the essence (strengthening the logic and content).Prewriting: Planning what to say and how to say it takes time. Professional writers have developed strategies for making this &ldquo;prewriting&rdquo; stage of composing more efficient. The journalist's &ldquo;who, what, when, where, and why&rdquo; litany is an example of a heuristic intended to help focus thoughts. There are many such heuristics, some dating back to Aristotle.Typically, software in this category automates an established strategy. The earliest invention software was modeled on Joseph Weizenbaum's ELIZA. Even today, implementation frequently takes the form of a dialogue, with the program asking significant questions and making appropriate comments on the writer's responses. The writer then uses the recorded information as the raw materials for the paper.Two drawbacks show up in current systems. First, many strategies when used by professional writers are comparable to a rule-of-thumb; thus, the effectiveness and appropriateness of a heuristic varies with the task. They lose most of their spontaneity and flexibility when automated. Second, the dialogue metaphor &mdash; which attributes a personality to the computer &mdash; is an embarrassing affectation in most programs.Outliners: This category has generated more commercial interest than the other three. Though frequently called &ldquo;idea processors,&rdquo; the label seems more honorific than earned. Most use a top-down (general-to-specific) knowledge representation as their bases. In other words, the writer is encouraged to find hierarchical relationships in her raw material by filling in an open-ended tree-structure. On some systems, levels can be hidden, thus focusing attention and reducing the cognitive load inherent in the writing process.Writing Environments: The more interesting of these programs are still in their infancy, and can be represented by WE (Writing Environment developed at the University of North Carolina&mdash;Chapel Hill) and CICILE (developed at the Center for Applied Cognitive Science in Toronto, Ontario). In something like the Writer's Workbenchtm, analytical programs are separate entities, and the writer is free to pick and choose among them. On the other hand, the suite of tools in a &ldquo;writing environment&rdquo; is integrated and part of a rigorously structured cognitive model of the writing process. In essence, a well-designed writing environment orchestrates the writing process by emulating stages of thinking. Few, if any, writing environments include AI applications as we normally define them (e.g. expert systems). Nevertheless, because the whole system supports and guides the activities of thinking, these knowledge-making habitats should be characterized as &ldquo;intelligent.&rdquo;I like the concept, but I am just a bit uneasy with the implementation. First, all of the examples I am aware of are theory-laden and exit as heavily-funded projects at large research universities or government-sponsored laboratories. In fact, these systems seem to be testbeds for doing high-powered research on the writing process more than practical tools for professionals. Second, because of the amount of &ldquo;scaffolding&rdquo; each system provides for the writer, they seem more appropriate as a learning environment. In short, they are more tutors than tools. And third, their heavy commitment to a definitive cognitive model of writing seems to ignore what historians of technology have taught us: new tools engender new habits of mind, and the tool &mdash; over time &mdash; can change the nature of the task.In summary then, word processing precipitated interest in computer-aided writing (CAW). Once text could be represented as bits and bytes, computational software for analyzing prose patterns became feasible. My objection is that the patterns used are too fine-grained and that the evaluation is too rigorous to qualify as a comfortable cognitive tool. I have never used a CAW product that, eventually, didn't pinch and constrain my writing process by being distractingly intrusive, nit-pickingly atomistic, or down-right tedious and misleading in the advice it returned.I view writing as one manifestation of the controlled creativity we call design. The cognitive activities of design take place in a cyclical rather than linear fashion. First, we decompose or partition the task into its components to get an idea of what we are trying to do. Then, we work on the pieces for a while, step back to compare interim results with higher-level goals, consolidate gains, jettison unrealistic expectations or excessive constraints, reorder plans, and move back to working on the pieces again. The cycle takes place over and over during the writing session. Good writers excel where poor writers fail because of this flexibility, this ability to move smoothly between top-down and bottom-up strategies. To my mind, a good cognitive tool for composing has two functions: (1) to serve as a peripheral brain that helps with the cognitive overload inherent in a complex task, and (2) to act as an expert associate that provides counsel in the iterative, &ldquo;prototype and feedback&rdquo; process of design.
ID:580
CLASS:6
Title: Neural nets and alphabets: introducing students to neural networks
Abstract: Three student projects involving neural networks are described. The projects include recognizing handwritten letters of the alphabet, determining the orientation of an imaged line, and recognizing particular rooms of a house based on samples of furniture found in the rooms. All projects were run on a back propagation neural network program implemented in Modula-2. A description of the program is presented and a sample module for simulating the behavior of an OR gate is included as an appendix. The program has been successfully used in several Artificial Intelligence classes for classroom demonstrations and carrying out various cognitive science experiments.
ID:581
CLASS:6
Title: Neural networks and pattern recognition in human-computer interaction
Abstract: This paper reports on the activities of the workshop held on Sunday 28th April at the CHI'91 conference. Participants were there to discuss different ideas, methods and approaches to using pattern recognition in human-computer interaction.The workshop aimed to bring together researchers using novel methodologies, such as neural networks, in HCI applications, as well as practitioners using alternative or more traditional methods to perform pattern recognition tasks in HCI. The intention was to explore the scope and limitations of each type of approach and its requirements, for example in terms of representation and resources. The workshop considered the relationships between the different approaches and the possibility of developing hybrid methodologies to resolve HCI problems.Researchers working with both traditional and novel pattern recognition methods that have applications to human-computer interaction, and those with strong views either way, submitted position statements outlining their interest and viewpoints. Their research results are summarised in this report; in addition, the discussions on methods, on how the work reported interrelates, and on future areas of interest are presented. Major results from the use of neural network systems and other pattern recognition systems in the interface are presented, with application areas ranging from the interpretation of gestural input to the automatic determination of user task. Fuller details of the research can be found in a book based on the proceedings[2].
ID:582
CLASS:6
Title: Documents, concepts and neural networks
Abstract: In this paper we investigate the relevance of neural networks to the problem of document classification. We show that textual documents can be represented numerically in a semantically meaningful way, so that the back-propagation learning algorithm can be used to build a document classifying neural network. We show that the network can be taught to classify natural language text according to predefined specifications. The convergence properties of the prototype NeuroZ described in this paper make it clear that neural networks provide a new platform for the automatic classification of semantically similar documents and that a system can be built which distinguishes between relatively complex linguistic patterns.
ID:583
CLASS:6
Title: Mobile user tracking using a hybrid neural network
Abstract: In this paper, a novel technique for location prediction of mobile users has been proposed, and a paging technique based on this predicted location is developed. As a mobile user always travels with a destination in mind, the movements of users, are, in general, preplanned, and are highly dependent on the individual characteristics. Hence, neural networks with its learning and generalization ability may act as a suitable tool to predict the location of a terminal provided it is trained appropriately by the personal mobility profile of individual user. For prediction, the performance of a multi-layer perceptron (MLP) network has been studied first. Next, to recognize the inherent clusters in the input data, and to process it accordingly, a hybrid network composed of a self-organizing feature map (SOFM) network followed by a number of MLP networks has been employed. Simulation studies show that the latter performs better for location management. This approach is free from all unrealistic assumptions about the movement of the users. It is applicable to any arbitrary cell architecture. It attempts to reduce the total location management cost and paging delay, in general.
ID:584
CLASS:6
Title: A neural network approach to the validation of simulation models
Abstract: We tackle the problem of validating simulation models using neural networks. We propose a neural-network-based method that first learns key properties of the behaviour of alternative simulation models, and then classifies real system behaviour as coming from one of the models. We investigate the use of multi-layer perceptron and radial basis function networks, both of which are popular pattern classification techniques. By a computational experiment, we show that our method successfully allows to distinguish valid from invalid models for a multiserver queueing system.
ID:585
CLASS:6
Title: Effect of data compression of ERP sign preprocessed by FWT algorithm upon a neural network classifier
Abstract: Earlier research at the Navy Personnel Research and Development Center revealed that measures of the brain response to sensory stimuli, known as Event Related Potentials (ERP) may be used to assess unique process-related variance that is dependent upon human performance. For example, it was found that the sensitivity of individual subjects to dynamic color contrast in computer displays can be assessed by visual ERP's. It has also been observed that RMS measures of the P1-N1-P2 complex and the P300 component of the ERP are related to signal detection and classification measures of performance. The present effort is to classify the ERP response to stimuli followed by a motor action of the subject, using neural networks. The stimuli consist of flashing a light which could be either one of two distinct colors. The following motor-action corresponds to pushing one of the two available buttons representing the two colors. The features will be selected from the Fast Walsh Transformation (FWT) of ERP observations and will be applied to appropriate Neural Networks to obtain a prediction of a possible response. Since a small number of features is derived from a large section of available processed data and since the ERP data is diluted with noise and artifacts, there is a need to evaluate the effectiveness of this data compression towards efficient prediction of the future action of the subject. The present paper deals with this study.
ID:586
CLASS:6
Title: Coevolution of neural networks using a layered pareto archive
Abstract: The Layered Pareto Coevolution Archive (LAPCA) was recently proposed as an effective Coevolutionary Memory (CM) which, under certain assumptions, approximates monotonic progress in coevolution. In this paper, a technique is developed that interfaces the LAPCA algorithm with NeuroEvolution of Augmenting Topologies (NEAT), a method to evolve neural networks with demonstrated efficiency in game playing domains. In addition, the behavior of LAPCA is analyzed for the first time in a complex game-playing domain: evolving neural network controllers for the game Pong. The technique is shown to keep the total number of evaluations in the order of those required by NEAT, making it applicable to complex domains. Pong players evolved with a LAPCA and with the Hall of Fame (HOF) perform equally well, but the LAPCA is shown to require significantly less space than the HOF. Therefore, combining NEAT and LAPCA is found to be an effective approach to coevolution.
ID:587
CLASS:6
Title: Neural network approach to solving the Traveling Salesman Problem
Abstract: The Traveling Salesman Problem involves mapping a route for a salesman to visit each city, without stopping in the same city twice, in the shortest route possible. A map of Germany is used as the test data for the study in this report.
ID:588
CLASS:6
Title: Simulating neural network learning with TEST/NIL
Abstract: TEST/NIL is a neural network learning simulation tool. TEST (Threshold Element Simulation Tool) is both an input language compiler and a simulation environment builder. NIL is the Network Input Language. TEST/NIL was designed to investigate biologically plausible learning theories at a level lower than that of cognitive psychology and higher than that of chemical pulse transmission. The discrete event simulation technique used by TEST/NIL is designed for possible optimization via parallelism. Appendices summarize the input language and provide a programming example.
ID:589
CLASS:6
Title: A new cache replacement scheme based on backpropagation neural networks
Abstract: In this paper, we present a new neural network-based algorithm, KORA (Khalid ShadOw Replacement Algorithm), that uses backpropagation neural network (BPNN) for the purpose of guiding the line/block replacement decisions in cache. This work is a continuation of our previous research presented in [1]-[3]. The KORA algorithm attempts to approximate the replacement decisions made by the optimal scheme (OPT). The key to our algorithm is to identify and subsequently discard the dead lines in cache memories. This allows our algorithm to provide better cache performance as compared to the conventional LRU (Least Recently Used), MRU (Most Recently Used), and FIFO (First In First Out) replacement policies. Extensive trace-driven simulations were performed for 30 different cache configurations using different SPEC (Standard Performance Evaluation Corp.) programs. Simulation results have shown that KORA can provide substantial improvement in the miss ratio over the conventional algorithms. Our work opens new dimensions for research in the development of new and improved page replacement schemes for virtual memory systems and disk caches.
ID:590
CLASS:6
Title: Efficient learning of continuous neural networks
Abstract: We describe an efficient algorithm for learning from examples a class of feedforward neural networks with real inputs and outputs in a real-value generalization of the Probably Approximately Correct (PAC) model. These networks can approximate an arbitrary function with an arbitrary precision. The learning algorithm can accommodate a fairly general worst-case noise model. The main improvement over previous work is that the running time of the algorithm grows only polynomially as the size of the target network increases (there is still an exponential dependence on the dimension of the input space, however). The main computational tool is an iterative &ldquo;loading&rdquo; algorithm which adds new hidden units to the hypothesis network sequentially. This avoids the difficult problem of   optimizing the weights of all units simultaneously.
ID:591
CLASS:6
Title: Synthesis of Embedded SystemC Design: A Case Study of Digital Neural Networks
Abstract: This work presents the whole System-on-Silicon design flow using SystemC system specification language. In this study, SystemC is used to design a multilayer perceptron neural network, which is applied to an electrocardiogram pattern recognition system. The objective of this work is to exemplify the synthesis of RTL- and behavioral integrated systems. To achievethis, a preprocessing methodology was used to optimize the three main constraints of hardware neural network (HNN) design: accuracy, space and processing speed. This allows a complex HNN to be implemented on a single Field Programmable Gate Array (FPGA). Thehigh level SystemC synthesis allows the straightforward translation of system level into hardware level, avoiding the error prone and the time consuming translation into another hardware description language.
ID:592
CLASS:7
Title: Stability analysis of active clock deskewing systems using a control theoretic approach
Abstract: In this paper, a methodology for analyzing closed loop clock distribution and active deskewing networks is proposed. An active clock distribution and deskewing network is modelled as a closed loop feedback control system using state space equations. The state space models of the system were then used to simulate the clock deskewing scheme, and most importantly, to analyze the stability using the integral quadratic constraints method. Such a systematic analysis method can be very useful to designers as they will be able to determine how the deskewing network behaves, thus, avoiding repeated simulations. The proposed approach can be further extended to determine performance of such systems under different configurations. We show how the proposed method is applied to an experimental clock deskewing system for performance and stability analysis.
ID:593
CLASS:7
Title: Systematic Analysis of Active Clock Deskewing Systems Using Control Theory
Abstract: A formal methodology for the analysis of a closed loop clock distribution and active deskewing network is proposed. In this paper an active clock distribution and deskewing network is modeled as a closed loop feedback system using state space equations. State space analysis allows systematic analysis of any clock distribution and deskewing systems to determine various conditions under which a system can over-compensate and become potentially unstable. Such an analysis can be very useful to designers as they will be able to determine analytically as to how the clock deskewing system behaves. By using the proposed approach, repeated simulations can be greatly limited and maybe entirely avoided. We applied the proposed method to an experimental clock deskewing system to illustrate the effectiveness of the proposed approach. The proposed approach can be further extended to determine performance of such systems under different configurations.
ID:594
CLASS:7
Title: An enhanced GA to improve the search process reliability in tuning of control systems
Abstract: Evolutionary Algorithms (EAs) have been largely applied to optimisation and synthesis of controllers. In spite of several successful applications and competitive solutions, the stochastic nature of EAs and the uncertainty of the results have considerably hindered their use in industrial applications. In this paper we propose a Genetic Algorithm (GA) for tuning controllers for classical first and second order plants with actuator nonlinearities. To increase the robustness of the algorithm we introduce two features: 1) genetic operators that perform directional mutations, 2) selection tournaments organized by genome vicinity. The experiment results show that the proposed GA is able to guarantee high performance and low variance in the results from different runs. The increased reliability, compared to the results from a classical GA, seems to favour particularly the application of Evolutionary Computation (EC) in tuning of control systems, where, thanks to this approach, a large search space can be searched repeatedly with high consistency in the solutions.
ID:595
CLASS:7
Title: Nonlinear instabilities in TCP-RED
Abstract: This work develops a discrete-time dynamical feedback system model for a simplified TCP network with RED control and provides a nonlinear analysis that can help in understanding observed parametric sensitivities. The model describes network dynamics over large parameter variations. The dynamical model is used to analyze the TCP-RED operating point and its stability with respect to various RED controller and system parameters. Bifurcations are shown to occur as system parameters are varied. These bifurcations, which involve the emergence of oscillatory and/or chaotic behavior, shed light on the parametric sensitivity observed in practice. The bifurcations arise due to the presence of a nonlinearity in the TCP throughput characteristic as a function of drop probability at the gateway. Among the bifurcations observed in the system are period doubling and border collision bifurcations. The bifurcations are studied analytically, numerically, and experimentally.
ID:596
CLASS:7
Title: Adaptive nonlinear congestion controller for a differentiated-services framework
Abstract: The growing demand of computer usage requires efficient ways of managing network traffic in order to avoid or at least limit the level of congestion in cases where increases in bandwidth are not desirable or possible. In this paper we developed and analyzed a generic Integrated Dynamic Congestion Control (IDCC) scheme for controlling traffic using information on the status of each queue in the network. The IDCC scheme is designed using nonlinear control theory based on a nonlinear model of the network that is generated using fluid flow considerations. The methodology used is general and independent of technology, as for example TCP/IP or ATM. We assume a differentiated-services network framework and formulate our control strategy in the same spirit as IP DiffServ for three types of services: Premium Service, Ordinary Service, and Best Effort Service. The three differentiated classes of traffic operate at each output port of a router/switch. An IDCC scheme is designed for each output port, and a simple to implement nonlinear controller, with proven performance, is designed and analyzed. Using analysis performance bounds are derived for provable controlled network behavior, as dictated by reference values of the desired or acceptable length of the associated queues. By tightly controlling each output port, the overall network performance is also expected to be tightly controlled. The IDCC methodology has been applied to an ATM network. We use OPNET simulations to demonstrate that the proposed control methodology achieves the desired behavior of the network, and possesses important attributes, as e.g., stable and robust behavior, high utilization with bounded delay and loss, together with good steady-state and transient behavior.
ID:597
CLASS:7
Title: Control system development tools
Abstract: This paper provides a core of APL algorithms for control system development and demonstrates their use by solving a typical control problem. In doing so it outlines useful numerical techniques for simulating dynamic systems and for solving some of the central equations of control theory.Although some sections of the paper are addressed to APL2 users, the majority of the paper applies to APL. Moreover, by doing a little extra work to handle complex numbers and by installing a &ldquo;callable&rdquo; compiled eigenvalue-eigenvector routine, all of the material presented can be adapted to any APL system.While APL is a comfortable environment for control system development, APL2 contains two especially useful enhancements: 1) complex numbers included in a natural way, and 2) the function EIGEN.APL2's facility with complex numbers permits the direct and clear coding of frequency domain methods such as root locus, bode plots, and the generation of transfer functions. APL2's facility with complex numbers also makes it possible to include a native eigenvalue-eigenvector utility function, EIGEN. This function generates the eigenvalues and eigenvectors of general square matrices, which can then be used for root locus studies, for transforming system equations to canonical forms, and for efficiently solving the Riccati and Lyapunov equations.Non-EIGEN-based functions are also provided so that all APL users will find enough tools to model, simulate, analyze, and develop regulators, observers, and filters for linear dynamic systems.
ID:598
CLASS:7
Title: Projection frameworks for model reduction of weakly nonlinear systems
Abstract: In this paper we present a generalization of popular linear model reduction methods, such as Lanczos- and Arnoldi-based algorithms based on rational approximation, to systems whose response to interesting external inputs can be described by a few terms in a functional series expansion such as a Volterra series. The approach allows automatic generation of macromodels that include frequency-dependent nonlinear effects.
ID:599
CLASS:7
Title: Reduction of the order of a nonlinear system
Abstract: A steam-driven electrical power generating station was designed with the aid of a hybrid computer simulation. In the process, it was advantageous to reduce the order of the system of nonlinear equations describing the boilers and turbines. Since there is no general procedure for reducing the order of nonlinear systems, each case must be treated individually. However, the methods used in one system are sometimes applicable to others. The approach taken in the above reduction is presented with this in mind.
ID:600
CLASS:7
Title: The effect of uncertain time-variant delays in ATM networks with explicit rate feedback: a control theoretic approach
Abstract: A new, more realistic model for the available bit rate traffic class in ATM network congestion control with explicit rate feedback is introduced and analyzed. This model is based on recent results by Ekanayake regarding discrete time models for time-variant delays. The discrete time model takes into account the effect of time-variant buffer occupancy levels of ATM switches, thus treating the case of time-variant delays between a single congested node and the connected sources. For highly dynamic situations, such a model is crucial for a valid analysis of the resulting feedback system. The new model also handles the effects of the mismatch between the resource management cell rates and the variable bit rate controller sampling rate as well as buffer and rate nonlinearities. A brief stability study shows that an equilibrium in the buffer occupancy is impossible to achieve in the presence of time-variant forward path delays. Stability conditions for the case of time-variant delays in the return path are presented. Finally, illustrative examples are provided.
ID:601
CLASS:7
Title: Formal online methods for voltage/frequency control in multiple clock domain microprocessors
Abstract: Multiple Clock Domain (MCD) processors are a promising future alternative to today's fully synchronous designs. Dynamic Voltage and Frequency Scaling (DVFS) in an MCD processor has the extra flexibility to adjust the voltage and frequency in each domain independently. Most existing DVFS approaches are profile-based offline schemes which are mainly suitable for applications whose execution char-acteristics are constrained and repeatable. While some work has been published about online DVFS schemes, the prior approaches are typically heuristic-based. In this paper, we present an effective online DVFS scheme for an MCD processor which takes a formal analytic approach, is driven by dynamic workloads, and is suitable for all applications. In our approach, we model an MCD processor as a queue-domain network and the online DVFS as a feedback control problem with issue queue occupancies as feedback signals. A dynamic stochastic queuing model is first proposed and linearized through an accu-rate linearization technique. A controller is then designed and verified by stability analysis. Finally we evaluate our DVFS scheme through a cycle-accurate simulation with a broad set of applications selected from MediaBench and SPEC2000 benchmark suites. Compared to the best-known prior approach, which is heuristic-based, the proposed online DVFS scheme is substantially more effective due to its automatic regulation ability. For example, we have achieved a 2-3 fold increase in efficiency in terms of energy-delay product improvement. In addition, our control theoretic technique is more resilient, requires less tuning effort, and has better scalability as compared to prior online DVFS schemes.We believe that the techniques and methodology described in this paper can be generalized for energy control in processors other than MCD, such as tiled stream processors.
ID:602
CLASS:7
Title: Computer-aided design of nonlinear dynamic systems
Abstract: With the advent of large memory, fast digital computers with convenient input-output devices and for which high-level problem-oriented languages are available in a time-shared enviromment, it is becoming increasingly desirable to develop digital computer programs for the design of complex control systems. For some time the analog computer has been used for this purpose. In this paper a large digital computer program, called OLDS (On Line dynamic system Design System), useful for the interactive design of nonlinear control systems in the time domain, is described. The program is the preliminary result of a research project with broader objectives.
ID:603
CLASS:7
Title: Queueing properties of feedback flow control systems
Abstract: In this paper, we consider a network with both controllable and uncontrollable flows. Uncontrollable flows are typically generated from applications with stringent QoS requirements and are given high priority. On the other hand, controllable flows are typically generated by elastic applications and can adapt to the available link capacities in the network. We provide a general model of such a system and analyze its queueing behavior. Specially, we obtain a lower bound and an asymptotic upper bound for the tail of the workload distribution at each link in the network. These queueing results provide us with guidelines on how to design a feedback flow control system. Simulation results show that the lower bound and asymptotic upper bound are quite accurate and that our feedback control method can effectively control the queue length in the presence of both controllable and uncontrollable traffic. Finally, we describe a distributed strategy that uses the notion of Active Queue Management (AQM) for implementing our flow control solution.
ID:604
CLASS:7
Title: A Numerical Method for Solving Control Differential Equations on Digital Computers
Abstract: Frequently, as in missile control systems, linear differential equations are simultaneous with nonlinear but slower acting differential equations. The numerical solution of this type of system on a digital computer is significantly speeded up by approximating the forcing functions with polynomials, solving the linear equations exactly, and numerically integrating the nonlinear equations with Milne integration. Automatic interval adjustment is possible by comparing errors in the nonlinear integration. The interval selected is related to the shortest time constant of the nonlinear equations rather than the shortest of all the equations. With this system, both detailed transient response and steady state conditions are revealed with a minimum of machine time.
ID:605
CLASS:7
Title: Algorithms for symbolic/numeric control of affine dynamical systems
Abstract: We consider a general linear dynamical system and want to control its behavior. The goal is to reach a given target by minimizing a cost function. We provide a new generic algorithm with together exact, symbolic and numerical modules. In particular new efficient methods computing a block Kalman canonical exact decomposition and the optimal solutions are presented. We also propose a new numerical algorithm under-approximating the controllable domain in view of its analytical resolution in the context of singular sub-arcs.
ID:606
CLASS:7
Title: Hierarchical model-based autonomic control of software systems
Abstract: Various control algorithms are used in autonomic control to maintain Quality of Service (QoS) and Service Level Agreements (SLAs). Controllers are all based to some extent on models of the relationship between resources, QoS measures, and the workload imposed by the environment. This work discusses the range of algorithms with an emphasis on richer and more powerful models to describe non-linear performance relationships, and strong interactions among the system resources. A hierarchical framework is described which accommodates different scopes and timescales of control actions, and different control algorithms. The control algorithms and architectures can be considered in three stages: tuning, load balancing and provisioning. Different situations warrant different solutions, so this work shows how different control algorithms and architectures at the three stages can be combined to fit into different autonomic environments to meet QoS and SLAs across a large variety of workloads.
ID:607
CLASS:7
Title: An interface optimization and application for the numerical solution of optimal control problems
Abstract: An interface between the application problem and the nonlinear optimization algorithm is proposed for the numerical solution of distributed optimal control problems. By using this interface, numerical optimization algorithms can be designed to take advantage of inherent problem features like the splitting of the variables into states and controls and the scaling inherited from the functional scalar products. Further, the interface allows the optimization algorithm to make efficient use of user-provided function evaluations and derivative calculations.
ID:608
CLASS:7
Title: Modeling, simulation, sensitivity analysis, and optimization of hybrid systems
Abstract: Hybrid (discrete/continuous) systems exhibit both discrete state and continuous state dynamics which interact to such a significant extent that they cannot be decoupled and must be analyzed simultaneously. We present an overview of the work that has been done in the modeling, simulation, sensitivity analysis, and optimization of hybrid systems, paying particular attention to the interaction between discrete and continuous dynamics. A concise intuitive framework for hybrid system modeling is presented, together with discussions on robust state event location, transfer functions of the continuous state at discontinuities, parametric sensitivity analysis of hybrid systems, and challenges in optimization.
ID:609
CLASS:7
Title: Triage: Performance differentiation for storage systems using adaptive control
Abstract: Ensuring performance isolation and differentiation among workloads that share a storage infrastructure is a basic requirement in consolidated data centers. Existing management tools rely on resource provisioning to meet performance goals; they require detailed knowledge of the system characteristics and the workloads. Provisioning is inherently slow to react to system and workload dynamics and, in the general case, it is not practical to provision for the worst case.We propose a software-only solution that ensures predictable performance for storage access. It is applicable to a wide range of storage systems and makes no assumptions about workload characteristics. We use an online feedback loop with an adaptive controller that throttles storage access requests to ensure that the available system throughput is shared among workloads according to their performance goals and their relative importance. The controller considers the system as a &ldquo;black box&rdquo; and adapts automatically to system and workload changes. The controller is distributed to ensure high availability under overload conditions, and it can be used for both block and file access protocols. The evaluation of Triage, our experimental prototype, demonstrates workload isolation and differentiation in an overloaded cluster file-system where workloads and system components are changing.
ID:610
CLASS:7
Title: Existence of limit cycles in a non linear dynamic system with random parameters
Abstract: The paper describes an efficient technique for investigating the performance of a nonlinear dynamic system with uncertain parameters. An analysis is made to determine the probability that a nonlinear control system will exhibit limit cycle (sustained periodic oscillation) behavior. The quadrature method of Evans, which provides an efficient computational tool for characterizing the moments of a function of random variables, is used in the analysis.
ID:611
CLASS:7
Title: SUNDIALS: Suite of nonlinear and differential/algebraic equation solvers
Abstract: SUNDIALS is a suite of advanced computational codes for solving large-scale problems that can be modeled as a system of nonlinear algebraic equations, or as initial-value problems in ordinary differential or differential-algebraic equations. The basic versions of these codes are called KINSOL, CVODE, and IDA, respectively. The codes are written in ANSI standard C and are suitable for either serial or parallel machine environments. Common and notable features of these codes include inexact Newton-Krylov methods for solving large-scale nonlinear systems; linear multistep methods for time-dependent problems; a highly modular structure to allow incorporation of different preconditioning and/or linear solver methods; and clear interfaces allowing for users to provide their own data structures underneath the solvers. We describe the current capabilities of the codes, along with some of the algorithms and heuristics used to achieve efficiency and robustness. We also describe how the codes stem from previous and widely used Fortran 77 solvers, and how the codes have been augmented with forward and adjoint methods for carrying out first-order sensitivity analysis with respect to model parameters or initial conditions.
ID:612
CLASS:7
Title: NORM: compact model order reduction of weakly nonlinear systems
Abstract: This paper presents a compact Nonlinear model Order Reduction Method (NORM) that is applicable for time-invariant and time-varying weakly nonlinear systems. NORM is suitable for reducing a class of weakly nonlinear systems that can be well characterized by low order Volterra functional series. Unlike existing projection based reduction methods [6]-[8], NORM begins with the general matrix-form Volterra nonlinear transfer functions to derive a set of minimum Krylov subspaces for order reduction. Direct moment matching of the nonlinear transfer functions by projection of the original system onto this set of minimum Krylov subspaces leads to a significant reduction of model size. As we will demonstrate as part of our comparison with existing methods, the efficacy of model order for weakly nonlinear systems is determined by the extend to which models can be reduced. Our results further indicate that a multiple-point version of NORM can substantially reduce the model size and approach the ultimate model compactness that is achievable for nonlinear system reduction. We demonstrate the practical utility of NORM for macro-modeling weakly nonlinear RF circuits with time-varying behavior.
ID:613
CLASS:7
Title: Smarter control variables: regression-adjusted linear and nonlinear controls
Abstract: Nonlinear regression-adjusted control variables are investigated for improving variance reduction in statistical and system simulations. Simple control variables are transformed using linear and nonlinear transformations, and parameters of these transformations are selected using linear or nonlinear least squares regression. As an example, piecewise powertransformed variables are used in the estimation of the mean for the two variable Anderson-Darling goodness-of-fit statistic W22. Substantial variance reduction over straightforward controls is obtained. These parametric transformations are compared against optimal, additive, nonparametric transformations from ACE and are shown to be nearly optimal.
ID:614
CLASS:7
Title: Word-length optimization for differentiable nonlinear systems
Abstract: This article introduces an automatic design procedure for determining the sensitivity of outputs in a digital signal processing design to small errors introduced by rounding or truncation of internal variables. The proposed approach can be applied to both linear and nonlinear designs. By analyzing the resulting sensitivity values, the proposed procedure is able to determine an appropriate distinct word-length for each internal variable in a fixed-point hardware implementation. In addition, the power-optimizing capabilities of word-length optimization are studied. Application of the proposed procedure to adaptive filters and polynomial evaluation circuits realized in a Xilinx Virtex FPGA has resulted in area reductions of up to 80&percnt; (mean 66&percnt;) combined with power reductions of up to 98&percnt; (mean 87&percnt;) and speed-up of up to 36&percnt;(mean 20&percnt;) over common alternative design strategies.
ID:615
CLASS:7
Title: A state event detection algorithm for numerically simulating hybrid systems with model singularities
Abstract: This article describes an algorithm for detecting the occurrence of events, which signify discontinuities in the first derivative of the state variables, while simulating a set of nonsmooth differential equations. Such combined-discrete continuous systems arise in many contexts and are often referred to as hybrid systems, switched systems, or nonsmooth systems. In all cases, the state events are triggered at simulated times which generate states corresponding to the zeros of some algebraic &ldquo;event&rdquo; function. It has been noted that all existing simulators are prone to failure when these events occur in the neighborhood of model singularities---regions of the state space where the right-hand side of the differential equation is undefined. Such model singularities are often the impetus for using nonsmooth models in the first place. This failure occurs because existing algorithms blindly attempt to interpolate across singular regions, checking for possible events after the fact. The event detection algorithm described here overcomes this limitation using an approach inspired by feedback control theory. A carefully constructed extrapolation polynomial is used to select the integration step size by checking for potential future events, avoiding the need to evaluate the differential equation in potentially singular regions. It is shown that this alternate approach gives added functionality with little impact on the simulation efficiency.
ID:616
CLASS:7
Title: Global stability conditions for rate control with arbitrary communication delays
Abstract: We analyze Kelly's optimization framework for a rate allocation problem in communication networks and provide stability conditions with arbitrary fixed communication delays. We demonstrate the existence of a fundamental tradeoff between users' price elasticity of demand and the responsiveness of resource through a choice of price function. We also show that the stability of the system can be studied by looking at a much simpler discrete time system that emerges from the underlying market structure of the rate control system with a homogeneous delay. We study the effects of nonresponsive traffic on system stability and show that the presence of nonresponsive traffic enhances the stability of system. We also investigate the system behavior beyond stable regime.
ID:617
CLASS:7
Title: A cooperative uplink power control scheme for elastic data services in wireless CDMA systems
Abstract: We consider the uplink power control problem in a single cell CDMA wireless data system. Each user specifies upper and lower QoS bounds. We formulate the considered problem as a game, and first examine the non-cooperative case. We then compare it to its cooperative counterpart (through the Nash bargaining solution). The use of the cooperative scheme shows significant reduction in the transmission power of the mobile terminals, while the achieved QoS is slightly compromised, compared to the non-cooperative scheme.
ID:618
CLASS:7
Title: A predictive flow control scheme for efficient network utilization and QoS
Abstract: In this paper, we develop a new predictive flow control scheme and analyze its performance. This scheme controls the nonreal-time (controllable) traffic based on predicting the real-time (uncontrollable) traffic. The goal of the work is to operate the network in a low congestion, high throughput regime. We provide a rigorous analysis of the performance of our flow control method and show that the algorithm has attractive and useful properties. From our analysis we obtain an explicit condition that gives us design guidelines on how to choose a predictor. We learn that it is especially important to take the queueing effect into account in developing the predictor. We also provide numerical results comparing different predictors that use varying degrees of information from the network.
ID:619
CLASS:7
Title: A control theoretic approach to run-time energy optimization of pipelined processing in MPSoCs
Abstract: In this work we take a control-theoretic approach to feedback-based dynamic voltage scaling (DVS) in Multi Processor System on Chip (MPSoC) pipelined architectures. We present and discuss a novel feedback approach based on both linear and non-linear techniques aimed at controlling interprocessor queue occupancy. Theoretical analysis and experiments, carried out on a cycle-accurate multiprocessor simulation platform, show that feedback-based control reduces energy consumption with respect to standard local DVS policies and highlight that non-linear strategies allows a more flexible and robust implementation in presence of variable workload conditions.
ID:620
CLASS:7
Title: A positive systems model of TCP-like congestion control: asymptotic results
Abstract: We study communication networks that employ drop-tail queueing and Additive-Increase Multiplicative-Decrease (AIMD) congestion control algorithms. It is shown that the theory of nonnegative matrices may be employed to model such networks. In particular, important network properties, such as: 1) fairness; 2) rate of convergence; and 3) throughput, can be characterized by certain nonnegative matrices. We demonstrate that these results can be used to develop tools for analyzing the behavior of AIMD communication networks. The accuracy of the models is demonstrated by several NS studies.
ID:621
CLASS:7
Title: Control variates in nonlinear regression
Abstract: Control variates can be applied to Monte Carlo sampling experiments to improve the precision of the results. This method is especially useful in statistical problems were low order approximators of a particular variate of interest are available and possibly several statistical properties of the variate are to be investigated. In this paper a control variate scheme based on the linear approximator &sgr; of the nonlinear parameter estimator (@@@@) is used to improve the precision of the first four moments of (@@@@) and the covariance matrix of the paramter estimates. The control variate method is shown to improve the effectiveness of the Monte Carlo results without substantially increasing the estimation effort, and it is effective over a wide range of nonlinearities. An approximate expression for the effectiveness of the control variate method based on the Beale measure of nonlinearity N&thgr; is given.
ID:622
CLASS:7
Title: Interval constraint solving for camera control and motion planning
Abstract: Many problems in robust control and motion planning can be reduced to either finding a sound approximation of the solution space determined by a set of nonlinear inequalities, or to the "guaranteed tuning problem" as defined by Jaulin and Walter, which amounts to finding a value for some tuning parameter such that a set of inequalities be verified for all the possible values of some perturbation vector. A classical approach to solving these problems, which satisfies the strong soundness requirement, involves some quantifier elimination procedure such as Collins' Cylindrical Algebraic Decomposition symbolic method. Sound numerical methods using interval arithmetic and local consistency enforcement to prune the search space are presented in this article as much faster alternatives for both soundly solving systems of nonlinear inequalities, and addressing the guaranteed tuning problem whenever the perturbation vector has dimension 1. The use of these methods in camera control is investigated, and experiments with the prototype of a declarative modeler to express camera motion using a cinematic language are reported and commented upon.
ID:623
CLASS:7
Title: Methodological frameworks for large-scale network analysis and design
Abstract: This paper emphasizes the need for methodological frameworks for analysis and design of large scale networks which are independent of specific design innovations and their advocacy, with the aim of making networking a more systematic engineering discipline. Networking problems have largely confounded existing theory, and innovation based on intuition has dominated design. This paper will illustrate potential pitfalls of this practice. The general aim is to illustrate universal aspects of theoretical and methodological research that can be applied to network design and verification. The issues focused on will include the choice of models, including the relationship between flow and packet level descriptions, the need to account for uncertainty generated by modelling abstractions, and the challenges of dealing with network scale. The rigorous comparison of proposed schemes will be illustrated using various abstractions. While standard tools from robust control theory have been applied in this area, we will also illustrate how network-specific challenges can drive the development of new mathematics that expand their range of applicability, and how many enormous challenges remain.
ID:624
CLASS:7
Title: Performance of a cooperative algorithm for power control in cellular systems with a time-varying link gain matrix
Abstract: A new version of the Cooperative Algorithm is proposed for distributed power control in time&dash;varying cellular environment. Unlike other approaches which assume a fixed link gain matrix, we consider a time&dash;varying model for shadow fading. We consider the performance of three schemes, namely, instantaneous SIR balancing, slow path loss compensation and the Cooperative Algorithm. Analytical results are obtained for the case where there are two cochannel users. We have shown that the sequence of signal&dash;to&dash;interference ratio &lpar;SIR&rpar; converges in distribution. The outage probability in steady state is obtained. It is shown that when the power control sampling period is small, the performance of the Cooperative Algorithm approaches that of instantaneous SIR balancing. For more than two users, a detailed simulation is performed.
ID:625
CLASS:7
Title: Evaluation of an adaptive traffic control technique with underlying system changes
Abstract: A key problem in traffic engineering is the optimization of the flow of vehicles through a given road network. Improving the timing of the traffic signals at intersections in the network is generally the most powerful and cost-effective means of achieving this goal. Recent efforts have resulted in the development of a fundamentally different approach for optimal centralized signal timing control that eliminates the need for an open-loop model of the traffic network dynamics. The approach is based on a neural network (NN) serving as the basis for the control law, with the internal NN weight estimation occurring real-time in closed-loop mode via the simultaneous perturbation stochastic approximation (SPSA) algorithm. The paper investigates the application of such a non-network-model-based approach and illustrates the approach through a simulation on a nine-intersection, mid-Manhattan, New York network. The simulated traffic network contains varying short and long-term congestion behavior and short-term stochastic, nonlinear effects. The approach results in a net 10% reduction in vehicle wait time relative to the performance of the existing, in-place strategy.
ID:626
CLASS:7
Title: Efficient Monte-Carlo simulation of a product-form model for a cellular system with dynamic resource sharing
Abstract: There are many ways for users to share the radio spectrum allocated to a cell in a cellular phone system. We analyze a commonly proposed scheme wh ere the cell is divided into s sectors. Each sector has exclusive access to a certain number of channels. The remaining channels reside in a &ldquo;common pool&rdquo; and are shared among the sectors. The smallest unit of bandwidth that can be borrowed from the common pool is a &ldquo;carrier,&rdquo; which consists of c channels. When viewed as a multidimensional birth-death process, the steady-state distribution of the number of active channels in each sector has a &ldquo;product form,&rdquo; but because the state space is large and has a nonlinear boundary, direct calculation of quantities of interest is usually impractical. Ross and Wang have developed a Monte-Carlo technique that  applies to our problem. We significantly improve the efficiency of their technique when applied to our problem by including certain (nonlinear) control variates. The kinds of control variates we use can be applied to other loss systems as well. We also explore the effect of importance sampling for our system. In many cases the variance reduction achieved from the combination of importance sampling and control variates is far greater than from either method alone. For systems with blocking probabilities in the range 0.001 to 0.1, the variance of the system-blocking probability estimator can be reduced by several orders of magnitude.
ID:627
CLASS:7
Title: Direct Nonlinear Order Reduction with Variational Analysis
Abstract: The variational analysis [11] has been employed in [7] for order reduction of weakly nonlinear systems. For a relatively strong nonlinear system, this method will mostly lose efficiency because of the exponentially increased number of inputs in higher order variational equations caused by the individual reduction process of the variational systems. Moreover, the inexact inputs into the higher order variational equations indispensably introduce extra errors in theorder reduction process. Inspired by the variational analysis, we propose a direct model order reduction method. The order of the approximate polynomial system of the original nonlinear system is directly reduced by one project space. The proposed direct reduction technique can easily avoid the errors brought by inexact inputs and the exponentially increased inputs. We show theoretically and experimentally that the proposed method can achieve much more accurate reduced system with smaller order size than the conventional variational equation order reduction method.
ID:628
CLASS:7
Title: Call admission policies based on calculated power control setpoints in SIR-based power-controlled DS-CDMA cellular networks
Abstract: In this paper, we develop call admission control algorithms for SIR-based power-controlled DS-CDMA cellular networks. We consider networks that handle both voice and data services. When a new call (or a handoff call) arrives at a base station requesting for admission, our algorithms will calculate the desired power control setpoints for the new call and all existing calls. We will provide necessary and sufficient conditions under which the power control algorithm will have a feasible solution. These conditions are obtained through deriving the inverse of the matrix used in the calculation of power control setpoints. If there is no feasible solution to power control or if the desired power levels to be received at the base station for some calls are larger than the maximum allowable power limits, the admission request will be rejected. Otherwise, the admission request will be granted. When higher priority is desired for handoff calls, we will allow different thresholds (i.e., different maximum allowable power limits) for new calls and handoff calls. We will develop an adaptive algorithm that adjusts these thresholds in real-time as environment changes. The performance of our algorithms will be shown through computer simulation and compared with existing algorithms.
ID:629
CLASS:7
Title: Kernel Predictive Linear Gaussian models for nonlinear stochastic dynamical systems
Abstract: The recent Predictive Linear Gaussian model (or PLG) improves upon traditional linear dynamical system models by using a predictive representation of state, which makes consistent parameter estimation possible without any loss of modeling power and while using fewer parameters. In this paper we extend the PLG to model stochastic, nonlinear dynamical systems by using kernel methods. With a Gaussian kernel, the model admits closed form solutions to the state update equations due to conjugacy between the dynamics and the state representation. We also explore an efficient sigma-point approximation to the state updates, and show how all of the model parameters can be learned directly from data (and can be learned on-line with the Kernel Recursive Least-Squares algorithm). We empirically compare the model and its approximation to the original PLG and discuss their relative advantages.
ID:630
CLASS:7
Title: A control theoretical approach to congestion control in packet networks
Abstract: In this paper, we introduce a control theoretical analysis of the closed-loop congestion control problem in packet networks. The control theoretical approach is used in a proportional rate controller, where packets are admitted into the network in accordance with network buffer occupancy. A Smith Predictor is used to deal with large propagation delays, common to high speed backbone networks. The analytical approach leads to accurate predictions regarding both transients as well as steady-state behavior of buffers and input rates. Moreover, it exposes tradeoffs regarding buffer dimensioning, packet loss, and throughput.
ID:631
CLASS:7
Title: Adaptive control algorithms for decentralized optimal traffic engineering in the internet
Abstract: In this paper, we address the problem of optimal decentralized traffic engineering when multiple paths are available for each call. More precisely, given a set of possible paths for each call, we aim at distributing the traffic among the available paths in order to maximize a given utility function. To solve this problem, we propose a large family of decentralized sending rate control laws having the property that each of the members of this family "steers" the traffic allocation to an optimal operation point. The approach taken relies on the control theory concept of Sliding Modes. These control laws allow each ingress node to independently adjust its traffic sending rates and/or redistribute its sending rates among multiple paths. The only nonlocal information needed is binary feedback from each congested node in the path. The control laws presented are applicable to a large class of utility functions, namely, utility functions that can be expressed as the sum of concave functions of the sending rates. We show that the technique can be applied not only to usual rate adaptive traffic with multiple paths, but also to rate adaptive traffic with minimum service requirements and/or maximum allowed sending rate and to assured service with targeted rate guarantee, all allowing for multiple paths. It is also shown that these control laws are robust with respect to failures; i.e., they automatically reroute traffic if a link failure occurs. Finally, we provide some insight on how to choose the "right" control law. In particular, we provide a way of choosing a member of the family of control laws that reduces the sending rate oscillation caused by implementation constraints like delays and quantization. An example of application of the approach delineated in this paper is also presented. This example provides some insights on the implementation aspects and illustrates the robustness of the control laws developed in this paper.
ID:632
CLASS:7
Title: Introducing consciousness in UWB networks by hybrid modelling of admission control
Abstract: We formalize a model for a self-organizing network of nodes that operate according to the UWB principle based on hybrid modelling formalism. We design the rules that lead to the formation of the network and in particular an admission control procedure that is capable to handle both continuous and discrete perturbations, while maintaining the network in a condition of stability. Cognition is introduced in the model by allowing nodes to adjust their rules of operation based on the perception of the environment by an elected node, serving as the observer, that is aware of context, evaluates, and selects one strategy of operation.
ID:633
CLASS:7
Title: Haptic techniques for media control
Abstract: We introduce a set of techniques for haptically manipulating digital media such as video, audio, voicemail and computer graphics, utilizing virtual mediating dynamic models based on intuitive physical metaphors. For example, a video sequence can be modeled by linking its motion to a heavy spinning virtual wheel: the user browses by grasping a physical force-feedback knob and engaging the virtual wheel through a simulated clutch to spin or brake it, while feeling the passage of individual frames. These systems were implemented on a collection of single axis actuated displays (knobs and sliders), equipped with orthogonal force sensing to enhance their expressive potential. We demonstrate how continuous interaction through a haptically actuated device rather than discrete button and key presses can produce simple yet powerful tools that leverage physical intuition.
ID:634
CLASS:7
Title: Integrating network optimization capabilities into a high-level modeling language
Abstract: Research in network optimization has reached the stage where large-scale problems-linear or non-linear, pure or generalized-are solved very efficiently with minimal computing resources. Representing such problems for solution on the computer, however, remains a rather cumbersome task. Taking advantage of developments in high-level modeling languages, we design and implement integrated systems to facilitate the representation and solution of network problems. Such systems integrate the flexibility and robustness of modeling languages with the efficiency of network optimizers.We describe two alternative modes for this integration, which can be achieved for linear and nonlinear problems alike. The use of the resulting systems is demonstrated with the solution of large-scale problems from diverse applications and with the implementation of network decomposition algorithms.
ID:635
CLASS:7
Title: Modeling and design monitor agent using layered control architecture
Abstract: A software agent is defined as an autonomous software entity that can interact with its environment. It is capable of responding to other agent and/or its environment to some degree. It has some degree of control over its internal state and actions based on its own model. The behavior of an agent has been described by BDI theory as a processing cycle. According to this theory we have developed the processing cycle with software feedback mechanism. Software feedback or loop-back control mechanism is capable acting without direct external intervention. A feedback mechanism continuously monitors the output of the system under control (the target system), compares the result against preset values (goals of the feedback control) and feeds the difference back to adjust the behavior of the target system in one processing cycle. This paper considers the modeling and design of a monitor agent with layered control architecture for autonomy and adaptation. The architecture consists three layers: schedule layer, optimization layer and regulator layer. The regulator layer utilizes software feedback and control methods in a process cycle, the optimization layer will help to adapt to the changing environment more precisely, the schedule layer generates the long-term goal for the agent. Also this paper gives a example of the agent for monitoring the mail server running Lotus Notes. Such sort of computing systems typically have two competitive control goals, namely: maximization of the throughput and minimization of the response time. A set of experimental results showing the effectiveness of the monitor agent for email server has been presented.
ID:636
CLASS:7
Title: Multi-path TCP: a joint congestion control and routing scheme to exploit path diversity in the internet
Abstract: We consider the problem of congestion-aware multi-path routing in the Internet. Currently, Internet routing protocols select only a single path between a source and a destination. However, due to many policy routing decisions, single-path routing may limit the achievable throughput. In this paper, we envision a scenario where multi-path routing is enabled in the Internet to take advantage of path diversity. Using minimal congestion feedback signals from the routers, we present a class of algorithms that can be implemented at the sources to stably and optimally split the flow between each source-destination pair. We then show that the connection-level throughput region of such multi-path routing/congestion control algorithms can be larger than that of a single-path congestion control scheme.
ID:637
CLASS:7
Title: Process control and improvement: prediction of process parameters for intelligent control of freezing tunnels using simulation
Abstract: Various analytical and empirical methods assuming the existence of steady state and requiring homogenous properties of the product have been used with limited success in estimating freezing times in the food processing industry. Irrespective of the method adopted for estimating freezing time requirements, a critical process issue that needs to be considered is that of system control. Simulation models suggest that a feed-forward control strategy, as discussed in this paper, can be used to control a freezing tunnel and obtain considerable energy savings while ensuring 'appropriate' freezing of all products. The control strategy discussed in this paper, involves the continuous monitoring of product input and controlling either or both of the refrigerant flow and conveyor speed. The primary objective of this paper is to demonstrate the use of simulation to predict process parameters for 'intelligent control' of freezing tunnels, and provide an estimate of potential energy savings.
ID:638
CLASS:7
Title: Abstracts from math programming symposium
Abstract: About half of the abstracts were published in the last Newsletter. In this issue we present the remaining ones. Each author was invited to update his abstract following his oral presentation to include the latest ideas. The updated abstracts were used when available, otherwise the original abstracts were used. Where an abstract are multiple authors to a paper, the one giving the paper is underlined and the address refers to that author.
ID:639
CLASS:7
Title: Simulation optimization for decision support in operating a robotic manufacturing system
Abstract: The operation of a robotic manufacturing system can be a complex task for which little experience is now available. Simulation has often been used as a means of modeling large complex systems. Optimization methods use such models to make good choices for system parameters. This paper describes a simulation-optimization approach combined with pattern recognition to develop an operating procedure for a manufacturing system which contains robots. This procedure is adaptive in the sense that it is updated on a periodic basis to account for changing shop load and pending orders.
ID:640
CLASS:7
Title: Analog Macromodeling using Kernel Methods
Abstract: In this paper we explore the potential of using a general class offunctional representation techniques, kernel-based regression, inthe nonlinear model reduction problem. The kernel-based view-pointprovides a convenient computational framework for regression,unifying and extending the previously proposed polynomialand piecewise-linear reduction methods. Furthermore, as many familiarmethods for linear system manipulation can be leveraged ina nonlinear context, kernels provide insight into how new, morepowerful, nonlinear modeling strategies can be constructed. Wepresent an SVD-like technique for automatic compression of non-linearmodels that allows systematic identification of model redundanciesand rigorous control of approximation error.
ID:641
CLASS:7
Title: TP-PPV: piecewise nonlinear, time-shifted oscillator macromodel extraction for fast, accurate PLL simulation
Abstract: We present a novel method for generating small, accurate PLL macromodels that capture transient response and jitter performance with unprecedented accuracy, while offering large speedups. The method extracts and uses a highly accurate oscillator phase macromodel termed the TP-PPV macromodel. The core idea behind the novel extraction procedure is to combine concepts from strongly nonlinear trajectory piecewise macromodeling techniques together with PPV-based timeshifted nonlinear phase macromodels. As a result, TP-PPV generated macromodels offer excellent global as well as local fidelity. These properties are necessary for handing large excursions in PLL control voltages during capture/lock in, e.g., hopping frequency synthesizers. We validate TP-PPV on a 5-stage interpolative ring VCO based PLL and compare results against full simulation, as well as against prior macromodels. We show that, unlike prior macromodels that only work well when the control voltage of the VCO has small excursions, the TP-PPV macromodel provides near-perfect matches against full SPICElevel simulation over a wide range of design scenarios, while achieving speedups of about three orders of magnitude.
ID:642
CLASS:7
Title: Lyapunov design for safe reinforcement learning
Abstract: Lyapunov design methods are used widely in control engineering to design controllers that achieve qualitative objectives, such as stabilizing a system or maintaining a system's state in a desired operating range. We propose a method for constructing safe, reliable reinforcement learning agents based on Lyapunov design principles. In our approach, an agent learns to control a system by switching among a number of given, base-level controllers. These controllers are designed using Lyapunov domain knowledge so that any switching policy is safe and enjoys basic performance guarantees. Our approach thus ensures qualitatively satisfactory agent behavior for virtually any reinforcement learning algorithm and at all times, including while the agent is learning and taking exploratory actions. We demonstrate the process of designing safe agents for four different control problems. In simulation experiments, we find that our theoretically motivated designs also enjoy a number of practical benefits, including reasonable performance initially and throughout learning, and accelerated learning.
ID:643
CLASS:7
Title: Flight midcourse guidance control based on genetic algorithm
Abstract: An advanced flight midcourse guidance law based on genetic algorithm (GA) is proposed. The proposed midcourse guidance formulation minimizes the flight time and maximizes the terminal energy subject to a terminal intercept condition. GA is used to search the optimal attack angle for the flight trajectory. By combining GA and singular perturbation technique (SPT), the optimal flight guidance law is obtained consequently. SPT is applied to approximate the terminal flight time. Meanwhile, the paper completely eliminates the need for solving two-point boundary-value problems (TBPVP), which is too complex for derivation and implementation. The simulation results show that the resulting guidance law is near-optimal and the proposed method is valid. Especially, the GA guidance law can apply to intercept the maneuver targets successfully.
ID:644
CLASS:7
Title: Fluid control using the adjoint method
Abstract: We describe a novel method for controlling physics-based fluid simulations through gradient-based nonlinear optimization. Using a technique known as the adjoint method, derivatives can be computed efficiently, even for large 3D simulations with millions of control parameters. In addition, we introduce the first method for the full control of free-surface liquids. We show how to compute adjoint derivatives through each step of the simulation, including the fast marching algorithm, and describe a new set of control parameters specifically designed for liquids.
ID:645
CLASS:7
Title: Variance reduction of quantile estimates via nonlinear control
Abstract: Linear controls are a well known technique for achieving variance reduction in computer simulation. Unfortunately the effectiveness of a linear control depends upon the correlation between the statistic of interest and the control which is often low. Since statistics are often nonlinear functions of the control this implies that nonlinear controls offer a means for improvement over linear controls. Nonlinear controls have had success in increasing the variance reduction over a linear control. This current work focuses on the use of nonlinear controls for reducing the variance of quantile estimates. The paper begins with a short discussion of linear controls. It describes nonlinear controls and the possibility for improved performance. The final sections discuss quantiles as controls and the potential of nonlinear controls for variance reduction in quantile estimation.
ID:646
CLASS:7
Title: Performance animation from low-dimensional control signals
Abstract: This paper introduces an approach to performance animation that employs video cameras and a small set of retro-reflective markers to create a low-cost, easy-to-use system that might someday be practical for home use. The low-dimensional control signals from the user's performance are supplemented by a database of pre-recorded human motion. At run time, the system automatically learns a series of local models from a set of motion capture examples that are a close match to the marker locations captured by the cameras. These local models are then used to reconstruct the motion of the user as a full-body animation. We demonstrate the power of this approach with real-time control of six different behaviors using two video cameras and a small set of retro-reflective markers. We compare the resulting animation to animation from commercial motion capture equipment with a full set of markers.
ID:647
CLASS:7
Title: From control effects to typed continuation passing
Abstract: First-class continuations are a powerful computational effect, allowing the programmer to express any form of jumping. Types and effect systems can be used to reason about continuations, both in the source language and in the target language of the continuation-passing transform. In this paper, we establish the connection between an effect system for first-class continuations and typed versions of continuation-passing style. A region in the effect system determines a local answer type for continuations, such that the continuation transforms of pure expressions are parametrically polymorphic in their answer types. We use this polymorphism to derive transforms that make use of effect information, in particular, a mixed linear/non-linear continuation-passing transform, in which expressions without control effects are passed their continuations linearly.
ID:648
CLASS:7
Title: A realizable driving point model for on-chip interconnect with inductance
Abstract: In this paper we present a generalization of popular linear model reduction methods, such as Lanczos- and Arnoldi-based algorithms based on rational approximation, to systems whose response to interesting external inputs can be described by a few terms in a functional series expansion such as a Volterra series. The approach allows automatic generation of macromodels that include frequency-dependent nonlinear effects.
ID:649
CLASS:7
Title: A control-based framework for self-managing distributed computing systems
Abstract: This paper describes an online control framework to design self-managing distributed computing systems that continually optimize their performance in response to changing computing demands and environmental conditions. An online control technique is used in conjunction with predictive filters to tune the performance of individual system components based on their forecast behavior. In a distributed setting, a global controller is used to manage the interaction between components such that overall system requirements are satisfied.
ID:650
CLASS:7
Title: Subspace gradient domain mesh deformation
Abstract: In this paper we present a general framework for performing constrained mesh deformation tasks with gradient domain techniques. We present a gradient domain technique that works well with a wide variety of linear and nonlinear constraints. The constraints we introduce include the nonlinear volume constraint for volume preservation, the nonlinear skeleton constraint for maintaining the rigidity of limb segments of articulated figures, and the projection constraint for easy manipulation of the mesh without having to frequently switch between multiple viewpoints. To handle nonlinear constraints, we cast mesh deformation as a nonlinear energy minimization problem and solve the problem using an iterative algorithm. The main challenges in solving this nonlinear problem are the slow convergence and numerical instability of the iterative solver. To address these issues, we develop a subspace technique that builds a coarse control mesh around the original mesh and projects the deformation energy and constraints onto the control mesh vertices using the mean value interpolation. The energy minimization is then carried out in the subspace formed by the control mesh vertices. Running in this subspace, our energy minimization solver is both fast and stable and it provides interactive responses. We demonstrate our deformation constraints and subspace deformation technique with a variety of constrained deformation examples.
ID:651
CLASS:7
Title: A TBR-based trajectory piecewise-linear algorithm for generating accurate low-order models for nonlinear analog circuits and MEMS
Abstract: In this paper we propose a method for generating reduced models for a class of nonlinear dynamical systems, based on truncated balanced realization (TBR) algorithm and a recently developed trajectory piecewise-linear (TPWL) model order reduction approach. We also present a scheme which uses both Krylov-based and TBR-based projections. Computational results, obtained for examples of nonlinear circuits and a micro-electro-mechanical system (MEMS), indicate that the proposed reduction scheme generates nonlinear macromodels with superior accuracy as compared to reduction algorithms based solely on Krylov subspace projections, while maintaining a relatively low model extraction cost.
ID:652
CLASS:7
Title: CDMA uplink power control as a noncooperative game
Abstract: We present a game-theoretic treatment of distributed power control in CDMA wireless systems. We make use of the conceptual framework of noncooperative game theory to obtain a distributed and market-based control mechanism. Thus, we address not only the power control problem, but also pricing and allocation of a single resource among several users. A cost function is introduced as the difference between the pricing and utility functions, and the existence of a unique Nash equilibrium is established. In addition, two update algorithms, namely, parallel update and random update, are shown to be globally stable under specific conditions. Convergence properties and robustness of each algorithm are also studied through extensive simulations.
ID:653
CLASS:7
Title: 3D screen-space widgets for non-linear projection
Abstract: Linear perspective is a good approximation to the format in which the human visual system conveys 3D scene information to the brain. Artists expressing 3D scenes, however, create nonlinear projections that balance their linear perspective view of a scene with elements of aesthetic style, layout and relative importance of scene objects. Manipulating the many parameters of a linear perspective camera to achieve a desired view is not easy. Controlling and combining multiple such cameras to specify a nonlinear projection is an even more cumbersome task. This paper presents a direct interface, where an artist manipulates in 2D the desired projection of a few features of the 3D scene. The features represent a rich set of constraints which define the overall projection of the 3D scene. Desirable properties of local linear perspective and global scene coherence drive a heuristic algorithm that attempts to interactively satisfy the given constraints as a weight-averaged projection of a minimal set of linear perspective cameras. This paper shows that 2D feature constraints are a direct and effective approach to control both the 2D layout of scene objects and the conceptually complex, high dimensional parameter space of nonlinear scene projection.
ID:654
CLASS:7
Title: Mlps (mono layer polynomials and multi layer perceptrons) for nonlinear modeling
Abstract: This paper presents a model selection procedure which stresses the importance of the classic polynomial models as tools for evaluating the complexity of a given modeling problem, and for removing non-significant input variables. If the complexity of the problem makes a neural network necessary, the selection among neural candidates can be performed in two phases. In an additive phase, the most important one, candidate neural networks with an increasing number of hidden neurons are trained. The addition of hidden neurons is stopped when the effect of the round-off errors becomes significant, so that, for instance, confidence intervals cannot be accurately estimated. This phase leads to a set of approved candidate networks. In a subsequent subtractive phase, a selection among approved networks is performed using statistical Fisher tests. The series of tests starts from a possibly too large unbiased network (the full network), and ends with the smallest unbiased network whose input variables and hidden neurons all have a significant contribution to the regression estimate. This method was successfully tested against the real-world regression problems proposed at the NIPS2000 Unlabeled Data Supervised Learning Competition; two of them are included here as illustrative examples.
ID:655
CLASS:7
Title: Control-theoretic dynamic frequency and voltage scaling for multimedia workloads
Abstract: This paper describes a formal feedback-control algorithm for dynamic voltage/frequency scaling (DVS) in a portable multimedia system to save power while maintaining a desired playback rate. Our algorithm is similar in complexity to the previously-proposed change-point detection algorithm [19] but does a better job of maintaining stable throughput and is not dependent on the assumption of an exponential distribution of the frame decoding rate. For approximately the same energy savings as reported by [19], our controller is able to keep the average frame delay within 10% of the target more than 90% of the time, whereas the change-point detection algorithm kept the average frame delay with 10% of the target only 70% or less of the time executing the same workload.
ID:656
CLASS:7
Title: Chaos engineering in Japan
Abstract: Since deterministic chaos is not only a profound concept in science but also a ubiquitous phenomenon in real-world nonlinear systems, extending to a variety of temporal and spatial scales, it can be naturally related to applications in science and technology [4]. In fact, it is not difficult to find the buds of such possible applications in historical papers by Van der Pol and Van der Mark [22], Ulam and von Neumann [21], and Kalman [12], although the term deterministic chaos was not used in those days.
ID:657
CLASS:7
Title: Interactive animation of dynamic manipulation
Abstract: Lifelike animation of object manipulation requires dynamic interaction between animated characters, objects, and their environment. These interactions can be animated automatically with physically based simulations but proper controls are needed to animate characters that move realistically and that accomplish tasks in spite of unexpected disturbances. This paper describes an efficient control algorithm that generates realistic animations by incorporating motion data into task execution. The end result is a versatile system for interactive animation of dynamic manipulation tasks such as lifting, catching, and throwing.
ID:658
CLASS:7
Title: Automated container transport system between inland port and terminals
Abstract: In this article we propose a new concept called automated container transportation system between inland port and terminals (ACTIPOT) which involves the use of automated trucks to transfer containers between an inland port and container terminals. The inland port is located a few miles away from the terminals and is used for storing and processing import/export containers before distribution to customers or transfer to the terminals. We design and analyze the ACTIPOT system with particular attention paid to the overall supervisory controller that synchronizes all the operations inside the ACTIPOT system. We employ the technique of truck platooning in order to simplify the control of the overall system and to minimize the possibility of deadlocks, congestion, and failures. A microscopic simulation model is developed and used to demonstrate the overall performance of the ACTIPOT system. The contribution of this article is the design, analysis, and evaluation of the new concept ACTIPOT.
ID:659
CLASS:7
Title: Engineering applications I: A digital nonlinear function generator
Abstract: A NONLINEAR FUNCTION GENERATOR is designed to produce the command information for a digital control system where path control is desired in such a manner that a constant velocity along the path is maintained. Included are an error analysis and the synthesis procedure used to obtain a satisfactory design.
ID:660
CLASS:7
Title: External control variance reduction for nonstationary simulation
Abstract: A statistically efficient method for performing simulation experimentation of nonstationary queueing models is outlined. The method utilizes a nonstationary queueing approximation as an external control variate system. A simple nonstationary tandem queueing model serves as an example of this variance reduction method.
ID:661
CLASS:7
Title: Simulation of large networks: modeling and simulation of telecommunication networks for control and management
Abstract: In this paper we describe methodologies for telecommunication networks modeling and simulation that are targeted to be useful as tools in on-line and off-line decision making of the type encountered in network control, management and planning problems. We describe the development, validation and use of self-similar and multi-fractal models, queuing control and performance evaluation, assessing the incremental utility of various models, hierarchical models based on aggregation, analytic approximation models for various performance metrics, trade-off and sensitivity analysis using a multi-objective optimization framework and automatic differentiation. We also describe four illustrative examples of applying these methodologies to dynamic network control and management problems. The examples involve primarily mobile ad hoc wireless and satellite networks in changing environments.
ID:662
CLASS:7
Title: Coopetitive visual surveillance using model predictive control
Abstract: Active cooperative sensing with multiple sensors is being actively researched in visual surveillance. However, active cooperative sensing often suffers from the delay in information exchange among the sensors and also from sensor reaction delays. This is because simplistic control strategies like Proportional Integral Differential (PID), that do not employ the look-ahead strategy, often fail to counterbalance these delays at real time. Hence, there is a need for more sophisticated interaction and control mechanisms that can overcome the delay problems. In this paper, we propose a coopetitive framework using Model Predictive Control (MPC) which allows the sensors to not only 'compete' as well as 'cooperate' with each other to perform the designated task in the best possible manner but also to dynamically swap their roles and sub-goals rather than just the parameters. MPC is used as a feedback control mechanism to allow sensors to react not only based on past observations but also on possible future events. We demonstrate the utility of our framework in a dual camera surveillance setup with the goal of capturing the high resolution images of intruders in the surveyed rectangular area e.g. an ATM lobby or a museum. The results are promising and clearly establish the efficacy of coopetition as an effective form of interaction between sensors and MPC as a superior feedback mechanism than the PID.
ID:663
CLASS:7
Title: TCP westwood: end-to-end congestion control for wired/wireless networks
Abstract: TCP Westwood (TCPW) is a sender-side modification of the TCP congestion window algorithm that improves upon the performance of TCP Reno in wired as well as wireless networks. The improvement is most significant in wireless networks with lossy links. In fact, TCPW performance is not very sensitive to random errors, while TCP Reno is equally sensitive to random loss and congestion loss and cannot discriminate between them. Hence, the tendency of TCP Reno to overreact to errors. An important distinguishing feature of TCP Westwood with respect to previous wireless TCP "extensions" is that it does not require inspection and/or interception of TCP packets at intermediate (proxy) nodes. Rather, TCPW fully complies with the end-to-end TCP design principle. The key innovative idea is to continuously measure at the TCP sender side the bandwidth used by the connection via monitoring the rate of returning ACKs. The estimate is then used to compute congestion window and slow start threshold after a congestion episode, that is, after three duplicate acknowledgments or after a timeout. The rationale of this strategy is simple: in contrast with TCP Reno which "blindly" halves the congestion window after three duplicate ACKs, TCP Westwood attempts to select a slow start threshold and a congestion window which are consistent with the effective bandwidth used at the time congestion is experienced. We call this mechanism faster recovery. The proposed mechanism is particularly effective over wireless links where sporadic losses due to radio channel problems are often misinterpreted as a symptom of congestion by current TCP schemes and thus lead to an unnecessary window reduction. Experimental studies reveal improvements in throughput performance, as well as in fairness. In addition, friendliness with TCP Reno was observed in a set of experiments showing that TCP Reno connections are not starved by TCPW connections. Most importantly, TCPW is extremely effective in mixed wired and wireless networks where throughput improvements of up to 550% are observed. Finally, TCPW performs almost as well as localized link layer approaches such as the popular Snoop scheme, without incurring the overhead of a specialized link layer protocol.
ID:664
CLASS:7
Title: An asynchronous integration and event detection algorithm for simulating multi-agent hybrid systems
Abstract: A simulation algorithm is presented for multi-agent hybrid systems---systems consisting of many sets of nonsmooth differential equations---such as systems involving multiple rigid bodies, vehicles, or airplanes. The differential equations are partitioned into coupled subsystems, called "agents"; and the conditions which trigger the discontinuities in the derivatives, called "events", may depend on the global state vector. Such systems normally require significant computational resources to simulate because a global time step is used to ensure the discontinuity is properly handled. When the number of systems is large, forcing all system to be simulated at the same rate creates a computational bottleneck, dramatically decreasing efficiency. By using a control systems approach for selecting integration step sizes, we avoid using a global time step. Each subsystem can be simulated asynchronously when the state is away from the event. As the state approaches the event, the simulation is able to synchronize each of the local time clocks in such a way that the discontinuities are properly handled without the need for "roll back". The algorithm's operation and utility is demonstrated on an example problem inspired by autonomous highway vehicles. Using a combination of stochastic modelling and numerical experiments we show that the algorithm requires significantly less computation time when compared with traditional simulation techniques for such problems, and scales more favorably with problem size.
ID:665
CLASS:7
Title: Application-oriented flow control: fundamentals, algorithms and fairness
Abstract: This paper is concerned with flow control and resource allocation problems in computer networks in which real-time applications may have hard quality of service (QoS) requirements. Recent optimal flow control approaches are unable to deal with these problems since QoS utility functions generally do not satisfy the strict concavity condition in real-time applications. For elastic traffic, we show that bandwidth allocations using the existing optimal flow control strategy can be quite unfair. If we consider different QoS requirements among network users, it may be undesirable to allocate bandwidth simply according to the traditional max-min fairness or proportional fairness. Instead, a network should have the ability to allocate bandwidth resources to various users, addressing their real utility requirements. For these reasons, this paper proposes a new distributed flow control algorithm for multiservice networks, where the application's utility is only assumed to be continuously increasing over the available bandwidth. In this, we show that the algorithm converges, and that at convergence, the utility achieved by each application is well balanced in a proportionally (or max-min) fair manner.
ID:666
CLASS:7
Title: APL, dynamic programming, and the optimal control of electromagnetic brake retarders
Abstract: The basis of dynamic programming is described. It is a method for finding the best way to control a system when that system can be controlled by its inputs. As an example, the optimal (or best) control of an electromagnetic brake retarder is derived. In this example the current to the brake retarder is the input being controlled. APL is shown to be a convenient language for writing a dynamic programming algorithm. A set of APL subroutines is provided for solving two-dimensional problems with one input. Adept APL programmers may enjoy the challenge of generalizing this code to problems of arbitrary dimensions.
ID:667
CLASS:7
Title: Fixed point approximation for multirate multihop loss networks with state-dependent routing
Abstract: In this paper we consider a class of loss networks that have arbitrary topologies and routes of arbitrary length. Multiple traffic classes are present, each with different bandwidth requirement, and each routed according to a state-dependent routing scheme. In particular, we consider the least loaded routing method generalized to routes of arbitrary number of hops. The connection level performance metric of interest is the end-to-end blocking probability. We are interested in developing fast evaluation methods to provide reasonably accurate estimates of the blocking probability, especially under heavy traffic load. Our algorithms are based on the fixed-point method framework, also known as the reduced load approximation. In addition to what commonly examined by previous work, two more factors contribute to the complexity of the computation in the scenario under consideration in this paper. One is the state-dependent nature of the routing mechanism, the other is the possible overlapping between routes due to the general multihop topology of the network. We present two fast approximation algorithms to evaluate the blocking probability with state-dependent routing by simplifying the route overlapping computation. We discuss the computational complexity of our algorithms as well as sources of approximation error. We then compare the numerical results with that of simulation and show that our algorithms provide fairly accurate blocking probability estimates especially under heavy traffic load.
ID:668
CLASS:7
Title: Augmenting linear control variates using transformations
Abstract: Linear control variates provide a convenient means for variance reduction in a wide variety of simulation and Monte Carlo sampling problems. The effectiveness of control variates can often be improved using appropriate transformations which increase the correlation between the primary variate and the transformed control variate. Possible transformations include polynomials, the Box-Cox family of power transformations, and the inverse-normal probability transformation. Here we consider generalized transformations using cubic splines which provide a virtually unrestricted source of transformations. To illustrate the transformation methodology, a nonlinear regression problem is used. In the illustration, the transformed control is compared to the control when estimating the mean of the sampling distribution of the estimated parameters.
ID:669
CLASS:7
Title: Design of a robust active queue management algorithm based on feedback compensation
Abstract: Active Queue Management (AQM) is a very active research area in networking. The main objective of an AQM mechanism is to provide low delay and low loss service in best-effort service networks. In this paper we propose a new AQM algorithm based on the feedback compensation technique in control theory. The algorithm is called Proportional Integral based series compensation, and Position feedback compensation (PIP). By choosing the appropriate feedback compensation element and its parameters, the properties of the corrected system become dependent, to a great extent, on the series and feedback compensatory elements. Thus, PIP can eliminate the error incurred by the inaccuracy in the linear system model as well as eliminate the sensitivity to the changes in system parameters like load level, propagation delay, etc. The performance of PIP is compared to PI using ns simulations. From the experiments and analysis we conclude that PIP is more responsive to and robust under time-varying network conditions than PI.
ID:670
CLASS:7
Title: Time-Domain Simulation of Sampled Weakly Nonlinear Systems Using Analytical Integration and Orthogonal Polynomial Series
Abstract: This paper presents a novel method for simulation of sampled systems with weakly nonlinear behavior. These systems can be characterized by adding weakly nonlinear terms to the linear state-space equations of the system resulting in an extended state-space model. Perturbation theory is used to split these equations in an ideal linear behavior and a non-ideal small perturbation. The linear equations are solved analytically which reduces simulation time compared to numerical evaluation. The solution of the perturbation equations is approximated by orthogonal polynomials. This methodology not only reduces simulation time compared to traditional numerical simulations, but also deals naturally with clock jitter and the discontinuous behavior ofsampled systems. An implementation of the methodology has been used to analyze systems including switched filters and continuous-time  modulators.
ID:671
CLASS:7
Title: Decentralized optimal traffic engineering in the internet
Abstract: Distributed optimal traffic engineering in the presence of multiple paths has been found to be a difficult problem to solve. In this paper, we introduce a new approach in an attempt to tackle this problem. This approach has its basis in nonlinear control theory. More precisely, it relies on the concept of Sliding Modes. We develop a family of control laws, each of them having the property that the steady-state network resource allocation yields the maximum of the given utility function, subject to the network resource constraints. These control laws not only allow each ingress node to independently adjust its traffic sending rate but also provide a scheme for optimal traffic load redistribution among multiple paths. The only nonlocal information needed is binary feedback from each congested node in the path. Moreover, the algorithms presented are applicable to a large class of utility functions, namely, utility functions that can be expressed as the sum of concave functions of the sending rates. We show that the technique can be applied not only to rate adaptive traffic with multiple paths, but also to assured service traffic with multiple paths. Preliminary case studies show that this technique is potentially very useful for optimal traffic engineering in a multiple-class-of-service and multiple-path enabled Internet, e.g., differentiated services enabled multi-protocol label switching networks.
ID:672
CLASS:7
Title: Interactive skeleton-driven dynamic deformations
Abstract: This paper presents a framework for the skeleton-driven animation of elastically deformable characters. A character is embedded in a coarse volumetric control lattice, which provides the structure needed to apply the finite element method. To incorporate skeletal controls, we introduce line constraints along the bones of simple skeletons. The bones are made to coincide with edges of the control lattice, which enables us to apply the constraints efficiently using algebraic methods. To accelerate computation, we associate regions of the volumetric mesh with particular bones and perform locally linearized simulations, which are blended at each time step. We define a hierarchical basis on the control lattice, so for detailed interactions the simulation can adapt the level of detail. We demonstrate the ability to animate complex models using simple skeletons and coarse volumetric meshes in a manner that simulates secondary motions at interactive rates.
ID:673
CLASS:7
Title: Computational modeling for the computer animation of legged figures
Abstract: Modeling techniques for animating legged figures are described which are used in the PODA animation system. PODA utilizes pseudoinverse control in order to solve the problems associated with manipulating kinematically redundant limbs. PODA builds on this capability to synthesize a kinematic model of legged locomotion which allows animators to control the complex relationships between the motion of the body of a figure and the coordination of its legs. Finally, PODA provides for the integration of a simple model of legged locomotion dynamics which insures that the accelerations of a figure's body are synchronized with the timing of the forces applied by its legs.
ID:674
CLASS:7
Title: Algorithm 852: RealPaver: an interval solver using constraint satisfaction techniques
Abstract: RealPaver is an interval software for modeling and solving nonlinear systems. Reliable approximations of continuous or discrete solution sets are computed using Cartesian products of intervals. Systems are given by sets of equations or inequality constraints over integer and real variables. Moreover, they may have different natures, being square or nonsquare, sparse or dense, linear, polynomial, or involving transcendental functions.The modeling language permits stating constraint models and tuning parameters of solving algorithms which efficiently combine interval methods and constraint satisfaction techniques. Several consistency techniques (box, hull, and 3B) are implemented. The distribution includes C sources, executables for different machine architectures, documentation, and benchmarks. The portability is ensured by the GNU C compiler.
ID:675
CLASS:7
Title: The determination of digital simulation models for continuous systems by direct-search minimization
Abstract: The representation of system characteristics by a discrete time model poses a fundamental problem in the field of digital simulation. In the majority of circumstances the problem consists of approximating a continuous time model well-defined in terms of a transfer function or state variable equation by difference equations or other forms convenient for computer processing. Examples representative of this approach are the Tustin method,1 Boxer-Thaler method,2 the Madwed-Truxal method,3 or the State Transition method.4 Equally important in the simulation field are the numerical integration methods such as the RungeKutta-Blum technique,5 the Runge-Kutta-Merson technique,6 or the Adams-Moulton technique.7 The latter class of techniques is applicable when the system equations are given in terms of first-order (linear or nonlinear) differential equations. A summary and comparison of the above techniques is given by Fryer and Schultz8 and Martens.9 The derivation of discrete analogs for continuous systems has also been discussed by Shaw.10
ID:676
CLASS:7
Title: Tracking time-varying parameters in software systems with extended Kalman filters
Abstract: Autonomic control of a service system can take advantage of a performance model only if a way can be found to track the changes in the system. A Kalman Filter provides a framework for integrating various kinds of measured data, and for tracking changes in any time-varying system. This work evaluates the effectiveness of such a filter in tracking changes in performance parameters of a software system that occur at different rates and amplitudes. The time-varying system is a Web application deployed in a data centre with layered queuing resources, in which parameter variations happen at random instants. The tracking filter is based on a layered queuing model of this system, with parameters representing CPU demands and the user load intensity. Experiments were performed to evaluate the effectiveness of the filter in tracking the changes, and the requirements for the filter settings for fast and slow variations in the parameters. The target application is autonomic control of a service centre.
ID:677
CLASS:7
Title: Ultracomputers
Abstract: A class of parallel processors potentially involving thousands of individual processing elements is described. The architecture is based on the perfect shuffle connection and has two favorable characteristics: (1) Each processor communicates with a fixed number of other processors. (2) Important communication functions can be accomplished in time proportional to the logarithm of the number of processors. A number of basic algorithms for these &ldquo;ultracomputers&rdquo; are presented, and physical design considerations are discussed in a preliminary fashion.
ID:678
CLASS:7
Title: Numerical analysis in a Ph.D. computer science program
Abstract: Numerical Analysis is the study of methods and procedures used to obtain&ldquo;approximate solutions&rdquo; to mathematical problems. Much of the emphasis is on scientific calculation. The difficulties of education in such a broad area center around the question of background and emphasis. The Numerical Analysis program in the Computer Science Department should emphasize an awareness of the problems of computer implementation and experimental procedures. Nevertheless, there is a need for a solid background in applied mathematics.
ID:679
CLASS:7
Title: Sum of roots with positive real parts
Abstract: In this paper we present a method to compute or estimate the sum of roots with positive real parts (SORPRP) of a polynomial, which is related to a certain index of "average" stability in optimal control, without computing explicit numerical values of the roots. The method is based on symbolic and algebraic computations and enables us to deal with polynomials with parametric coefficients for their SORPRP. This leads to provide a novel systematic method to achieve optimal regulator design in control by combining the method with quantifier elimination. We also report some experimental result for a typical class of plants and confirm the effectiveness of the proposed method.
ID:680
CLASS:7
Title: Evolution of Voronoi based fuzzy recurrent controllers
Abstract: A fuzzy controller is usually designed by formulating the knowledge of a human expert into a set of linguistic variables and fuzzy rules. Among the most successful methods to automate the fuzzy controllers development process are evolutionary algorithms. In this work, we propose the Recurrent Fuzzy Voronoi (RFV) model, a representation for recurrent fuzzy systems. It is an extension of the FV model proposed by Kavka and Schoenauer that extends the application domain to include temporal problems. The FV model is a representation for fuzzy controllers based on Voronoi diagrams that can represent fuzzy systems with synergistic rules, fulfilling the &#949;-completeness property and providing a simple way to introduce a priory knowledge. In the proposed representation, the temporal relations are embedded by including internal units that provide feedback by connecting outputs to inputs. These internal units act as memory elements. In the RFV model, the semantic of the internal units can be specified together with the a priori rules. The geometric interpretation of the rules allows the use of geometric variational operators during the evolution. The representation and the algorithms are validated in two problems in the area of system identification and evolutionary robotics.
ID:681
CLASS:7
Title: Deterministic fluid models of congestion control in high-speed networks
Abstract: Congestion control algorithms, such as TCP or the closely-related additive increase-multiplicative decrease algorithms, are extremely difficult to simulate on a large scale. The reasons for this include the complexity of the actual implementation of the algorithm and the randomness introduced in the packet arrival and service processes due to many factors such as arrivals and departures of sources and uncontrollable short flows in the network. To make the simulation tractable, often deterministic fluid approximations of these algorithms are used. These fluid approximations are in the form of deterministic delay differential equations. In this paper, we ignore the complexity introduced by the window-based implementation of such algorithms and focus on the randomness in the network. We justify the use of deterministic models for proportionally-fair congestion controllers under a limiting regime where the number of sources in a network is large.
ID:682
CLASS:7
Title: A probabilistic algorithm to test local algebraic observability in polynomial time
Abstract: The following questions are often encountered in system and control theory. Given an algebraic model of a physical process, which variables can be, in theory, deduced from the input-output behavior of an experiment? How many of the remaining variables should we assume to be known in order to determine all the others? These questions are parts of the local algebraic observability problem which is concerned with the existence of a non trivial Lie subalgebra of the symmetries of the model letting the inputs and the outputs invariant.
ID:683
CLASS:7
Title: Adaptive proportional delay differentiated services: characterization and performance evaluation
Abstract: We examine a proportional-delay model for Internet differentiated services. Under this model, an ISP can control the waiting-time "spacings" between different classes of traffic. Specifically, the ISP tries to ensure that the average waiting time of class i traffic relative to that of class i - 1 traffic is kept at a constant specified ratio. If the waiting-time ratio of class i - 1 to class i is greater than one, the ISP can legitimately charge users of class i traffic a higher tariff rate (compared to the rate for class i - 1 traffic), since class i users consistently enjoy better performance than class i - 1 users. To realize such proportional-delay differentiated services, we use the time-dependent priority scheduling algorithm. We formally characterize the feasible regions in which given delay ratios can be achieved. Moreover, a set of control parameters for obtaining the desired delay ratios can be determined by an efficient iterative algorithm. We also use an adaptive control algorithm to maintain the correctness of these parameters in response to changing system load. Experiments are carried out to illustrate the short-term, medium-term and long-term relative waiting-time performances for different service classes under Poisson, Pareto, MMPP and mixed traffic workloads. We also carry out experiments to evaluate the achieved end-to-end accumulative waiting times for different classes of traffic which traverse multiple hops under our service model.
ID:684
CLASS:7
Title: The CLP(<inline-equation> <f> <sc>R</sc></f> </inline-equation> ) language and system
Abstract: The CLPR programming language is defined, its underlyingphilosophy and programming methodology are discussed, importantimplementation issues are explored in detail, and finally, a prototypeinterpreter is described.CLPRis designed to be an instance of the Constraint LogicProgramming Scheme, a family of rule-based constraint programminglanguages defined by Jaffar and Lassez. The domain of computationRof this particular instance is the algebraic structureconsisting of uninterpreted functors over real numbers. An importantproperty of CLPRis that the constraints are treated uniformly in thesense that they are used to specify the input parameters to a program,they are the only primitives used in the execution of a program, andthey are used to describe the output of a program.Implementation of a CLP language, and of CLPRin particular, raises new problems in the design of aconstraint-solver. For example, the constraint solver must beincremental in the sensethat solving additional constraints must not entail the resolving of oldconstraints. In our system, constraints are filtered through aninference engine, an engine/solver interface, an equation solver and aninequality solver. This sequence of modules reflects a classificationand prioritization of the classes of constraints. Modules solving higherpriority constraints are isolated from the complexities of modulessolving lower priority constraints. This multiple-phase solving ofconstraints, together with a set of associated algorithms, gives rise toa practical system.
ID:685
CLASS:7
Title: End-to-end congestion control for the internet: delays and stability
Abstract: Under the assumption that queueing delays will eventually become small relative to propagation delays, we derive stability results for a fluid flow model of end-to-end Internet congestion control. The theoretical results of the paper are intended to be decentralized and locally implemented: each end system needs knowledge only of its own round-trip delay. Criteria for local stability and rate of convergence are completely characterized for a single resource, single user system. Stability criteria are also described for networks where all users share the same round-trip delay. Numerical experiments investigate extensions to more general networks. Through simulations, we are able to evaluate the relative importance of queueing delays and propagation delays on network stability. Finally, we suggest how these results may be used to design network resources.
ID:686
CLASS:7
Title: The development of an automated flight test management system for flight test planning and monitoring
Abstract: This paper describes the development of an automated flight test management system (ATMS) as a component of a rapid-prototyping flight research facility for AI-based flight systems concepts. The ATMS provides a flight test engineer (FTE) with a set of tools that assist in flight test planning, monitoring and simulation. This system is also capable of controlling an aircraft during flight test by performing closed-loop guidance functions, range management, and maneuver-quality monitoring. The ATMS is being used as the prototypical system to develop a flight research facility for AI-based flight systems concepts at NASA Ames-Dryden.
ID:687
CLASS:7
Title: A physically-based motion retargeting filter
Abstract: This article presents a novel constraint-based motion editing technique. On the basis of animator-specified kinematic and dynamic constraints, the method converts a given captured or animated motion to a physically plausible motion. In contrast to previous methods using spacetime optimization, we cast the motion editing problem as a constrained state estimation problem, based on the per-frame Kalman filter framework. The method works as a filter that sequentially scans the input motion to produce a stream of output motion frames at a stable interactive rate. Animators can tune several filter parameters to adjust to different motions, turn the constraints on or off based on their contributions to the final result, or provide a rough sketch (kinematic hint) as an effective way of producing the desired motion. Experiments on various systems show that the technique processes the motions of a human with 54 degrees of freedom, at about 150 fps when only kinematic constraints are applied, and at about 10 fps when both kinematic and dynamic constraints are applied. Experiments on various types of motion show that the proposed method produces remarkably realistic animations.
ID:688
CLASS:7
Title: A capacity-based RAU selection scheme for downlink transmission in distributed antenna system
Abstract: As a promising wireless network structure, distributed antenna system (DAS) has recently attracted some investigations for its application in future wireless communication systems. Previous work had studied the performance of DAS in indoor and outdoor cellular mobile communications system and investigated signal transmit schemes and power allocation methods for DAS performance analysis. In this paper, we discuss the optimization problem of downlink channel capacity of DAS and analyze the downlink channel capacity in cellular and Manhattan scenarios. Based on the characteristics of nonlinear variation of downlink ergodic capacity with increase of transmitting remote antenna unit (RAU) number, a capacity based RAU selection (CBRS) scheme which combines optimal capacity RAU selection and power allocation is proposed for effective system control of downlink transmission in DAS. Simulation results show that the CBRS control scheme can achieve better capacity performance compared with blanket transmission and selection diversity schemes.
ID:689
CLASS:1
Title: Time-space trade-off lower bounds for randomized computation of decision problems
Abstract: We prove the first time-space lower bound trade-offs for randomized computation of decision problems. The bounds hold even in the case that the computation is allowed to have arbitrary probability of error on a small fraction of inputs. Our techniques are extension of those used by Ajtai and by Beame, Jayram, and Saks that applied to deterministic branching programs. Our results also give a quantitative improvement over the previous results.Previous time-space trade-off results for decision problems can be divided naturally into results for functions with Boolean domain, that is, each input variable is &lcub;0,1&rcub;-valued, and the case of large domain, where each input variable takes on values from a set whose size grows with the number of variables.In the case of Boolean domain, Ajtai exhibited an explicit class of functions, and proved that any deterministic Boolean branching program or RAM using space S = o(n) requires superlinear time T to compute them. The functional form of the superlinear bound is not given in his paper, but optimizing the parameters in his arguments gives T = &Omega;(n log log n/log log log n) for S = O(n1&minus;&epsis;). For the same functions considered by Ajtai, we prove a time-space trade-off (for randomized branching programs with error) of the form T = &Omega;(n &sqrt; log(n/S)/log log (n/S)). In particular, for space O(n1&minus;&epsis;), this improves the lower bound on time to &Omega;(n&sqrt; log n/log log n).In the large domain case, we prove lower bounds of the form T = &Omega;(n&sqrt; log(n/S)/log log (n/S)) for randomized computation of the element distinctness function and lower bounds of the form T = &Omega;(n log (n/S)) for randomized computation of Ajtai's Hamming closeness problem and of certain functions associated with quadratic forms over large fields.
ID:690
CLASS:1
Title: Relativizing complexity classes with sparse oracles
Abstract: Baker, Gill, and Solovay constructed sparse sets A and B such that P(A) &ne; NP(A) and NP(B) &ne; co-NP(B). In contrast to their results, we prove that P = NP if and only if for every tally language T, P(T) = NP( T), and that NP = co-NP if and only if for every tally language T, NP(T) = co-NP(T). We show that the polynomial hierarchy collapses if and only if there is a sparse set S such that the polynomial hierarchy relative to S collapses. Similar results are obtained for several other complexity classes.
ID:691
CLASS:1
Title: Computing shortest paths for any number of hops
Abstract: In this paper, we introduce and investigate a "new" path optimization problem that we denote the all hops optimal path (AHOP) problem. The problem involves identifying, for all hop counts, the optimal, i.e., minimum weight, path(s) between a given source and destination(s). The AHOP problem arises naturally in the context of quality-of-service (QoS) routing in networks, where routes (paths) need to be computed that provide services guarantees, e.g., delay or bandwidth, at the minimum possible "cost" (amount of resources required) to the network. Because service guarantees are typically provided through some form of resource allocation on the path (links) computed for a new request, the hop count, which captures the number of links over which resources are allocated, is a commonly used cost measure. As a result, a standard approach for determining the cheapest path available that meets a desired level of service guarantees is to compute a minimum hop shortest (optimal) path. Furthermore, for efficiency purposes, it is desirable to precompute such optimal minimum hop paths for all possible service requests. Providing this information gives rise to solving the AHOP problem. The paper's contributions are to investigate the computational complexity of solving the AHOP problem for two of the most prevalent cost functions (path weights) in networks, namely, additive and bottleneck weights. In particular, we establish that a solution based on the Bellman-Ford algorithm is optimal for additive weights, but show that this does not hold for bottleneck weights for which a lower complexity solution exists.
ID:692
CLASS:1
Title: Computing an equidimensional decomposition of an algebraic variety by means of geometric resolutions
Abstract: Let &fnof;1, &hellip; , &fnof;s be polynomials in n variables over a field of characteristic zero and d be the maximum of their total degree. We propose a new probabilistic algorithm for computing a geometric resolution of each equidimensional part of the variety defined by the system &fnof;1 = &middot;&middot;&middot; = &fnof;s = 0. The returned resolutions are encoded by means of Straight-Line Programs and the complexity of the algorithm is polynomial in a geometric degree of the system. In the worst case this complexity is asymptotically polynomial in sdn.
ID:693
CLASS:1
Title: Lower bounds on communication complexity in distributed computer networks
Abstract: The main result of this paper is a general technique for determining lower bounds on the communication complexity of problems on various distributed computer networks. This general technique is derived by simulating the general network by a linear array and then using a lower bound on the communication complexity of the problem on the linear array. Applications of this technique yield optimal bounds on the communication complexity of merging, ranking, uniqueness, and triangle-detection problems on a ring of processors. Nontrivial near-optimal lower bounds on the communication complexity of distinctness, merging, and ranking on meshes and complete binary trees are also derived.
ID:694
CLASS:1
Title: A complexity theory based on Boolean algebra
Abstract: A projection of a Boolean function is a function obtained by substituting for each of its variables a variable, the negation of a variable, or a constant. Reducibilities among computational problems under this relation of projection are considered. It is shown that much of what is of everyday relevance in Turing-machine-based complexity theory can be replicated easily and naturally in this elementary framework. Finer distinctions about the computational relationships among natural problems can be made than in previous formulations and some negative results are proved.
ID:695
CLASS:1
Title: P-uniform circuit complexity
Abstract: Much complexity-theoretic work on parallelism has focused on the class NC, which is defined in terms of logspace-uniform circuits. Yet P-uniform circuit complexity is in some ways a more natural setting for studying feasible parallelism. In this paper, P-uniform NC (PUNC) is characterized in terms of space-bounded AuxPDAs and alternating Turing Machines with bounded access to the input. The notions of general-purpose and special-purpose computation are considered, and a general-purpose parallel computer for PUNC is presented. It is also shown that NC = PUNC if all tally languages in P are in NC; this implies that the NC = PUNC question and the NC = P question are both instances of the ASPACE(S(n)) = ASPACE,TIME(S(n), S(n)o(1)) question. As a corollary, it follows that NC = PUNC implies PSPACE = DTIME(2no(1)).
ID:696
CLASS:1
Title: Managing access control complexity using metrices
Abstract: General access control models enable flexible expression of access control policies, but they make the verification of whether a particular access control configuration is safe (i.e., prevents the leakage of a permission to an unauthorized subject) difficult.  The current approach to expressing safety policy in such models is to use constraints.  When the constraints are verified, then the configuration is verified to be safe.  However, the addition of constraints to an access control configuration significantly increases its complexity, so it quickly becomes difficult to understand the access control policy expressed in the configuration such that future changes can be made correctly.  We propose an approach whereby the complexity of each access control configuration is estimated, so the administrators can see the effect of a configuration change on the future ability to maintain the configuration.  We identify metrics for making complexity estimates and evaluate these metrics on some constraint examples.  Our goal is to enable the use of flexible access control models for safety-critical systems by permitting limited use of constraints that do not complicate the configuration beyond a maintainable complexity.
ID:697
CLASS:1
Title: The existence and density of generalized complexity cores
Abstract: If C is a class of sets and A is not in C, then an infinite set H is a proper hard core for A with respect to C, if H &sube; A and for every C &egr; C such that C &sube; A, C &Cap; H is finite. It is shown that if C is a countable class of sets of strings that is closed under finite union and finite variation, then every infinite set not in C has a proper hard core with respect to C. In addition, the density of such generalized complexity cores is studied.
ID:698
CLASS:1
Title: The complexity of probabilistic verification
Abstract: We determine the complexity of testing whether a finite state, sequential or concurrent probabilistic program satisfies its specification expressed in linear-time temporal logic. For sequential programs, we present an algorithm that runs in time linear in the program and exponential in the specification, and also show that the problem is in PSPACE, matching the known lower bound. For concurrent programs, we show that the problem can be solved in time polynomial in the program and doubly exponential in the specification, and prove that it is complete for double exponential time. We also address these questions for specifications described by &ohgr;-automata or formulas in extended temporal logic.
ID:699
CLASS:1
Title: The interface between computational and combinatorial geometry
Abstract: We illustrate the rich interface between computational and combinatorial geometry by a series of examples, including k-sets, randomized incremental algorithms, random sampling and partitioning, and analysis of geometric arrangements.
ID:700
CLASS:1
Title: Complexity classes of partial recursive functions (Preliminary Version)
Abstract: This paper studies possible extensions of the concept of complexity class of recursive functions to partial recursive functions. Many of the well-known results for total complexity classes are shown to have corresponding, though not exactly identical, statements for partial classes. In particular, with two important exceptions, all results on the presentation and decision problems of membership for the two most reasonable definitions of partial classes are the same as for total classes. The exceptions concern presentations of the complements and maximum difficulty for decision problems of the more restricted form of partial classes. The last section of this paper shows that it is not possible to have an &ldquo;Intersection Theorem&rdquo;, corresponding to the Union Theorem of McCreight and Meyer, either for complexity classes or complexity index sets.
ID:701
CLASS:1
Title: Straight-line program length as a parameter for complexity measures
Abstract: This paper represents a continuation of work in [LBI] and [LB2] directed toward the development of a unified, relative model for complexity theory. The earlier papers establish a simple, natural and fairly general model, and demonstrated its attractiveness by using it to state and prove a variety of technical results. The present paper uses the same model but deals more specifically with the problems involved in stating complexity bounds in a usable closed form for arbitrary operations on arbitrary data types. Work currently in progress is directed toward similar unified treatment of complexity of data structures.
ID:702
CLASS:1
Title: Computational complexity in two-level morphology
Abstract: Morphological analysis must take into account the spelling-change processes of a language as well as its possible configurations of stems, affixes, and inflectional markings. The computational difficulty of the task can be clarified by investigating specific models of morphological processing. The use of finite-state machinery in the "two-level" model by Kimmo Koskenniemi gives it the appearance of computational efficiency, but closer examination shows the model does not guarantee efficient processing. Reductions of the satisfiability problem show that finding the proper lexical/surface correspondence in a two-level generation or recognition problem can be computationally difficult. The difficulty increases if unrestricted deletions (null characters) are allowed.
ID:703
CLASS:1
Title: On classes of computable functions
Abstract: The complexity closure of a computable function is defined by a set of axioms. The axioms are satisfied by complexity classes that are computation time closed and also by other complexity classes which do not have this property. It is then shown that there exist honest recursive functions whose complexity closure are setwise incomparable. Further that there exist chains of honest recursive functions whose complexity closures are densely ordered under set inclusion.
ID:704
CLASS:1
Title: The complexity of XPath query evaluation
Abstract: In this paper, we study the precise complexity of XPath 1.0 query processing. Even though heavily used by its incorporation into a variety of XML-related standards, the precise cost of evaluating an XPath query is not yet wellunderstood. The first polynomial-time algorithm for XPath processing (with respect to combined complexity) was proposed only recently, and even to this day all major XPath engines take time exponential in the size of the input queries. From the standpoint of theory, the precise complexity of XPath query evaluation is open, and it is thus unknown whether the query evaluation problem can be parallelized.In this work, we show that both the data complexity and the query complexity of XPath 1.0 fall into lower (highly parallelizable) complexity classes, but that the combined complexity is PTIME-hard. Subsequently, we study the sources of this hardness and identify a large and practically important fragment of XPath 1.0 for which the combined complexity is LOGCFL-complete and, therefore, in the highly parallelizable complexity class NC2.
ID:705
CLASS:1
Title: On the theory of average case complexity
Abstract: This paper takes the next step in developing the theory of average case complexity initiated by Leonid A. Levin. Previous works [Levin 84, Gurevich 87, Venkatesan and Levin 88] have focused on the existence of complete problems. We widen the scope to other basic questions in computational complexity. Our results include:the equivalence of search and decision problems in the context of average case complexity;an initial analysis of the structure of distributional-NP under reductions which preserve average polynomial-time;a proof that if all distributional-NP is in average polynomial-time then non-deterministic exponential-time equals deterministic exponential time (i.e., a collapse in the worst case hierarchy);definitions and basic theorems regarding other complexity classes such as average log-space.
ID:706
CLASS:1
Title: SIGACT News Complexity Theory Column 10
Abstract: We survey techniques and results obtained over the past thirty years for space-bounded probabilistic computation. We use a common framework for the study of both finite-state automata and logarithmic-space-bounded Turing machines. In particular, we present recent advances on space-efficient deterministic simulation of probabilistic automata.
ID:707
CLASS:1
Title: Interaction in quantum communication and the complexity of set disjointness
Abstract: One of the most intriguing facts about communication using quantum states is that these states cannot be used to transmit more classical bits than the number of qubits used, yet in some scenarios there are ways of conveying information with exponentially fewer qubits than possible classically [3, 26]. Moreover, these methods have a very simple structure---they involve only few message exchanges between the communicating parties.We consider the question as to whether every classical protocol may be transformed to a &ldquo;simpler&rdquo; quantum protocol---one that has similar efficiency, but uses fewer message exchanges. We show that for any constant k, there is a problem such that its k+1 message classical communication complexity is exponentially  smaller than its k message quantum communication complexity, thus answering the above question in the negative. This in particular proves a round hierarchy theorem for quantum communication complexity, and implies via a simple reduction, an \Omega(N^{1/k}) lower bound for k message protocols for Set Disjointness for constant~k.Our result builds on two primitives, local transitions in bi-partite states (based on previous work) and average encoding which may be of significance in other contexts as well.
ID:708
CLASS:1
Title: On the complexity of dataflow analysis of logic programs
Abstract: It is widely held that there is a correlation between complexity and precision in dataflow analysis, in the sense that the more precise an analysis algorithm, the more computationally expensive it must be. The details of this relationship, however, appear to not have been explored extensively. This article reports some results on this correlation in the context of logic programs. A formal notion of the &ldquo;precision&rdquo; of an analysis algorithm is proposed, and this is used to characterize the worst-case computational complexity of a number of dataflow analyses with different degrees of precision. While this article considers the analysis of logic programs, the technique proposed, namely the use of &ldquo;exactness sets&rdquo; to study relationships between complexity and precision of analyses, is not specific to logic programming in any way, and is equally applicable to flow analyses of other language families.
ID:709
CLASS:2
Title: Harp: a distributed query system for legacy public libraries and structured databases
Abstract: The main purpose of a digital library is to facilitate users easy access to enormous amount of globally networked information. Typically, this information includes preexisting public library catalog data, digitized document collections, and other databases. In this article, we describe the distributed query system of a digital library prototype system known as HARP. In the HARP project, we have designed and implemented a distributed query processor and its query front-end to support integrated queries to preexisting public library catalogs and structured databases. This article describes our experiences in the design of an extended Sequel (SQL) query language known as HarpSQL. It also presents the design and implementation of the distributed query system. Our experience in  distributed query processor and user interface design and development will be highlighted. We believe that our prototyping effort will provide useful lessons to the development of a complete digital library infrastructure.
ID:710
CLASS:2
Title: Active database systems
Abstract: Active database systems support mechanisms that enable them to respond automatically to events that are taking place either inside or outside the database system itself. Considerable effort has been directed towards improving understanding of such systems in recent years, and many different proposals have been made and applications suggested. This high level of activity has not yielded a single agreed-upon standard approach to the integration of active functionality with conventional database systems, but has led to improved understanding of active behavior description languages, execution models, and architectures. This survey presents the fundamental characteristics of active database systems, describes a collection of representative systems within a common framework, considers  the consequences for implementations of certain design decisions, and discusses tools for developing active applications.
ID:711
CLASS:2
Title: Object-oriented database systems
Abstract: This paper describes my vision of the current state of object-oriented database research. I first briefly define this field by its objectives, and relate it to other database subfields. I describe what I consider to be the main characteristics of an object oriented system, i.e. those which are important to integrate in a database system: encapsulation, object identity, classes or types, inheritance, overriding and late binding. I point out the differences between an object oriented system and an object oriented database system. I also point out the advantages and drawbacks of an object oriented database system with respect to a relational system. Finally, I list some research issues.
ID:712
CLASS:2
Title: Research directions in object-oriented database systems
Abstract: The set of object-oriented concepts found in object-oriented programming languages forms a good basis for a data model for post-relational database systems which will extend the domain of database applications beyond conventional business data processing. However, despite the high level of research and development activities during the past several years, there is no standard object-oriented data model, and criticisms and concerns about the field still remain. In this paper, I will first provide a historical perspective on the emergence of object-oriented database systems in order to derive a definition of object-oriented database systems. I will then examine a number of major challenge which remain for researchers and implementers of object-oriented database systems.
ID:713
CLASS:2
Title: Performance of database workloads on shared-memory systems with out-of-order processors
Abstract: Database applications such as online transaction processing (OLTP) and decision support systems (DSS) constitute the largest and fastest-growing segment of the market for multiprocessor servers. However, most current system designs have been optimized to perform well on scientific and engineering workloads. Given the radically different behavior of database workloads (especially OLTP), it is important to re-evaluate key system design decisions in the context of this important class of applications.This paper examines the behavior of database workloads on shared-memory multiprocessors with aggressive out-of-order processors, and considers simple optimizations that can provide further performance improvements. Our study is based on detailed simulations of the Oracle commercial database engine. The results show that the combination of out-of-order execution and multiple instruction issue is indeed effective in improving performance of database workloads, providing gains of 1.5 and 2.6 times over an in-order single-issue processor for OLTP and DSS, respectively. In addition, speculative techniques enable optimized implementations of memory consistency models that significantly improve the performance of stricter consistency models, bringing the performance to within 10--15% of the performance of more relaxed models.The second part of our study focuses on the more challenging OLTP workload. We show that an instruction stream buffer is effective in reducing the remaining instruction stalls in OLTP, providing a 17% reduction in execution time (approaching a perfect instruction cache to within 15%). Furthermore, our characterization shows that a large fraction of the data communication misses in OLTP exhibit migratory behavior; our preliminary results show that software prefetch and writeback/flush hints can be used for this data to further reduce execution time by 12%.
ID:714
CLASS:2
Title: An analysis of geometric modeling in database systems
Abstract: The data-modeling and computational requirements for integrated computer aided manufacturing (CAM) databases are analyzed, and the most common representation schemes for modeling solid geometric objects in a computer are described. The primitive instancing model, the boundary representation, and the constructive solid geometry model are presented from the viewpoint of database representation. Depending on the representation scheme, one can apply geometric transformations to the stored geometric objects. The standard transformations, scaling, translation, and rotation, are outlined with respect to the data structure aspects. Some of the more recent developments in the area of engineering databases with regard to supporting these representation schemes are then explored, and a classification scheme for technical database management systems is presented that distinguishes the systems according to their level of object orientation: structural or behavioral object orientation. First, several systems that are extensions to the relational model are surveyed, then the functional data model DAPLEX, the nonnormalized relational model NF2, and the database system R2D2 that provides abstract data types in the NF2 model are described.
ID:715
CLASS:2
Title: Extending a database system with procedures
Abstract: This paper suggests that more powerful database systems (DBMS) can be built by supporting database procedures as full-fledged database objects. In particular, allowing fields of a database to be a collection of queries in the query language of the system is shown to allow the natural expression of complex data relationships. Moreover, many of the features present in object-oriented systems and semantic data models can be supported by this facility.In order to implement this construct, extensions to a typical relational query language must be made, and considerable work on the execution engine of the underlying DBMS must be accomplished. This paper reports on the extensions for one particular query language and data manager and then gives performance figures for a prototype implementation. Even though the performance of the prototype is competitive with that of a conventional system, suggestions for improvement are presented.
ID:716
CLASS:2
Title: Lore: a database management system for semistructured data
Abstract: Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.
ID:717
CLASS:2
Title: ProbView: a flexible probabilistic database system
Abstract: Probability theory is mathematically the best understood paradigm for modeling and manipulating uncertain information.  Probabilities of complex events can be computed from those of basic events on which they depend, using any of a number of strategies.  Which strategy is appropriate depends very much on the known interdependencies among the events involved.  Previous work on probabilistic databases has assumed a fixed and restrictivecombination strategy (e.g., assuming all events are pairwise independent).  In this article, we characterize, using postulates, whole classes of strategies for conjunction, disjunction, and negation, meaningful from the viewpoint of probability theory.  (1) We propose a probabilistic relational data model and a  genericprobabilistic relational algebra that neatly captures various strategiessatisfying the postulates, within a single unified framework. (2) We show that as long as the chosen strategies can be computed in polynomial time, queries in the positive fragment of the probabilistic relational algebra have essentially the same data complexity as classical relational algebra.  (3) We establish various containments and equivalences between algebraic expressions, similar in spirit to those in classical algebra.  (4) We develop algorithms for maintaining materialized probabilistic views.  (5) Based on these ideas, we have developed a prototype probabilistic database system called ProbView on top of Dbase V.0.  We validate our complexity results  with experiments and show that rewriting certain types of queries to other equivalent forms often yields substantial savings.
ID:718
CLASS:2
Title: Secure transaction processing in firm real-time database systems
Abstract: Many real-time database applications arise in safety-critical installations and military systems where enforcing security is crucial to the success of the enterprise. A secure real-time database system has to simultaneously satisfy who requirements guarantee data security and minimize the number of missed transaction deadlines. We investigate here the performance implications, in terms of missed deadlines, of guaranteeing security in a real-time database system. In particular, we focus on the concurrency control aspects of this issue.Our main contributions are the following: First, we identify which among the previously proposed real-time concurrency control protocols are capable of providing protection against both direct and indirect (covert channels) means of unauthorized access to data. Second, using a detailed simulation model of a firm-deadline real-time database system, we profile the real-time performance of a representative set of these secure concurrency control protocols. Our experiments show that a prioritized optimistic concurrency control protocol. OPT-WAIT, provides the best overall performance. Third, we propose and evaluate a novel dual approach to secure transaction concurrency control that allows the real-time database system to simultaneously use different concurrency control mechanisms for guaranteeing security and for improving real-time performance. By appropriately choosing these different mechanisms, we have been able to design hybrid concurrency control algorithms that provide even better performance than OPT-WAIT.
ID:719
CLASS:2
Title: A probabilistic relational algebra for the integration of information retrieval and database systems
Abstract: We present a probabilistic relational algebra (PRA) which is a generalization of standard relational algebra. In PRA, tuples are assigned probabilistic weights giving the probability that a tuple belongs to a relation. Based on intensional semantics, the tuple weights of the result of a PRA expression always conform to the underlying probabilistic model. We also show for which expressions extensional semantics yields the same results. Furthermore, we discuss complexity issues and indicate possibilities for optimization. With regard to databases, the approach allows for representing imprecise attribute values, whereas for information retrieval, probabilistic document indexing and probabilistic search term weighting can be modeled. We introduce the concept of vague predicates which  yield probabilistic weights instead of Boolean values, thus allowing for queries with vague selection conditions. With these features, PRA implements uncertainty and vagueness in combination with the relational model.
ID:720
CLASS:2
Title: The model-assisted global query system for multiple databases in distributed enterprises
Abstract: Today's enterprises typically employ multiple information systems, which are independently developed, locally administered, and different in logical or physical designs. Therefore, a fundamental challenge in enterprise information management is the sharing of information for enterprise users across organizational boundaries; this requires a global query system capable of providing on-line intelligent assistance to users. Conventional technologies, such as schema-based query languages and hard-coded schema integration, are not sufficient to solve this problem. This article develops a new approach, a &ldquo;model-assisted global query system,&rdquo; that utilizes an on-line repository of enterprise metadata&mdash;the Metadatabase&mdash;to facilitate global query formulation and processing with  certain desirable properties such as adaptiveness and open-systems architecture. A definitional model characterizing the various classes and roles of the required metadata as knowledge for the system is presented. The significance of possessing this knowledge (via a Metadatabase) toward improving the global query capabilities available previously is analyzed. On this basis, a direct method using model traversal and a query language using global model constructs are developed along with other new methods required for this approach. It is then tested through a prototype system in a computer-integrated manufacturing (CIM) setting.
ID:721
CLASS:2
Title: Semantics for update rule programs and implementation in a relational database management system
Abstract: In this paper, we present our research on defining a correct semantics for a class of update rule (UR) programs, and discuss implemanting these programs in a DBMS environment. Update rules execute by updating relations in a database which may cause the further execution of rules. A correct semantics must guarantee that the execution of the rules will terminate and that it will produce a minimal updated database. The class of UR programs is syntactically identified, based upon a concept that is similar to stratification. We extend that strict definition of stratification and allow a relaxed criterion for partitioning of the rules in the UR program. This relaxation allows a limited degree of nondeterminism in rule execution. We define an execution semantics based upon a monotonic fixpoint operator TUR, resulting in a set of fixpoints for UR. The monotionicity of the operator is maintained nby explicitly representing the effect of asserting and retracting tuples in the database.  A declarative semantics for the update rule program is obtained by associating a normal logic program UR to represent the UR program. We use the stable model semantics which characterize a normal logic program by a set of minimal models which are called stable models. We show the equivalence between the set of fixpoints for UR and the set of stable models for UR. We briefly discuss implementing the fixpoint semantics of the UR program in a DBMS environment. Relations that can be updated by the rules are updatable relations and they are extended with two flags. An update rule is represented by a database query, which queries the updatable relations as well as database relaions, i.e., those relations which are not update by rules. We describe an algorithm to process the queries and compute a fixpoint in the DBMS environment and obtain a final database.
ID:722
CLASS:2
Title: The active database management system manifesto: a rulebase of ADBMS features
Abstract: Active database systems have been a hot research topic for quite some years now. However, while &ldquo;active functionality&rdquo; has been claimed for many systems, and notions such as &ldquo;active objects&rdquo; or &ldquo;events&rdquo; are used in many research areas (even beyond database technology), it is not yet clear which functionality a database management system must support in order to be legitimately considered as an active system. In this paper, we attempt to clarify the notion of &ldquo;active database management system&rdquo; as well as the functionality it has to support. We thereby distinguish mandatory features that are needed to qualify as an active database system, and desired features which are nice to have. Finally, we perform a classification of applications of active database systems and identify the requirements for an active database management system in order to be applicable in these application areas.
ID:723
CLASS:2
Title: Foundations of multimedia database systems
Abstract: Though numerous multimedia systems exist in the commercial market today, relatively little work has been done on developing the mathematical foundation of multimedia technology. We attempt to take some initial steps towards the development of a theoretical basis for a multimedia information system. To do so, we develop the motion of a structured multimedia database system. We begin by defining a mathematical model of a media-instance. A media-instance may be thought of as &ldquo;glue&rdquo; residing on top of a specific physical media-representation (such as video, audio, documents, etc). Using this &ldquo;glue&rdquo;, it is possible to define a general purpose logical query language to query multimedia data. This glue consists of a set of &ldquo;states&rdquo; (e.g., video frames, audio tracks, etc.) and &ldquo;features&rdquo;, together with relationships between states and/or features. A structured multimedia database system imposes a certain mathematical structures on the set of features/states. Using this notion of a structure, we are able to define indexing structures for processing queries, methods to relax queries when answers do not exist to those queries, as well as sound, complete and terminating procedures to answer such queries (and their relaxations, when appropriate). We show how a media-presentation can be generated by processing a sequence of queries, and furthermore we show that when these queries are extended to include constraints, then these queries can not only generate presentations, but also generate temporal synchronization properties and spatial layout properties for such presentations. We describe the architecture of a prototype multimedia database system based on the principles described in this paper.
ID:724
CLASS:2
Title: A shared, segmented memory system for an object-oriented database
Abstract: This paper describes the basic data model of an object-oriented database and the basic architecture of the system implementing it. In particular, a secondary storage segmentation scheme and a transaction-processing scheme are discussed. The segmentation scheme allows for arbitrary clustering of objects, including duplicates. The transaction scheme allows for many different sharing protocols ranging from those that enforce serializability to those that are nonserializable and require communication with the server only on demand. The interaction of these two features is described such that segment-level transfer and object-level locking is achieved.
ID:725
CLASS:2
Title: Temporal conditions and integrity constraints in active database systems
Abstract: In this paper, we present a unified formalism, based on Past Temporal Logic, for specifying conditions and events in the rules for active database system. This language permits specification of many time varying properties of database systems. It also permits specification of temporal aggregates. We present an efficient incremental algorithm for detecting conditions specified in this language. The given algorithm, for a subclass of the logic, was implemented on top of Sybase.
ID:726
CLASS:2
Title: Applying update streams in a soft real-time database system
Abstract: Many papers have examined how to efficiently export a materialized view but to our knowledge none have studied how to efficiently import one. To import a view, i.e., to install a stream of updates, a real-time database system must process new updates in a timely fashion to keep the database "fresh," but at the same time must process transactions and ensure they meet their time constraints. In this paper, we discuss the various properties of updates and views (including staleness) that affect this tradeoff. We also examine, through simulation, four algorithms for scheduling transactions and installing updates in a soft real-time database.
ID:727
CLASS:2
Title: Recovery protocols for shared memory database systems
Abstract: Significant performance advantages can be gained by implementing a database system on a cache-coherent shared memory multiprocessor. However, problems arise when failures occur. A single node (where a node refers to a processor/memory pair) crash may require a reboot of the entire shared memory system. Fortunately, shared memory multiprocessors that isolate individual node failures are currently being developed. Even with these, because of the side effects of the cache coherency protocol, a transaction executing strictly on a single node may become dependent on the validity of the memory of many nodes thereby inducing unnecessary transaction aborts. This happens when database objects, such as records, and database support structures, such as index structures and shared lock tables, are stored in shared memory.In this paper, we propose crash recovery protocols for shared memory database systems which avoid the unnecessary transaction aborts induced by the effects of using shared physical memory. Our recovery protocols guarantee that if one or more nodes crash, all the effects of active transactions running on the crashed nodes will be undone, and no effects of transactions running on nodes which did not crash will be undone. In order to show the practicality of our protocols, we discuss how existing features of cache-coherent multiprocessors can be utilized to implement these recovery protocols. Specifically, we demonstrate that (1) for many types of database objects and support structures, volatile (in-memory) logging is sufficient to avoid unnecessary transaction aborts, and (2) a very low overhead implementation of this strategy can be achieved with existing multiprocessor features.
ID:728
CLASS:2
Title: METU interoperable database system
Abstract: METU INteroperable Database System (MIND) is a multidatabase system that aims at achieving interoperability among heterogeneous, federated DBMSs. MIND architecture if based on OMG distributed object management model. It is implemented on top of a CORBA compliant ORB, namely, ObjectBroker. MIND provides users a single ODMG-93 compliant common data model, and a single global query language based on SQL. This makes it possible to incorporate both relational and object oriented databases into the system. Currently Oracle 7, Sybase and METU OODBMS (MOOD) have been incorporated into MIND. The main components of MIND are a global query processor, a global transaction manager, a schema integrator, interfaces to supported database systems and a user graphical interface.In MIND all local databases are encapsulated in a generic database object with a well defined single interface. This approach hides the differences between local databases from the rest of the system. The integration of export schemas is currently performed manually by using an object definition language (ODL) which is based on OMG's interface definition language. The DBA builds the integrated schema as a view over export schemas. the functionalities of ODL allow selection and restructuring of schema elements from existing local schemas.MIND global query optimizer aims at maximizing the parallel execution of the intersite joins of the global subqueries. Through MIND global transaction manager, the serializable execution of the global transactions are provided.
ID:729
CLASS:3
Title: Management perspectives on usability in a public authority: a case study
Abstract: In trying to understand the problem of poor usability in computer-supported work, this article looks at management and their perspective on usability in a public authority. What are their underlying basic values, assumptions and attitudes? Why do managers interpret usability as they do, and what are the consequences for the organization and for usability? The empirical basis is an interpretive case study where 19 semi-structured interviews were conducted. Results indicate that usability is interpreted differently, depending on the formal roles of informants. Furthermore, a majority of the informants express personal, but limited, responsibility for usability. Moreover, we found that basic values are based on an instrumental view of work where efficiency and economy are important constituents. We identified that even though users participate in IT development, they have no formal responsibility or authority. They have become IT workers in that they perform highly technical tasks such as integral testing.
ID:730
CLASS:3
Title: An ethnographic study of distributed problem solving in spreadsheet development
Abstract: In contrast to the common view of spreadsheets as &ldquo;single-user&rdquo; programs, we have found that spreadsheets offer surprisingly strong support for cooperative development of a wide variety of applications. Ethnographic interviews with spreadsheet users showed that nearly all of the spreadsheets used in the work environments studied were the result of collaborative work by people with different levels of programming and domain expertise. Cooperation among spreadsheet users was spontaneous and casual; users activated existing informal social networks to initiate collaboration.
ID:731
CLASS:3
Title: Predictive human performance modeling made easy
Abstract: Although engineering models of user behavior have enjoyed a rich history in HCI, they have yet to have a widespread impact due to the complexities of the modeling process. In this paper we describe a development system in which designers generate predictive cognitive models of user behavior simply by demonstrating tasks on HTML mock-ups of new interfaces. Keystroke-Level Models are produced automatically using new rules for placing mental operators, then implemented in the ACT-R cognitive architecture. They interact with the mock-up through integrated perceptual and motor modules, generating behavior that is automatically quantified and easily examined. Using a query-entry user interface as an example [19], we demonstrate that this new system enables more rapid development of predictive models, with more accurate results, than previously published models of these tasks.
ID:732
CLASS:3
Title: Isolating the effects of visual impairment: exploring the effect of AMD on the utility of multimodal feedback
Abstract: This study examines the effects of multimodal feedback on the performance of older adults with an ocular disease, Age-Related Macular Degeneration (AMD), when completing a simple computer-based task. Visually healthy older users (n = 6) and older users with AMD (n = 6) performed a series of drag-and-drop tasks that incorporated a variety of different feedback modalities. The user groups were equivalent with respect to traditional visual function metrics and measured subject cofactors, aside from the presence or absence of AMD. Results indicate that users with AMD exhibited decreased performance, with respect to required feedback exposure time. Some non-visual and multimodal feedback forms show potential as solutions to enhance performance, for those with AMD as well as for visually healthy older adults.
ID:733
CLASS:3
Title: Tactons: structured tactile messages for non-visual information display
Abstract: Tactile displays are now becoming available in a form that can be easily used in a user interface. This paper describes a new form of tactile output. Tactons, or tactile icons, are structured, abstract messages that can be used to communicate messages non-visually. A range of different parameters can be used for Tacton construction including: frequency, amplitude and duration of a tactile pulse, plus other parameters such as rhythm and location. Tactons have the potential to improve interaction in a range of different areas, particularly where the visual display is overloaded, limited in size or not available, such as interfaces for blind people or in mobile and wearable devices. This paper describes Tactons, the parameters used to construct them and some possible ways to design them. Examples of where Tactons might prove useful in user interfaces are given.
ID:734
CLASS:3
Title: Report from the ICSE workshop: bridging the gaps between software engineering and human-computer interaction
Abstract: The First International Workshop on the Relationships between Software Engineering and Human-Computer Interaction was held on May 3--4, 2003 as part of the 2003 International Conference on Software Engineering, in Portland, OR, U.S.A. This workshop was motivated by a perception among researchers, practitioners, and educators that the fields of Human-Computer Interaction and Software Engineering were largely ignoring each other and that they needed to work together more closely and to understand each other better. This report describes the motivation, goals, organization, and outputs of the workshop and the significant activity that it has subsequently fostered.
ID:735
CLASS:3
Title: Service learning models connecting computer science to the community
Abstract: Service learning is an educational experience that enables students to apply material learned in the classroom by volunteering in a real-world situation. This paper provides a brief review of service learning and describes two models that the computer science department at Saint Anselm College implemented successfully.
ID:736
CLASS:3
Title: Promoting universal usability with multi-layer interface design
Abstract: Increased interest in universal usability is causing some researchers to study advanced strategies for satisfying first-time as well as intermittent and expert users. This paper promotes the idea of multi-layer interface designs that enable first-time and novice users to begin with a limited set of features at layer 1. They can remain at layer 1, then move up to higher layers when needed or when they have time to learn further features. The arguments for and against multi-layer interfaces are presented with two example systems: a word processor with 8 layers and an interactive map with 3 layers. New research methods and directions are proposed.
ID:737
CLASS:3
Title: What's going on?: discovering what children understand about handwriting recognition interfaces
Abstract: When people use interactive technology, they construct a 'mental model' of the processes that are going on. This model assists the user in error repair and in task completion. The mental models that children have of computer systems are known to be brittle and incomplete. This paper describes how three different methods - structured interview, questionnaire, and talk back, were used with 7 and 8-year-old children to identify children's mental models of a handwriting-recognition based interface. The time taken by both the child and the researcher, the insights reported by the children, and the ease of use of each of the three methods is reported. The three methods are then compared, both in terms of cost/benefit and with relation to the influence of the researcher in the process. The paper concludes that the interview and questionnaire were both effective in this study, and that questionnaires can be surprisingly informative with children of this age.
ID:738
CLASS:3
Title: The many facets of HCI
Abstract: In the last ten years HCI, the study and practice of usability, has emerged as a multidisciplinary, multifaceted field. HCI is an essential knowledge area that pervades the computing field and should be included in every computing professional's education. Computing professionals need to create software, and other technologies, so that users will want to use them and will be able to effectively use them. User advocacy distinguishes the Information Technology discipline from the other computing disciplines. Graduates need to understand the many facets of HCI. These include not only understanding the design of the interface, but also the broader issues of the user experience, process and business concerns, challenges of distributed computing, the emergence of supportive technologies, and the impact of ubiquitous computing. This paper will introduce the key HCI concepts, and discuss the challenges, issues and future developments of the field that will drive Information Technology curriculum development.
ID:739
CLASS:3
Title: An empirical method based on protocol analysis to analyze technical review meetings
Abstract: This paper presents an empirical approach based on protocol analysis to study conversations in a technical review meeting. Empirical methods based on observation and analysis of professional work, which are called protocol analyses, are well known in Cognitive Science. This project is an effort to understand and rationalize all the steps used to define a coding scheme used in protocol analysis. Two independent but complementary teams composed of psychologists and software engineers have realized the approach presented. This empirical approach is illustrated with examples taken from a protocol analysis of a Technical Review Meeting held in an industrial environment.
ID:740
CLASS:3
Title: Community design of community simulations
Abstract: We report on a participatory design workshop in which residents of a community collaborated in learning about and designing projects for a visual simulation environment. Nine participants (five middle school teachers, four senior citizens) first conducted a participatory evaluation of a tutorial developed for the Stagecast Creator simulation tool. They then worked in pairs to brainstorm ideas for Creator simulation projects that would help raise and promote discussion of issues relevant to their community. After sharing these ideas, each pair chose 2--3 simulation ideas to refine as a specification for subsequent implementation. We discuss the participants' learning and design activities, as well as their contributions to our long term goal of supporting cross-generational collaboration and learning through community simulation projects.
ID:741
CLASS:3
Title: The case against user interface consistency
Abstract: Designers striving for user interface consistency can resemble Supreme Court justices trying to define pornography: each of us feels we know it when we see it, but people often disagree and a precise definition remains elusive. A close examination suggests that consistency is an unreliable guide and that designers would often do better to focus on users' work environments.
ID:742
CLASS:3
Title: Report on the workshop on analytical models
Abstract: Analytical models are extremely useful and important tools for design engineering in many fields. They provide engineers with performance predictions for a specified design, and they also can assist in design improvements by identifying the sources of performance deficiencies.Currently analytical models are not commonly used for design engineering in HCI. However, university research has produced some promising results, and has led to exploratory development projects in several major corporations.
ID:743
CLASS:3
Title: "Are you there Margaret? It's me, Margaret": speech recognition as a mirror
Abstract: This paper briefly examines the design space of speech recognition as an input device. I discuss the particular advantages of speech recognition over other button-pushing modes of input. The goal of this paper is to examine speech devices that operate as a reflective tool for Human-Computer Interaction. I detail the construction and implementation of a simple speech recognition device, the Speech Patternizer, which specifies the frequency of indicated words in recorded speech. Through the prototyping of this device and a longitudinal study of it, I inspect the more nuanced models of Human-Computer Interaction that make speech patterns more transparent and interpretable. The value of the ability to examine and assess speech interaction between computers and humans is also discussed.
ID:744
CLASS:3
Title: Programmable user modelling analysis in theory and in practice
Abstract: This tutorial provides a general introduction to cognitive modelling as it relates to work in Human-Computer Interaction and, in particular, to Programmable User Modelling Analysis (PUMA). PUMA is an approach to predictive usability evaluation that focuses on describing the user's knowledge and problem-solving behaviour. The tutorial covers underlying cognitive theory as well as the method of analysis. Examples of various sizes are used to demonstrate how PUMA can be applied within design practice.
ID:745
CLASS:3
Title: Human-computer interaction: introduction and overview
Abstract: The objective of this special introductory seminar is to provide newcomers to Human-Computer Interaction (HCI) with an introduction and overview of the field. The material will begin with a brief history of the field, followed by presentation and discussion of how good application development methods pull on the interdisciplinary technologies of HCI. The topics will include the psychology of human-computer interaction, usability engineering, psychologically-based design methods and tools, user interface media and tools, and introduction to user interface architecture.
ID:746
CLASS:3
Title: Overcoming the Lack of Screen Space on Mobile Computers
Abstract: One difficulty for interface design on mobile computers is lack of screen space caused by their small size. This paper describes a small pilot study and two formal experiments that investigate the usability of sonically-enhanced buttons of different sizes. The underlying hypothesis being that presenting information about the buttons in sound would increase their usability and allow their size to be reduced. An experimental interface was created that ran on a 3Com Palm III mobile computer and used a simple calculator-style interface to enter data. The buttons of the calculator were changed in size between 4&times;4, 8&times;8 and 16&times;16 pixels and used a range of different types of sound from basic to complex. Results showed that sounds significantly improved usability for both standard and small button sizes &ndash; more data could be entered with sonically-enhanced buttons and subjective workload reduced. More sophisticated sounds that presented more information about the state of the buttons were shown to be more effective than the standard Palm III sounds. The results showed that if sound was added to buttons then they could be reduced in size from 16&times;16 to 8&times;8 pixels without much loss in quantitative performance. This reduction in size, however, caused a significant increase in subjective workload. Results also showed that when a mobile device was used in a more realistic situation (whilst walking outside) usability was significantly reduced (with increased workload and less data entered) than when used in a usability laboratory. These studies show that sound can be beneficial for usability and that care must be taken to do testing in realistic environments to get a good measure of mobile device usability.
ID:747
CLASS:3
Title: Designing for accountability
Abstract: Accountability is an important issue for design, in more than one sense. In software engineering literature, accountability is mainly seen as a goal for quality assurance of design processes. In ethnomethodological studies, accountability is a central concept for understanding how people organize their everyday actions and interactions. Where the different research approaches meet, in Human Computer Interaction (HCI) and Computer Supported Cooperative Work (CSCW) literature, new and hybrid understandings of accountability arise. In this paper, I explore and compare uses of the concept of accountability in a selection of texts. Finally, using a specific case as an example, I discuss what focusing on ethnomethodological understandings of accountability might imply for design of information technologies.
ID:748
CLASS:3
Title: WebThumb: interaction techniques for small-screen browsers
Abstract: The proliferation of wireless handheld devices is placing the World Wide Web in the palms of users, but this convenience comes at a high interactive cost. The Web that came of age on the desktop is ill-suited for use on the small displays of handhelds. Today, handheld browsing often feels like browsing on a PC with a shrunken desktop. Overreliance on scrolling is a big problem in current handheld browsing. Users confined to viewing a small portion of each page often lack a sense of the overall context --- they may feel lost in a large page and be forced to remember the locations of items as those items scroll out of view. In this paper, we present a synthesis of interaction techniques to address these problems. We implemented these techniques in a prototype, WebThumb, that can browse the live Web.
ID:749
CLASS:4
Title: Support vector machine active learning for image retrieval
Abstract: Relevance feedback is often a critical component when designing image databases. With these databases it is difficult to specify queries directly and explicitly. Relevance feedback interactively determinines a user's desired output or query concept by asking the user whether certain proposed images are relevant or not. For a relevance feedback algorithm to be effective, it must grasp a user's query concept accurately and quickly, while also only asking the user to label a small number of images. We propose the use of a support vector machine active learning algorithm for conducting effective relevance feedback for image retrieval. The algorithm selects the most informative images to query a user and quickly learns a boundary that separates the images that satisfy the user's query concept from the rest of the dataset. Experimental results show that our algorithm achieves significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback.
ID:750
CLASS:4
Title: Context-sensitive learning methods for text categorization
Abstract: Two recently implemented machine-learning algorithms, RIPPERand sleeping-experts for phrases, are evaluated on a number of large text categorization problems. These algorithms both construct classifiers that allow the &ldquo;context&rdquo; of a word w to affect how (or even whether) the presence or absence of w will contribute to a classification. However, RIPPER and sleeping-experts differ radically in many other respects: differences include different notions as to what constitutes a context, different ways of combining contexts to construct a classifier, different methods to search for a combination of contexts, and different criteria as to what contexts should be included in such a combination. In spite of these differences,  both RIPPER and sleeping-experts perform extremely well across a wide variety of categorization problems, generally outperforming previously applied learning methods. We view this result as a confirmation of the usefulness of classifiers that represent contextual information.
ID:751
CLASS:4
Title: Training linear SVMs in linear time
Abstract: Linear Support Vector Machines (SVMs) have become one of the most prominent machine learning techniques for high-dimensional sparse data commonly encountered in applications like text classification, word-sense disambiguation, and drug design. These applications involve a large number of examples n as well as a large number of features N, while each example has only s &lt;&lt; N non-zero features. This paper presents a Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n))for ordinal regression problems. The algorithm is based on an alternative, but equivalent formulation of the SVM optimization problem. Empirically, the Cutting-Plane Algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets.
ID:752
CLASS:4
Title: Improving cooperative GP ensemble with clustering and pruning for pattern classification
Abstract: A boosting algorithm based on cellular genetic programming to build an ensemble of predictors is proposed. The method evolves a population of trees for a fixed number of rounds and, after each round, it chooses the predictors to include into the ensemble by applying a clustering algorithm to the population of classifiers. The method proposed runs on a distributed hybrid multi-island environment that combines the island and cellular models of parallel genetic programming. The large amount of memory required to store the ensemble makes the method heavy to deploy. The paper shows that by applying suitable pruning strategies it is possible to select a subset of the classifiers without increasing misclassification errors; indeed, up to 20 of pruning, ensemble accuracy increases. Experiments on several data sets show that combining clustering and pruning enhances classification accuracy of the ensemble approach.
ID:753
CLASS:4
Title: Feature value acquisition in testing: a sequential batch test algorithm
Abstract: In medical diagnosis, doctors often have to order sets of medical tests in sequence in order to make an accurate diagnosis of patient diseases. While doing so they have to make a trade-off between the cost of the tests and possible misdiagnosis. In this paper, we use cost-sensitive learning to model this process. We assume that test examples (new patients) may contain missing values, and their actual values can be acquired at cost (similar to doing medical tests) in order to reduce misclassification errors (misdiagnosis). We propose a novel Sequential Batch Test algorithm that can acquire sets of attribute values in sequence, similar to sets of medical tests ordered by doctors in sequence. The goal of our algorithm is to minimize the total cost (i.e., the trade-off) of acquiring attribute values and misclassifications. We demonstrate the effectiveness of our algorithm, and show that it outperforms previous methods significantly. Our algorithm can be readily applied in real-world diagnosis tasks. A case study on the heart disease is given in the paper.
ID:754
CLASS:4
Title: Simpler knowledge-based support vector machines
Abstract: If appropriately used, prior knowledge can significantly improve the predictive accuracy of learning algorithms or reduce the amount of training data needed. In this paper we introduce a simple method to incorporate prior knowledge in support vector machines by modifying the hypothesis space rather than the optimization problem. The optimization problem is amenable to solution by the constrained concave convex procedure, which finds a local optimum. The paper discusses different kinds of prior knowledge and demonstrates the applicability of the approach in some characteristic experiments.
ID:755
CLASS:4
Title: A regularization framework for multiple-instance learning
Abstract: This paper focuses on kernel methods for multi-instance learning. Existing methods require the prediction of the bag to be identical to the maximum of those of its individual instances. However, this is too restrictive as only the sign is important in classification. In this paper, we provide a more complete regularization framework for MI learning by allowing the use of different loss functions between the outputs of a bag and its associated instances. This is especially important as we generalize this for multi-instance regression. Moreover, both bag and instance information can now be directly used in the optimization. Instead of using heuristics to solve the resultant non-linear optimization problem, we use the constrained concave-convex procedure which has well-studied convergence properties. Experiments on both classification and regression data sets show that the proposed method leads to improved performance.
ID:756
CLASS:4
Title: Ranking on graph data
Abstract: In ranking, one is given examples of order relationships among objects, and the goal is to learn from these examples a real-valued ranking function that induces a ranking or ordering over the object space. We consider the problem of learning such a ranking function when the data is represented as a graph, in which vertices correspond to objects and edges encode similarities between objects. Building on recent developments in regularization theory for graphs and corresponding Laplacian-based methods for classification, we develop an algorithmic framework for learning ranking functions on graph data. We provide generalization guarantees for our algorithms via recent results based on the notion of algorithmic stability, and give experimental evidence of the potential benefits of our framework.
ID:757
CLASS:4
Title: Who should fix this bug?
Abstract: Open source development projects typically support an open bug repository to which both developers and users can report bugs. The reports that appear in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open source developments are burdened by the rate at which new bug reports appear in the bug repository. In this paper, we present a semi-automated approach intended to ease one part of this process, the assignment of reports to a developer. Our approach applies a machine learning algorithm to the open bug repository to learn the kinds of reports each developer resolves. When a new report arrives, the classifier produced by the machine learning technique suggests a small number of developers suitable to resolve the report. With this approach, we have reached precision levels of 57% and 64% on the Eclipse and Firefox development projects respectively. We have also applied our approach to the gcc open source development with less positive results. We describe the conditions under which the approach is applicable and also report on the lessons we learned about applying machine learning to repositories used in open source development.
ID:758
CLASS:4
Title: An empirical study of the domain dependence of supervised word sense disambiguation systems
Abstract: This paper describes a set of experiments carried out to explore the domain dependence of alternative supervised Word Sense Disambiguation algorithms. The aim of the work is threefold: studying the performance of these algorithms when tested on a different corpus from that they were trained on; exploring their ability to tune to new domains, and demonstrating empirically that the Lazy-Boosting algorithm outperforms state-of-the-art supervised WSD algorithms in both previous situations.
ID:759
CLASS:4
Title: Wrapper-based computation and evaluation of sampling methods for imbalanced datasets
Abstract: Learning from imbalanced datasets presents an interesting problem both from modeling and economy standpoints. When the imbalance is large, classification accuracy on the smaller class(es) tends to be lower. In particular, when a class is of great interest but occurs relatively rarely such as cases of fraud, instances of disease, and regions of interest in largescale simulations, it is important to accurately identify it. It then becomes more costly to misclassify the interesting class. In this paper, we implement a wrapper approach that computes the amount of under-sampling and synthetic generation of the minority class examples (SMOTE) to improve minority class accuracy. The f-value serves as the evaluation function. Experimental results show the wrapper approach is effective in optimization of the composite f-value, and reduces the average cost per test example for the datasets considered. We report both average cost per test example and the cost curves in the paper. The true positive rate of the minority class increases significantly without causing a significant change in the f-value. We also obtain the lowest cost per test example, compared to any result we are aware of for the KDD Cup-99 intrusion detection data set.
ID:760
CLASS:4
Title: Variable selection and ranking for analyzing automobile traffic accident data
Abstract: Variable ranking and feature selection are important concepts in data mining and machine learning. This paper introduces a new variable ranking technique named Sum Max Gain Ratio (SMGR). The new technique is evaluated within the domain of traffic accident data and against a more generalized dataset. In certain cases, SMGR is empirically shown to provide similar results to established approaches with significantly better runtime performance.
ID:761
CLASS:4
Title: Fast Binary Feature Selection with Conditional Mutual Information
Abstract: We propose in this paper a very fast feature selection technique based on conditional mutual information. By picking features which maximize their mutual information with the class to predict conditional to any feature already picked, it ensures the selection of features which are both individually informative and two-by-two weakly dependant. We show that this feature selection method outperforms other classical algorithms, and that a naive Bayesian classifier built with features selected that way achieves error rates similar to those of state-of-the-art methods such as boosting or SVMs. The implementation we propose selects 50 features among 40,000, based on a training set of 500 examples in a tenth of a second on a standard 1Ghz PC.
ID:762
CLASS:4
Title: Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning
Abstract: Policy gradient methods for reinforcement learning avoid some of the undesirable properties of the value function approaches, such as policy degradation (Baxter and Bartlett, 2001). However, the variance of the performance gradient estimates obtained from the simulation is sometimes excessive. In this paper, we consider variance reduction methods that were developed for Monte Carlo estimates of integrals. We study two commonly used policy gradient techniques, the baseline and actor-critic methods, from this perspective. Both can be interpreted as additive control variate variance reduction methods. We consider the expected average reward performance measure, and we focus on the GPOMDP algorithm for estimating performance gradients in partially observable Markov decision processes controlled by stochastic reactive policies. We give bounds for the estimation error of the gradient estimates for both baseline and actor-critic algorithms, in terms of the sample size and mixing properties of the controlled system. For the baseline technique, we compute the optimal baseline, and show that the popular approach of using the average reward to define the baseline can be suboptimal. For actor-critic algorithms, we show that using the true value function as the critic can be suboptimal. We also discuss algorithms for estimating the optimal baseline and approximate value function.
ID:763
CLASS:4
Title: Statistical Analysis of Some Multi-Category Large Margin Classification Methods
Abstract: The purpose of this paper is to investigate statistical properties of risk minimization based multi-category classification methods. These methods can be considered as natural extensions of binary large margin classification. We establish conditions that guarantee the consistency of classifiers obtained in the risk minimization framework with respect to the classification error. Examples are provided for four specific forms of the general formulation, which extend a number of known methods. Using these examples, we show that some risk minimization formulations can also be used to obtain conditional probability estimates for the underlying problem. Such conditional probability information can be useful for statistical inferencing tasks beyond classification.
ID:764
CLASS:4
Title: Redundancy based feature selection for microarray data
Abstract: In gene expression microarray data analysis, selecting a small number of discriminative genes from thousands of genes is an important problem for accurate classification of diseases or phenotypes. The problem becomes particularly challenging due to the large number of features (genes) and small sample size. Traditional gene selection methods often select the top-ranked genes according to their individual discriminative power without handling the high degree of redundancy among the genes. Latest research shows that removing redundant genes among selected ones can achieve a better representation of the characteristics of the targeted phenotypes and lead to improved classification accuracy. Hence, we study in this paper the relationship between feature relevance and redundancy and propose an efficient method that can effectively remove redundant genes. The efficiency and effectiveness of our method in comparison with representative methods has been demonstrated through an empirical study using public microarray data sets.
ID:765
CLASS:4
Title: Diagnosing extrapolation: tree-based density estimation
Abstract: There has historically been very little concern with extrapolation in Machine Learning, yet extrapolation can be critical to diagnose. Predictor functions are almost always learned on a set of highly correlated data comprising a very small segment of predictor space. Moreover, flexible predictors, by their very nature, are not controlled at points of extrapolation. This becomes a problem for diagnostic tools that require evaluation on a product distribution. It is also an issue when we are trying to optimize a response over some variable in the input space. Finally, it can be a problem in non-static systems in which the underlying predictor distribution gradually drifts with time or when typographical errors misrecord the values of some predictors.We present a diagnosis for extrapolation as a statistical test for a point originating from the data distribution as opposed to a null hypothesis uniform distribution. This allows us to employ general classification methods for estimating such a test statistic. Further, we observe that CART can be modified to accept an exact distribution as an argument, providing a better classification tool which becomes our extrapolation-detection procedure. We explore some of the advantages of this approach and present examples of its practical application.
ID:766
CLASS:4
Title: C4.5 competence map: a phase transition-inspired approach
Abstract: How to determine a priori whether a learning algorithm is suited to a learning problem instance is a major scientific and technological challenge. A first step toward this goal, inspired by the Phase Transition (PT) paradigm developed in the Constraint Satisfaction domain, is presented in this paper.Based on the PT paradigm, extensive and principled experiments allow for constructing the Competence Map associated to a learning algorithm, describing the regions where this algorithm on average fails or succeeds. The approach is illustrated on the long and widely used C4.5 algorithm. A non trivial failure region in the landscape of k-term DNF languages is observed and some interpretations are offered for the experimental results.
ID:767
CLASS:4
Title: A Monte Carlo analysis of ensemble classification
Abstract: In this paper we extend previous results providing a theoretical analysis of a new Monte Carlo ensemble classifier. The framework allows us to characterize the conditions under which the ensemble approach can be expected to outperform the single hypothesis classifier. Moreover, we provide a closed form expression for the distribution of the true ensemble accuracy, as well as of its mean and variance. We then exploit this result in order to analyze the expected error behavior in a particularly interesting case.
ID:768
CLASS:4
Title: Kernel-based discriminative learning algorithms for labeling sequences, trees, and graphs
Abstract: We introduce a new perceptron-based discriminative learning algorithm for labeling structured data such as sequences, trees, and graphs. Since it is fully kernelized and uses pointwise label prediction, large features, including arbitrary number of hidden variables, can be incorporated with polynomial time complexity. This is in contrast to existing labelers that can handle only features of a small number of hidden variables, such as Maximum Entropy Markov Models and Conditional Random Fields. We also introduce several kernel functions for labeling sequences, trees, and graphs and efficient algorithms for them.
ID:769
CLASS:5
Title: Transition-independent decentralized markov decision processes
Abstract: There has been substantial progress with formal models for sequential decision making by individual agents using the Markov decision process (MDP). However, similar treatment of multi-agent systems is lacking. A recent complexity result, showing that solving decentralized MDPs is NEXP-hard, provides a partial explanation. To overcome this complexity barrier, we identify a general class of transition-independent decentralized MDPs that is widely applicable. The class consists of independent collaborating agents that are tied together through a global reward function that depends upon both of their histories. We present a novel algorithm for solving this class of problems and examine its properties. The result is the first effective technique to solve optimally a class of decentralized MDPs. This lays the foundation for further work in this area on both exact and approximate solutions.
ID:770
CLASS:5
Title: Coalition formation in proportionally fair divisible auctions
Abstract: One method for agents to improve their performance is to form coalitions with other agents. One reason why this might occur is because different agents could have been created by the same owner so an incentive to cooperate naturally exists. Competing agents can also choose to coordinate their actions when there is a mutually beneficial result. The emergence and effects of cooperation depend on the structure of the game being played. In this paper, we study a proportionally fair divisible auction to manage agents bidding for service from network and computational resources. We first show that cooperation is a dominant strategy against any fixed level of competition. We then investigate whether collusion can undermine a noncooperative equilibrium solution, i.e. allow an agent priced out of the noncooperative game to enter the game by teaming with other agents. We are able to show that agents not receiving service after a bid equilibrium is reached cannot obtain service by forming coalitions. However, cooperation does allow the possibility that agents with positive allocations can improve their performance. To know whether or not to cooperate with another agent, one must devise a way of assigning a value to every coalition. In classical cooperative game theory, the value of a team is the total utility of the team under the worst case response of all other agents, as a coalition is viewed as a threat by the remaining agents. We show that this analysis is not appropriate in our case. The formation of a coalition under a proportionally fair divisible auction improves the performance of those outside the coalition. This then creates an incentive structure where team play is encouraged.
ID:771
CLASS:5
Title: Incentive compatible multi unit combinatorial auctions
Abstract: This paper deals with multi-unit combinatorial auctions where there are n types of goods for sale, and for each good there is some fixed number of units. We focus on the case where each bidder desires a relatively small number of units of each good. In particular, this includes the case where each good has exactly k units, and each bidder desires no more than a single unit of each good. We provide incentive compatible mechanisms for combinatorial auctions for the general case where bidders are not limited to single minded valuations. The mechanisms we give have approximation ratios close to the best possible for both on-line and off-line scenarios. This is the first result where non-VCG mechanisms are derived for non-single minded bidders for a natural model of combinatorial auctions.
ID:772
CLASS:5
Title: Intelligent agents for automated one-to-many e-commerce negotiation
Abstract: Negotiation is a process in which two or more parties with different criteria, constraints, and preferences, jointly reach an agreement on the terms of a transaction. Many current automated negotiation systems support one-to-one negotiation. One-to-many negotiation has been mostly automated using various kinds of auction mechanisms, which have a number of limitations such as the lack of the ability to perform two-way communication of offers and counteroffers. Moreover, in auctions, there is no way of exercising different negotiation strategies with different opponents. Even though auction-based online trading is suitable for many applications, there are some in which there is a need for such greater flexibility. There has been a significant body of work towards sophisticated one-to-one automated negotiation. In this paper, we present a framework for one-to-many negotiation by means of conducting a number of concurrent coordinated one-to-one negotiations. In our framework, a number of agents, all working on behalf of one party, negotiate individually with other parties. After each negotiation cycle, these agents report back to a coordinating agent that evaluates how well each agent has done, and issues new instructions accordingly. Each individual agent conducts reasoning by using constraint-based techniques. We outline two levels of strategies that can be exercised on two levels, the individual negotiation level, and the coordination level. We also show that our one-to-many negotiation architecture can be directly used to support many-to-many negotiations. In our prototype Intelligent Trading Agency (ITA), agents autonomously negotiate multi- attribute terms of transactions in an e-commerce environment tested with a personal computer trading scenario.
ID:773
CLASS:5
Title: Self-stabilization as multiagent systems property
Abstract: Self-Stabilization is an important concept for distributed computing and communication networks. It describes a system's ability to recover automatically from unexpected failure. It is also an important issue for multiagent systems, as they are distributed and communicative systems. Therefore, we investigate self-stabilization for multiagent systems and how it can be achieved. We define the functionality of an agent society and the task to be achieved in terms of roles which agents may fulfill. Self-Stabilization can thus be achieved by ensuring role adopting and role fulfillment within an agent society.
ID:774
CLASS:5
Title: A scalable, distributed algorithm for efficient task allocation
Abstract: We present a distributed algorithm for task allocation in multi-agent systems for settings in which agents and tasks are geographically dispersed in two-dimensional space. We describe a method that enables agents to determine individually how to move so that they are, as a group, efficiently assigned to tasks. The method comprises two algorithms and is especially useful in environments with very large numbers of agent and task nodes. One algorithm adapts computational geometry techniques to determine adjacency information for the agent nodes given the geographical positions of agents and tasks. This adjacency information is used to determine the visible nodes that are most relevant to an agent's decision making process and to eliminate those that it should not consider. The second algorithm uses local heuristics based solely on an agent's adjacent nodes to determine its course of action. This method yields improved task allocations compared to previous algorithms proposed for similar environments. We also present a modification to the second algorithm that improves performance in environments in which multiple agents are required to complete a single task.
ID:775
CLASS:5
Title: Automating supply-chain management
Abstract: This paper explores a linguistic approach to coordination modeling as a formal basis for supply-chain management (SCM) in manufacturing. The approach promotes the interchange of standard documents: enterprises need only describe their supply processes using OAG business object documents and UML interaction diagrams. Our methodology and tools analyze the documents and interactions in terms of four linguistic primitives and convert the diagrams into specifications and implementations of software agents. The agents then cooperate in automating the resultant supply chain. We evaluate our methodology in the context of several industrial scenarios. We conclude that supply-chain automation using software-agent technology is feasible.
ID:776
CLASS:5
Title: Extreme programming of multi-agent systems
Abstract: The complexity of communication scenarios between agents make multi-agent systems difficult to build. Most of the existing Agent-Oriented Software Engineering methodologies face this complexity by guiding the developers through a rather waterfall-based process with a series of intermediate modeling artifacts. While these methodologies lead to executable prototypes relatively late and are expensive when requirements change, we explore a rather evolutionary approach with explicit support for change and rapid feedback. In particular, we apply Extreme Programming, a modern agile methodology from object-oriented software technology, for the design and implementation of multi-agent systems. The only modeling artifacts that are being maintained in our approach are a process model with which domain experts and developers design and communicate agent application scenarios, and the executable agent source code including automated test cases. We have successfully applied our approach for the development of a prototypical multi-agent system for clinical information logistics.
ID:777
CLASS:5
Title: Just-in-time information sharing architectures in multiagent systems
Abstract: ACORN (Agent-based Community Oriented Routing Network) is a distributed multi-agent architecture for the search, distribution and management of information across networks. ACORN utilises the concept of 'information as agent' together with an application of Stanley Milgram's Small World Problem (the idea of the Six Degrees of Separation) in order to route individual items of information around a network of people and agents. This paper describes additions made to the ACORN architecture and the implementation. A directory server that facilitates real-time communication between a client and corresponding agent is implemented. This server allows for instant feedback and modification to the agent by the client. The concept of an anonymous service provider is introduced to allow clients to generate anonymous agents that cannot be traced back to the original creator of the agent. This service is vital for maintaining some privacy aspects of the user. ACORN consists of a set of information-sharing locations referred to as Caf&eacute;s. A dynamic caf&eacute; clustering method is developed. Using the proposed clustering method, caf&eacute;s are dynamically created / destroyed to most accurately reflect the collective interests of the given members of said caf&eacute;. The performance evaluation of the proposed structure for the caf&eacute; using a testbed of multiple virtual users shows that the addition of multi-caf&eacute; component to ACORN's architecture improves its information sharing efficiency and leads to significant reduction in unnecessary mingling. Lastly, the concept of a fat and thin agent is introduced. A fat/thin agent architecture allows for minimizing network traffic as agents traverse the network in search of or distribution of knowledge.
ID:778
CLASS:5
Title: Robustness of reputation-based trust: boolean case
Abstract: We consider the problem of user agents selecting processor agents to processor tasks. We assume that processor agents are drawn from two populations: high and low-performing processors with different averages but similar variance in performance. For selecting a processor, a user agent queries other user agents for their high/low rating of different processors. We assume that a known percentage of "liar" users, who give inverse estimates of processors. We develop a trust mechanism that determines the number of users to query given a target guarantee threshold likelihood of choosing high-performance processors in the face of such "noisy" reputation mechanisms. We evaluate the robustness of this reputation-based trusting mechanism over varying environmental parameters like percentage of liars, performance difference and variances for high and low-performing agents, learning rates, etc.
ID:779
CLASS:5
Title: A multiagent interaction paradigm for physiological process control
Abstract: Multiagent systems are powerful and flexible tools for controlling complex phenomena. The complexity of a phenomenon can be tackled in such a way that each agent embeds the controller for a portion of the phenomenon [3]. In this perspective, the interaction among the agents results in a complex controller for the whole phenomenon. The actions the agents undertake to control their portions of the phenomenon may conflict, as a result of the "overlapping" of the controlled portions; hence a mediated interaction is needed. A class of complex phenomena that present several difficulties in their satisfactory modeling and controlling is the class of physiological processes.We illustrate the negotiation paradigm of a general regulating multiagent architecture called anthropic agency.
ID:780
CLASS:5
Title: Congregating and market formation
Abstract: Agents in a multiagent system are not typically entirely self-sufficient; instead, they frequently need to enlist other agents to perform tasks for them or to exchange goods or services with them. This creates a problem: how can an agent efficiently locate other agents to work or trade with? As the number of agents grows, the cost of this computation can become prohibitively large. One solution to this is for the system to self-organize into smaller groups of agents. In this paper, we apply the idea of congregating to a model of an information economy. We illustrate how participants in this economy can self-organize into a set of markets such that agents are able to find suitable partners while retaining low computational costs. We show how congregating can help allocation problems scale to large populations by allowing agents to interact locally.
ID:781
CLASS:5
Title: Designing an auction protocol under asymmetric information on nature's selection
Abstract: Internet auctions are becoming an especially popular part of Electronic Commerce and auction protocols have been studied very widely in the field of multi-agent systems and AI. However, correctly judging the quality of auctioned goods is often difficult for non-experts (amateurs), in particular, on the Internet auctions. In this paper, we formalize such a situation so that Nature selects the quality of the auctioned good. Experts can observe Nature's selection (i.e., the quality of the good) correctly, while amateurs including the auctioneer cannot. In other words, the information on Nature's selection is asymmetric between experts and amateurs. In this situation, it is difficult to attain an efficient allocation, since experts have a clear advantage over amateurs and they would not reveal their valuable information without some reward. Thus, in this paper, we develop a new auction protocol in which truth-telling is a dominant strategy for each expert. This can be done by putting these experts in a situation similar to Prisoner's Dilemma. If they cooperate and tell lies, they can exclude amateurs, but betraying is a dominant strategy. By making experts to elicit their information on the quality of the good, the protocol can achieve a socially desirable, i.e., Pareto efficient allocation if certain assumptions are satisfied.
ID:782
CLASS:5
Title: Multiagent systems specification by UML statecharts aiming at intelligent manufacturing
Abstract: Multiagent systems are a promising new paradigm in computing, which are contributing to various fields. Many theories and technologies have been developed in order to design and specify multiagent systems, however, no standard procedure is used at present. Industrial applications often have a complex structure and need plenty of working resources. They require a standard specification method as well. As the standard method to design and specify software systems, we believe that one of the key words is simplicity for their wide acceptance. In this paper, we propose a method to specify multiagent systems, namely with UML statecharts. We use them for specifying almost all aspects of multiagent systems, because we think that it is an advantage to keep everything in one type of diagram.We apply our method to different domains, namely to robotic soccer and a network application. This approach enables not only standardized design of multiagent systems, but also almost automatic translation of the specification into a running implementation (here: into Prolog). Moreover, the verification or formal analysis is feasible, because of the rigidly formal manner of the system specification. We concentrate on the formal specification of multiagent systems in general and its application to robotic soccer, which is already implemented, and to networking. The application to different domains---with homogeneous or heterogeneous agents---corroborates the generality of the proposed approach.
ID:783
CLASS:5
Title: Our <i>guest</i> agents are welcome to your agent platforms
Abstract: Multiagent applications will appear more and more in open, heterogeneous, evolving and distributed environments, such as the Internet. In order to run in such environments, agents will need to adapt themselves to new platforms and protocols. We propose a model of agents that are able to run, communicate and move between different multiagent platforms. This model is based on a middleware between such agents and platforms. It has been implemented on a kind of agents called Guest which can be used the same way regardless of what kind of servers they run on, notably, through tools such as a graphical management tool or a server launcher. Also, we provide a mechanism based on plug-ins that allows modification of basic agent behaviors like manipulation of messages, control of migration, etc. Lastly, we propose two kinds of dynamic agent hierarchies based on plug-ins or middleware.
ID:784
CLASS:5
Title: Improving the agent-oriented modeling process by roles
Abstract: The agent-oriented modeling process is divided in a typical sequence of activities, i.e., \emph{requirements specification}, \emph{analysis}, and \emph{design}. The \emph{requirements} are specified by descriptions of the system's functionality and by exemplary scenarios of essential interactions. In \emph{analysis} the system's structure is captured and mandatory behavior of agents is prescribed. The \emph{design} model describes system behavior by means of local operations. The problem arises how the transition between these different stages of the modeling process can be performed. In this paper, we introduce a concept of roles in order to support the transition in a systematic way and thereby improving the agent-oriented modeling process.
ID:785
CLASS:5
Title: A stable and efficient buyer coalition formation scheme for e-marketplaces
Abstract: Buyer coalitions are beneficial in e-marketplaces because they allow buyers to take advantage of volume discounts.  However, existing buyer coalition formation schemes do not provide buyers with any means to declare and match their preferences or to calculate the division of the surplus in a stable manner.  Concepts and algorithms for coalition formation have been investigated in game theory and multi-agent systems research, but because of the computational complexity, they cannot deal with thousands of buyers which could join a coalition in practice.  In this paper, we propose a new buyer coalition formation scheme GroupBuyAuction.  At GroupBuyAuction, buyers form a group based on a category of items. A buyer can post an OR-asking for multiple items within a category. An OR-asking is a list of items indicating that the buyer would buy any one of the items in the list with some particular reservation price.  Sellers bid volume discount prices. The group leader agent splits the group into sub groups (coalitions), selects a winning seller for each coalition, and calculates surplus division among buyers.  We prove that this scheme guarantees the stability in surplus division within each coalition in terms of the core in game theory. Simulation results show that, under most conditions, our scheme increases buyers' utility, and allows more buyers to obtain items compared to traditional group buying schemes, such as those used at existing commercial WWW sites.
ID:786
CLASS:5
Title: Agents teaching agents to share meaning
Abstract: The promise of intelligent agents acting on behalf of users' personalized knowledge sharing needs may be hampered by the insistence that these agents begin with a predefined, common ontology instead of personalized, diverse ontologies. Only until recently have researchers diverged from the last decades common &ldquo;ontology paradigm&rdquo; to a paradigm involving agents that can share knowledge using diverse ontologies. This paper describes how we address this agent knowledge sharing problem of how agents deal with diverse ontologies by introducing a methodology and algorithms for multi- agent knowledge sharing and learning. We demonstrate how this approach will enable multi- agent systems to assist groups of people in locating, translating, and sharing knowledge using our Distributed Ontology  Gathering Group Integration Environment (DOGGIE) and describe our proof- of- concept experiments. DOGGIE synthesizes agent communication, machine learning, and reasoning for information sharing in the Web domain.
ID:787
CLASS:5
Title: RAJA: a resource-adaptive Java agent infrastructure
Abstract: This paper presents RAJA, a Resource-Adaptive Java Agent Infrastructure. RAJA is easily accessible to agent developers, since it allows structured program\-ming by using a multi-level architecture to clearly separate domain-specific functionality from resource and adaptation concerns. It is generic, since it is applicable to a wide range of adaptation strategies. These two key features are illustrated by several applications, where the RAJA concept has been successfully applied to solve real world problems. They stem from very different domains (video streaming and spatial reasoning), which demonstrates the wide range of application and the flexibility of the proposed infrastructure.
ID:788
CLASS:5
Title: On the semantics of conditional commitment
Abstract: In this paper, we identify some problems with current formalizations of conditional commitments, i.e. commitments to achieve a goal if some condition becomes true. We present a solution to these problems. We also formalize two types of communicative actions that can be used by an agent to request another agent to achieve a goal or perform an action provided that some condition becomes true. Our account is set within ECASL [9], a framework for modeling communicating agents based on the situation calculus.
ID:789
CLASS:6
Title: Generalization in partially connected layered neural networks
Abstract: We study the learning from examples in a partially connected single layer perceptron and a two-layer network. Partially connected student networks learn from fully connected teacher networks. We study the generalization in the annealed approximation. We consider a single layer perceptron with binary weights. When a student is weakly diluted, there is a first order phase transition from the poor learning to the good learning state similar to that of fully connected perceptron. With a strong dilution, the first order phase transition disappears and the generalization error decreases continuously. We also study learning of a two-layer committee machine with binary weights. Contrary to the perceptron learning, there always exist a first order transition irrespective of dilution. The permutation symmetry is broken at the transition point and the generalization error is reduced to a non-zero minimum value.
ID:790
CLASS:6
Title: Artificial neural networks: an emerging new technique
Abstract: The artificial neural network is at the heart of an emerging technique, which many academicians and practitioners are using very productively due to its high performance in addressing complex problems. Although the ANN has not yet reached its full potential, the technique has demonstrated the capability of enhancing performance in a broad range of problems. This article presents an overview of the artificial neural network, a taxonomy of its learning paradigm, and its application areas.
ID:791
CLASS:6
Title: Modular thinking: evolving modular neural networks for visual guidance of agents
Abstract: This paper investigates whether replacing non-modular artificial neural network brains of visual agents with modular brains improves their ability to solve difficult tasks, specifically, survive in a changing environment. A set of experiments was conducted and confirmed that agents with modular brains are in fact better. Further analysis of the evolved modules characterised their function and determined their mechanism of operation. The results indicate that the greater survival ability is obtained due to functional specialisation of the evolved modules; good agents do well because their modules are more specialised at tasks such as reproduction and consumption. Overall, the more specialised the modules, the fitter the agents.
ID:792
CLASS:6
Title: Some issues and problems in text tagging using neural networks
Abstract: This paper reports the results of several experiments conducted on automatic text tagging using neural networks. Error backpropagation networks were tested to see how effective they would be at correctly predicting the syntactic and semantic classification ("tag") of a word in a sentence, given some or no contextual information. The following contexts were examined: (1) the ending (last three letters) of the word alone, and (2) an encoded representation of the word, preceded by the representations of the three previous words in the sentence. Although each study suffered from some interesting problems with data representation, the results seemed promising and suggest that further investigation is warranted.
ID:793
CLASS:6
Title: Network restoration using recurrent neural networks
Abstract: In this article, a method is proposed for network restoration using a centralized, static restoration after failure, where the restoration initiated at the local node or at the source uses a hybrid strategy. &copy; 1998 John Wiley & Sons, Ltd.
ID:794
CLASS:6
Title: Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks
Abstract: Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.
ID:795
CLASS:6
Title: Incorporating agent based neural network model for adaptive meta-search
Abstract: In the current information age, the web is increasing at a very rapid pace, while the indexes of the current Search Engines are not scaling up at the same pace resulting in the loss of access to a good fraction of documents on the web. An intriguing alternative is a Meta-Search Engine, which provides a unified access to several Search Engines thereby increasing the coverage of the web. Though using Meta-Search Engines, the coverage of the web is increased, maintaining a good precision can be a problem especially if one or more of the Search Engine's returns irrelevant documents for certain user queries. This paper proposes a novel, intelligent, and adaptive approach to improve the precision of the meta-search results. This approach uses an adaptive agent based neural network model to improve the quality of the search results by incorporating user relevance feedback in to the system.
ID:796
CLASS:6
Title: A comparative study of neural network algorithms applied to optical character recognition
Abstract: Three simple general purpose networks are tested for pattern classification on an optical character recognition problem. The feed-forward (multi-layer perceptron) network, the Hopfield network and a competitive learning network are compared. The input patterns are obtained by optically scanning images of printed digits and uppercase letters. The resulting data is used as input for the networks with two-state input nodes; for others, features are extracted by template matching and pixel counting. The classification capabilities of the networks are compared with a nearest neighbour algorithm applied to the same feature vectors. The feed-forward network reaches the same recognition rates as the nearest neighbour algorithm, even when only a small percentage of the possible connections is used. The Hopfield network performs less well, and overloading of the network remains a problem. Recognition rates with the competitive learning network, if input patterns are clustered well, are again as high as the nearest neighbour algorithm.
ID:797
CLASS:6
Title: Using spreadsheets to simulate neural networks
Abstract: The field of neural networks is an active research area in artificial intelligence for solving problems ranging from image recognition to financial systems. Unfortunately, the solution to such problems using neural networks is often complex and highly mathematical. In teaching neural networks, spreadsheets are excellent tools for visualizing the problem as well as for describing the proposed scientific solution.
ID:798
CLASS:6
Title: A pareto archive evolutionary strategy based radial basis function neural network training algorithm for failure rate prediction in overhead feeders
Abstract: This paper outlines a radial basis function neural network approach to predict the failures in overhead distribution lines of power delivery systems. The RBF networks are trained using historical data. The network sizes and errors are simultaneously minimized using the Pareto Archive Evolutionary Strategy algorithm. Mutation of the network is carried out by invoking an orthogonal least square procedure. The performance of the proposed method was compared to a fuzzy inference approach and with multilayered perceptrons. The results suggest that this approach outperforms the other techniques for the prediction of failure rates.
ID:799
CLASS:6
Title: Discriminative training of a neural network statistical parser
Abstract: Discriminative methods have shown significant improvements over traditional generative methods in many machine learning applications, but there has been difficulty in extending them to natural language parsing. One problem is that much of the work on discriminative methods conflates changes to the learning method with changes to the parameterization of the problem. We show how a parser can be trained with a discriminative learning method while still parameterizing the problem according to a generative probability model. We present three methods for training a neural network to estimate the probabilities for a statistical parser, one generative, one discriminative, and one where the probability model is generative but the training criteria is discriminative. The latter model outperforms the previous two, achieving state-of-the-art levels of performance (90.1% F-measure on constituents).
ID:800
CLASS:6
Title: Neural network model of serial learning
Abstract: A brief introduction to neural networks is given, and correspondences between the elements of such networks and the primitive objects of APL are indicated. A neural network model is presented for simulating serial learning, the ability of an animal to respond in a prescribed sequence to an array of stimuli presented simultaneously. The particular network presented is intended to model stimulus-response (s-r) chaining, a behaviorist theory of serial learning. Some results obtained from running the network are discussed.
ID:801
CLASS:6
Title: Neural networks: a new technolgy for information processing
Abstract: Neural networks are an innovative approach to information processing. Their characteristics lend themselves to directed analysis of the contents of computer data-bases. Potential payoffs from this include increased sales, decreased risk, and improved profitability. This presentation includes a brief look at the technology with an emphasis on providing a basic understanding of its characteristics and how they can be applied.
ID:802
CLASS:6
Title: A concurrent neural network algorithm for the traveling salesman problem
Abstract: A binary neuromorphic data structure is used to encode the N &mdash; city Traveling Salesman Problem (TSP). In this representation the computational complexity, in terms of number of neurons, is reduced from Hopfield and Tank's &Ogr;(N2) to &Ogr;(N log2 N). A continuous synchronous neural network algorithm in conjunction with the LaGrange multiplier, is used to solve the problem. The algorithm has been implemented on the NCUBE hypercube multiprocessor. This algorithm converges faster and has a higher probability to reach a valid tour than previously available results.
ID:803
CLASS:6
Title: Artificial neural network-based image pattern recognition
Abstract: This paper addresses the use of a multi-layer fully-connected perceptron neural network for implementing a pattern recognizer. The input of the neural network is a set of seven standardized invariant moments in both the training procedure and recognition procedure. This standardization results in significantly increasing the accuracy of recognition. The neural network in this paper can recognize the shape of patterns regardless of the size, location or brightness. Images are captured and transformed to binary images through a global threshold technique. The weights of the network are computed using the back propagation algorithm. An example is given to demonstrate this neural network pattern recognizer.
ID:804
CLASS:6
Title: Transparent remote execution in LAHNOS by means of a neural network device
Abstract: LAHNOS is a Local Area Heterogeneous Operating System [1] being currently developed at the Universidad Nacional de San Luis over which distributed services are to be built. This paper shows some enhancements to be introduced into the original design in order to achieve automatic allocation of remote execution requests to the best fitted node under some chosen performance criteria.Following current trends in Distributed Systems we propose the insertion of a neural network device into the kernel of LAHNOS to reflect a selected allocation policy to provide improved system performance as long as to release the user of some cumbersome decisions.
ID:805
CLASS:6
Title: Neural networks and open texture
Abstract: In this paper some experiments designed to explore the suitability of using neural nets to tackle problems of open texture in law are described. Three key questions are investigated: can a net classify cases successfully; can an acceptable rationale be uncovered by an examination of the net; and can we derive rules describing the problem from an examination of the net?
ID:806
CLASS:6
Title: Computer \&ldquo;virus\&rdquo; identification by neural networks: An artificial intelligence connectionist implementation naturally made to work with fuzzy information
Abstract: Computer viruses are more and more numerous: around 400 in the year 1990 and this number is estimated to reach 1,000 for 1994-95. Users are not experts and need help in identifying the virus and carrying out the most appropriate cure in case of attack.Knowledge of viruses is necessary but public information offered by virus database or catalogs gives a powerful advantage to virus makers. On the other hand, not enough or no information to users is also a problem because then they use the product they have which does not necessarily provide the appropriate solution in case of virus attack. We propose an alternative solution to the dilemma found in a neural network, an artificial intelligence connectionist model which is fault tolerant, self adaptative to learn automatically, retaining experience to solve the problem of virus identification regarding fuzzy information on concerns and effects.Principles of the formal neuron and the neural network using hidden nodes is examined as well as the theoretical and practical apects of the gradient back propagation algorithm. An implementation of the algorithm is applied to virus identification with data referring to virus concerns and their obvious effects. First results have shown a correct identification of viruses while using fuzzy knowledge of end users introducing uncertaincy on answers or, even, forcing erroneous data. Such a system can be employed by ordinary users, system or computer security managers, as well as consultants as a complementary tool for virus warfare.Further work needs to be conducted to validate methodologically such an approach and to optimize input data coding, the choice for parameters and the learning strategy.
ID:807
CLASS:6
Title: Sentence generation and neural networks
Abstract: In this paper we describe a neural networks approach to generation. The task is to generate sentences with hotel-information from a structured database. The system is inspired by Karen Kukich's ANA, but expands on it by adding generality in the form of language independence in representations and lexical look-up.
ID:808
CLASS:6
Title: An integrated system for neural network simulations
Abstract: A specialist MIMD parallel processing computer has been constructed for high speed simulation of neural network systems. Custom-designed integrated circuits have been combined with conventional microprocessors to give a machine which is highly tuned for neural network computations but which runs a wide range of commercial software packages. In addition, an operating system is presented which assists in the partitioning of problems and allows code which has been developed remotely to be run in parallel on the machine without modification. The machine is intended for research purposes, particularly where large quantities of data need to be processed (e.g. in image processing applications).
ID:809
CLASS:7
Title: Hybrid dynamic systems: mode transition behavior in hybrid dynamic systems
Abstract: Physical system modeling benefits from the use of implicit equations because it is often an intuitive way to describe physical constraints and behaviors. To achieve efficient models, model abstraction may lead to idealized component behavior that switches between modes of operation (e.g., an electrical diode may be on or off) based on inequalities (e.g., voltage &gt; 0). In an explicit representation, the combination of these local mode switches leads to a combinatorial explosion of the number of global modes. It is shown how an implicit formulation can be used to formulate these mode switches, thereby circumventing the combinatorial problem. This leads to the use of differential and algebraic equations (DAEs) for each of the modes. In case these DAEs are of high index, jumps in generalized state variables may occur. In combination with the inequalities that define mode switching, this leads to rich and complex mode transition behavior. An overview of this mode switching behavior and an ontology is presented.
ID:810
CLASS:7
Title: Metabolic Flux Estimation-A Self-Adaptive Evolutionary Algorithm with Singular Value Decomposition
Abstract: Metabolic flux analysis is important for metabolic system regulation and intracellular pathway identification. A popular approach for intracellular flux estimation involves using ^{13}{\rm C} tracer experiments to label states that can be measured by nuclear magnetic resonance spectrometry or gas chromatography mass spectrometry. However, the bilinear balance equations derived from ^{13}{\rm C} tracer experiments and the noisy measurements require a nonlinear optimization approach to obtain the optimal solution. In this paper, the flux quantification problem is formulated as an error-minimization problem with equality and inequality constraints through the ^{13}{\rm C} balance and stoichiometric equations. The stoichiometric constraints are transformed to a null space by singular value decomposition. Self-adaptive evolutionary algorithms are then introduced for flux quantification. The performance of the evolutionary algorithm is compared with ordinary least squares estimation by the simulation of the central pentose phosphate pathway. The proposed algorithm is also applied to the central metabolism of Corynebacterium glutamicum under lysine-producing conditions. A comparison between the results from the proposed algorithm and data from the literature is given. The complexity of a metabolic system with bidirectional reactions is also investigated by analyzing the fluctuations in the flux estimates when available measurements are varied.
ID:811
CLASS:7
Title: A two-time-scale design for edge-based detection and rectification of uncooperative flows
Abstract: Existing Internet protocols rely on cooperative behavior of end users. We present a control-theoretic algorithm to counteract uncooperative users which change their congestion control schemes to gain larger bandwidth. This algorithm rectifies uncooperative users; that is, forces them to comply with their fair share, by adjusting the prices fed back to them. It is to be implemented at the edge of the network (e.g., by ISPs), and can be used with any congestion notification policy deployed by the network. Our design achieves a separation of time-scales between the network congestion feedback loop and the price-adjustment loop, thus recovering the fair allocation of bandwidth upon a fast transient phase.
ID:812
CLASS:7
Title: An energy-aware data-centric generic utility based approach in wireless sensor networks
Abstract: Distinct from wireless ad hoc networks, wireless sensor networks are data-centric, application-oriented, collaborative, and energy-constrained in nature. In this paper, formulate the problem of data transport in sensor networks as an optimization problem whose objective function is to maximize the amount of information (utility) collected at sinks (subscribers), subject to the flow, energy and channel bandwidth constraints. Also, based on a Markov model extended from [3], we derive the link delay and the node capacity in both the single and multi-hop environments, and figure them in the problem formulation. We study three special cases under the problem formulation. In particular, we consider the energy-aware flow control problem, derive an energy aware flow control solution, and investigate via ns-2 simulation its performance. The simulation results show that the proposed energy-aware flow control solution can achieve high utility and low delay without congesting the network.
ID:813
CLASS:7
Title: ATPC: adaptive transmission power control for wireless sensor networks
Abstract: Extensive empirical studies presented in this paper confirm that the quality of radio communication between low power sensor devices varies significantly with time and environment. This phenomenon indicates that the previous topology control solutions, which use static transmission power, transmission range, and link quality, might not be effective in the physical world. To address this issue, online transmission power control that adapts to external changes is necessary. This paper presents ATPC, a lightweight algorithm of Adaptive Transmission Power Control for wireless sensor networks. In ATPC, each node builds a model for each of its neighbors, describing the correlation between transmission power and link quality. With this model, we employ a feedback-based transmission power control algorithm to dynamically maintain individual link quality over time. The intellectual contribution of this work lies in a novel pairwise transmission power control, which is significantly different from existing node-level or network-level power control methods. Also different from most existing simulation work, the ATPC design is guided by extensive field experiments of link quality dynamics at various locations and over a long period of time. The results from the real-world experiments demonstrate that 1) with pairwise adjustment, ATPC achieves more energy savings with a finer tuning capability and 2) with online control, ATPC is robust even with environmental changes over time.
ID:814
CLASS:7
Title: Geometric completion of differential systems using numeric-symbolic continuation
Abstract: Symbolic algorithms using a finite number of exact differentiations and eliminations are able to reduce over and under-determined systems of polynomially nonlinear differential equations to involutive form. The output involutive form enables the identification of consistent initial values, and eases the application of exact or numerical integration methods.Motivated to avoid expression swell of pure symbolic approaches and with the desire to handle systems with approximate coefficients, we propose the use of homotopy continuation methods to perform the differential-elimination process on such non-square systems. Examples such as the classic index 3 Pendulum illustrate the new procedure. Our approach uses slicing by random linear subspaces to intersect its jet components in finitely many points. Generation of enough generic points enables irreducible jet components of the differential system to be interpolated.
ID:815
CLASS:7
Title: Adoption and focus: practical linear types for imperative programming
Abstract: A type system with linearity is useful for checking software protocols andresource management at compile time. Linearity provides powerful reasoning about state changes, but at the price of restrictions on aliasing. The hard division between linear and nonlinear types forces the programmer to make a trade-off between checking a protocol on an object and aliasing the object. Most onerous is the restriction that any type with a linear component must itself be linear. Because of this, checking a protocol on an object imposes aliasing restrictions on any data structure that directly or indirectly points to the object. We propose a new type system that reduces these restrictions with the adoption and focus constructs. Adoption safely allows a programmer to alias objects on which she is checking protocols, and focus allows the reverse. A programmer can alias data structures that point to linear objects and use focus for safe access to those objects. We discuss how we implemented these ideas in the Vault programming language.
ID:816
CLASS:7
Title: Mobility modelling and trajectory prediction for cellular networks with mobile base stations
Abstract: This paper provides mobility estimation and prediction for a variant of GSM network which resembles an adhoc wireless mobile network where base stations and users are both mobile. We propose using Robust Extended Kalman Filter (REKF)as a location heading altitude estimator of mobile user for next node (mobile-base station)in order to improve the connection reliability and bandwidth efficiency of the underlying system. Through analysis we demonstrate that our algorithm can successfully track the mobile users with less system complexity as it requires either one or two closest mobile-basestation measurements. Further, the technique is robust against system uncertainties due to inherent deterministic nature in the mobility model. Through simulation, we show the accuracy and simplicity in implementation of our prediction algorithm.
ID:817
CLASS:7
Title: Sticky splines: definition and manipulation of spline structures with maintained topological relations
Abstract: This paper describes an augmentation to the spline concept to account for topological relations between different spline curves. These topological relations include incidence relations, constraining the extremes of spline curves to other spline curves, and also more general geometric relations, for example, involving the tangents of spline curves in their extremes. To maintain these incidence relations, some spline curves may have to be transformed (translated, rotated, scaled), or even deformed (i.e., the shape of the curve may change) as a result of modifying other spline curves. A data structure and algorithms are given to implement the propagation of these transformations and deformations.Based on the augmented spline concept, to be called sticky  splines, both a script system to represent spline structures and an interactive system for editing drawings while automatically, maintaining their topological structure are presented.
ID:818
CLASS:7
Title: Principles and applications of chaotic systems
Abstract: There lies a behavior between rigid regularity and randomness based on pure chance. It's called a chaotic system, or chaos for short [5]. Chaos is all around us. Our notions of physical motion or dynamic systems have encompassed the precise clock-like ticking of periodic systems and the vagaries of dice-throwing chance, but have often been overlooked as a way to account for the more commonly observed chaotic behavior between these two extremes. When we see irregularity we cling to randomness and disorder for explanations. Why should this be so? Why is it that when the ubiquitous irregularity of engineering, physical, biological, and other systems are studied, it is assumed to be random and the whole vast machinery of probability and statistics is applied? Rather recently, however, we have begun to realize that the tools of chaos theory can be applied toward the understanding, manipulation, and control of a variety of systems, with many of the practical applications coming after 1990. To understand why this is true, one must start with a working knowledge of how chaotic systems behave&mdash;profoundly, but sometimes subtly different, from the behavior of random systems.
ID:819
CLASS:7
Title: Discrete-time optimal control problems with general constraints
Abstract: This paper presents a computational procedure for solving combined discrete-time optimal control and optimal parameter selection problems subject to general constraints. The approach adopted is to convert the problem into a nonlinear programming problem which can be solved using standard optimization software. The main features of the procedure are the way the controls are parametrized and the conversion of all constraints into a standard form suitable for computation. The software is available commercially as a FORTRAN program DMISER3 together with a companion program MISER3 for solving continuous-time problems.
ID:820
CLASS:7
Title: Encoding of high dynamic range video with a model of human cones
Abstract: A recently developed quantitative model describing the dynamical response characteristics of primate cones is used for rendering high dynamic range (HDR) video. The model provides range compression, as well as luminance-dependent noise suppression. The steady-state (static) version of the model provides a global tone mapping algorithm for rendering HDR images. Both the static and dynamic cone models can be inverted, enabling expansion of the HDR images and video that were compressed with the cone model.
ID:821
CLASS:7
Title: Nonlinear programming on generalized networks
Abstract: We describe a specialization of the primal truncated Newton algorithm for solving nonlinear optimization problems on networks with gains. The algorithm and its implementation are able to capitalize on the special structure of the constraints. Extensive computational tests show that the algorithm is capable of solving very large problems. Testing of numerous tactical issues are described, including maximal basis, projected line search, and pivot strategies. Comparisons with NLPNET, a nonlinear network code, and MINOS, a general-purpose nonlinear programming code, are also included.
ID:822
CLASS:7
Title: Automatic data layout for distributed-memory machines
Abstract: The goal of languages like Fortran D or High Performance Fortran (HPF) is to provide a simple yet efficient machine-independent parallel programming model. After the algorithm selection, the data layout choice is the key intellectual challenge in writing an efficient program in such languages. The performance of a data layout depends on the target compilation system, the target machine, the problem size, and the number of available processors. This makes the choice of a good layout extremely difficult for most users of such languages. If languages such as HPF are to find general acceptance, the need for data layout selection support has to  be addressed. We beleive that the appropriate way to provide the needed support is through a tool that generates data layout specifications automatically. This article discusses the design and implementation of a data layout selection tool that generates HPF-style data layout specifications automatically. Because layout is done in a tool that is not embedded in the target compiler and hence will be run only a few times during the tuning phase of an application, it can use techniques such as integer programming that may be considered too computationally expensive for inclusion in production compilers. The proposed framework for automatic data layout selection builds and examines search spaces of candidate data layouts. A candidate layout is an efficient layout for some part of the program. After the generation of search spaces, a single candidate layout is selected for each program part, resulting in a data layout for the entire program. A good overall data layout may require the remapping of arrays between   program parts. A performance estimator based on a compiler model, an execution model, and a machine model are needed to predict the execution time of each candidate layout and the costs of possible remappings between candidate data layouts. In the proposed framework, instances of NP-complete problems are solved during the construction of candidate layout search spaces and the final selection of candidate layouts from each search space. Rather than resorting to heuristics, the framework capitalizes on state-of-the-art 0-1 integer programming technology to compute optimal solutions of these NP-complete problems. A prototype data layout assistant tool based on our framework has been implemented as part of the D system currently under development at Rice University. The article reports  preliminary experimental results. The results indicate that the framework is efficient and allows the generation of data layouts of high quality.
ID:823
CLASS:7
Title: Simulation, mathematical modeling and optimization in industrial design: When and how to apply it?
Abstract: With the current technology, optimization has become a feasible and profitable avenue for problem solving in industrial design. This in turn brings simulation and modeling into the design process in order to obtain a mathematical description of the process in question. This paper addresses the problem of &ldquo;When and How to Apply Optimization in Industrial Design?&rdquo;. In addition, it reviews the most common optimization algorithms and how they should be selected. Furthermore, some of the common optimization packages are discussed. Four industrial applications are briefly studied. The first two are in the area of simulation, in particular, the simulation of a foundry and a computer workload simulation. The other two examples deal with optimization. The first of which develops an algorithm for economic tube grouping and the second one discusses the optimization of a boiler's circulation system.
ID:824
CLASS:7
Title: SystemC-AMS Requirements, Design Objectives and Rationale
Abstract: This paper presents and discusses the foundations on which the analog and mixed-signal extensions of SystemC, named SystemC-AMS, will be developed. First, requirements from targeted application domains are identified. These are then used to derive design objectives and related rationales. Finally, some preliminary seed work is presented and the outline of the analog and mixed-signal extensions development work is given.
ID:825
CLASS:7
Title: CANDIDE: a learning system for process control
Abstract: The aim of this paper is to present an application of artificial intelligence techniques to control. Their use at a high level, as supervisor tools is shortly described and we focuse the attention onto their use at low level, inside the control loops. We describe our approach using artificial intelligence machine learning to acquire knowledge concerning the controlled system, to modelise it and finally to control it. As an example, CANDIDE learns to drive a car. We explain all the learning steps and shows the obtained results.
ID:826
CLASS:7
Title: Coding for system-on-chip networks: a unified framework
Abstract: In this paper, we present a coding framework derived from a communication-theoretic view of a DSM bus to jointly address power, delay, and reliability. In this framework, the data is first passed through a nonlinear source coder that reduces self and coupling transition activity and imposes a constraint on the peak coupling transitions on the bus. Next, a linear error control coder adds redundancy to enable error detection and correction. The framework is employed to efficiently combine existing codes and to derive novel codes that span a wide range of trade-offs between bus delay, codec latency, power, area, and reliability. Simulation results, for a 1-cm 32-bit bus in a 0.18-$mu$m CMOS technology, show that 31 reduction in energy and 62 reduction in energy-delay product are achievable.
ID:827
CLASS:7
Title: Making simulation relevant in business: integrated development of nonlinear process planning and simulation-based shop floor control
Abstract: Although several methods of simulation-based SFC have been suggested for the SFCS, these researches paid only attention to the generation of a target simulation code and could not be fully integrated with the SFCS. Hence, this paper focuses on the conceptual architecture for the rapid and adaptive realization of a simulation-based SFCS for a discrete part manufacturing system. The developed simulation-based SFCS can process non-linear process plans. To this end, the new simulator engine must be developed. It advances the simulation clock and drives the simulation-based SFCS by investigating the information contents specified in the process and resource models.
ID:828
CLASS:7
Title: Precomputing interactive dynamic deformable scenes
Abstract: We present an approach for precomputing data-driven models of interactive physically based deformable scenes. The method permits real-time hardware synthesis of nonlinear deformation dynamics, including self-contact and global illumination effects, and supports real-time user interaction. We use data-driven tabulation of the system's deterministic state space dynamics, and model reduction to build efficient low-rank parameterizations of the deformed shapes. To support runtime interaction, we also tabulate impulse response functions for a palette of external excitations. Although our approach simulates particular systems under very particular interaction conditions, it has several advantages. First, parameterizing all possible scene deformations enables us to precompute novel reduced coparameterizations of global scene illumination for low-frequency lighting conditions. Second, because the deformation dynamics are precomputed and parameterized as a whole, collisions are resolved within the scene during precomputation so that runtime self-collision handling is implicit. Optionally, the data-driven models can be synthesized on programmable graphics hardware, leaving only the low-dimensional state space dynamics and appearance data models to be computed by the main CPU.
ID:829
CLASS:1
Title: Modeling and verification of adaptive navigation in web applications
Abstract: The navigation of a web application is the possible sequences of web pages a user can visit. In the simplest case the next page is determined by the current page and the action (e.g. link, button) selected by the user. However, many web applications now incorporate adaptive navigation, where the next page also depends on the user's mode, for example whether they are a customer or an administrator, or depends on what pages the user has visited previously.Navigation models are useful for clarifying requirements and specifying implementation behavior. When a model is formal, it can also be used to generate design or implementation artifacts, and can be verified for properties such as broken links or length of navigation path. These uses are all important for the case of simple navigation, but even more important for adaptive navigation because of the added complexity. However, none of the current formal approaches can support adaptive navigation.In this paper we present an approach that uses Statecharts to formally model adaptive navigation, and show how important properties of a navigation model are verified using existing model-checking tools. We summarize the kinds of properties that can be checked with such a model, and describe how to use the Symbolic Model Verifier (SMV) tool to perform the verification. Finally, we use the Blockbuster web site as a case study to demonstrate how our approach can uncover navigation problems that arise when new requirements are imposed.
ID:830
CLASS:4
Title: Mining relationships among interval-based events for classification
Abstract: Existing temporal pattern mining assumes that events do not have any duration. However, events in many real world applications have durations, and the relationships among these events are often complex. These relationships are modeled using a hierarchical representation that extends Allen's interval algebra. However, this representation is lossy as the exact relationships among the events cannot be fully recovered. In this paper, we augment the hierarchical representation with additional information to achieve a lossless representation. An efficient algorithm called IEMiner is designed to discover frequent temporal patterns from interval-based events. The algorithm employs two optimization techniques to reduce the search space and remove non-promising candidates. From the discovered temporal patterns, we build an interval-based classifier called IEClassifier to differentiate closely related classes. Experiments on both synthetic and real world datasets indicate the efficiency and scalability of the proposed approach, as well as the improved accuracy of IEClassifier.
ID:831
CLASS:5
Title: Detecting social interactions of the elderly in a nursing home environment
Abstract: Social interaction plays an important role in our daily lives. It is one of the most important indicators of physical or mental changes in aging patients. In this article, we investigate the problem of detecting social interaction patterns of patients in a skilled nursing facility using audio/visual records. Our studies consist of both a Wizard of Oz style study and an experimental study of various sensors and detection models for detecting and summarizing social interactions among aging patients and caregivers. We first simulate plausible sensors using human labeling on top of audio and visual data collected from a skilled nursing facility. The most useful sensors and robust detection models are determined using the simulated sensors. We then present the implementation of some real sensors based on video and audio analysis techniques and evaluate the performance of these implementations in detecting interactions. We conclude the article with discussions and future work.
ID:832
CLASS:6
Title: Knowing a web page by the company it keeps
Abstract: Web page classification is important to many tasks in information retrieval and web mining. However, applying traditional textual classifiers on web data often produces unsatisfying results. Fortunately, hyperlink information provides important clues to the categorization of a web page. In this paper, an improved method is proposed to enhance web page classification by utilizing the class information from neighboring pages in the link graph. The categories represented by four kinds of neighbors (parents, children, siblings and spouses) are combined to help with the page in question. In experiments to study the effect of these factors on our algorithm, we find that the method proposed is able to boost the classification accuracy of common textual classifiers from around 70% to more than 90% on a large dataset of pages from the Open Directory Project, and outperforms existing algorithms. Unlike prior techniques, our approach utilizes same-host links and can improve classification accuracy even when neighboring pages are unlabeled. Finally, while all neighbor types can contribute, sibling pages are found to be the most important.
ID:833
CLASS:4
Title: A synergistic approach for evolutionary optimization
Abstract: One of the major causes of premature convergence in Evolutionary Algorithm (EA) is loss of population diversity, which pushes the search space to a homogeneous or a near-homogeneous configuration. In particular, this can be a more complicated issue in case of high dimensional complex problem domains. In [13, 14], we presented two novel EA frameworks to curb premature convergence by maintaining constructive diversity in the population. The COMMUNITY_GA or COUNTER_NICHING_GA in [13] uses an informed exploration technique to maintain constructive diversity. In addition to this, the POPULATION_GA model in [14] balances exploration and exploitation using a hierarchical multi-population approach. The current research presents further investigation on the later model which synergistically uses an exploration controlling mechanism through informed genetic operators along with a multi-tier hierarchical dynamic population architecture, which allows initially less fit individuals a fair chance to survive and evolve. Simulations using a set of popular benchmark test functions showed promising results.
ID:834
CLASS:4
Title: A bayesian logistic regression model for active relevance feedback
Abstract: Relevance feedback, which traditionally uses the terms in the relevant documents to enrich the user's initial query, is an effective method for improving retrieval performance. The traditional relevance feedback algorithms lead to overfitting because of the limited amount of training data and large term space. This paper introduces an online Bayesian logistic regression algorithm to incorporate relevance feedback information. The new approach addresses the overfitting problem by projecting the original feature space onto a more compact set which retains the necessary information. The new set of features consist of the original retrieval score, the distance to the relevant documents and the distance to non-relevant documents. To reduce the human evaluation effort in ascertaining relevance, we introduce a new active learning algorithm based on variance reduction to actively select documents for user evaluation. The new active learning algorithm aims to select feedback documents to reduce the model variance. The variance reduction approach leads to capturing relevance, diversity and uncertainty of the unlabeled documents in a principled manner. These are the critical factors of active learning indicated in previous literature. Experiments with several TREC datasets demonstrate the effectiveness of the proposed approach.
ID:835
CLASS:5
Title: Optimizing scrip systems: efficiency, crashes, hoarders, and altruists
Abstract: We discuss the design of efficient scrip systems and develop tools for empirically analyzing them. For those interested in the empirical study of scrip systems, we demonstrate how characteristics of agents in a system can be inferred from the equilibrium distribution of money. From the perspective of a system designer, we examine the effect of the money supply on social welfare and show that social welfare is maximizedby increasing the money supply up to the point that the system experiences a "monetary crash," where money is sufficiently devalued that no agent is willing to perform a service. We alsoexamine the implications of the presence of altruists and hoarders on the performance of the system. While a small number of altruists may improve social welfare, too many can also cause the system to experience a monetary crash, which may be bad for social welfare. Hoarders generally decrease social welfare but, surprisingly, they also promote system stability by helping prevent monetary crashes. In addition, we provide new technical tools for analyzing and computing equilibria by showing that our model exhibits strategic complementarities, which implies that there exist equilibria in pure strategies that can be computed efficiently.
ID:836
CLASS:5
Title: Sequential versus simultaneous auctions: a case study
Abstract: Sequential and simultaneous auctions are two important mechanisms for buying and selling multiple objects. These two mechanisms yield different outcomes (i.e., different surpluses, different revenues, and also different profits to the winning bidders). Given this, we compare the outcomes for the sequential and simultaneous mechanisms for the following scenario. There are multiple similar objects for sale, each object is sold in a separate auction, and each bidder needs only one object. Furthermore, each object has both common and private value components and bidders are  uncertain  about these values. We first determine the equilibrium bidding strategies for each individual auction for the simultaneous and sequential cases. We do this for the English auction rules, the first-price sealed bid rules, and the second-price sealed bid rules. We then consider the case where the private and common values have a uniform distribution and compare the two mechanisms in terms of a bidder's ex-ante expected profit, the auctioneer's expected cumulative revenue, and the expected cumulative surplus. Our study shows that, for all the three auction rules, the expected cumulative surplus and the auctioneer's expected cumulative revenue is higher for the sequential mechanism. However, the mechanism that generates a higher ex-ante expected profit for the bidders depends on the number of objects being auctioned and the number of participating bidders, and it is sometimes higher for sequential mechanism and sometimes for the simultaneous one.
ID:837
CLASS:4
Title: An analysis of bootstrapping for the recognition of temporal expressions
Abstract: We present a semi-supervised (bootstrapping) approach to the extraction of time expression mentions in large unlabelled corpora. Because the only supervision is in the form of seed examples, it becomes necessary to resort to heuristics to rank and filter out spurious patterns and candidate time expressions. The application of bootstrapping to time expression recognition is, to the best of our knowledge, novel. In this paper, we describe one such architecture for bootstrapping Information Extraction (IE) patterns ---suited to the extraction of entities, as opposed to events or relations--- and summarize our experimental findings. These point out to the fact that a pattern set with a good increase in recall with respect to the seeds is achievable within our framework while, on the other side, the decrease in precision in successive iterations is succesfully controlled through the use of ranking and selection heuristics. Experiments are still underway to achieve the best use of these heuristics and other parameters of the bootstrapping algorithm.
