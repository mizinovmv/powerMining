<?xml version="1.0" encoding="UTF-8" standalone="no"?><documents><document><year>2004</year><authors>Peter D. Karp1 | Vinay K. Chaudhri2  | Suzanne M. Paley3 </authors><title>A Collaborative Environment for Authoring Large Knowledge Bases</title><content>Collaborative knowledge base (KB) authoring environments are critical for the construction of high-performance KBs. Such environments must support rapid construction of KBs by a collaborative effort of teams of knowledge engineers through reuse of existing knowledge and software components. They should support the manipulation of knowledge by diverse problem-solving engines even if that knowledge is encoded in different languages and by different researchers. They should support large KBs and provide a scalable and interoperable development infrastructure. In this paper, we present an environment that satisfies many of these goals.We present an architecture for scalable frame representation systems (FRSs). The Generic Frame Protocol (GFP) provides infrastructure for reuse of software components. It is a procedural interface to frame representation systems that provides a common means of accessing and modifying frame KBs. The Generic KB Editor (GKB-EDITOR) provides graphical KB browsing, editing, and comprehension services for large KBs. Scalability of loading and saving time is provided by a storage system (PERK) which submerges a database management system in an FRS. Multi-user access is controlled through a collaboration subsystem that uses a novel optimistic concurrency control algorithm. All the results have been implemented and tested in the development of several real KBs.</content></document><document><year>2004</year><authors>Athanasios Kehagias1| Vassilios Petridis2| Vassilis G. Kaburlasos3 | Pavlina Fragkou2</authors><title>A Comparison of Word- and Sense-Based Text Categorization Using Several Classification Algorithms</title><content>Most of the text categorization algorithms in the literature represent documents as collections of words. An alternative which has not been sufficiently explored is the use of word meanings, also known as senses. In this paper, using several algorithms, we compare the categorization accuracy of classifiers based on words to that of classifiers based on senses. The document collection on which this comparison takes place is a subset of the annotated Brown Corpus semantic concordance. A series of experiments indicates that the use of senses does not result in any significant categorization improvement.</content></document><document><year>2004</year><authors>Qiang Yang1 | Joshua Zhexue Huang2  | Michael Ng3 </authors><title>A Data Cube Model for Prediction-Based Web Prefetching</title><content>Reducing the web latency is one of the primary concerns of Internet research. Web caching and web prefetching are two effective techniques to latency reduction. A primary method for intelligent prefetching is to rank potential web documents based on prediction models that are trained on the past web server and proxy server log data, and to prefetch the highly ranked objects. For this method to work well, the prediction model must be updated constantly, and different queries must be answered efficiently. In this paper we present a data-cube model to represent Web access sessions for data mining for supporting the prediction model construction. The cube model organizes session data into three dimensions. With the data cube in place, we apply efficient data mining algorithms for clustering and correlation analysis. As a result of the analysis, the web page clusters can then be used to guide the prefetching system. In this paper, we propose an integrated web-caching and web-prefetching model, where the issues of prefetching aggressiveness, replacement policy and increased network traffic are addressed together in an integrated framework. The core of our integrated solution is a prediction model based on statistical correlation between web objects. This model can be frequently updated by querying the data cube of web server logs. This integrated data cube and prediction based prefetching framework represents a first such effort in our knowledge.</content></document><document><year>2004</year><authors>Mira Balaban1 | Yoram Kornatzky1</authors><title>A data model for processes based on relative time</title><content>Advanced database applications such as automated manufacturing, scheduling, and computer-aided software engineering, demand an explicit representation of processes, including their decomposition into subprocesses, where subprocesses may be repeated or shared. Temporal information on these processes is inherently relative to particular temporal frames of reference, that may be different from that of a complex process containing them. We suggest the Rtime object-oriented data model in which processes are first-class citizens and complex processes are built, using standard type constructors, from their component processes. The relative timing of component processes is a key feature of the suggested model. It allows for a modular construction of complex process objects that may be repeated and shared. Standard object-oriented query languages can be used for temporal queries on processes, by providing an operator for translating timing information between different temporal frames of reference.</content></document><document><year>2004</year><authors>Vilas Wuwongse1 | Kiyoshi Akama2 | Chutiporn Anutariya1  | Ekawit Nantajeewarawat3 </authors><title>A Data Model for XML Databases</title><content>In the proposed data model for XML databases, an XML element is directly represented as a ground (variable-free) XML expression&amp;#x2014;a generalization of an XML element by incorporation of variables for representation of implicit information and enhancement of its expressive power&amp;#x2014;while a collection of XML documents as a set of ground expressions, each describing an XML element in the documents. Axioms and relationships among elements in the collection as well as structural and integrity constraints are formalized as XML clauses. An XML database, consisting of: (i) a document collection (or an extensional database), (ii) a set of axioms and relationships (or an intensional database), (iii) a set of integrity constraints, is therefore modeled as an XML declarative description comprising a set of ground XML expressions and XML clauses. Its semantics is a set of ground XML expressions, which are explicitly described by the extensional database or implicitly derived from the database, based on the defined intensional database, and satisfy all the specified set of constraints. Thus, selective and complex queries, formulated as sets of XML clauses, about information satisfying specific criteria and possibly implicit in the database, become expressible and computable. The model thereby serves as an effective and well-founded XML database management framework with succinct representational and operational uniformity, reasoning capabilities as well as complex and deductive query supports.</content></document><document><year>2004</year><authors>Antonio Brogi1 | V.S. Subrahmanian2  | Carlo Zaniolo3 </authors><title>A Deductive Database Approach to A.I. Planning</title><content>In this paper, we show that the classical A.I. planning problem can be modelled using simple database constructs with logic-based semantics. The approach is similar to that used to model updates and nondeterminism in active database rules. We begin by showing that planning problems can be automatically converted to Datalog1S programs with nondeterministic choice constructs, for which we provide a formal semantics using the concept of stable models. The resulting programs are characterized by a syntactic structure (XY-stratification) that makes them amenable to efficient implementation using compilation and fixpoint computation techniques developed for deductive database systems. We first develop the approach for sequential plans, and then we illustrate its flexibility and expressiveness by formalizing a model for parallel plans, where several actions can be executed simultaneously. The characterization of parallel plans as partially ordered plans allows us to develop (parallel) versions of partially ordered plans that can often be executed faster than the original partially ordered plans.</content></document><document><year>2004</year><authors>Anthony Hunter1 </authors><title>A Default Logic Based Framework for Context-Dependent Reasoning with Lexical Knowledge</title><content>Lexical knowledge is increasingly important in information systems&amp;#x2014;for example in indexing documents using keywords, or disambiguating words in a query to an information retrieval system, or a natural language interface. However, it is a difficult kind of knowledge to represent and reason with. Existing approaches to formalizing lexical knowledge have used languages with limited expressibility, such as those based on inheritance hierarchies, and in particular, they have not adequately addressed the context-dependent nature of lexical knowledge. Here we present a framework, based on default logic, called the dex framework, for capturing context-dependent reasoning with lexical knowledge. Default logic is a first-order logic offering a more expressive formalisation than inheritance hierarchies: (1) First-order formulae capturing lexical knowledge about words can be inferred; (2) Preferences over formulae can be based on specificity, reasoning about exceptions, or explicit priorities; (3) Information about contexts can be reasoned with as first-order formulae formulae; and (4) Information about contexts can be derived as default inferences. In the dex framework, a word for which lexical knowledge is sought is called a query word. The context for a query word is derived from further words, such as words in the same sentence as the query word. These further words are used with a form of decision tree called a context classification tree to identify which contexts hold for the query word. We show how we can use these contexts in default logic to identify lexical knowledge about the query word such as synonyms, antonyms, specializations, meronyms, and more sophisticated first-order semantic knowledge. We also show how we can use a standard machine learning algorithm to generate context classification trees.</content></document><document><year>2004</year><authors>Ata Kab&amp;aacute n1  | Mark A. Girolami1 </authors><title>A Dynamic Probabilistic Model to Visualise Topic Evolution in Text Streams</title><content>We propose a novel probabilistic method, based on latent variable models, for unsupervised topographic visualisation of dynamically evolving, coherent textual information. This can be seen as a complementary tool for topic detection and tracking applications. This is achieved by the exploitation of the a priori domain knowledge available, that there are relatively homogeneous temporal segments in the data stream. In a different manner from topographical techniques previously utilized for static text collections, the topography is an outcome of the coherence in time of the data stream in the proposed model. Simulation results on both toy-data settings and an actual application on Internet chat line discussion analysis is presented by way of demonstration.</content></document><document><year>2004</year><authors>P. Fragkou| V. Petridis | Ath. Kehagias </authors><title>A Dynamic Programming Algorithm for Linear Text Segmentation</title><content>In this paper we introduce a dynamic programming algorithm which performs linear text segmentation by global minimization of a segmentation cost function which incorporates two factors: (a) within-segment word similarity and (b) prior information about segment length. We evaluate segmentation accuracy of the algorithm by precision, recall and Beeferman's segmentation metric. On a segmentation task which involves Choi's text collection, the algorithm achieves the best segmentation accuracy so far reported in the literature. The algorithm also achieves high accuracy on a second task which involves previously unused texts.</content></document><document><year>2004</year><authors>Giovanna Guerrini1 | Elisa Bertino2  | Ren&amp;eacute  Bal3 </authors><title>A Formal Definition of the Chimera Object-Oriented Data Model</title><content>In this paper we formalize the object-oriented data model of the Chimera language. This language supports all the common features of object-oriented data models such as object identity, complex objects and user-defined operations, classes, inheritance. Constraints may be defined by means of deductive rules, used also to specify derived attributes. In addition, class attributes, operations, and constraints that collectively apply to classes are supported.The main contribution of our work is to define a complete formal model for an object-oriented data model, and to address on a formal basis several issues deriving from the introduction of rules into an object-oriented data model.</content></document><document><year>2004</year><authors>Alex|ra Poulovassilis1 | Swarup Reddi2  | Carol Small2 </authors><title>A formal semantics for an active functional DBPL</title><content>We describe how the functional database programming language PFL is extended with an active component without compromising either its declarative semantics or its syntax. We give a formal specification of the active component using PFL itself, including event specification and detection, parameter-binding, reaction scheduling and abort handling. We describe how a user-specified function can be cast as a primitive event, and discuss the expressiveness of events and the optimisation of event detection.</content></document><document><year>2004</year><authors>J. William Murdock1 | Ashok K. Goel1 | Michael J. Donahoo2  | Shamkant Navathe1 </authors><title>A Framework for Method-Specific Knowledge Compilation from Databases</title><content>Generality and scale are important but difficult issues in knowledge engineering. At the root of the difficulty lie two challenging issues: how to accumulate huge volumes of knowledge and how to support heterogeneous knowledge and processing. One approach to the first issue is to reuse legacy knowledge systems, integrate knowledge systems with legacy databases, and enable sharing of the databases by multiple knowledge systems. We present an architecture called HIPED for realizing this approach. HIPED converts the second issue above into a new form: how to convert data accessed from a legacy database into a form appropriate to the processing method used in a legacy knowledge system. One approach to this reformed issue is to use method-specific compilation of data into knowledge. We describe an experiment in which a legacy knowledge system called INTERACTIVE KRITIK is integrated with an ORACLE database. The experiment indicates the computational feasibility of method-specific data-to-knowledge compilation.</content></document><document><year>2004</year><authors>Elisa Bertino1 | Ahmed K. Elmagarmid2  | Moh|-Sa&amp;iuml d Hacid3 </authors><title>A Knowledge-Based Approach to Visual Information</title><content>We propose an approach based on description logics for the representation and retrieval of visual information. We first consider objects as having shapes which are described by means of semi-algebraic sets.1 We propose a model which consists of three layers: (1) Shape Layer, which provides the geometric shapes of image objects; (2) Object Layer, intended to contain objects of interest and their description; and (3) Schema Layer, which contains the structured abstractions of objects, i.e., a general schema about the classes of objects represented in the Object Layer. We propose two abstract languages on the basis of description logics: one for describing knowledge of the object and schema layers, and the other, more expressive, for making queries. Queries can refer to the form dimension (i.e., information of the Shape Layer) or to the semantic dimension (i.e., information of the Object Layer). We show how this framework can be easily extended to accommodate the visual layer (e.g., color and texture).</content></document><document><year>2004</year><authors>Len Seligman1  | Larry Kerschberg2 </authors><title>A Mediator for Approximate Consistency: Supporting Good Enough Materialized Views</title><content>This paper addresses the needs of application designers whowould like to tell an automated assistant the following: Here is aquery that defines a view I want to materialize within myapplication. I need this view to remain approximately consistent with the state of the data sources from which the view isderived, in accordance with declaratively specified stalenesspredicates. When the view becomes stale, follow the refresh strategyI specify (e.g., eager, lazy, hybrid). You must do this inheterogeneous environments containing both active and passive datasources. This paper describes an architecture that realizes this vision. Theapproach supports materialized, object-based views, called quasi-views,defined over shared databases. Quasi-views arerefreshed according to the consistency conditions and refreshstrategies specified declaratively by application designers. Theseconditions allow for the deviation of quasi-views from their databasecounterparts according to well-defined and monitored approximateconsistency predicates. A layer of software called a Mediator for Approximate Consistencyautomatically generates the databaseobjects necessary to enforce these consistency conditions, shieldingthe application developer from the implementation details ofconsistency maintenance. In addition, it does this for both activeand passive (e.g., legacy) data sources.</content></document><document><year>2004</year><authors>Fabio Crestani1  | Cornelis J. van Rijsbergen2 </authors><title>A Model for Adaptive Information Retrieval</title><content>The paper presents a network model that can be used toproduce conceptual and logical schemas for Information Retrievalapplications. The model has interesting adaptability characteristicsand can be instantiated in various effective ways. The paper alsoreports the results of an experimental investigation into theeffectiveness of implementing associative and adaptive retrieval onthe proposed model by means of Neural Networks. The implementationmakes use of the learning and generalisation capabilities of theBackpropagation learning algorithm to build up and use applicationdomain knowledge in a sub-symbolic form. The knowledge is acquiredfrom examples of queries and relevant documents. Three differentlearning strategies are introduced, their performance is analysed andcompared with the performance of a traditional Information Retrievalsystem.</content></document><document><year>2004</year><authors>Lachlan M. Mackinnon1 | David H. Marwick1  | M. Howard Williams1 </authors><title>A Model for Query Decomposition and Answer Construction in Heterogeneous Distributed Database Systems</title><content>The problem of retrieving information from a collection of heterogeneous distributed databases has attracted a number of solutions. However, the task of integrating established database systems is complicated not only by the differences between the database systems themselves, but also by the differences in structure and semantics of the information contained within them. The problem is exacerbated when one needs to provide access to such a system for naive end-users.This paper is concerned with a Knowledge-Based Systems approach to solving this problem for clearly bounded situations, in which both the domain and the types of query are constrained. At the user interface, dialogue is conducted in terms of concepts with which the user is familiar, and these are then mapped into appropriate database queries. To achieve this a model for query decomposition and answer construction has been used. This model is based around the development of an Intensional Structure containing information necessary for the recapture of semantic information lost in the query decomposition process and required in the answer construction process. The model has been successfully implemented in combination with an embedded KBS, within a five-layer representation model.</content></document><document><year>2004</year><authors>Elpida T. Keravnou1 </authors><title>A Multidimensional and Multigranular Model of Time for Medical Knowledge-Based Systems</title><content>In temporal reasoning there are two interrelated issues; how to model time per se and how to model occurrences. In medical temporal reasoning the need for multiple granularities and multiple conceptual temporal contexts arises in relation to a model of time. Some occurrence can then be expressed with respect to different temporal contexts. This paper presents a multidimensional and multigranular model of time for knowledge-based problem solving, primarily for medical applications. Both the conceptual issues and the design issues underlying the implementation of the proposed model are discussed. The presented model of time has been developed in the context of a time ontology for medical knowledge engineering, whose principal primitives are the time-axis and the time-object. The notion of a time-axis constitutes the primitive for the proposed model of time, while the notion of a time-object aims to integrate time with other essential forms of knowledge, such as structural and causal knowledge, in the expression of different types of occurrences, thus resulting in the integral embodiment of time in such occurrences. The notion of a time-object and the overall ontology of occurrences is given only a cursory mention in this paper. The focus of the paper is the time model. More specifically, the paper presents the notion of a time-axis in the context of the overall time ontology and discusses at length the two classes of time-axes, namely the atomic axes and the spanning axes. The assertion language which has been developed, for the entire ontology, for the expression of axioms (deductive rules and integrity constraints), attribute constraints and propagation methods is presented and illustrated. The implementation of the time model in terms of a layered object-based system is also presented.</content></document><document><year>2004</year><authors>Shu-Ching Chen1| Mei-Ling Shyu2| Chengcui Zhang1 | Jeff Strickrott1</authors><title>A Multimedia Data Mining Framework: Mining Information from Traffic Video Sequences</title><content>The analysis and mining of traffic video sequences to discover important but previously unknown knowledge such as vehicle identification, traffic flow, queue detection, incident detection, and the spatio-temporal relations of the vehicles at intersections, provide an economic approach for daily traffic monitoring operations. To meet such demands, a multimedia data mining framework is proposed in this paper. The proposed multimedia data mining framework analyzes the traffic video sequences using background subtraction, image/video segmentation, vehicle tracking, and modeling with the multimedia augmented transition network (MATN) model and multimedia input strings, in the domain of traffic monitoring over traffic intersections. The spatio-temporal relationships of the vehicle objects in each frame are discovered and accurately captured and modeled. Such an additional level of sophistication enabled by the proposed multimedia data mining framework in terms of spatio-temporal tracking generates a capability for automation. This capability alone can significantly influence and enhance current data processing and implementation strategies for several problems vis-&amp;agrave;-vis traffic operations. Three real-life traffic video sequences obtained from different sources and with different weather conditions are used to illustrate the effectiveness and robustness of the proposed multimedia data mining framework by demonstrating how the proposed framework can be applied to traffic applications to answer the spatio-temporal queries.</content></document><document><year>2004</year><authors>Andrzej Skowron1 | Zbigniew Suraj2</authors><title>A parallel algorithm for real-time decision making: A rough set approach</title><content>We consider decision tables with the values of conditional attributes (conditions) measured by sensors. These sensors produce outputs after an unknown but finite number of time units. We construct an algorithm for computing a highly parallel program represented by a Petri net from a given decision table. The constructed net allows to identify objects in decision tables to an extent which makes appropriate decisions possible. The outputs from sensors are propagated through the net with maximal speed. This is done by an appropriate implementation of all rules true in a given decision table. Our approach is based on rough set theory (Pawlak, 1991). It also seems to have some significance for theoretical foundations of real-time systems.</content></document><document><year>2004</year><authors>Alexei Vinokourov1  | Mark Girolami2 </authors><title>A Probabilistic Framework for the Hierarchic Organisation and Classification of Document Collections</title><content>This paper presents a probabilistic mixture modeling framework for the hierarchic organisation of document collections. It is demonstrated that the probabilistic corpus model which emerges from the automatic or unsupervised hierarchical organisation of a document collection can be further exploited to create a kernel which boosts the performance of state-of-the-art support vector machine document classifiers. It is shown that the performance of such a classifier is further enhanced when employing the kernel derived from an appropriate hierarchic mixture model used for partitioning a document corpus rather than the kernel associated with a flat non-hierarchic mixture model. This has important implications for document classification when a hierarchic ordering of topics exists. This can be considered as the effective combination of documents with no topic or class labels (unlabeled data), labeled documents, and prior domain knowledge (in the form of the known hierarchic structure), in providing enhanced document classification performance.</content></document><document><year>2004</year><authors>H. Papageorgiou1 | Fragkiskos Pentaris2 | Eirini Theodorou2 | Maria Vardaki1  | Michalis Petrakos1 </authors><title>A Statistical Metadata Model for Simultaneous Manipulation of both Data and Metadata</title><content>There is a growing demand for more cost-efficient production processes in Statistical Institutes. One way to address this need is to equip Statistical Information Systems (SIS) with the ability to automatically produce statistical data and metadata of high quality and deliver them to the user via the Internet. Current approaches, although provide for the storage of appropriate metadata, do not use process metadata for guiding the production process. In this paper we present an approach on creating SISs that permits metadata-guided statistical processing based on an object-based, statistical metadata model. The model is not domain specific and can accommodate both microdata and macrodata. We have equipped the model with a set of transformations that can be used to automatically manipulate data and metadata. We show the applicability of transformations with some examples using actual statistical data for R&amp;amp;D expenditures. Finally, we demonstrate how the presented framework can be exploited for the construction of a web site that offers ad hoc query capabilities to the users of statistical data.</content></document><document><year>2004</year><authors>Yonatan Aumann1  | Yehuda Lindell2 </authors><title>A Statistical Theory for Quantitative Association Rules</title><content>Association rules are a key data-mining tool and as such have been well researched. So far, this research has focused predominantly on databases containing categorical data only. However, many real-world databases contain quantitative attributes and current solutions for this case are so far inadequate. In this paper we introduce a new definition of quantitative association rules based on statistical inference theory. Our definition reflects the intuition that the goal of association rules is to find extraordinary and therefore interesting phenomena in databases. We also introduce the concept of sub-rules which can be applied to any type of association rule. Rigorous experimental evaluation on real-world datasets is presented, demonstrating the usefulness and characteristics of rules mined according to our definition.</content></document><document><year>2004</year><authors>Soosup Song1 | Sangho Lee2</authors><title>A Strategy of Dynamic Reasoning in Knowledge-Based System with Fuzzy Production Rules</title><content>Fuzzy production rules have been successfully applied to represent uncertainty in a knowledge-based system. The knowledge organized as a knowledge base is static. On the other hand, a real system such as the stock market is dynamic in nature. Therefore we need a strategy to reflect the dynamic nature of a system when we make reasoning with a knowledge-based system.This paper proposes a strategy of dynamic reasoning that can be used to takes account the dynamic behavior of decision-making with the knowledge-based system consisted of fuzzy rules. A degree of match (DM) between actual input information and antecedent of a rule is represented by a value in interval [0, 1]. Weights of relative importance of attributes in a rule are obtained by the AHP (Analytic Hierarchy Process) method. Then these weights are applied as exponents for the DM, and the DMs in a rule are combined, with the Min operator, into a single DM for the rule. In this way, the importance of attributes of a rule, which can be changed from time to time, can be reflected to reasoning in knowledge-based system with fuzzy rules.</content></document><document><year>2004</year><authors>Yiming Yang1 | Se&amp;aacute n Slattery1  | Rayid Ghani2| 3 </authors><title>A Study of Approaches to Hypertext Categorization</title><content>Hypertext poses new research challenges for text classification. Hyperlinks, HTML tags, category labels distributed over linked documents, and meta data extracted from related Web sites all provide rich information for classifying hypertext documents. How to appropriately represent that information and automatically learn statistical patterns for solving hypertext classification problems is an open question. This paper seeks a principled approach to providing the answers. Specifically, we define five hypertext regularities which may (or may not) hold in a particular application domain, and whose presence (or absence) may significantly influence the optimal design of a classifier. Using three hypertext datasets and three well-known learning algorithms (Naive Bayes, Nearest Neighbor, and First Order Inductive Learner), we examine these regularities in different domains, and compare alternative ways to exploit them. Our results show that the identification of hypertext regularities in the data and the selection of appropriate representations for hypertext in particular domains are crucial, but seldom obvious, in real-world problems. We find that adding the words in the linked neighborhood to the page having those links (both inlinks and outlinks) were helpful for all our classifiers on one data set, but more harmful than helpful for two out of the three classifiers on the remaining datasets. We also observed that extracting meta data from related Web sites was extremely useful for improving classification accuracy in some of those domains. Finally, the relative performance of the classifiers being tested provided insights into their strengths and limitations for solving classification problems involving diverse and often noisy Web pages.</content></document><document><year>2004</year><authors>Joshua Grass1  | Shlomo Zilberstein2 </authors><title>A Value-Driven System for Autonomous Information Gathering</title><content>This paper presents a system for autonomous information gathering in an information rich domain under time and monetary resource restrictions. The system gathers information using an explicit representation of the user's decision model and a database of information sources. Information gathering is performed by repeatedly selecting the query with the highest marginal value. This value is determined by the value of the information with respect to the decision being made, the responsiveness of the information source, and a given resource cost function. Finally, we compare the value-driven approach to several base-line techniques and show that the overhead of the meta-level control is made up for by the increased decision quality.</content></document><document><year>2004</year><authors>Ahmed Guessoum1 </authors><title>Abductive Knowledge Base Updates for Contextual Reasoning</title><content>We show in this paper how procedures that update knowledge bases can naturally be adapted to a number of problems related to contextual reasoning. The fact that the update procedures are abductive in nature is favourably exploited to tackle problems related to human-computer dialogue systems. We consider as examples aspects of pronoun resolution,goal formulation , and the problem of restoring the consistency of a knowledge base after some knowledge update is carried out. We state these problems in terms of the update problem and abductive reasoning and show how procedures that update knowledge bases yield some interesting results. We also explain how these procedures can naturally be used to model various forms of hypothetical reasoning such as hypothesizing inconsistencies and performing some look ahead form of reasoning.We do not claim thaT the problems presented here are solved entirely within the update framework. However, we believe that the flexibility of the representation and of the problem-solving approach suggest that the problems could be solved by adding more details about each problem. What is most interesting in our understanding is that all the aforementioned problems are expressed and tackled within the same framework.</content></document><document><year>2004</year><authors>James Bailey1  | Alex|ra Poulovassilis1</authors><title>Abstract Interpretation for Termination Analysis in Functional Active Databases</title><content>An active database consists of a traditional database supplemented by a set of Event-Condition-Action (ECA) rules. One of the key questions for active database designers is that of termination of the ECA rules. The behaviour of the ECA rules may be obscure and their semantics is often not specified formally. Consequently, developing termination analysis algorithms and proving their correctness is a challenging task. In this paper we address this problem for functional active databases by adopting an abstract interpretation approach. By functional active databases we mean active databases whose transaction execution semantics have been expressed in a purely functional language. Although we demonstrate our techniques for a specific active DBMS which supports a functional database programming language interface, these techniques are directly applicable to other active DBMSs whose execution semantics have been specified using a functional or a denotational approach.</content></document><document><year>2004</year><authors>Susan D. Urban1 | Michael K. Tschudi1| Suzanne W. Dietrich1 | Anton P. Karadimce1</authors><title>Active Rule Termination Analysis: An Implementation and Evaluation of the Refined Triggering Graph Method</title><content>This paper describes the implementation of the Refined Triggering Graph (RTG) method for active rule termination analysis and provides an evaluation of the approach based on the application of the method to a sample active application. The RTG method has been defined in the context of an active-deductive, object-oriented database language known as CDOL (Comprehensive, Declarative, Object Language). The RTG method studies the contents of rule pairs and rule cycles in a triggering graph and tests for: (1) the successful unification of one rule's action with another rule's triggering event, and (2) the satisfiability of active rule conditions, asking whether it is possible for the condition of a triggered rule to evaluate to true in the context of the triggering rule's condition. If the analysis can provably demonstrate that one rule cannot trigger another rule, the directed vector connecting the two rules in a basic triggering graph can be removed, thus refining the triggering graph. An important aspect in the implementation of the method is the development of a satisfiability algorithm for CDOL conditions. This paper presents the tool that was developed based on the RTG method, describing how techniques from constraint logic programming are integrated with other techniques for testing the satisfiability of rule triggering conditions. The effectiveness of the approach within the context of a sample application is also addressed.</content></document><document><year>2004</year><authors>Ibrahim Imam1| 1  | Larry Kerschberg1| 1 </authors><title>Adaptive Intelligent Agents</title><content>Without Abstract</content></document><document><year>2004</year><authors>Manfred Reichert1  | Peter Dadam2 </authors><title>Adeptflex&amp;#x2014;Supporting Dynamic Changes of Workflows Without Losing Control</title><content>Today's workflow management systems (WFMSs) are only applicable in a secure and safe manner if the business process (BP) to be supported is well-structured and there is no need for ad hoc deviations at run-time. As only few BPs are static in this sense, this significantly limits the applicability of current workflow (WF) technology. On the other hand, to support dynamic deviations from premodeled task sequences must not mean that the responsibility for the avoidance of consistency problems and run-time errors is now completely shifted to the (naive) end user. In this paper we present a formal foundation for the support of dynamic structural changes of running WF instances. Based upon a formal WF model (ADEPT), we define a complete and minimal set of change operations (ADEPTflex) that support users in modifying the structure of a running WF, while maintaining its (structural) correctness and consistency. The correctness properties defined by ADEPT are used to determine whether a specific change can be applied to a given WF instance or not. If these properties are violated, the change is either rejected or the correctness must be restored by handling the exceptions resulting from the change. We discuss basic issues with respect to the management of changes and the undoing of temporary changes at the instance level. Recently we have started the design and implementation of ADEPTworkflow, the ADEPT workflow engine, which will make use of the change facilities presented in this paper.</content></document><document><year>2004</year><authors>Jens Lechtenb&amp;ouml rger1 | Hua Shu2 | Gottfried Vossen1</authors><title>Aggregate Queries Over Conditional Tables</title><content>Conditional tables have been identified long ago as a way to capture unknown or incomplete information. However, queries over conditional tables have never been allowed to involve column functions such as aggregates. In this paper, the theory of conditional tables is extended in this direction, and it is shown that a strong representation system exists which has the closure property that the result of an aggregate query over a conditional table can be again represented by a conditional table. It turns out, however, that the number of tuples in a conditional table representing the result of an aggregate query may grow exponentially in the number of variables in the table. This phenomenon is analyzed in detail, and tight upper and lower bounds concerning the number of tuples contained in the result of an aggregate query are given. Finally, representation techniques are sketched that approximate aggregation results in tables of reasonable size.</content></document><document><year>2004</year><authors>Francesco M. Donini1 | Maurizio Lenzerini1 | Daniele Nardi1  | Andrea Schaerf1 </authors><title>AL-log: Integrating Datalog and Description Logics</title><content>We present an integrated system for knowledge representation, calledAL -log, based on description logics and the deductive database language Datalog. AL-log embodies two subsystems, called structural and relational. The former allows for the definition of structural knowledge about classes of interest (concepts) and membership relation between objects and classes. The latter allows for the definition of relational knowledge about objects described in the structural component. The interaction between the two components is obtained by allowing constraints within Datalog clauses, thus requiring the variables in the clauses to range over the set of instances of a specified concept. We propose a method for query answering in AL-log based on constrained resolution, where the usual deduction procedure defined for Datalog is integrated with a method for reasoning on the structural knowledge.</content></document><document><year>2004</year><authors>Kenneth A. Kaufman1  | Ryszard S. Michalski2| 3 </authors><title>An Adjustable Description Quality Measure for Pattern Discovery Using the AQ Methodology</title><content>In concept learning and data mining tasks, the learner is typically faced with a choice of many possible hypotheses or patterns characterizing the input data. If one can assume that training data contain no noise, then the primary conditions a hypothesis must satisfy are consistency and completeness with regard to the data. In real-world applications, however, data are often noisy, and the insistence on the full completeness and consistency of the hypothesis is no longer valid. In such situations, the problem is to determine a hypothesis that represents the best trade-off between completeness and consistency. This paper presents an approach to this problem in which a learner seeks rules optimizing a rule quality criterion that combines the rule coverage (a measure of completeness) and training accuracy (a measure of inconsistency). These factors are combined into a single rule quality measure through a lexicographical evaluation functional (LEF). The method has been implemented in the AQ18 learning system for natural induction and pattern discovery, and compared with several other methods. Experiments have shown that the proposed method can be easily tailored to different problems and can simulate different rule learners by modifying the parameter of the rule quality criterion.</content></document><document><year>2004</year><authors>Victor Fresno1  | Angela Ribeiro2 </authors><title>An Analytical Approach to Concept Extraction in HTML Environments</title><content>The core of the Internet and World Wide Web revolution comes from their capacity to efficiently share the huge quantity of data, but the rapid and chaotic growth of the Net has extremely complicated the task of sharing or mining useful information. Each inference process, from Internet information, requires an adequate characterization of the Web pages. The textual part of a page is one of the most important aspects that should be considered to appropriately perform a page characterization. The textual characterization should be made through the extraction of an appropriate set of relevant concepts that properly represent the text included in the Web page. This paper presents a method to obtain such a set of relevant concepts from a Web page, essentially based on a relevance estimation of each word in the text of a Web page. The word-relevance is defined by a combination of criteria that take into account characteristics of the HTML language as well as more classical measures such as the frequency and the position of a word in a document. Besides, heuristic rules to obtain the most suitable fusion of criteria is achieved via a statistical study. Several experiments are conducted to test the performance of the proposed concept extraction method compared to other approaches including a commercial tool. The results obtained here exhibit a greater success in the concept extraction by the proposed technique against other tested methods.</content></document><document><year>2004</year><authors>Art Goldschmidt1 </authors><title>An approach to information mediation in the industrial domain</title><content>This paper presents an approach to information mediation in the industrial domain resulting from the NIIIP infrastructure. In this environment, representations of computing resources normally managed behind distinct organizational firewalls are shared by members of a Virtual Enterprise. Issues of descriptive heterogeneity, multiple sources of control, and semantic mismatch are addressed by providing a common representation of both physical resources and natural language tokens as linked objects. The focus of this paper is on the algorithm or stopping rule that causes the mediation portion of the system to be invoked to learn to resolve object/action level conflicts by adding high-level abstractions in the form of triplet tokens to the Virtual Enterprise's Knowledge Base Management System.</content></document><document><year>2004</year><authors>Jinsuk Kim1  | Myoung Ho Kim2 </authors><title>An Evaluation of Passage-Based Text Categorization</title><content>Researches in text categorization have been confined to whole-document-level classification, probably due to lack of full-text test collections. However, full-length documents available today in large quantities pose renewed interests in text classification. A document is usually written in an organized structure to present its main topic(s). This structure can be expressed as a sequence of subtopic text blocks, or passages. In order to reflect the subtopic structure of a document, we propose a new passage-level or passage-based text categorization model, which segments a test document into several passages, assigns categories to each passage, and merges the passage categories to the document categories. Compared with traditional document-level categorization, two additional steps, passage splitting and category merging, are required in this model. Using four subsets of the Reuters text categorization test collection and a full-text test collection of which documents are varying from tens of kilobytes to hundreds, we evaluate the proposed model, especially the effectiveness of various passage types and the importance of passage location in category merging. Our results show simple windows are best for all test collections tested in these experiments. We also found that passages have different degrees of contribution to the main topic(s), depending on their location in the test document.</content></document><document><year>2004</year><authors>S.K.M. Wong1 </authors><title>An Extended Relational Data Model For Probabilistic Reasoning</title><content>Probabilistic methods provide a formalism for reasoning aboutpartial beliefs under conditions of uncertainty. This paper suggests a newrepresentation of probabilistic knowledge. This representation encompassesthe traditional relational database model. In particular, it is shown thatprobabilistic conditional independence is equivalent to the notion of generalized multivalued dependency. More importantly,a Markov network can be viewed as a generalized acyclic joindependency. This linkage between these two apparently different butclosely related knowledge representations provides a foundation fordeveloping a unified model for probabilistic reasoning and relationaldatabase systems.</content></document><document><year>2004</year><authors>Kewen Wang1  | Lizhu Zhou2</authors><title>An Extension to GCWA and Query Evaluation for Disjunctive Deductive Databases</title><content>We present a simple and intuitive extension GCWAG of the generalized closed world assumption (GCWA) from positive disjunctive deductive databases to general disjunctive deductive databases (with default negation). This semantics is defined in terms of unfounded sets and possesses an argumentation-theoretic characterization. We also provide a top-down procedure for GCWAG, which is sound and complete with respect to GCWAG. We investigate two query evaluation methods for GCWAG: database partition, and database splitting. The basic idea of these methods is to divide the original deductive database into several smaller sub-databases and the query evaluation in the original database is transformed into the problem of query evaluation in smaller or simplified components. We prove that these two methods of query evaluation are all sound with respect to GCWAG.</content></document><document><year>2004</year><authors>Alfredo Go&amp;ntilde i1 | Arantza Illarramendi2 | Eduardo Mena2  | Jos&amp;eacute  Miguel Blanco2 </authors><title>An Optimal Cache for a Federated Database System</title><content>Federated database systems allow users to query different autonomousdatabases with a single request. The answer to those requests mustbe found on the underlying databases. This answering process can beimproved if some data are cached within the federated databasesystem. The article presents an approach that allows the definitionof an optimal cache for a federated database system according to aset of parameters. We show the types of objects to be cached, thecost model used to decide which ones are worth caching and the methodto find the optimal set of objects to cache. Moreover, this approachcontinuously updates the set of parameter values and periodicallyredefines the optimal cache in order to reflect changes in the userrequirements or in the implementation features of the underlyingdatabases. The article also presents how cached data can be used toanswer a user query. Furthermore, the advantages of using a KnowledgeRepresentation System based on Description Logics in order to definean optimal cache for a federated database system are shown throughthe paper.</content></document><document><year>2004</year><authors>Mengchi Liu </authors><title>An Overview of the Rule-Based Object Language</title><content>This paper presents an overview of a novel strongly typed deductive object database language, called Rule-based Object Language, which is being developed at the University of Regina. Rule-based Object Language is a uniform language for defining, querying, and manipulating a database, which integrates important features of deductive databases and object databases. It supports object identity, complex objects, classes, class hierarchies, multiple inheritance with overriding and blocking, and schema definition. It also supports structured values such as functor objects and sets, treating them as first class citizens and providing powerful mechanisms for representing both partial and complete information about sets. Important integrity constraints such as domain, referential, functional dependency, multi-valued dependency, and cardinality are built-in in a uniform framework. Rule-based Object Language directly supports non-first normal form relations and is an extension of the pure valued-oriented deductive languages such as Datalog and LDL (without grouping) and subsumes them as special cases. It supports schema, object, fact and rule queries in a uniform framework. It also supports schema, fact and rule updates.</content></document><document><year>2004</year><authors>Alicja A. Wieczorkowska1  | Jan M. ytkow2</authors><title>Analysis of Feature Dependencies in Sound Description</title><content>Multimedia data, including sound databases, require signal processing and parameterization to enable automatic searching for a specific content. Indexing of musical audio material with high-level timbre information requires extraction of low-level sound parameters first. In this paper, we analyze regularities in musical sound description, for the data representing musical instrument sounds by means of spectral and time-domain features. We examined digital audio recordings of singular sounds for 11 instruments of definite pitch. Woodwinds, brass, and strings used in contemporary orchestras were investigated, for various fundamental frequencies of sound and articulation techniques. General-purpose data mining system Forty-Niner was applied to investigate dependencies between the sound attributes, and the results of the experiments are presented and discussed. We also indicate a broad range of possible industry applications, which may influence directions of further research in this domain. We summarize our paper with conclusions on representation of musical instrument sound, and the emerging issue of exploration of audio databases.</content></document><document><year>2004</year><authors>Maureen Mellody1 | Mark A. Bartsch2  | Gregory H. Wakefield2 </authors><title>Analysis of Vowels in Sung Queries for a Music Information Retrieval System</title><content>A method for analyzing and categorizing the vowels of a sung query is described and analyzed. This query system uses a combination of spectral analysis and parametric clustering techniques to divide a single query into different vowel regions. The method is applied separately to each query, so no training or repeated measures are necessary. The vowel regions are then transformed into strings and string search methods are used to compare the results from various songs. We apply this method to a small pilot study consisting of 40 sung queries from each of 7 songs. Approximately 60% of the queries are correctly identified with their corresponding song, using only the vowel stream as the identifier.</content></document><document><year>2004</year><authors>Laurence Cholvy1  | Christophe Garion2 </authors><title>Answering Queries Addressed to Several Databases According to a Majority Merging Approach</title><content>The general context of this work is the problem of merging data provided by several sources which can be contradictory. Focusing on the case when the information sources do not contain any disjunction, this paper first defines a propositional modal logic for reasoning with data obtained by merging several information sources according to a majority approach. Then it defines a theorem prover to automatically deduce these merged data. Finally, it shows how to use this prover to implement a query evaluator which answers queries addressed to several databases. This evaluator is such that the answer to a query is the one that could be computed by a classical evaluator if the query was addressed to the merged databases. The databases we consider are made of an extensional part, i.e. a set of positive or negative ground literals, and an intensional part i.e. a set of first order function-free clauses. A restriction is imposed to these databases in order to avoid disjunctive data.</content></document><document><year>2004</year><authors>Alicja A. Wieczorkowska1 | Jakub Wr&amp;oacute blewski1 | Piotr Synak1  | Dominik le&amp;cedil zak2| 1 </authors><title>Application of Temporal Descriptors to Musical Instrument Sound Recognition</title><content>An automatic content extraction from multimedia files is recently being extensively explored. However, an automatic content description of musical sounds has not been broadly investigated and still needs an intensive research. In this paper, we investigate how to optimize sound representation in terms of musical instrument recognition purposes. We propose to trace trends in the evolution of values of MPEG-7 descriptors in time, as well as their combinations. Described process is a typical example of KDD application, consisting of data preparation, feature extraction and decision model construction. Discussion of efficiency of applied classifiers illustrates capabilities of possible progress in the optimization of sound representation. We believe that further research in this area would provide background for an automatic multimedia content description.</content></document><document><year>2004</year><authors>Jorge R. Bernardino1| Pedro S. Furtado2  | Henrique C. Madeira2 </authors><title>Approximate Query Answering Using Data Warehouse Striping</title><content>This paper presents and evaluates a simple but very effective method to implement large data warehouses on an arbitrary number of computers, achieving very high query execution performance and scalability. The data is distributed and processed in a potentially large number of autonomous computers using our technique called data warehouse striping (DWS). The major problem of DWS technique is that it would require a very expensive cluster of computers with fault tolerant capabilities to prevent a fault in a single computer to stop the whole system. In this paper, we propose a radically different approach to deal with the problem of the unavailability of one or more computers in the cluster, allowing the use of DWS with a very large number of inexpensive computers. The proposed approach is based on approximate query answering techniques that make it possible to deliver an approximate answer to the user even when one or more computers in the cluster are not available. The evaluation presented in the paper shows both analytically and experimentally that the approximate results obtained this way have a very small error that can be negligible in most of the cases.</content></document><document><year>2004</year><authors>Shengli Wu1 | Amit Sheth2 | John Miller2  | Zongwei Luo3 </authors><title>Authorization and Access Control of Application Data in Workflow Systems</title><content>Workflow Management Systems (WfMSs) are used to support the modeling and coordinated execution of business processes within an organization or across organizational boundaries. Although some research efforts have addressed requirements for authorization and access control for workflow systems, little attention has been paid to the requirements as they apply to application data accessed or managed by WfMSs. In this paper, we discuss key access control requirements for application data in workflow applications using examples from the healthcare domain, introduce a classification of application data used in workflow systems by analyzing their sources, and then propose a comprehensive data authorization and access control mechanism for WfMSs. This involves four aspects: role, task, process instance-based user group, and data content. For implementation, a predicate-based access control method is used. We believe that the proposed model is applicable to workflow applications and WfMSs with diverse access control requirements.</content></document><document><year>2004</year><authors>Ralph D. Semmel1  | James Mayfield2</authors><title>Automated Query Formulation Using an Entity&amp;#x2013;Relationship Conceptual Schema</title><content>Significant effort is expended in developing a high-level conceptual schema for a relational database. However, criticalknowledge is often discarded when the conceptual schema is mapped to aset of relation schemas. As a result, designers and users must employsparser logical-level knowledge to access data. Unfortunately, naiveusers do not possess the detailed logical-level knowledge required toformulate queries corresponding to ad hoc requests. Auniversal relation interface can shield users from underlyingdesign details. However, most universal relation systems have beenbased on abstractions not typically used by database designers.Consequently, the usefulness of these interfaces has been limited.This article demonstrates how an Entity-Relationship (ER) conceptualschema can be used by a high-level interface to formulate queriesautomatically. The notion of contextsis introduced to describethe segmentation of an ER conceptual schema into overlapping subgraphsthat correspond to sets of relations that can be joined in a losslessmanner. Given a set of contexts, natural join query formulation isstraightforward. As demonstrated with a case study using theprototype QUICK system, the techniques presented facilitate theconstruction of high-level, intelligent interfaces.</content></document><document><year>2004</year><authors>Michelle X. Zhou1  | Steven K. Feiner1 </authors><title>Automated Visual Presentation: From Heterogeneous Information to Coherent Visual Discourse</title><content>Automated visual presentation systems should be able to design effective presentations for heterogeneous (quantitative and qualitative) information. They should also be able to work in static or interactive environments and capable of employing a wide range of visual media and visual techniques. In this paper, we focus on three tasks in building visual production systems: establishing a thorough understanding of the presentation-related characteristics of domain-specific information; classifying several types of visual information and capturing their distinct syntactic, semantic, and pragmatic features; and formulating a set of design principles.We define a data-analysis taxonomy to characterize heterogeneous information. In addition, we have modeled presentation context information such as audience identity to produce user-centered visual design. To utilize and manipulate visual information, we have classified it into visual objects and visual tools based on its role in the visual production process. To guide the visual design process, we have formulated a set of design principles that ensure the expressiveness and effectiveness of a design. To test and evaluate our work, we have developed a prototype system called IMPROVISE based on the research results. We use examples generated by IMPROVISE to illustrate how it constructs visual presentations.</content></document><document><year>2004</year><authors>Colin Meek1  | William P. Birmingham1</authors><title>Automatic Thematic Extractor</title><content>We have created a system that identifies musical keywords or themes. The system searches for all patterns composed of melodic (intervallic for our purposes) repetition in a piece. This process generally uncovers a large number of patterns, many of which are either uninteresting or only superficially important. Filters reduce the number or prevalence, or both, of such patterns. Patterns are then rated according to perceptually significant characteristics. The top-ranked patterns correspond to important thematic or motivic musical content, as has been verified by comparisons with published musical thematic catalogs. The system operates robustly across a broad range of styles, and relies on no meta-data on its input, allowing it to independently and efficiently catalog multimedia data.</content></document><document><year>2004</year><authors>Yutaka Matsuo1 | Yukio Ohsawa2  | Mitsuru Ishizuka3 </authors><title>Average-Clicks: A New Measure of Distance on the World Wide Web</title><content>The pages and hyperlinks of the World Wide Web may be viewed as nodes and edges in a directed graph. In this paper, we propose a new definition of the distance between two pages, called average-clicks. It is based on the probability to click a link through random surfing. We compare the average-clicks measure to the classical measure of clicks between two pages, and show the average-clicks fits better to the users' intuition of distance.</content></document><document><year>2004</year><authors>Jong P. Yoon1 | Vijay Raghavan1 | Venu Chakilam1  | Larry Kerschberg2 </authors><title>BitCube: A Three-Dimensional Bitmap Indexing for XML Documents</title><content>XML is a new standard for exchanging and representing information on the Internet. Documents can be hierarchically represented by XML-elements. In this paper, we propose that an XML document collection be represented and indexed using a bitmap indexing technique. We define the similarity and popularity operations suitable for bitmap indexes. We also define statistical measurements in the BitCube: center, and radius. Based on these measurements, we describe a new bitmap indexing based technique to cluster XML documents. The techniques for clustering are motivated by the fact that the bitmap indexes are expected to be very sparse.Furthermore, a 2-dimensional bitmap index is extended to a 3-dimensional bitmap index, called the BitCube. Sophisticated querying of XML document collections can be performed using primitive operations such as slice, project, and dice. Experiments show that the BitCube can be created efficiently and the primitive operations can be performed more efficiently with the BitCube than with other alternatives.</content></document><document><year>2004</year><authors>Yonatan Aumann1 | Ronen Feldman1| Orly Lipshtat1 | Heikki Manilla1</authors><title>Borders: An Efficient Algorithm for Association Generation in Dynamic Databases</title><content>We consider the problem of finding association rules in a database with binary attributes. Most algorithms for finding such rules assume that all the data is available at the start of the data mining session. In practice, the data in the database may change over time, with records being added and deleted. At any given time, the rules for the current set of data are of interest. The naive, and highly inefficient, solution would be to rerun the association generation algorithm from scratch following the arrival of each new batch of data. This paper describes the Borders algorithm, which provides an efficient method for generating associations incrementally, from dynamically changing databases. Experimental results show an improved performance of the new algorithm when compared with previous solutions to the problem.</content></document><document><year>2004</year><authors>Hasan M. Jamil1 </authors><title>Bottom-Up Association Rule Mining in Relational Databases</title><content>Although knowledge discovery from large relational databases has gained popularity and its significance is well recognized, the prohibitive nature of the cost associated with extracting such knowledge, as well as the lack of suitable declarative query language support act as limiting factors. Surprisingly, little or no relational technology has yet been significantly exploited in data mining even though data often reside in relational tables. Consequently, no relational optimization has yet been possible for data mining. We exploit the transitive nature of large item sets and the so called anti-monotonicity property of support thresholds of large item sets to develop a natural least fixpoint operator for set oriented data mining from relational databases. The operator proposed has several advantages including optimization opportunities, and traditional candidate set free large item set generation. We present an SQL3 expression for association rule mining and discuss its mapping to the least fixpoint operator developed in this paper.</content></document><document><year>2004</year><authors>Rajiv Bagai1 | Rajshekhar Sunderraman1</authors><title>Bottom-up computation of the Fitting model for general deductive databases</title><content>General logic programs are those that contain both positive and negative subgoals in their clause bodies. For such programs Fitting proposed an elegant 3-valued minimum model semantics that avoids some impracticalities of previous approaches. Here we present a method to compute this Fitting model for deductive databases. We introducepartial relations, which are the semantic objects associated with predicate symbols, and define algebraic operators over them. The first step in our model computation method is to convert the database rules into partial relation definitions involving these operators. The second step is to build the minimum model iteratively. We give algorithms for both steps and show their termination and correctness. We also suggest extensions to our method for computing the well-founded model proposed by van Gelder, Ross and Schlipf.</content></document><document><year>2004</year><authors>Robert M. Bruckner1  | A Min Tjoa1 </authors><title>Capturing Delays and Valid Times in Data Warehouses&amp;#x2014;Towards Timely Consistent Analyses</title><content>Real-world changes are generally discovered delayed by computer systems. The typical update patterns for traditional data warehouses on an overnight or even weekly basis increase this propagation delay until the information is available to knowledge workers. Typically, traditional data warehouses focus on summarized data (at some level) rather than detailed data.For active data warehouse environments, detailed data about entities is required for checking the data conditions and triggering actions to automize routine decision tasks. Hence, keeping data current (by minimizing the latency from when data is captured until it is available to knowledge workers) and consistent in that context is a difficult task.</content></document><document><year>2004</year><authors>Ekow J. Otoo1 | Arie Shoshani1  | Seung-Won Hwang2</authors><title>Clustering High Dimensional Massive Scientific Datasets</title><content>Many scientific applications can benefit from an efficient clustering algorithm of massively large high dimensional datasets. However most of the developed algorithms are impractical to use when the amount of data is very large. Given N objects each defined by an M-dimensional feature vector, any clustering technique for handling very large datasets in high dimensional space should run in time O(MN) at best, and O(MN log N) in the worst case, using no more than O(MN) storage, for it to be practical. We introduce a hybrid algorithm, called HyCeltyc, for clustering massively large high dimensional datasets in O(MN) time which is linear in the size of the data. HyCeltyc, which stands for HybridCell Density Clustering method, combines a cell-density based algorithm with a hierarchical agglomerative method to identify clusters in linear time. The main steps of the algorithm involve sampling, dimensionality reduction, selection of significant features on which to cluster the data and a grid-based clustering algorihm that is linear in the data size.</content></document><document><year>2004</year><authors>Wesley W. Chu1 | Hua Yang1 | Kuorong Chiang1| Michael Minock1| Gladys Chow1 | Chris Larson1</authors><title>CoBase: A scalable and extensible cooperative information system</title><content>A new generation of information systems that integrates knowledge base technology with database systems is presented for providing cooperative (approximate, conceptual, and associative) query answering. Based on the database schema and application characteristics, data are organized into Type Abstraction Hierarchies (TAHs). The higher levels of the hierarchy provide a more abstract data representation than the lower levels. Generalization (moving up in the hierarchy), specialization (moving down the hierarchy), and association (moving between hierarchies) are the three key operations in deriving cooperative query answers for the user. Based on the context, the TAHs can be constructed automatically from databases. An intelligent dictionary/directory in the system lists the location and characteristics (e.g., context and user type) of the TAHs. CoBase also has a relaxation manager to provide control for query relaxations. In addition, an explanation system is included to describe the relaxation and association processes and to provide the quality of the relaxed answers. CoBase uses a mediator architecture to provide scalability and extensibility. Each cooperative module, such as relaxation, association, explanation, and TAH management, is implemented as a mediator. Further, an intelligent directory mediator is provided to direct mediator requests to the appropriate service mediators. Mediators communicate with each other via KQML. The GUI includes a map server which allows users to specify queries graphically and incrementally on the map, greatly improving querying capabilities. CoBase has been demonstrated to answer imprecise queries for transportation and logistic planning applications. Currently, we are applying the CoBase methodology to match medical image (X-ray, MRI) features and approximate matching of emitter signals in electronic warfare applications.</content></document><document><year>2004</year><authors>Christian B&amp;ouml hm1 | Hans-Peter Kriegel2  | Thomas Seidl3 </authors><title>Combining Approximation Techniques and Vector Quantization for Adaptable Similarity Search</title><content>Adaptable similarity queries based on quadratic form distance functions are widely popular in data mining application domains including multimedia, CAD, molecular biology or medical image databases. Recently it has been recognized that quantization of feature vectors can substantially improve query processing for Euclidean distance functions, as demonstrated by the scan-based VA-file and the index structure IQ-tree. In this paper, we address the problem that determining quadratic form distances between quantized vectors is difficult and computationally expensive. Our solution provides a variety of new approximation techniques for quantized vectors which are combined by an extended multistep query processing architecture. In our analysis section, we show that the filter steps complement each other. Consequently, it is useful to apply our filters in combination. We show the superiority of our approach over other architectures and over competitive query processing methods. In our experimental evaluation, the sequential scan is outperformed by a factor of 2.3. Compared to the X-tree on 64 dimensional color histogram data, we measured an improvement factor of 5.7.</content></document><document><year>2004</year><authors>Hidetoshi Nonaka1 </authors><title>Communication Interface with Eye-Gaze and Head Gesture Using Successive DP Matching and Fuzzy Inference</title><content>This paper proposes a communication interface with eye-gaze and head gesture. Visual sensorimotor integration with eye-head cooperation is considered, especially head gesture accompanied with vestibulo-ocular reflex is used for selecting object in the screen. Eye-mark recorder and motion tracking system were used to tracking eye movement and head movement, respectively. Nonverbal response animation with eye contact was introduced in the interaction system. In identifying the head gesture, we adopted a modified dynamic programming matching method and fuzzy inference.</content></document><document><year>2004</year><authors>Jos&amp;eacute  Luis Ambite1 | Craig A. Knoblock1 | Ion Muslea1  | Andrew G. Philpot1 </authors><title>Compiling Source Descriptions for Efficient and Flexible Information Integration</title><content>Integrating data from heterogeneous data sources is a critical problem that has received a great deal of attention in recent years. There are two competing approaches to address this problem. The traditional approach, which first appeared in Multibase and more recently in HERMES and TSIMMIS, often called global-as-view, defines the global model as a view on the sources. A more recent approach, sometimes referred to as local-as-view or view rewriting, defines the sources as views on the global model. The disadvantage of the first approach is that a person must re-engineer the definitions of the global model whenever any of the sources change or when new sources are added. The view rewriting approach does not suffer from this drawback, but the problem of rewriting queries into equivalent plans using views is computationally hard and must be performed for each query at run-time.In this paper we propose a hybrid approach that amortizes the cost of query processing over all queries by pre-compiling the source descriptions into a minimal set of integration axioms. Using this approach, the sources are defined in terms of the global model and then compiled into axioms that define the global model in terms of the sources. These axioms can be efficiently instantiated at run-time to determine the most appropriate rewriting to answer a query and facilitate traditional cost-based query optimization. Our approach combines the flexibility of the local-as-view approach with the run-time efficiency of the query processing in global-as-view systems. We have implemented this approach for the SIMS and Ariadne information mediators and provide empirical results that demonstrate that in practice the approach scales to large numbers of sources and that the approach can compile the axioms for a variety of real-world domains in a matter of seconds.</content></document><document><year>2004</year><authors>Ashish Mehta1 | James Geller2 | Yehoshua Perl2 | Peter Fankhauser3</authors><title>Computing access relevance for path-method generation in OODB and IM-OODB</title><content>A path-method (PM) is a mechanism to retrieve or to update information relevant to one class, in an object-oriented database (OODB), that is not stored with that class but with some other class. The PM traverses a chain of classes and connections that ends at the class where the required information is stored. However, it is a difficult task for a user to write PMs. This is because it might require comprehensive knowledge of many classes of the conceptual schema. But a typical user has often incomplete or even inconsistent knowledge of the schema. Currently we are developing a system, called Path-Method Generator (PMG), which generates PMs automatically according to a naive user's requests. One algorithm of PMG uses numerical access relevance between pairs of classes as a guide for the traversal of an OODB schema. In this paper we define the notion of access relevance to measure the significance of the (indirect) connection between any two classes in an OODB and present efficient algorithms to compute access relevance. The manual PM generation in an interoperable multi object-oriented database (IM-OODB) is even more difficult than for one OODB since a user has to be familiar with several OODBs. We use a hierarchical approach for developing efficient online algorithms for the computation of access relevances in an IM-OODB, based on precomputed access relevances for each autonomous OODB. In an IM-OODB the access relevances are used as a guide in generating PMs between the classes of different OODBs.</content></document><document><year>2004</year><authors>Arul Siromoney1  | K. Inoue2 </authors><title>Consistency and Completeness in Rough Sets</title><content>Consistency and completeness are defined in the context of rough set theory and shown to be related to the lower approximation and upper approximation, respectively. A member of a composed set (union of elementary sets) that is consistent with respect to a concept, surely belongs to the concept. An element that is not a member of a composed set that is complete with respect to a concept, surely does not belong to the concept. A consistent rule and a complete rule are useful in addition to any other rules learnt to describe a concept. When an element satisfies the consistent rule, it surely belongs to the concept, and when it does not satisfy the complete rule, it surely does not belong to the concept. In other cases, the other learnt rules are used. The results in the finite universe are extended to the infinite universe, thus introducing a rough set model for the learning from examples paradigm. The results in this paper have application in knowledge discovery or learning from database environments that are inconsistent, but at the same time demand accurate and definite knowledge. This study of consistency and completeness in rough sets also lays the foundation for related work at the intersection of rough set theory and inductive logic programming.</content></document><document><year>2004</year><authors>Contributing authors</authors><title/></document><document><year>2009</year><authors>Surya B. Yadav1 </authors><title>A conceptual model for user-centered quality information retrieval on the World Wide Web      </title><content>Information retrieval from the Internet is becoming a commonplace phenomenon. Users and consumers are browsing websites and         seeking various kinds of information for personal use. Retrieving quality information from the Internet can be challenging         even for the computer-savvy. There are several search engines, even some personalized, to help users search for information         on the Internet. In spite of all the claims about search engines, users still have difficult time retrieving relevant information         quickly. This paper proposes a general conceptual model for user-centered quality information retrieval (UCQIR) from the Internet.         The UCQIR conceptual model is presented in an architectural form. The UCQIR architectural model uses the concept of &amp;#8220;Task-performer&amp;#8221;         to present various aspects of an information retrieval system at the knowledge level. Task-performer is an abstract construct         used to conceptualize the idea of an entity that is competent in doing its tasks. The UCQIR architectural model can be used         to easily design and develop domain-specific, user-centered quality information retrieval systems. The proposed UCQIR conceptual         model is unique and comprehensive. The use of the conceptual model is illustrated through a design of a patient-centered quality         medical information retrieval for the medical domain. We also present an experimental evaluation of a UCQIR prototype based         upon real user experiences. The experimental results are very positive.      </content></document><document><year>2009</year><authors>Frdric Flouvat1 | Fabien De Marchi2  | Jean-Marc Petit3 </authors><title>A new classification of datasets for frequent itemsets      </title><content>The discovery of frequent patterns is a famous problem in data mining. While plenty of algorithms have been proposed during         the last decade, only a few contributions have tried to understand the influence of datasets on the algorithms behavior. Being         able to explain why certain algorithms are likely to perform very well or very poorly on some datasets is still an open question.         In this setting, we describe a thorough experimental study of datasets with respect to frequent itemsets. We study the distribution of frequent itemsets with respect to itemsets size together with the distribution of three concise representations: frequent         closed, frequent free and frequent essential itemsets. For each of them, we also study the distribution of their positive         and negative borders whenever possible. The main outcome of these experiments is a new classification of datasets invariant         w.r.t. minsup variations and robust to explain efficiency of several implementations.      </content></document><document><year>2009</year><authors>Davide Buscaldi1 | Paolo Rosso1 | Jos Manuel Gmez-Soriano2  | Emilio Sanchis1 </authors><title>Answering questions with an n-gram based passage retrieval engine      </title><content>In this paper, we present a Question Answering system based on redundancy and a Passage Retrieval method that is specifically         oriented to Question Answering. We suppose that in a large enough document collection the answer to a given question may appear         in several different forms. Therefore, it is possible to find one or more sentences that contain the answer and that also         include tokens from the original question. The Passage Retrieval engine is almost language-independent since it is based on         n-gram structures. Question classification and answer extraction modules are based on shallow patterns.      </content></document><document><year>2009</year><authors>Sinuh Arroyo1  | Miguel-Angel Sicilia1 </authors><title>Architecture and algorithms of the SOPHIE choreography framework      </title><content>Services communicate with each other by exchanging self-contained messages, enabling them to make or to respond to requests.         Depending on the specific application requirements a number of mismatches affecting the semantics, sequence, cardinality and         structure of messages can occur, which prevent interoperation among a prior compatible services. Current technologies present         an &amp;#8220;ad-hoc&amp;#8221; approach for overcoming mismatches. Initiatives to overcome mismatches based on semantic descriptions and mediators,         i.e. choreography service, are envisioned as promising in solving these problems. The SOPHIE framework tackles precisely these         objectives. It supports the conceptualization and mediation of ontology-based choreographies among interacting services, as         a realization of a fully fledged Service Oriented Architecture (SOA). This paper provides an overview of the architecture         and algorithms behind SOPHIE. In detail, the service topologies that define the different ways in which parties can be linked         and the structure they define are depicted. The operational algorithms that model the mechanisms to generate mediators for         overcoming heterogeneity among the Message Exchange Patterns (MEP) of interacting parties are presented. Finally, the correlation         algorithms that put in place the required logic to link the messages sent by one party to the ones expected by another are         described.      </content></document><document><year>2009</year><authors>M. Belkhatir1 </authors><title>CLOVIS: towards precision-oriented text-based video retrieval through the unification of automatically-extracted concepts         and relations of the visual and audio/speech contents      </title><content>Traditional multimedia (video) retrieval systems use the keyword-based approach in order to make the search process fast although         this approach has several shortcomings and limitations related to the way the user is able to formulate her/his information         need. Typical Web multimedia retrieval systems illustrate this paradigm in the sense that the result of a search consists         of a collection of thousands of multimedia documents, many of which would be irrelevant or not fully exploited by the typical         user. Indeed, according to studies related to users&amp;#8217; behavior, an individual is mostly interested in the initial documents         returned during a search session and therefore a multimedia retrieval system is to model the multimedia content as precisely         as possible to allow for the first retrieved images to be fully relevant to the user&amp;#8217;s information need. For this, the keyword-based         approach proves to be clearly insufficient and the need for a high-level index and query language, addressing the issue of         combining modalities within expressive frameworks for video indexing and retrieval is of huge importance and the only solution         for achieving significant retrieval performance. This paper presents a multi-facetted conceptual framework integrating multiple         characterizations of the visual and audio contents for automatic video retrieval. It relies on an expressive representation         formalism handling high-level video descriptions and a full-text query framework in an attempt to operate video indexing and         retrieval beyond trivial low-level processes, keyword-annotation frameworks and state-of-the art architectures loosely-coupling         visual and audio descriptions. Experiments on the multimedia topic search task of the TRECVID evaluation campaign validate         our proposal.      </content></document><document><year>2009</year><authors>Liliana Patricia Santacruz-Valencia1 | Antonio Navarro2 | Ignacio Aedo3  | Carlos Delgado Kloos4 </authors><title>Comparison of knowledge during the assembly process of learning objects      </title><content>This paper describes the conceptual framework OntoGlue for the assembly of learning objects (LOs). To permit a coherent assembling process from the point of view of requirements         and competencies, OntoGlue enhances the definition of LOs by including associated knowledge (i.e. requirements and competencies) in their conceptual data schema. This associated knowledge is defined in terms of classes         of educational ontologies (used as taxonomies), possibly related by mappings. There are several advantages associated with         the OntoGlue approach. Firstly, it provides an enhanced description of the LOs, which permits their search and reuse by considering         requirements and competencies. Secondly, during the assembly process of two LOs, OntoGlue checks that the competencies of         the first LO cover the requirements of the second LO, guaranteeing a coherent assembling process from the requirements and         competencies&amp;#8217; point of view. Thirdly, OntoGlue automatically calculates the meta-data of the resulting assembled LO. Finally,         the definition of the associated knowledge in terms of classes of ontologies, possibly related by mappings, permits an advanced         comparison of requirements and competencies during assembly and search processes.      </content></document><document><year>2009</year><authors>Kamel Aouiche1  | Jrme Darmont1 </authors><title>Data mining-based materialized view and index selection in data warehouses      </title><content>Materialized views and indexes are physical structures for accelerating data access that are casually used in data warehouses.         However, these data structures generate some maintenance overhead. They also share the same storage space. Most existing studies         about materialized view and index selection consider these structures separately. In this paper, we adopt the opposite stance         and couple materialized view and index selection to take view&amp;#8211;index interactions into account and achieve efficient storage         space sharing. Candidate materialized views and indexes are selected through a data mining process. We also exploit cost models         that evaluate the respective benefit of indexing and view materialization, and help select a relevant configuration of indexes         and materialized views among the candidates. Experimental results show that our strategy performs better than an independent         selection of materialized views and indexes.      </content></document><document><year>2009</year><authors>Janne Jmsen1 | Timo Niemi1  | Kalervo Jrvelin2 </authors><title>Derived types in semantic association discovery      </title><content>Semantic associations are direct or indirect linkages between two entities that are construed from existing associations among         entities. In this paper we extend our previous query language approach for discovering semantic associations with an ability         to retrieve semantic associations that, besides explicitly stated (base) associations, may contain associations derived using         logic-based derivation rules. As will be shown, this makes it possible to find semantic associations that are both compact         and intuitive. To implement this new feature, we introduce a rewriting principle that utilizes derived associations to reduce         resulting semantic associations if possible. Other proposed means to assist the interpretation of query results include answer         expansion and the ordering of answers. The incorporated answer expansion feature lets the user investigate rewritten semantic         associations in a query result at the desired level of detail. The ordering of answers is based on the lengths of the resulting         semantic associations, whereby priority is given to shorter semantic associations which often express close and relevant relationships.      </content></document><document><year>2009</year><authors>Michael Jason Minock1 </authors><title>Describing and deriving certain answers over partial databases      </title><content>Although there has been much work in recent years on answering queries using views, there has been less work on deriving answers         from partial databases. That is given a partial database state D                     V            , materialized via the view V, what queries can be asked over D                     V             that can be answered with certainty using only the instance of the partial database and standard query evaluation mechanisms. We define these as the derivable answers and show several special cases in which we can compute and intensionally describe them.      </content></document><document><year>2009</year><authors>M. Zaki1  | A. A. Hamouda1</authors><title>Design of a multi_agent system for worm spreading_reduction      </title><content>With the explosive growth of Internet applications, the threats of network worms against computer systems and network security         are seriously increasing. Many recent researches concentrate on providing a propagation model and early warning. In fact,         the defense against worms in a realistic environment is an open problem. In this work, we present WSRMAS (worm spreading_reduction         multi_agent system) as a system that includes a worm defense mechanism to considerably reduce the rate at which hosts are         infected. As WSRMAS needs a suitable infra-structure, its architecture was elaborated and an agent platform was designed and         implemented to support WSRMAS functions. The proposed system was provided once with a centralized plan and second with a decentralized         (distributed) plan. In both cases the system performance was evaluated. Also different communication capabilities using Knowledge         Query Manipulation Language (KQML) were exploited to improve WSRMAS performance. The ratio between worm and anti-worm spreading         was studied to investigate its influence on the defense efficiency. Taking into account that some machines may not deploy         WSRMAS, consequently, the effectiveness of WSRMAS under different operational conditions has been studied.      </content></document><document><year>2009</year><authors>Bing Zhou1  | Yiyu Yao1 </authors><title>Evaluating information retrieval system performance based on user preference      </title><content>One of the challenges of modern information retrieval is to rank the most relevant documents at the top of the large system         output. This calls for choosing the proper methods to evaluate the system performance. The traditional performance measures,         such as precision and recall, are based on binary relevance judgment and are not appropriate for multi-grade relevance. The         main objective of this paper is to propose a framework for system evaluation based on user preference of documents. It is         shown that the notion of user preference is general and flexible for formally defining and interpreting multi-grade relevance.         We review 12 evaluation methods and compare their similarities and differences. We find that the normalized distance performance         measure is a good choice in terms of the sensitivity to document rank order and gives higher credits to systems for their         ability to retrieve highly relevant documents.      </content></document><document><year>2009</year><authors>Robert Bembenik1  | Henryk Rybi&amp;#324 ski1 </authors><title>FARICS: a method of mining spatial association rules and collocations using clustering and Delaunay diagrams      </title><content>The paper presents problems pertaining to spatial data mining. Based on the existing solutions a new method of knowledge extraction         in the form of spatial association rules and collocations has been worked out and is proposed herein. Delaunay diagram is         used for determining neighborhoods. Based on the neighborhood notion, spatial association rules and collocations are defined.         A novel algorithm for finding spatial rules and collocations has been presented. The approach allows eliminating the parameters         defining neighborhood of objects, thus avoiding multiple &amp;#8220;test and trial&amp;#8221; repetitions of the process of mining for various         parameter values. The presented method has been implemented and tested. The results of the experiments have been discussed.      </content></document><document><year>2009</year><authors>Hyunki Kim1 | Chee-Yoong Choo2  | Su-Shing Chen3 </authors><title>Generating a meta-DL by federating search on OAI and non-OAI servers      </title><content>Federation of intelligent systems is important to practice in applications. More recently, Digital Library (DL) interoperability         has played an important role towards providing more visibility and accessibility to the broad range of rich digital resources         collected by digital libraries and web-based services worldwide. In this paper, we describe how DL interoperability can be         applied to mediating federated intelligent systems. Exemplified by the Digital Library for Life Science Learners (DLLSL) federated         search project and the Open Archives Initiatives Protocol for Metadata Harvesting (OAI-PMH), we develop the design of an integrated         DL system using harvesting methods towards DL interoperability, and the proposal of integrating federated search of non-OAI         and harvesting of OAI repositories, which provides an even broader accessibility to intelligent systems with digital resources.         This Meta-DL system helps researchers to locate, explore and use the resources in the expanding body of scholarly information         by a simple middleware, such as Emerge (see &amp;#8220;Section;2&amp;#8221;).      </content></document><document><year>2009</year><authors>Yanfang Ye1 | Tao Li2 | Kai Huang3| Qingshan Jiang3  | Yong Chen4 </authors><title>Hierarchical associative classifier (HAC) for malware detection from the large and imbalanced gray list      </title><content>Nowadays, numerous attacks made by the malware (e.g., viruses, backdoors, spyware, trojans and worms) have presented a major         security threat to computer users. Currently, the most significant line of defense against malware is anti-virus products         which focus on authenticating valid software from a whitelist, blocking invalid software from a blacklist, and running any         unknown software (i.e., the gray list) in a controlled manner. The gray list, containing unknown software programs which could         be either normal or malicious, is usually authenticated or rejected manually by virus analysts. Unfortunately, along with         the development of the malware writing techniques, the number of file samples in the gray list that need to be analyzed by         virus analysts on a daily basis is constantly increasing. The gray list is not only large in size, but also has an imbalanced         class distribution where malware is the minority class. In this paper, we describe our research effort on building automatic,         effective, and interpretable classifiers resting on the analysis of Application Programming Interfaces (APIs) called by Windows         Portable Executable (PE) files for detecting malware from the large and imbalanced gray list. Our effort is based on associative         classifiers due to their high interpretability as well as their capability of discovering interesting relationships among         API calls. We first adapt several different post-processing techniques of associative classification, including rule pruning         and rule re-ordering, for building effective associative classifiers from large collections of training data. In order to         help the virus analysts detect malware from the imbalanced gray list, we then develop the Hierarchical Associative Classifier         (HAC). HAC constructs a two-level associative classifier to maximize precision and recall of the minority (malware) class:         in the first level, it uses high precision rules of majority (benign file samples) class and low precision rules of minority         class to achieve high recall; and in the second level, it ranks the minority class files and optimizes the precision. Finally,         since our case studies are based on a large and real data collection obtained from the Anti-virus Lab of Kingsoft corporation,         including 8,000,000 malware, 8,000,000 benign files, and 100,000 file samples from the gray list, we empirically examine the         sampling strategy to build the classifiers for such a large data collection to avoid over-fitting and achieve great effectiveness         as well as high efficiency. Promising experimental results demonstrate the effectiveness and efficiency of the HAC classifier.         HAC has already been incorporated into the scanning tool of Kingsoft&amp;#8217;s Anti-Virus software.      </content></document><document><year>2009</year><authors>Alej|ro Figueroa1  | John Atkinson2 </authors><title>Intelligent answering location questions from the web using molecular alignment      </title><content>In this paper, a new molecular alignment based recognition method for question answering from from the Web is proposed. This         identifies locations using an molecular alignment sequence algorithm according to their similarity with a user natural-language         question. Different experiments and results concerning questions on locations are discussed. The high accuracy of the proposed         alignment strategy shows the promise of approach to effectively deal with questions extracted from natural-language corpus         which contain many complex patterns.      </content></document><document><year>2009</year><authors>Hakim Hacid1  | Tetsuya Yoshida2 </authors><title>Neighborhood graphs for indexing and retrieving multi-dimensional data      </title><content>We propose a methodology based on a structure called neighborhood graphs for indexing and retrieving multi-dimensional data.         In accordance with the increase of the quantity of data, it gets more and more important to process multi-dimensional data.         Processing of data includes various tasks, for instance, mining, classifying, clustering, to name a few. However, to enable         the effective processing of such multi-dimensional data, it is often necessary to locate each data precisely in the multi-dimensional         space where the data reside so that each data can be effectively retrieved for processing. This amounts to solving the point         location problem (neighborhood search) for multi-dimensional space. In this paper, in order to utilize the structure of neighborhood         graphs as an indexing structure for multi-dimensional data, we propose the following: i) a local insertion and deletion method,         and ii) an incremental neighborhood graph construction method. The first method enables to cope with the problem incurred         from the updating of the graph. The second method realizes fast neighborhood graph construction from scratch, through the         recursive application of the first method. Several experiments are conducted to evaluate the proposed approach, and the results         indicate the effectiveness of our approach.      </content></document><document><year>2009</year><authors>Qinmin Hu1  | Jimmy Xiangji Huang2 </authors><title>Passage extraction and result combination for genomics information retrieval      </title><content>In this paper, we first propose algorithms for passage extraction to build indices for the purpose of generating more accurate         passages as query answers. Second, we propose a basic result combination method and an improved result combination method         to combine the retrieved results from different indices for the purpose of selecting and merging relevant passages as outputs.         For passage extraction, three new algorithms are proposed, namely paragraphParsed, sentenceParsed and wordSentenceParsed.         For result combination, a novel method is proposed, in which we use factor analysis to generate a better baseline result for         combination by finding some hidden common factors that can be used to estimate the importance of keywords and keyword associations.         Finally, we report the experimental results that confirm the effectiveness and superiority of the factor analysis based method         for result combination. Our proposed approaches achieve excellent results on the TREC 2006 and 2007 Genomics data sets, which         provide a promising avenue for constructing high performance information retrieval systems in biomedicine.      </content></document><document><year>2009</year><authors>Liang Zhu1| 2 | Weiyi Meng3 | Chunnian Liu1 | Wenzhu Yang2  | Dazhong Liu2 </authors><title>Processing top-N relational queries by learning      </title><content>A top-N selection query against a relation is to find the N tuples that satisfy the query condition the best but not necessarily completely. In this paper, we propose a new method for         evaluating top-N queries against a relation. This method employs a learning-based strategy. Initially, this method finds and saves the optimal         search spaces for a small number of random top-N queries. The learned knowledge is then used to evaluate new queries. Extensive experiments are carried out to measure the         performance of this strategy and the results indicate that it is highly competitive with existing techniques for both low-dimensional         and high-dimensional data. Furthermore, the knowledge base can be updated based on new user queries to reflect new query patterns         so that frequently submitted queries can be processed most efficiently. The maintenance and stability of the knowledge base         are also addressed in the paper.      </content></document><document><year>2009</year><authors>Jarek Gryz1 | Qiong Wang2| Xiaoyan Qian2 | Calisto Zuzarte2</authors><title>Queries with CASE expressions      </title><content>In recent years more and more queries are generated automatically by query managers/builders with end-users providing only         specific parameters through GUIs. Queries generated automatically can be quite different from queries written by humans. In         particular, they contain non-declarative features, most notorious of which is the CASE expression. Current query optimizers are often ill-prepared for the new types of queries as they do not deal well with procedural         &amp;#8216;insertions&amp;#8217;. In this paper, we discuss the inefficiencies of CASE expressions and present several new optimization techniques to address them. We also describe experimental evaluation of         the prototype implemented in DB2 UDB V8.2.      </content></document><document><year>2009</year><authors>Iluju Kiringa1  | Alfredo Gabaldon2| 3 </authors><title>Synthesizing advanced transaction models using the situation calculus      </title><content>The situation calculus is a versatile logic for reasoning about actions and formalizing dynamic domains. Using the non-Markovian         action theories formulated in the situation calculus, one can specify and reason about the effects of database actions under         the constraints of the classical, flat database transactions, which constitute the state of the art in database systems. Classical         transactions are characterized by the so-called ACID properties. With non-Markovian action theories, one can also specify,         reason about, and even synthesize various extensions of the flat transaction model, generally called advanced transaction models (ATMs). In this paper, we show how to use non-Markovian theories of the situation calculus to specify and reason about the         properties of ATMs. In these theories, one may refer to past states other than the previous one. ATMs are expressed as such         non-Markovian theories using the situation calculus. We illustrate our method by specifying (and sometimes reasoning about         the properties of) several classical models and known ATMs.      </content></document><document><year>2009</year><authors>M. Campos1 | J. M. Jurez2| J. Palma2 | R. Marn2</authors><title>Using temporal constraints for temporal abstraction      </title><content>The need to provide high level descriptions of the evolution of data is evident in fields like medicine. For being able to         perform task such as diagnostic or monitoring, it is very important to facilitate a high level representation and management         of temporal data. With this representation two main benefits are obtained: it becomes easier to compare data with generic         knowledge, and the volume of data can be reduced. Several models have been proposed for time representation and management.         Temporal constraints have been extensively used as a liable model in problems where temporal imprecision or missing data exist.         The imprecision is usually present when data are manually collected, or when the data are based on subjective observations.         The aim of this paper is to demonstrate how temporal constraints can be used as a formalism in which temporal abstraction         of concepts can be performed. To this end, in the first place, we introduce the fuzzy temporal constraint network as the formalism         for representing temporal information. Then, we present an algorithm for obtaining a state representation from a sequence         of observations. We show the complexity and applicability of the approach.      </content></document><document><year>2009</year><authors>Anthony Lo1| Tansel zyer2| Keivan Kianmehr1 | Reda Alhajj1| 3 </authors><title>VIREX and VRXQuery: interactive approach for visual querying of relational databases to produce XML      </title><content>VIREX provides an interactive approach for querying and integrating relational databases to produce XML documents and the         corresponding schemas. VIREX connects to each database specified by the user; analyzes the catalogue to derive an interactive         diagram equivalent to the extended entity-relationship diagram; allows the user to display sample records from the tables         in the database; allows the user to rename columns and relations by modifying directly the interactive diagram; facilitates         the conversion of the relational database into XML; and derives the XML schema. VIREX works even when the catalogue of the         relational database is missing; it extracts the required catalogue information by analyzing the database content. Further,         VIREX supports VRXQuery, which is a visual naive-users-oriented query language that allows users to specify queries and define         views directly on the interactive diagram as a sequence of mouse clicks with minimum keyboard input. The user is expected         to interactively decide on certain factors to be considered in producing the XML result. Such factors include: 1);selecting         the relations/attributes to be converted into XML; 2);specifying a predicate to be satisfied by the information to be converted         into XML; 3);deciding on the order of nesting between the relations to be converted into XML; 4);ordering for the result.         VRXQuery supports selection, projection, nesting/join, union, difference, and order-by. As the result of a query, VIREX displays         on the screen the XML schema that satisfies the specified characteristics and generates colored (easy to read) XML;document(s).         Further, VIREX allows the user to display and review the SQL and XQuery equivalent to each query expressed in VRXQuery.      </content></document><document><year>2009</year><authors>Raymond Wong1| Jiuyong Li2 | Ada Fu3 | Ke Wang4</authors><title>(&amp;#945;, k)-anonymous data publishing      </title><content>Privacy preservation is an important issue in the release of data for mining purposes. The k-anonymity model has been introduced for protecting individual identification. Recent studies show that a more sophisticated         model is necessary to protect the association of individuals to sensitive information. In this paper, we propose an (&amp;#945;, k)-anonymity model to protect both identifications and relationships to sensitive information in data. We discuss the properties         of (&amp;#945;, k)-anonymity model. We prove that the optimal (&amp;#945;, k)-anonymity problem is NP-hard. We first present an optimal global-recoding method for the (&amp;#945;, k)-anonymity problem. Next we propose two scalable local-recoding algorithms which are both more scalable and result in less         data distortion. The effectiveness and efficiency are shown by experiments. We also describe how the model can be extended         to more general cases.      </content></document><document><year>2008</year><authors>Krishnaprasad Thirunarayan1  | Trivikram Immaneni1 </authors><title>A coherent query language for XML      </title><content>Text search engines are inadequate for indexing and searching XML documents because they ignore metadata and aggregation structure         implicit in the XML documents. On the other hand, the query languages supported by specialized XML search engines are very         complex. In this paper, we present a simple yet flexible query language, and develop its semantics to enable intuitively appealing         extraction of relevant fragments of information while simultaneously falling back on retrieval through plain text search if necessary. Our approach combines and generalizes several available techniques to obtain precise         and coherent results.      </content></document><document><year>2008</year><authors>Abdelkamel Tari1 | Islam Elgedawy2 | Abdelnasser Dahmani1</authors><title>A dual-layered model for web services representation and composition      </title><content>Nowadays more and more companies and organizations implement their business services in the Internet due to the tremendous         progress made recently in the field of Web services. It becomes possible to publish, locate and invoke applications across         the Web. Thus, the ability to select efficiently and integrate at runtime services located in different sites on the Web is         an important issue. In some situations, if no single Web service can satisfy the request of the user, there should be a possibility         to combine existing services together in order to meet the user&amp;#8217;s request. This paper provides a dual-layered model for web         services, where the first model layer captures the high-level functional specifications (namely goals, achievement contexts,         and external behaviours), and the second model layer captures the low-level functional specifications (namely interfaces).         This model allows the service composition process to be performed on both high-level and low-level specifications. We also         introduce the composition operators (both high-level and low-level) to allow composition of virtual services.      </content></document><document><year>2008</year><authors>Masahiko Sato1 </authors><title>A framework for checking proofs naturally      </title><content>We propose a logical framework, called Natural Framework (NF), which supports formal reasoning about computation and logic         (CAL) on a computer. NF is based on a theory of Judgments and Derivations. NF is designed by observing how working mathematical         theories are created and developed. Our observation is that the notions of judgments and derivations are the two fundamental         notions used in any mathematical activity. We have therefore developed a theory of judgments and derivations and designed         a framework in which the theory provides a uniform and common play ground on which various mathematical theories can be defined as derivation games and can be played, namely, can write and check proofs. NF is equipped with a higher-order intuitionistic logic and derivations         (proofs) are described following Gentzen&amp;#8217;s natural deduction style. NF is part of an interactive computer environment CAL         and it is also referred to as NF/CAL. CAL is written in Emacs Lisp and it is run within a special buffer of the Emacs editor.         CAL consists of user interface, a general purpose parser and a checker for checking proofs of NF derivation games. NF/CAL         system has been successfully used as an education system for teaching CAL for undergraduate students for about 8;years. We         will give an overview of the NF/CAL system both from theoretical and practical sides.      </content></document><document><year>2008</year><authors>Aless|ro Campi1 | Ernesto Damiani2 | Sam Guinea1 | Stefania Marrara2 | Gabriella Pasi3  | Paola Spoletini4 </authors><title>A fuzzy extension of the XPath query language      </title><content>Today the current state of the art in querying XML data is represented by XPath and XQuery, both of which rely on Boolean         conditions for node selection. Boolean selection is too restrictive when users do not use or even know the data structure         precisely, e.g. when queries are written based on a summary rather than on a schema. In this paper we describe a XML querying         framework, called FuzzyXPath, based on Fuzzy Set Theory, which relies on fuzzy conditions for the definition of flexible constraints         on stored data. A function called &amp;#8220;deep-similar&amp;#8221; is introduced to replace XPath&amp;#8217;s typical &amp;#8220;deep-equal&amp;#8221; function. The main         goal is to provide a degree of similarity between two XML trees, assessing whether they are similar both structure-wise and         content-wise. Several query examples are discussed in the field of XML based metadata for e-learning.      </content></document><document><year>2008</year><authors>Taisuke Sato1 </authors><title>A glimpse of symbolic-statistical modeling by PRISM      </title><content>We give a brief overview of a logic-based symbolic modeling language PRISM which provides a unified approach to generative         probabilistic models including Bayesian networks, hidden Markov models and probabilistic context free grammars. We include         some experimental result with a probabilistic context free grammar extracted from the Penn Treebank. We also show EM learning         of a probabilistic context free graph grammar as an example of exploring a new area.      </content></document><document><year>2008</year><authors>C. J. Butz1 | H. Yao1  | S. Hua1 </authors><title>A join tree probability propagation architecture for semantic modeling      </title><content>We propose the first join tree (JT) propagation architecture that labels the probability information passed between JT nodes in terms of conditional probability tables (CPTs) rather than potentials. By modeling the task of inference involving evidence, we can generate three work schedules         that are more time-efficient for LAZY propagation. Our experimental results, involving five real-world or benchmark Bayesian         networks (BNs), demonstrate a reasonable improvement over LAZY propagation. Our architecture also models inference not involving         evidence. After the CPTs identified by our architecture have been physically constructed, we show that each JT node has a         sound, local BN that preserves all conditional independencies of the original BN. Exploiting inference not involving evidence         is used to develop an automated procedure for building multiply sectioned BNs. It also allows direct computation techniques         to answer localized queries in local BNs, for which the empirical results on a real-world medical BN are promising. Screen         shots of our implemented system demonstrate the improvements in semantic knowledge.      </content></document><document><year>2008</year><authors>Manuel Martn-Merino1  | ngela Blanco1</authors><title>A local semi-supervised Sammon algorithm for textual data visualization      </title><content>Sammon&amp;#8217;s mapping is a powerful non-linear technique that allow us to visualize high dimensional object relationships. It has         been applied to a broad range of practical problems and particularly to the visualization of the semantic relations among         terms in textual databases. The word maps generated by the Sammon mapping suffer from a low discriminant power due to the         well known &amp;#8220;curse of dimensionality&amp;#8221; and to the unsupervised nature of the algorithm. Fortunately the textual databases provide         frequently a manually created classification for a subset of documents that may help to overcome this problem. In this paper         we first introduce a modification of the Sammon mapping (SSammon) that enhances the local topology reducing the sensibility         to the &amp;#8217;curse of dimensionality&amp;#8217;. Next a semi-supervised version is proposed that takes advantage of the a priori categorization         of a subset of documents to improve the discriminant power of the word maps generated. The new algorithm has been applied         to the challenging problem of word map generation. The experimental results suggest that the new model improves significantly         well known unsupervised alternatives.      </content></document><document><year>2008</year><authors>Tao Ban1 | Changshui Zhang2  | Shigeo Abe3 </authors><title>A new approach to discover interlacing data structures in high-dimensional space      </title><content>The discovery of structures hidden in high-dimensional data space is of great significance for understanding and further processing         of the data. Real world datasets are often composed of multiple low dimensional patterns, the interlacement of which may impede         our ability to understand the distribution rule of the data. Few of the existing methods focus on the detection and extraction         of the manifolds representing distinct patterns. Inspired by the nonlinear dimensionality reduction method ISOmap, in this         paper we present a novel approach called Multi-Manifold Partition to identify the interlacing low dimensional patterns. The         algorithm has three steps: first a neighborhood graph is built to capture the intrinsic topological structure of the input         data, then the dimensional uniformity of neighboring nodes is analyzed to discover the segments of patterns, finally the segments         which are possibly from the same low-dimensional structure are combined to obtain a global representation of distribution         rules. Experiments on synthetic data as well as real problems are reported. The results show that this new approach to exploratory         data analysis is effective and may enhance our understanding of the data distribution.      </content></document><document><year>2008</year><authors>Antoine Cornujols1  | Michle Sebag2 </authors><title>A note on phase transitions and computational pitfalls of learning from sequences      </title><content>An ever greater range of applications call for learning from sequences. Grammar induction is one prominent tool for sequence         learning, it is therefore important to know its properties and limits. This paper presents a new type of analysis for inductive         learning. A few years ago, the discovery of a phase transition phenomenon in inductive logic programming proved that fundamental         characteristics of the learning problems may affect the very possibility of learning under very general conditions. We show         that, in the case of grammatical inference, while there is no phase transition when considering the whole hypothesis space,         there is a much more severe &amp;#8220;gap&amp;#8221; phenomenon affecting the effective search space of standard grammatical induction algorithms         for deterministic finite automata (DFA). Focusing on standard search heuristics, we show that they overcome this difficulty         to some extent, but that they are subject to overgeneralization. The paper last suggests some directions to alleviate this         problem.      </content></document><document><year>2008</year><authors>Lijie Wen1| 2| 3 | Jianmin Wang1| 2 | Wil M. P. van der Aalst4 | Biqing Huang3  | Jiaguang Sun1| 2 </authors><title>A novel approach for process mining based on event types      </title><content>Despite the omnipresence of event logs in transactional information systems (cf. WFM, ERP, CRM, SCM, and B2B systems), historic         information is rarely used to analyze the underlying processes. Process mining aims at improving this by providing techniques         and tools for discovering process, control, data, organizational, and social structures from event logs, i.e., the basic idea         of process mining is to diagnose business processes by mining event logs for knowledge. Given its potential and challenges         it is no surprise that recently process mining has become a vivid research area. In this paper, a novel approach for process         mining based on two event types, i.e., START and COMPLETE, is proposed. Information about the start and completion of tasks         can be used to explicitly detect parallelism. The algorithm presented in this paper overcomes some of the limitations of existing         algorithms such as the &amp;#945;-algorithm (e.g., short-loops) and therefore enhances the applicability of process mining.      </content></document><document><year>2008</year><authors>Ioannis Katakis1 | Grigorios Tsoumakas1 | Evangelos Banos1 | Nick Bassiliades1  | Ioannis Vlahavas1 </authors><title>An adaptive personalized news dissemination system      </title><content>With the explosive growth of the Word Wide Web, information overload became a crucial concern. In a data-rich information-poor         environment like the Web, the discrimination of useful or desirable information out of tons of mostly worthless data became         a tedious task. The role of Machine Learning in tackling this problem is thoroughly discussed in the literature, but few systems         are available for public use. In this work, we bridge theory to practice, by implementing a web-based news reader enhanced         with a specifically designed machine learning framework for dynamic content personalization. This way, we get the chance to         examine applicability and implementation issues and discuss the effectiveness of machine learning methods for the classification         of real-world text streams. The main features of our system named PersoNews are: (a) the aggregation of many different news         sources that offer an RSS version of their content, (b) incremental filtering, offering dynamic personalization of the content         not only per user but also per each feed a user is subscribed to, and (c) the ability for every user to watch a more abstracted         topic of interest by filtering through a taxonomy of topics. PersoNews is freely available for public use on the WWW (http://news.csd.auth.gr).      </content></document><document><year>2008</year><authors>Indrakshi Ray1 | Indrajit Ray1  | Sudip Chakraborty1 </authors><title>An interoperable context sensitive model of trust      </title><content>Although the notion of trust is widely used in secure information systems, very few works attempt to formally define it or         reason about it. Moreover, in most works, trust is defined as a binary concept&amp;#8212;either an entity is completely trusted or not         at all. Absolute trust on an entity requires one to have complete knowledge about the entity. This is rarely the case in real-world         applications. Not trusting an entity, on the other hand, prohibits all communications with the entity rendering it useless.         In short, treating trust as a binary concept is not acceptable in practice. Consequently, a model is needed that incorporates         the notion of different degrees of trust. We propose a model that allows us to formalize trust relationships. The trust relationship         between a truster and a trustee is associated with a context and depends on the experience, knowledge, and recommendation         that the truster has with respect to the trustee in the given context. We show how our model can measure trust and compare         two trust relationships in a given context. Sometimes enough information is not available about a given context to evaluate         trust. Towards this end we show how the relationships between different contexts can be captured using a context graph. Formalizing         the relationships between contexts allows us to extrapolate values from related contexts to approximate the trust of an entity         even when all the information needed to calculate the trust is not available. Finally, we show how the semantic mismatch that         arises because of different sources using different context graphs can be resolved and the trust of information obtained from         these different sources compared.      </content></document><document><year>2008</year><authors>Michel de Rougemont1  | Adrien Vieilleribire2 </authors><title>Approximate schemas, source-consistency and query answering      </title><content>We use the Edit distance with Moves on words and trees and say that two regular (tree) languages are &amp;#949;-close if every word (tree) of one language is &amp;#949;-close to the other. A transducer model is introduced to compare tree languages (schemas) with different alphabets and attributes.         Using the statistical embedding of Fischer et al. (Proceedings of 21st IEEE Symposium on Logic in Computer Science, pp. 421&amp;#8211;430,;2006), we show that Source-Consistency and Approximate Query Answering are testable on words and trees, i.e. can be approximately         decided within &amp;#949; by only looking at a constant fraction of the input.      </content></document><document><year>2008</year><authors>Tiphaine Accary-Barbier1  | Sylvie Calabretto1 </authors><title>Building and using temporal knowledge in archaeological documentation      </title><content>Our previous work on specialized digital libraries showed that each expert was brought to enrich the documentary corpus with         his/her point of view (expressed by a model of knowledge). These points of view can be contradictory and therefore, it would         be interesting to provide expert communities with tools, enabling them to point out meaning dissensions. In Archaeology, the         comparison between different temporal models of knowledge make possible the publication of relative models of knowledge. These         different models tend towards the emerging of a global knowledge shared by a community. In this paper we present a method         and a set of concrete relations proposed to the annotator, which allows to extend some existing models of knowledge and to         build new temporal knowledge. The proposed model, based on temporal algebra relations, permits the use of some efficient constraint         propagation algorithms to extract new information.      </content></document><document><year>2008</year><authors>Alfredo Cuzzocrea1| 2 | Filippo Furfaro2  | Domenico Sacc1| 2 </authors><title>Enabling OLAP in mobile environments via intelligent data cube compression techniques      </title><content>The main drawbacks of handheld devices (small storage space, small size of the display screen, discontinuance of the connection         to the WLAN etc) are often incompatible with the need of querying and browsing information extracted from enormous amounts         of data which are accessible through the network. In this application scenario, data compression and summarization have a         leading role: data in a lossy compressed format can be transmitted more efficiently than the original ones, and can be effectively         stored in handheld devices (setting the compression ratio accordingly). In this paper, we introduce a very effective compression         technique for multidimensional data cubes, and the system Hand-OLAP, which exploits this technique to allow handheld devices         to extract and browse compressed two-dimensional OLAP views coming from multidimensional data cubes stored on a remote OLAP         server localized on the wired network. Hand-OLAP effectively and efficiently enables OLAP in mobile environments, and also         enlarges the potentialities of Decision Support Systems by taking advantage from the &amp;#8220;naturally&amp;#8221; decentralized nature of such         environments. The idea which the system is based on is: rather than querying the original multidimensional data cubes, it         may be more convenient to generate a compressed OLAP view of them, store such view into the handheld device, and query it         locally (off-line), thus obtaining approximate answers that are suitable for OLAP applications.      </content></document><document><year>2008</year><authors>Roelof K. Brouwer1| 2 </authors><title>Extending the rand, adjusted rand and jaccard indices to fuzzy partitions      </title><content>The first stage of knowledge acquisition and reduction of complexity concerning a group of entities is to partition or divide         the entities into groups or clusters based on their attributes or characteristics. Clustering is one of the most basic processes         that are performed in simplifying data and expressing knowledge in a scientific endeavor. It is akin to defining classes.         Since the output of clustering is a partition of the input data, the quality of the partition must be determined as a way         of measuring the quality of the partitioning (clustering) process. The problem of comparing two different partitions of a         finite set of objects reappears continually in the clustering literature. This paper looks at some commonly used clustering         measures including the rand index (RI), adjusted RI (ARI) and the jaccuard index(JI) that are already defined for crisp clustering         and extends them to fuzzy clustering measures giving FRI,FARI and FJI. These new indices give the same values as the original         indices do in the special case of crisp clustering. The extension is made by first finding equivalent expressions for the         parameters, a, b, c, and d of these indices in the case of crisp clustering. A relationship called bonding that describes         the degree to which two cluster members are in the same cluster or class is first defined. Through use in crisp clustering         and fuzzy clustering the effectiveness of the indices is demonstrated.      </content></document><document><year>2008</year><authors>Conor Nugent1| Dnal Doyle2 | Pdraig Cunningham3 </authors><title>Gaining insight through case-based explanation      </title><content>Traditional explanation strategies in machine learning have been dominated by rule and decision tree based approaches. Case-based         explanations represent an alternative approach which has inherent advantages in terms of transparency and user acceptability.         Case-based explanations are based on a strategy of presenting similar past examples in support of and as justification for         recommendations made. The traditional approach to such explanations, of simply supplying the nearest neighbour as an explanation,         has been found to have shortcomings. Cases should be selected based on their utility in forming useful explanations. However,         the relevance of the explanation case may not be clear to the end user as it is retrieved using domain knowledge which they         themselves may not have. In this paper the focus is on a knowledge-light approach to case-based explanations that works by         selecting cases based on explanation utility and offering insights into the effects of feature-value differences. In this         paper we examine to two such a knowledge-light frameworks for case-based explanation. We look at explanation oriented retrieval         (EOR) a strategy which explicitly models explanation utility and also at the knowledge-light explanation framework (KLEF)         that uses local logistic regression to support case-based explanation.      </content></document><document><year>2008</year><authors>Moh|-Said Hacid1 | Nicolas Spyratos2 | Yuzuru Tanaka3</authors><title>Guest editors&amp;#8217; introduction: special issue for ISIP 2005      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Patrick Bosc1 | Allel Hadjali1  | Gabriella Pasi2 </authors><title>Guest editors&amp;#8217; introduction to the special issue on flexible queries in information systems      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Patrick Bosc1 | Allel Hadjali1  | Olivier Pivert1 </authors><title>Incremental controlled relaxation of failing flexible queries      </title><content>In this paper, we discuss an approach for relaxing a failing query in the context of flexible (or fuzzy) querying. The approach         relies on the notion of a parameterized proximity relation which is defined in a relative way. We show how such a proximity         relation allows for transforming a gradual predicate into an enlarged one. The resulting predicate is semantically close to         the original one and it is obtained by a simple fuzzy arithmetic operation. Such a transformation provides the basis for a         flexible query relaxation which can be controlled in a non-empirical rigorous way without requiring any additional information         from the user. We also show how the search for a non-failing relaxed query over the lattice of relaxed queries can be improved         by exploiting the notion of Minimal Failing Sub-queries derived from the failing query.      </content></document><document><year>2008</year><authors>Irwin King1  | Christopher C. Yang2</authors><title>Introduction      </title><content>Without Abstract</content></document><document><year>2008</year><authors>S&amp;#322 awomir Zadro&amp;#380 ny1  | Janusz Kacprzyk1 </authors><title>Issues in the practical use of the OWA operators in fuzzy querying      </title><content>Flexible querying of the relational databases is considered. The applicability of some non-standard, mainly linguistic quantifier         driven aggregation, and via Yager&amp;#8217;s ordered weighted averaging (OWA) operators in particular, is shown. Their handling is         studied with a special emphasis on the selection and tuning of the OWA operator that is appropriate for the user needs. We         start with an OWA operator and intend to tune it to increase its ORness, but keeping the changes as limited as possible, or         to preserve consistency of the changes. These tasks are defined as optimization problems. The discussion is illustrated on         the example of the authors&amp;#8217; FQUERY for Access system.      </content></document><document><year>2008</year><authors>Peter Dolog1 | Heiner Stuckenschmidt2 | Holger Wache3  | Jrg Diederich4 </authors><title>Relaxing RDF queries based on user and domain preferences      </title><content>Research in cooperative query answering is triggered by the observation that users are often not able to correctly formulate         queries to databases such that they return the intended result. Due to lacking knowledge about the contents and the structure         of a database, users will often only be able to provide very broad queries. Existing methods for automatically refining such         queries based on user profiles often overshoot the target resulting in queries that do not return any answer. In this article,         we investigate methods for automatically relaxing such over-constrained queries based on domain knowledge and user preferences.         We describe a framework for information access that combines query refinement and relaxation in order to provide robust, personalized         access to heterogeneous resource description framework data as well as an implementation in terms of rewriting rules and explain         its application in the context of e-learning systems.      </content></document><document><year>2008</year><authors>Iluju Kiringa1 </authors><title>Specifying active databases as non-Markovian theories of actions      </title><content>Over the last 15 years, database management systems (DBMSs) have been enhanced by the addition of rule-based programming to         obtain active DBMSs. One of the greatest challenges in this area is to formally account for all the aspects of active behavior         using a uniform formalism. In this paper, we formalize active relational databases within the framework of the situation calculus         by uniformly accounting for them using theories embodying non-Markovian control in the situation calculus. We call these theories         active relational theories and use them to capture the dynamics of active databases. Transaction processing and rule execution is modelled as a theorem         proving task using active relational theories as background axioms. We show that the major components of an ADBMS, namely         the rule sets and the execution models, may be given a clear semantics using active relational theories. More precisely: we         represent the rule set as a program written in a suitable version of the situation calculus based language ConGolog; then         we extend an existing situation calculus based framework for modelling advanced transaction models to one for modelling the         execution models of active behaviors.      </content></document><document><year>2008</year><authors>Bela Stantic1| Abdul Sattar1 | Paolo Terenziani2 </authors><title>The POINT approach to represent now in bitemporal databases      </title><content>Most modern database applications involve a significant amount of time dependent data and a significant portion of this data         is now-relative. Now-relative data are a natural and meaningful part of every temporal database as well as being the focus of most queries. Previous studies         indicate that the choice of the representation of now significantly influences the efficiency of accessing bitemporal data. In this paper we propose and experimentally evaluate         a novel approach to represent now that we termed the POINT approach, in which now-relative facts are represented as points on the transaction-time and/or valid-time line. Furthermore, in the POINT approach we propose a logical query transformation that relies on the above representation and on the geometry features of         spatial access methods. Such a logical query transformation enables off-the-shelf spatial indexes to be used. We empirically         prove that the POINT approach is efficient on now-relative bitemporal data, outperforming the maximum timestamp approach that has been proven to the best approach to now-relative data         in the literature, independently of the indexing methodology (B         &amp;#8201;+&amp;#8201;- tree vs R         *- tree) being used. Specifically, if spatial indexing is used, the POINT approach outperforms the maximum timestamp approach to the extent of factor more than 10, both in number of disk accesses         and CPU usage.      </content></document><document><year>2008</year><authors>Xing Li1| Howard J. Hamilton1 | Kamran Karimi1 | Liqiang Geng1</authors><title>The Multi-Tree Cubing algorithm for computing iceberg cubes      </title><content>The computation of data cubes is one of the most expensive operations in on-line analytical processing (OLAP). To improve         efficiency, an iceberg cube represents only the cells whose aggregate values are above a given threshold (minimum support).         Top-down and bottom-up approaches are used to compute the iceberg cube for a data set, but both have performance limitations.         In this paper, a new algorithm, called Multi-Tree Cubing (MTC), is proposed for computing an iceberg cube. The Multi-Tree         Cubing algorithm is an integrated top-down and bottom-up approach. Overall control is handled in a top-down manner, so MTC         features shared computation. By processing the orderings in the opposite order from the Top-Down Computation algorithm, the         MTC algorithm is able to prune attributes. The Bottom Up Computation (BUC) algorithm and its variations also perform pruning         by relying on the processing of intermediate partitions. The MTC algorithm, however, prunes without processing such partitions.         The MTC algorithm is based on a specialized type of prefix tree data structure, called an Attribute&amp;#8211;Partition tree (AP-tree),         consisting of attribute and partition nodes. The AP-tree facilitates fast, in-memory sorting and APRIORI-like pruning. We         report on five series of experiments, which confirm that MTC is consistently as fast or faster than BUC, while finding the         same iceberg cubes.      </content></document><document><year>2008</year><authors>Fabien De Marchi1 | Stphane Lopes2  | Jean-Marc Petit3 </authors><title>Unary and n-ary inclusion dependency discovery in relational databases      </title><content>Foreign keys form one of the most fundamental constraints for relational databases. Since they are not always defined in existing         databases, the discovery of foreign keys turns out to be an important and challenging task. The underlying problem is known         to be the inclusion dependency (IND) inference problem. In this paper, data-mining algorithms are devised for IND inference         in a given database. We propose a two-step approach. In the first step, unary INDs are discovered thanks to a new preprocessing         stage which leads to a new algorithm and to an efficient implementation. In the second step, n-ary IND inference is achieved.         This step fits in the framework of levelwise algorithms used in many data-mining algorithms. Since real-world databases can         suffer from some data inconsistencies, approximate INDs, i.e. INDs which almost hold, are considered. We show how they can         be safely integrated into our unary and n-ary discovery algorithms. An implementation of these algorithms has been achieved         and tested against both synthetic and real-life databases. Up to our knowledge, no other algorithm does exist to solve this         data-mining problem.      </content></document><document><year>2006</year><authors>Byeong Man Kim1| Qing Li1| 2 | Chang Seok Park1| Si Gwan Kim1 | Ju Yeon Kim3</authors><title>A new approach for combining content-based and collaborative filters      </title><content>With the development of e-commerce and the proliferation of easily accessible information, recommender systems have become         a popular technique to prune large information spaces so that users are directed toward those items that best meet their needs         and preferences. A variety of techniques have been proposed for performing recommendations, including content-based and collaborative         techniques. Content-based filtering selects information based on semantic content, whereas collaborative filtering combines         the opinions of other users to make a prediction for a target user. In this paper, we describe a new filtering approach that         combines the content-based filter and collaborative filter to capitalize on their respective strengths, and thereby achieves         a good performance. We present a series of recommendations on the selection of the appropriate factors and also look into         different techniques for calculating user-user similarities based on the integrated information extracted from user profiles         and user ratings. Finally, we experimentally evaluate our approach and compare it with classic filters, the result of which         demonstrate the effectiveness of our approach.      </content></document><document><year>2006</year><authors>David J. Russomanno1 </authors><title>A plausible inference prototype for the Semantic Web      </title><content>This paper presents a prototype that is capable of drawing plausible inferences from Resource Description Framework (RDF)         assertions that are constituents of a distributed, Semantic Web knowledge system. The approach taken to build the prototype         can be viewed as the extension and adaptation of a classical approach to plausible inference to exploit the evolving infrastructure         being developed to represent declarative knowledge on the Semantic Web. The approach includes a knowledge representation formalism         that supports meta properties, which define precise semantics, enabling subsequent plausible inferences via extended composition         of RDF properties. Most research and development in the context of the Semantic Web has been devoted to representational infrastructure         and accompanying query and logical deduction formalisms to evolve the Web from a document repository to a set of distributed         knowledge bases. The work presented in this paper provides a functioning Semantic Web application in which the generation         of new inferences is not contained within the deductive closure of the knowledge and data expressed by a collection of information         sources represented using RDF. Moreover, the paper provides a concrete example of an RDF schema and a working system built         around it which demonstrates one potential use of meta data on the Web.      </content></document><document><year>2006</year><authors>S. M. Deen1  | R. Jayousi1| 2 </authors><title>A preference processing model for cooperative agents      </title><content>When multiple valid solutions are available to a problem, preferences can be used to indicate a choice. In a distributed system,         such a preference-based solution can be produced autonomous agents cooperating together, but the attempt will lead to contention         if the same resource is given preference by several user-agents. To resolve such contentions, this paper proposes a market-based         payment scheme for selling and buying preferences by the contenders, in which the best solution is defined as the one where         as many preferences as theoretically possible are globally met. After exploring the nature of preference, the paper develops         a preference processing model based on the market based scheme, and presents a theoretical performance model to verify the         correctness of the processing model. This verification is provided by a simulation study of the processing model.                     For the simulation study, a manufacturing environment is conjectured, where a set of tasks are resolved into subtasks by coordinator               agents, and then these subtasks are allocated to assembler agents through cooperation and negotiation, in which preferred               resources are exchanged against payments. The study shows that our agent based strategy not only produces convergence on the               total preference value for the whole system, but also reaches that final value irrespective of the initial orderof subtask               allocation to the assemblers.            </content></document><document><year>2006</year><authors>Zidrina Pabarskaite1  | Aistis Raudys1 </authors><title>A process of knowledge discovery from web log data: Systematization and critical review      </title><content>This paper presents a comprehensive survey of web log/usage mining based on over 100 research papers. This is the first survey         dedicated exclusively to web log/usage mining. The paper identifies several web log mining sub-topics including specific ones         such as data cleaning, user and session identification. Each sub-topic is explained, weaknesses and strong points are discussed         and possible solutions are presented. The paper describes examples of web log mining and lists some major web log mining software         packages.      </content></document><document><year>2006</year><authors>Yannis Tzitzikas1 | Carlo Meghini1  | Nicolas Spyratos1 </authors><title>A unified interaction scheme for information sources      </title><content>Commonly, for retrieving the desired information from an information source (knowledge base or information base), the user         has to use the query language that is provided by the system. This is a big barrier for many ordinary users and the resulting         interaction style is rather inflexible. In this paper we give the theoretical foundations of an interaction scheme that allows         users to retrieve the objects of interest without having to be familiar with the conceptual schema of the source or with the         supported query language. Specifically, we describe an interaction manager that provides a quite flexible interaction scheme by unifying several well-known interaction schemes. Furthermore, we show         how this scheme can be applied to taxonomy-based sources by providing all needed algorithms and reporting their computational         complexity.      </content></document><document><year>2006</year><authors>Qian Wan1  | Aijun An1 </authors><title>An efficient approach to mining indirect associations      </title><content>Discovering association rules is one of the important tasks in data mining. While most of the existing algorithms are developed         for efficient mining of frequent patterns, it has been noted recently that some of the infrequent patterns, such as indirect         associations, provide useful insight into the data. In this paper, we propose an efficient algorithm, called HI-mine, based on a new data structure, called HI-struct, for mining the complete set of indirect associations between items. Our experimental results show that HI-mine's performance is significantly better than that of the previously developed algorithm for mining indirect associations on         both synthetic and real world data sets over practical ranges of support specifications.      </content></document><document><year>2006</year><authors>Roberto Esposito1 | Rosa Meo1  | Marco Botta1 </authors><title>Answering constraint-based mining queries on itemsets using previous materialized results      </title><content>In recent years, researchers have begun to study inductive databases, a new generation of databases for leveraging decision support applications. In this context, the user interacts with the         DBMS using advanced, constraint-based languages for data mining where constraints have been specifically introduced to increase         the relevance of the results and, at the same time, to reduce its volume. In this paper we study the problem of mining frequent         itemsets using an inductive database. We propose a technique for query answering which consists in rewriting the query in         terms of union and intersection of the result sets of other queries, previously executed and materialized. Unfortunately,         the exploitation of past queries is not always applicable. We then present sufficient conditions for the optimization to apply         and show that these conditions are strictly connected with the presence of functional dependencies between the attributes         involved in the queries. We show some experiments on an initial prototype of an optimizer which demonstrates that this approach         to query answering is viable and in many practical cases it drastically reduces the query execution time.      </content></document><document><year>2006</year><authors>Lemuel R. Waitman1| Douglas H. Fisher2  | Paul H. King1</authors><title>Bootstrapping rule induction to achieve rule stability and reduction      </title><content>Most rule learning systems posit hard decision boundaries for continuous attributes and point estimates of rule accuracy,         with no measures of variance, which may seem arbitrary to a domain expert. These hard boundaries/points change with small         perturbations to the training data due to algorithm instability. Moreover, rule induction typically produces a large number         of rules that must be filtered and interpreted by an analyst. This paper describes a method of combining rules over multiple         bootstrap replications of rule induction so as to reduce the total number of rules presented to an analyst, to measure and         increase the stability of the rule induction process, and to provide a measure of variance to continuous attribute decision         boundaries and accuracy point estimates. A measure of similarity between rules is also introduced as a basis of multidimensional         scaling to visualize rule similarity. The method was applied to perioperative data and to the UCI (University of California,         Irvine) thyroid dataset.      </content></document><document><year>2006</year><authors>Hye-young Paik1 | Noureddine Mouaddib2 | Boualem Benatallah3 | Farouk Toumani4  | Mahbub Hassan3 </authors><title>Building and querying e-catalog networks using P2P and data summarisation techniques      </title><content>One of the critical issues in Web-based e-commerce has been how to efficiently and effectively integrate and query heterogeneous,         diverse e-catalogs. We propose an integration framework for building and querying catalogs. Our approach is based on a hybrid         of peer-to-peer data sharing paradigm and Web-services architecture. Peers in our system serve as domain-specific data integration         mediators. Links between peers are established based on the similarity of the domain they represent. The relationships are         used for routing queries among peers. As the number of catalogs involved grow larger, the need for filtering irrelevant data         sources will become increasingly high. We apply a summarisation technique to summarise the content of catalogs. The summaries         are used to pre-selecting data sources that are relevant to a user query.      </content></document><document><year>2006</year><authors>Katsumi Inoue1 | Koji Iwanuma2  | Hidetomo Nabeshima2 </authors><title>Consequence finding and computing answers with defaults      </title><content>Consequence finding has been recognized as an important technique in many intelligent systems involving inference. In previous         work, propositional or first-order clausal theories have been considered for consequence finding. In this paper, we consider         consequence finding from a default theory, which consists of a first-order clausal theory and a set of normal defaults. In         an extension of a default theory, consequence finding can be done with the generating defaults for the extension. Alternatively, all extensions can be represented at once with the conditional answer format, which represents how a conclusion depends on which defaults.                     We also propose a procedure for consequence finding and query answering in a default theory using the first-order consequence-finding               procedure SOL. In computing consequences from default theories efficiently, the notion of TCS-freeness is most important to prune a large number of irrational tableaux induced by the generating defaults for an extension. In               order to simulate the TCS-freeness, the refined SOL calculus called SOL-S(&amp;#915;) is adopted using skip preference and complement               checking.            </content></document><document><year>2006</year><authors>JuHum Kwon1 | O-Hoon Choi1 | Chang-Joo Moon2 | Soo-Hyun Park3  | Doo-Kwon Baik1 </authors><title>Deriving similarity for Semantic Web using similarity graph      </title><content>One important research challenge of current Semantic Web is resolving the interoperability issue across ontologies. The issue         is directly related to identifying semantics of resources residing in different domain ontologies. That is, the semantics         of a concept in an ontology differs from others according to the modeling style and intuition of the knowledge expert even         though they are the same forms of a concept in each respective ontology. In this paper, we propose a similarity measure to         resolve the interoperability issue by using a similarity graph. The strong point of this paper is that we provide a precise         mapping technique and similarity properties to derive the similarity. The novel contribution of this paper is that we provide         a core technique of computing similarity across ontologies of Semantic Web.      </content></document><document><year>2006</year><authors>Zakaria Maamar1 | Hamdi Yahyaoui2 | Qusay H. Mahmoud3</authors><title>Dynamic management of UDDI registries in a wireless environment of web services: Concepts, architecture, operation, and deployment      </title><content>This paper presents mechanisms for the dynamic management of the content of several Universal Description, Discovery, and         Integration;(UDDI) registries. These mechanisms are deployed in the context of a wireless environment of Web services. By         content, it is meant the announcements of Web services that providers submit to a UDDI registry. Unlike other initiatives         in the Web services domain that consider a single UDDI registry and a wired communication infrastructure, this paper is concerned         with the fact that: several UDDI registries are deployed, there is no wired communication infrastructure between the UDDI         registries, and absence of a centralized component for coordinating the UDDI-registries. The solution presented integrates         users and software agents into what we call messengers. Initially, software agents reside in users&amp;#8217; mobile devices and cache         a description of the Web services that satisfy their users&amp;#8217; needs. Each time a user is in the vicinity of a UDDI registry,         her software agent interacts with that registry, so the details stored on Web services are submitted.      </content></document><document><year>2006</year><authors>Didier Dubois1| Eyke Hllermeier2  | Henri Prade1</authors><title>Fuzzy methods for case-based recommendation and decision support      </title><content>The paper proposes two case-based methods for recommending decisions to users on the basis of information stored in a database.         In both approaches, fuzzy sets and related (approximate) reasoning techniques are used for modeling user preferences and decision         principles in a flexible manner. The first approach, case-based decision making, can principally be seen as a case-based counterpart         to classical decision principles well-known from statistical decision theory. The second approach, called case-based elicitation,         combines aspects from flexible querying of databases and case-based prediction. Roughly, imagine a user who aims at choosing         an optimal alternative among a given set of options. The preferences with respect to these alternatives are formalized in         terms of flexible constraints, the expression of which refers to cases stored in a database. As both types of decision support         might provide useful tools for recommender systems, we also place the methods in a broader context and discuss the role of         fuzzy set theory in some related fields.      </content></document><document><year>2006</year><authors>Patrice Buche1 | Juliette Dibie-Barthlemy1 | Ollivier Haemmerl2  | Galle Hignette1 </authors><title>Fuzzy semantic tagging and flexible querying of XML documents extracted from the Web      </title><content>The relational database model is widely used in real applications. We propose a way of complementing such a database with         an XML data warehouse. The approach we propose is generic, and driven by a domain ontology. The XML data warehouse is built         from data extracted from the Web, which are semantically tagged using terms belonging to the domain ontology. The semantic         tagging is fuzzy, since, instead of tagging the values of the Web document with one value of the domain ontology, we propose         to use tags expressed in terms of a possibility distribution representing a set of possible terms, each term being weighted         by a possibility degree. The querying of the XML data warehouse is also fuzzy: the end-users can express their preferences         by means of fuzzy selection criteria. We present our approach on a first application domain: predictive microbiology.      </content></document><document><year>2006</year><authors>Jun Du1 | Reda Alhajj1| 2  | Ken Barker1 </authors><title>Genetic algorithms based approach to database vertical partition      </title><content>Vertical partition clusters attributes of a relation to generate fragments suitable for subsequent allocation over a distributed         platform with the goal of improving performance. Vertical partition is an optimization problem that can resort to genetic         algorithms (GA). However, the performance of the classical GA application to vertical partition as well as to similar problems         such as clustering and grouping suffers from two major drawbacks&amp;#8212;redundant encoding and non-group oriented genetic operations.         This paper applies the restricted growth (RG) string Ruskey (1993) constraint to manipulate the chromosomes so that redundant         chromosomes are excluded during the GA process. On RG string compliant chromosomes, the group oriented crossover and mutation         become realizable. We thus propose a novel approach called Group oriented Restricted Growth String GA (GRGS-GA) which incorporates the two above features. Finally, we compare the proposed approach with a rudimental RG string         based approach and a classical GA based approach. The conducted experiments demonstrate a significant improvement of GRGS-GA         on partition speed and result, especially for large size vertical partition problems.      </content></document><document><year>2006</year><authors>Jarek Gryz1  | Dongming Liang1</authors><title>Holes in joins      </title><content>A join of two relations in real databases is usually much smaller than their Cartesian product. This means that most of the         combinations of tuples in the crossproduct of the respective relations do not appear together in the join result. We characterize         these combinations as ranges of attributes that do not appear together. We sketch an algorithm for finding such combinations         and present experimental results from real data sets. We then explore two potential applications of this knowledge in query         processing. In the first application, we model empty joins as materialized views, we show how they can be used for query optimization.         In the second application, we propose a strategy that uses information about empty joins for an improved join selectivity         estimation.      </content></document><document><year>2006</year><authors>Xintao Wu1 </authors><title>Incorporating large unlabeled data to enhance EM classification      </title><content>This paper investigates the problem of augmenting labeled data with unlabeled data to improve classification accuracy. This         is significant for many applications such as image classification where obtaining classification labels is expensive, while         large unlabeled examples are easily available. We investigate an Expectation Maximization (EM) algorithm for learning from         labeled and unlabeled data. The reason why unlabeled data boosts learning accuracy is because it provides the information         about the joint probability distribution. A theoretical argument shows that the more unlabeled examples are combined in learning,         the more accurate the result. We then introduce B-EM algorithm, based on the combination of EM with bootstrap method, to exploit         the large unlabeled data while avoiding prohibitive I/O cost. Experimental results over both synthetic and real data sets         show that the proposed approach has a satisfactory performance.      </content></document><document><year>2006</year><authors>Henning Christiansen | Moh|-Said Hacid</authors><title>Introduction      </title><content>Without Abstract</content></document><document><year>2006</year><authors>Salvatore Rinzivillo1  | Franco Turini1 </authors><title>Knowledge discovery from spatial transactions      </title><content>We propose a general mechanism to represent the spatial transactions in a way that allows the use of the existing data mining         methods. Our proposal allows the analyst to exploit the layered structure of geographical information systems in order to         define the layers of interest and the relevant spatial relations among them. Given a reference object, it is possible to describe         its neighborhood by considering the attribute of the object itself and the objects related by the chosen relations. The resulting         spatial transactions may be either considered like &amp;#8220;traditional&amp;#8221; transactions, by considering only the qualitative spatial         relations, or their spatial extension can be exploited during the data mining process. We explore both these cases. First         we tackle the problem of classifying a spatial dataset, by taking into account the spatial component of the data to compute         the statistical measure (i.e., the entropy) necessary to learn the model. Then, we consider the task of extracting spatial         association rules, by focusing on the qualitative representation of the spatial relations. The feasibility of the process         has been tested by implementing the proposed method on top of a GIS tool and by analyzing real world data.      </content></document><document><year>2006</year><authors>Yonghong Tian1| 2 | Tiejun Huang1| 2  | Wen Gao1| 2 </authors><title>Latent linkage semantic kernels for collective classification of link data      </title><content>Generally, links among objects demonstrate certain patterns and contain rich semantic clues. These important clues can be         used to improve classification accuracy. However, many real-world link data may exhibit more complex regularity. For example,         there may be some noisy links that carry no human editorial endorsement about semantic relationships. To effectively capture         such regularity, this paper proposes latent linkage semantic kernels (LLSKs) by first introducing the linkage kernels to model         the local and global dependency structure of a link graph and then applying the singular value decomposition (SVD) in the         kernel-induced space. For the computational efficiency on large datasets, we also develop a block-based algorithm for LLSKs.         A kernel-based contextual dependency network (KCDN) model is then presented to exploit the dependencies in a network of objects         for collective classification. We provide experimental results demonstrating that the KCDN model, together with LLSKs, demonstrates         relatively high robustness on the datasets with the complex link regularity, and the block-based computation method can scale         well with varying sizes of the problem.      </content></document><document><year>2006</year><authors>John Grant1  | Anthony Hunter2 </authors><title>Measuring inconsistency in knowledgebases      </title><content>It is well-known that knowledgebases may contain inconsistencies. We provide a measure to quantify the inconsistency of a         knowledgebase, thereby allowing for the comparison of the inconsistency of various knowledgebases, represented as first-order         logic formulas. We use quasi-classical (QC) logic for this purpose. QC logic is a formalism for reasoning and analysing inconsistent         information. It has been used as the basis of a framework for measuring inconsistency in propositional theories. Here we extend         this framework, by using a first-order logic version of QC logic for measuring inconsistency in first-order theories. We motivate         the QC logic approach by considering some formulae as database or knowledgebase integrity constraints. We then define a measure         of extrinsic inconsistency that can be used to compare the inconsistency of different knowledgebases. This measure takes into         account both the language used and the underlying domain. We show why this definition also captures the intrinsic inconsistency         of a knowledgebase. We also provide a formalization of paraconsistent equality, called quasi-equality, and we use this in         an extended example of an application for measuring inconsistency between heterogeneous sources of information and integrity         constraints prior to merging.      </content></document><document><year>2006</year><authors>Boris Galitsky1 </authors><title>Merging deductive and inductive reasoning for processing textual descriptions of inter-human conflicts      </title><content>We report on a novel approach to modeling a dynamic domain with limited knowledge. A domain may include participating agents         where we are uncertain about motivations and decision-making principles of some of these agents. Our reasoning setting for         such domains includes deductive, inductive, and abductive components. The deductive component is based on situation calculus         and describes the behavior of agents with complete information. The machine learning-based inductive and abductive components         involve the previous experience with the agents, whose actions are uncertain to the system. Suggested reasoning machinery         is applied to the problem of processing customer complaints in the form of textual messages that contain a multiagent conflict.         The task is to predict the future actions of an opponent agent to determine the required course of action to resolve a multiagent         conflict. This study demonstrates that the hybrid reasoning approach outperforms both stand-alone deductive and inductive         components. Suggested methodology reflects the general situation of reasoning in dynamic domains in the conditions of uncertainty,         merging analytical (rule-based) and analogy-based reasoning.      </content></document><document><year>2006</year><authors>Irene Pekerskaya1 | Jian Pei1  | Ke Wang1 </authors><title>Mining changing regions from access-constrained snapshots: a cluster-embedded decision tree approach      </title><content>Change detection on spatial data is important in many applications, such as environmental monitoring. Given a set of snapshots         of spatial objects at various temporal instants, a user may want to derive the changing regions between any two snapshots.         Most of the existing methods have to use at least one of the original data sets to detect changing regions. However, in some         important applications, due to data access constraints such as privacy concerns and limited data online availability, original         data may not be available for change analysis. In this paper, we tackle the problem by proposing a simple yet effective model-based         approach. In the model construction phase, data snapshots are summarized using the novel cluster-embedded decision trees as concise models. Once the models are built, the original data snapshots will not be accessed anymore. In the change detection         phase, to mine changing regions between any two instants, we compare the two corresponding cluster-embedded decision trees.         Our systematic experimental results on both real and synthetic data sets show that our approach can detect changes accurately         and effectively.      </content></document><document><year>2006</year><authors>Alice Marascu1  | Florent Masseglia1</authors><title>Mining sequential patterns from data;streams: a centroid approach      </title><content>In recent years, emerging applications introduced new constraints for data mining methods. These constraints are typical of         a new kind of data: the data streams. In data stream processing, memory usage is restricted, new elements are generated continuously and have to be considered         in a linear time, no blocking operator can be performed and the data can be examined only once. At this time, only a few methods         has been proposed for mining sequential patterns in data streams. We argue that the main reason is the combinatory phenomenon         related to sequential pattern mining. In this paper, we propose an algorithm based on sequences alignment for mining approximate         sequential patterns in Web usage data streams. To meet the constraint of one scan, a greedy clustering algorithm associated         to an alignment method is proposed. We will show that our proposal is able to extract relevant sequences with very low thresholds.      </content></document><document><year>2006</year><authors>Gennady Andrienko1| Donato Malerba2 | Michael May1 | Maguelonne Teisseire3</authors><title>Mining spatio-temporal data      </title><content>Without Abstract</content></document><document><year>2006</year><authors>Xing Wu1| 2 | Jin Chen1 | Ruqiang Li1| 3 | Weixiang Sun1 | Guicai Zhang1  | Fucai Li1 </authors><title>Modeling a web-based remote monitoring and fault diagnosis system with UML and component technology      </title><content>As the Machinery Condition Monitoring and Fault Diagnosis Systems (MCMFDSs) are more and more complex, the design and development         of these systems are becoming a challenge. The best way to manage the complexity and risk is to abstract and model them. This         paper presents a new method of modeling Web-based Remote Monitoring and Fault Diagnosis Systems (WRMFDSs) with Unified Modeling         Language (UML). A component framework model is put forward. A highly maintainable WRMFDS with three reusable component packages         was developed using component-based programming. This paper, which studies a reusable WRMFDS model, aims at making such advanced         information technologies be used widely in the condition monitoring and fault diagnosis domain, it can give developers a paradigm         to accomplish the similar systems.      </content></document><document><year>2006</year><authors>Hewijin Christine Jiau1 | Yi-Jen Su1 | Yeou-Min Lin1  | Shang-Rong Tsai1 </authors><title>MPM: a hierarchical clustering algorithm using matrix partitioning method for non-numeric data      </title><content>The Publisher regrets that the original article incorrectly listed the authors&amp;#8217; location as the &amp;#8220;People&amp;#8217;s Republic of China&amp;#8221;.         This was a typesetting error. Their correct location is the &amp;#8220;Republic of China&amp;#8221;, which is also known as Taiwan. Their full         affiliation is Department of Electrical Engineering, National Cheng Kung University, Tainan, Taiwan.      </content></document><document><year>2006</year><authors>Hewijin Christine Jiau1 | Yi-Jen Su1 | Yeou-Min Lin1  | Shang-Rong Tsai1 </authors><title>MPM: a hierarchical clustering algorithm using matrix partitioning method for non-numeric data      </title><content>Clustering has been widely adopted in numerous applications, including pattern recognition, data analysis, image processing,         and market research. When performing data mining, traditional clustering algorithms which use distance-based measurements         to calculate the difference between data are unsuitable for non-numeric attributes such as nominal, Boolean, and categorical         data. Applying an unsuitable similarity measurement in clustering may cause some valuable information embedded in the data         attributes to be lost, and hence low quality clusters will be created. This paper proposes a novel hierarchical clustering         algorithm, referred to as MPM, for the clustering of non-numeric data. The goals of MPM are to retain the data features of         interest while effectively grouping data objects into clusters with high intra-similarity and low inter-similarity. MPM achieves         these goals through two principal methods: (1) the adoption of a novel similarity measurement which has the ability to capture         the &amp;#8220;characterized properties&amp;#8221; of information, and (2) the application of matrix permutation and matrix participation partitioning         to the results of the similarity measurement (constructed in the form of a similarity matrix) in order to assign data to appropriate         clusters. This study also proposes a heuristic-based algorithm, the Heuristic_MPM, to reduce the processing times required         for matrix permutation and matrix partitioning, which together constitute the bulk of the total MPM execution time.      </content></document><document><year>2006</year><authors>W. A. Voglozin1 | G. Raschia1 | L. Ughetto1  | N. Mouaddib1 </authors><title>Querying a summary of database      </title><content>For some years, data summarization techniques have been developed to handle the growth of databases. However these techniques         are usually not provided with tools for end-users to efficiently use the produced summaries. This paper presents a first attempt         to develop a querying tool for the SAINTETIQ summarization model. The proposed search algorithm takes advantage of the hierarchical         structure of the SAINTETIQ summaries to efficiently answer questions such as &amp;#8220;how are, on some attributes, the tuples which         have specific characteristics?&amp;#8221; Moreover, this algorithm can be seen both as a boolean querying mechanism over a hierarchy         of summaries, and as a flexible querying mechanism over the underlying relational tuples.      </content></document><document><year>2006</year><authors>Michelangelo Ceci1  | Annalisa Appice1 </authors><title>Spatial associative classification: propositional vs structural approach      </title><content>In Spatial Data Mining, spatial dimension adds a substantial complexity to the data mining task. First, spatial objects are         characterized by a geometrical representation and relative positioning with respect to a reference system, which implicitly         define both spatial relationships and properties. Second, spatial phenomena are characterized by autocorrelation, i.e., observations         of spatially distributed random variables are not location-independent. Third, spatial objects can be considered at different         levels of abstraction (or granularity). The recently proposed SPADA algorithm deals with all these sources of complexity,         but it offers a solution for the task of spatial association rules discovery. In this paper the problem of mining spatial         classifiers is faced by building an associative classification framework on SPADA. We consider two alternative solutions for         associative classification: a propositional and a structural method. In the former, SPADA obtains a propositional representation of training data even in spatial domains which are inherently         non-propositional, thus allowing the application of traditional data mining algorithms. In the latter, the Bayesian framework         is extended following a multi-relational data mining approach in order to cope with spatial classification tasks. Both methods         are evaluated and compared on two real-world spatial datasets and results provide several empirical insights on them.      </content></document><document><year>2006</year><authors>Diansheng Guo1  | Mark Gahegan2 </authors><title>Spatial ordering and encoding for geographic data mining and visualization      </title><content>Geographic information (e.g., locations, networks, and nearest neighbors) are unique and different from other aspatial attributes         (e.g., population, sales, or income). It is a challenging problem in spatial data mining and visualization to take into account         both the geographic information and multiple aspatial variables in the detection of patterns. To tackle this problem, we present         and evaluate a variety of spatial ordering methods that can transform spatial relations into a one-dimensional ordering and         encoding which preserves spatial locality as much possible. The ordering can then be used to spatially sort temporal or multivariate         data series and thus help reveal patterns across different spaces. The encoding, as a materialization of spatial clusters         and neighboring relations, is also amenable for processing together with aspatial variables by any existing (non-spatial)         data mining methods. We design a set of measures to evaluate nine different ordering/encoding methods, including two space-filling         curves, six hierarchical clustering based methods, and a one-dimensional Sammon mapping (a multidimensional scaling approach).         Evaluation results with various data distributions show that the optimal ordering/encoding with the complete-linkage clustering         consistently gives the best overall performance, surpassing well-known space-filling curves in preserving spatial locality.         Moreover, clustering-based methods can encode not only simple geographic locations, e.g., x and y coordinates, but also a wide range of other spatial relations, e.g., network distances or arbitrarily weighted graphs.      </content></document><document><year>2006</year><authors>Mirco Nanni1  | Dino Pedreschi2 </authors><title>Time-focused clustering of trajectories of moving objects      </title><content>Spatio-temporal, geo-referenced datasets are growing rapidly, and will be more in the near future, due to both technological         and social/commercial reasons. From the data mining viewpoint, spatio-temporal trajectory data introduce new dimensions and,         correspondingly, novel issues in performing the analysis tasks. In this paper, we consider the clustering problem applied         to the trajectory data domain. In particular, we propose an adaptation of a density-based clustering algorithm to trajectory         data based on a simple notion of distance between trajectories. Then, a set of experiments on synthesized data is performed         in order to test the algorithm and to compare it with other standard clustering approaches. Finally, a new approach to the         trajectory clustering problem, called temporal focussing, is sketched, having the aim of exploiting the intrinsic semantics of the temporal dimension to improve the quality of trajectory         clustering.      </content></document><document><year>2006</year><authors>Chedy Rassi1| 2 | Pascal Poncelet1  | Maguelonne Teisseire2 </authors><title>Towards a new approach for mining frequent itemsets on data stream      </title><content>Mining frequent patterns on streaming data is a new challenging problem for the data mining community since data arrives sequentially         in the form of continuous rapid streams. In this paper we propose a new approach for mining itemsets. Our approach has the         following advantages: an efficient representation of items and a novel data structure to maintain frequent patterns coupled         with a fast pruning strategy. At any time, users can issue requests for frequent itemsets over an arbitrary time interval.         Furthermore our approach produces an approximate answer with an assurance that it will not bypass user-defined frequency and         temporal thresholds. Finally the proposed method is analyzed by a series of experiments on different datasets.      </content></document><document><year>2006</year><authors>Yaoyong Li1  | John Shawe-Taylor2</authors><title>Using KCCA for Japanese&amp;#8211;English cross-language information retrieval and document classification      </title><content>Kernel Canonical Correlation Analysis (KCCA) is a method of correlating linear relationship between two variables in a kernel         defined feature space. A machine learning algorithm based on KCCA is studied for cross-language information retrieval. We         apply the algorithm in Japanese&amp;#8211;English cross-language information retrieval. The results are quite encouraging and are significantly         better than those obtained by other state of the art methods. Computational complexity is an important issue when applying         KCCA to large dataset as in information retrieval. We experimentally evaluate several methods to alleviate the problem of         applying KCCA to large datasets. We also investigate cross-language document classification using KCCA as well as other methods.         Our results show that it is feasible to use a classifier learned in one language to classify the documents in other languages.      </content></document><document><year>2007</year><authors>William K. Michener1 | James H. Beach2 | Matthew B. Jones3 | Bertram Ludscher4 | Deana D. Pennington1 | Ricardo S. Pereira5 | Arcot Rajasekar4  | Mark Schildhauer3 </authors><title>A knowledge environment for the biodiversity and ecological sciences      </title><content>The Science Environment for Ecological Knowledge (SEEK) is a knowledge environment that is being developed to address many         of the current challenges associated with data accessibility and integration in the biodiversity and ecological sciences.         The SEEK information technology infrastructure encompasses three integrated systems: (1) EcoGrid&amp;#8212;an open architecture for         data access; (2) a Semantic Mediation System based on domain-specific ontologies; and (3) an Analysis and Modeling System         that supports semantically integrated analytical workflows. Multidisciplinary scientists and programmers from multiple institutions         comprise the core development team. SEEK design and development are informed by three multidisciplinary teams of scientists         organized in Working Groups. The Biodiversity and Ecological Analysis and Modeling Working Group informs development through         evaluation of SEEK efficacy in addressing biodiversity and ecological questions. The Knowledge Representation Working Group         provides knowledge representation requirements from the domain sciences and develops the corresponding knowledge representations         (ontologies) to support the assembly of analytical workflows in the Analysis and Modeling System, and the intelligent data         and service discovery in the EcoGrid. A Biological Classification and Nomenclature Working Group investigates solutions to         mediating among multiple taxonomies for naming organisms. A multifaceted education, outreach and training program ensures         that the SEEK research products, software, and information technology infrastructure optimally benefit the target communities.      </content></document><document><year>2007</year><authors>Young-In Song1 | Kyoung-Soo Han2 | Sang-Bum Kim2 | So-Young Park3  | Hae-Chang Rim1 </authors><title>A novel retrieval approach reflecting variability of syntactic phrase representation      </title><content>In this paper, we introduce variability of syntactic phrases and propose a new retrieval approach reflecting the variability of syntactic phrase representation.         With variability measure of a phrase, we can estimate how likely a phrase in a given query would appear in relevant documents and control         the impact of syntactic phrases in a retrieval model. Various experimental results over different types of queries and document         collections show that our retrieval model based on variability of syntactic phrases is very effective in terms of retrieval         performance, especially for long natural language queries.      </content></document><document><year>2007</year><authors>Sathish Govindarajan1 | Michael C. Dietze2 | Pankaj K. Agarwal3  | James S. Clark4 </authors><title>A scalable algorithm for dispersing population      </title><content>Models of forest ecosystems are needed to understand how climate and land-use change can impact biodiversity. In this paper         we describe an ecological dispersal model developed for the specific case of predicting seed dispersal by trees on a landscape         for use in a forest simulation model. We present efficient approximation algorithms for computing seed dispersal. These algorithms         allow us to simulate large landscapes for long periods of time. We also present experimental results that (1) quantify the         inherent uncertainty in the dispersal model and  (2) describe the variation of the approximation error as a function of the         approximation parameters. Based on these experiments, we provide guidelines for choosing the right approximation parameters,         for a given model simulation.      </content></document><document><year>2007</year><authors>Ferdin|o Villa1 </authors><title>A semantic framework and software design to enable the transparent integration, reorganization and discovery of natural systems         knowledge      </title><content>I present a conceptualization that attempts to unify diverse representations of natural knowledge while providing a workable         computational framework, based on current semantic web theory, for developing, communicating, and running integrated simulation         models. The approach is based on a long-standing principle of scientific investigation: the separation of the ontological         character of the object of study from the semantics of the observation context, the latter including location in space and time and other observation-related aspects. I will show how current Knowledge         Representation theories coupled with the object-oriented paradigm allow an efficient integration through the abstract model         of a domain, which relates to the idea of aspect in software engineering. This conceptualization allows us to factor out two fundamental causes of complexity and awkwardness         in the representation of knowledge about natural system: (a) the distinction between data and models, both seen here as generic         knowledge sources; (b) the multiplicity of states in data sources, handled through the hierarchical composition of independently         defined domain objects, each accounting for all states in one well-known observational dimension. This simplification leaves         modelers free to work with the bare conceptual bones of the problem, encapsulating complexities connected to data format,         and scale. I will then describe the design of a software system that implements the approach, referring to explicit ontologies         to unambiguously characterize the semantics of the objects of study, and allowing the independent definition of a global observation context that can be redefined as required. I will briefly discuss applications to multi-scale, multi-paradigm modeling, intelligent         database design, and web-based collaboration.      </content></document><document><year>2007</year><authors>P. Bosc1 | O. Pivert1  | D. Rocacher1 </authors><title>About quotient and division of crisp and fuzzy relations      </title><content>The role and properties of the division are very well known in the context of queries addressed to regular relational databases.         However, Boolean queries whose result is expressed in terms of all or nothing may turn out to be too limited to answer certain         user needs and it is desirable to envisage extended queries by introducing preferences in the conditions. In this paper, two         lines of extension of the division operator are studied: (i) operand relations of the division are fuzzy relations (i.e.,         they are made of weighted tuples) and (ii) the universal quantifier underlying the division is weakened. Various approaches         to these extensions can be considered and one of our goals is to point out those which ensure that the resulting relation         is a quotient (in reference to the characterization of the quotient of two integers). So doing, a sound semantics for the         extended division is guaranteed.      </content></document><document><year>2007</year><authors>Wooju Kim1 | Dae Woo Choi2  | Sangun Park3 </authors><title>Agent based intelligent search framework for product information using ontology mapping      </title><content>The Semantic Web and Web services provide many opportunities in various applications such as product search and comparison         in electronic commerce. We implemented an intelligent meta-search and recommendation system for products through consideration         of multiple attributes by using ontology mapping and Web services. Under the assumption that each shopping site offers product         ontology and product search service with Web services, we proposed a meta-search framework to configure a customer&amp;#8217;s search         intent, make and dispatch proper queries to each shopping site, evaluate search results from shopping sites, and show the         customer the relevant product list with associated rankings. Ontology mapping is used for generating proper queries for shopping         sites that have different product categories. We also implemented our framework and performed empirical evaluation of our         approach with two leading shopping sites in the world.      </content></document><document><year>2007</year><authors>Alex Spokoiny1  | Yuval Shahar1 </authors><title>An active database architecture for knowledge-based incremental abstraction of complex concepts from continuously arriving         time-oriented raw data      </title><content>An effective solution to the tasks of continuous monitoring and aggregation querying of complex domain-meaningful concepts and patterns in environments featuring large continuously changing data sets is very important for many domains. Typical domains include: making financial decisions, integrating intelligence information         from multiple sources, evaluating the effects of traffic controllers&amp;#8217; actions, detection of security threats in communication         networks, planning and monitoring in robotics, and management of chronic patients in medical domains. In this paper, we present         a general domain-independent method for an effective solution of these two tasks. Our method involves incremental creation         of meaningful, interval-based abstractions, from raw, time-stamped data continuously arriving from multiple sources, which         is supported by the accumulation and continuous validation of the created abstractions. We implemented our method in the Momentum system, which is an active knowledge-based time-oriented database&amp;#8212;a temporal extension of the active-database concept that we propose for incremental application of knowledge to continuously         arriving time-oriented data. We evaluated the Momentum system in a medical domain within a database of 1,000 patients monitored         after bone-marrow transplantation, and a knowledge base of complex abstractions regarding more than 100 raw-data types and         about 400 concept types derivable from them. Initial evaluations are highly encouraging with regards to the feasibility of         the whole approach.      </content></document><document><year>2007</year><authors>Vasileios Megalooikonomou1| 2| 6 | Despina Kontos1| 2| 6| Dragoljub Pokrajac3| 4| Aleks|ar Lazarevic5 | Zoran Obradovic2| 6</authors><title>An adaptive partitioning approach for mining discriminant regions in 3D image data      </title><content>Mining discriminative spatial patterns in image data is an emerging subject of interest in medical imaging, meteorology, engineering,         biology, and other fields. In this paper, we propose a novel approach for detecting spatial regions that are highly discriminative         among different classes of three dimensional (3D) image data. The main idea of our approach is to treat the initial 3D image         as a hyper-rectangle and search for discriminative regions by adaptively partitioning the space into progressively smaller         hyper-rectangles (sub-regions). We use statistical information about each hyper-rectangle to guide the selectivity of the         partitioning. A hyper-rectangle is partitioned only if its attribute cannot adequately discriminate among the distinct labeled         classes, and it is sufficiently large for further splitting. To evaluate the discriminative power of the attributes corresponding         to the detected regions, we performed classification experiments on artificial and real datasets. Our results show that the         proposed method outperforms major competitors, achieving 30% and 15% better classification accuracy on synthetic and real         data respectively while reducing by two orders of magnitude the number of statistical tests required by voxel-based approaches.      </content></document><document><year>2007</year><authors>Robert A. Morris1 | Robert D. Stevenson2 | William Haber3</authors><title>An architecture for electronic field guides      </title><content>People who classify and identify things based on their observable or deducible properties (called &amp;#8220;characters&amp;#8221; by biologists)         can benefit from databases and keys that assist them in naming a specimen. This paper discusses our approach to generating         an identification tool based on the field guide concept. Our software accepts character lists either expressed as XML (which         biologists rarely provide knowingly&amp;#8212;although most databases can now export in XML) or via ODBC connections to the data author&amp;#8217;s         relational database. The software then produces an Electronic Field Guide (EFG) implemented as a collection of Java servlets.         The resulting guide answers queries made locally to a backend, or to Internet data sources via http, and returns XML. If,         however, the query client requires HTML (e.g., if the EFG is responding to a human-centric browser interface that we or the         remote application provides), or if some specialized XML is required, then the EFG forwards the XML to a servlet that applies         an XSLT transformation to provide the look and feel that the client application requires. We compare our approach to the architecture         of other taxon identification tools. Finally, we discuss how we combine this service with other biodiversity data services         on the web to make integrated applications.      </content></document><document><year>2007</year><authors>Hassan Artail1  | Michel Abi-Aad1</authors><title>An enhanced Web page change detection approach based on limiting similarity computations to elements of same type      </title><content>This paper describes an efficient Web page detection approach based on restricting the similarity computations between two         versions of a given Web page to the nodes with the same HTML tag type. Before performing the similarity computations, the         HTML Web page is transformed into an XML-like structure in which a node corresponds to an open-closed HTML tag. Analytical         expressions and supporting experimental results are used to quantify the improvements that are made when comparing the proposed         approach to the traditional one, which computes the similarities across all nodes of both pages. It is shown that the improvements         are highly dependent on the diversity of tags in the page. That is, the more diverse the page is (i.e., contains mixed content         of text, images, links, etc.), the greater the improvements are, while the more uniform it is, the lesser they are.      </content></document><document><year>2007</year><authors>Xiaochun Yang1  | Yiu-Kai Ng2 </authors><title>Answering form-based web queries using the data-mining approach      </title><content>Web users often post queries through form-based interfaces on the Web to retrieve data from the Web; however, answers to these         queries are mostly computed according to keywords entered into different fields specified in a query interface, and their         precision and recall could be low. The precision and recall ratios in answering this type of query can be improved by considering         closely related previous queries submitted through the same interface, along with their answers. In this paper, we present         an approach for enhancing the retrieval of relevant answers to a form-based Web query by adopting the data-mining approach         using previous, relevant queries and their answers. Experimental results on a randomly selected set of 3,800 documents retrieved         from various Web sites show that our data-mining, query-rewriting approach achieves average precision and true positive ratios         on rewritten queries in the upper 80% range, whereas the average false positive ratio is less than 2.0%.      </content></document><document><year>2007</year><authors>Alfredo Cuzzocrea1  | Wei Wang2 </authors><title>Approximate range&amp;#8211;sum query answering on data cubes with probabilistic guarantees      </title><content>Approximate range aggregate queries are one of the most frequent and useful kinds of queries for Decision Support Systems         (DSS), as they are widely used in many data analysis tasks. Traditionally, sampling-based techniques have been proposed to         tackle this problem. However, their effectiveness degrade when the underlying data distribution is skewed. Another approach         based on the outlier management can limit the effect of data skews but fails to address other requirements of approximate         range aggregate queries, such as error guarantees and query processing efficiency. In this paper, we present a technique that         provides approximate answers to range aggregate queries on OLAP data cubes efficiently, with theoretical guarantees on the         errors. Our basic idea is to build different data structures to manage outliers and the rest of the data. Carefully chosen         outliers are organized in a quad-tree based indexing data structure to provide efficient access for query processing. A query-workload adaptive, tree-like synopsis data structure, called T         unable         P         artition-Tree (TP-Tree), is proposed to organize samples extracted from non-outlier data. Our experiments clearly demonstrate the merits of our         technique, by comparing with previous well-known techniques.      </content></document><document><year>2007</year><authors>Akinari Yamaguchi1| Shougo Shimizu2 | Yasunori Ishihara1  | Toru Fujiwara1 </authors><title>Bag-based data models for incomplete information and their closure properties      </title><content>In most practical database applications, incompleteness and duplication of facts should be carefully handled. We propose bag-based         data models for incomplete information, called V-bags, CV-bags, GV-bags, and CGV-bags. In V-bags, incompleteness is represented         by variables like C-tables by Imielinski and Lipski. GV-bags are a supermodel of V-bags, where global conditions that restrict         assignments over variables are attached. CV-bags and CGV-bags are submodels of V-bags and GV-bags, respectively, where the         usage of variables for representing the number of duplication of tuples is somewhat restricted. We also investigate the closure         properties of forward and inverse algebraic operations (selection, projection, product, union, difference, and unique) on         each of the data models under both CWA and OWA. Among these data models, CGV-bags have the most closed operations.      </content></document><document><year>2007</year><authors>Estevam R. Hruschka Jr.1 | Eduardo R. Hruschka2 | Nelson F. F. Ebecken3</authors><title>Bayesian networks for imputation in classification problems      </title><content>Missing values are an important problem in data mining. In order to tackle this problem in classification tasks, we propose         two imputation methods based on Bayesian networks. These methods are evaluated in the context of both prediction and classification         tasks. We compare the obtained results with those achieved by classical imputation methods (Expectation&amp;#8211;Maximization, Data         Augmentation, Decision Trees, and Mean/Mode). Our simulations were performed by means of four datasets (Congressional Voting         Records, Mushroom, Wisconsin Breast Cancer and Adult), which are benchmarks for data mining methods. Missing values were simulated         in these datasets by means of the elimination of some known values. Thus, it is possible to assess the prediction capability         of an imputation method, comparing the original values with the imputed ones. In addition, we propose a methodology to estimate         the bias inserted by imputation methods in classification tasks. In this sense, we use four classifiers (One Rule, Nave Bayes,         J4.8 Decision Tree and PART) to evaluate the employed imputation methods in classification scenarios. Computing times consumed         to perform imputations are also reported. Simulation results in terms of prediction, classification, and computing times allow         us performing several analyses, leading to interesting conclusions. Bayesian networks have shown to be competitive with classical         imputation methods.      </content></document><document><year>2007</year><authors>John L. Schnase1 | Judy Cushing2  | James A. Smith1 </authors><title>Biodiversity and ecosystem informatics      </title><content>The field of Biodiversity and Ecosystem Informatics (BDEI) brings together computer scientists, biologists, natural resource         managers, and others who wish to solve real-world challenges while advancing the underlying ecological, computer, and information         sciences. The potential for synergies among these disciplines is high, because our need to understand complex, ecosystem-scale         processes requires the solution to many groundbreaking technological problems. Fortunately, we are beginning to see increased         support for applied computer science and information technology research in the context of environmental problem-solving.         In July, 2001, the National Science Foundation (NSF), in collaboration with the United States Geological Survey (USGS), and         the National Aeronautics and Space Administration (NASA), invited proposals for high-risk, small-scale planning and incubation         activities to catalyze innovation and rapid advances in this new research community. The papers included in this special issue         are selected, peer-reviewed summaries from principal investigators involved in this first NSF BDEI effort. These papers provide         an overview of this emerging area and remind us that computer and information science and engineering play a crucial role         in creating the technologies from which advances in the natural sciences evolve.      </content></document><document><year>2007</year><authors>Michelangelo Ceci1  | Donato Malerba1 </authors><title>Classifying web documents in a hierarchy of categories: a comprehensive study      </title><content>Most of the research on text categorization has focused on classifying text documents into a set of categories with no structural         relationships among them (flat classification). However, in many information repositories documents are organized in a hierarchy         of categories to support a thematic search by browsing topics of interests. The consideration of the hierarchical relationship         among categories opens several additional issues in the development of methods for automated document classification. Questions         concern the representation of documents, the learning process, the classification process and the evaluation criteria of experimental         results. They are systematically investigated in this paper, whose main contribution is a general hierarchical text categorization         framework where the hierarchy of categories is involved in all phases of automated document classification, namely feature         selection, learning and classification of a new document. An automated threshold determination method for classification scores         is embedded in the proposed framework. It can be applied to any classifier that returns a degree of membership of a document         to a category. In this work three learning methods are considered for the construction of document classifiers, namely centroid-based,         nave Bayes and SVM. The proposed framework has been implemented in the system WebClassIII and has been tested on three datasets         (Yahoo, DMOZ, RCV1) which present a variety of situations in terms of hierarchical structure. Experimental results are reported         and several conclusions are drawn on the comparison of the flat vs. the hierarchical approach as well as on the comparison         of different hierarchical classifiers. The paper concludes with a review of related work and a discussion of previous findings         vs. our findings.      </content></document><document><year>2007</year><authors>Judith Bayard Cushing1 | Nalini Nadkarni1| Michael Finch3| Anne Fiala1| Emerson Murphy-Hill2| Lois Delcambre2 | David Maier2</authors><title>Component-based end-user database design for ecologists      </title><content>To solve today&amp;#8217;s ecological problems, scientists need well documented, validated, and coherent data archives. Historically,         however, ecologists have collected and stored data idiosyncratically, making data integration even among close collaborators         difficult. Further, effective ecology data warehouses and subsequent data mining require that individual databases be accurately         described with metadata against which the data themselves have been validated. Using database technology would make documenting         data sets for archiving, integration, and data mining easier, but few ecologists have expertise to use database technology         and they cannot afford to hire programmers. In this paper, we identify the benefits that would accrue from ecologists&amp;#8217; use         of modern information technology and the obstacles that prevent that use. We describe our prototype, the Canopy         DataBank, through which we aim to enable individual ecologists in the forest canopy research community to be their own database programmers.         The key feature that makes this possible is domain-specific database components, which we call templates. We also show how additional tools that reuse these components, such as for visualization, could provide gains in productivity         and motivate the use of new technology. Finally, we suggest ways in which communities might share database components and         how components might be used to foster easier data integration to solve new ecological problems.      </content></document><document><year>2007</year><authors>Antonio Arauzo-Azofra1 | Jose Manuel Benitez2  | Juan Luis Castro2 </authors><title>Consistency measures for feature selection      </title><content>The use of feature selection can improve accuracy, efficiency, applicability and understandability of a learning process.         For this reason, many methods of automatic feature selection have been developed. Some of these methods are based on the search         of the features that allows the data set to be considered consistent. In a search problem we usually evaluate the search states,         in the case of feature selection we measure the possible feature sets. This paper reviews the state of the art of consistency         based feature selection methods, identifying the measures used for feature sets. An in-deep study of these measures is conducted,         including the definition of a new measure necessary for completeness. After that, we perform an empirical evaluation of the         measures comparing them with the highly reputed wrapper approach. Consistency measures achieve similar results to those of         the wrapper approach with much better efficiency.      </content></document><document><year>2007</year><authors>Jian Pei1 | Jiawei Han2  | Wei Wang3 </authors><title>Constraint-based sequential pattern mining: the pattern-growth methods      </title><content>Constraints are essential for many sequential pattern mining applications. However, there is no systematic study on constraint-based sequential pattern mining. In this paper, we investigate this issue and point out that the framework developed for constrained frequent-pattern mining         does not fit our mission well. An extended framework is developed based on a sequential pattern growth methodology. Our study         shows that constraints can be effectively and efficiently pushed deep into the sequential pattern mining under this new framework.         Moreover, this framework can be extended to constraint-based structured pattern mining as well.      </content></document><document><year>2007</year><authors>Andreas Wichert1 </authors><title>Content-based image retrieval by hierarchical linear subspace method      </title><content>We describe a hierarchical linear subspace method to query large on-line image databases using image similarity as the basis         of the queries. The method is based on the generic multimedia indexing (GEMINI) approach which is used in the IBM query through         the image content search system. Our approach is demonstrated on image indexing, in which the subspaces correspond to different         resolutions of the images. During content-based image retrieval, the search starts in the subspace with the lowest resolution         of the images. In this subspace, the set of all possible similar images is determined. In the next subspace, additional metric         information corresponding to a higher resolution is used to reduce this set. This procedure is repeated until the similar         images can be determined. For evaluation we used three image databases and two different subspace sequences.      </content></document><document><year>2007</year><authors>Aviv Segev1 | Moshe Leshno2 | Moshe Zviran2</authors><title>Context recognition using internet as a knowledge base      </title><content>Context recognition is an important component of the common sense knowledge problem, which is one of the key research areas         in the field of Artificial Intelligence. The paper develops a model of context recognition using the Internet as a knowledge         base. The use of the Internet as a database for context recognition gives a context recognition model immediate access to         a nearly infinite amount of data in a multiplicity of fields. Context is represented here as any textual description that         is most commonly selected by a set of subjects to describe a given situation. The model input is based on any aspect of the         situation that can be translated into text (such as: voice recognition, image recognition, facial expression interpretation,         and smell identification). The research model is based on the streaming in text format of information that represents situations&amp;#8212;Internet         chats, e-mails, Shakespeare plays, or article abstracts. The comparison of the results of the algorithm with the results of         human subjects yielded a very high agreement and correlation. The results showed there was no significant difference in the         determination of context between the algorithm and the human subjects.      </content></document><document><year>2007</year><authors>G. Boccignone1 | A. Chianese2 | V. Moscato2  | A. Picariello2 </authors><title>Context-sensitive queries for image retrieval in digital libraries      </title><content>In this paper we show how to achieve a more effective Query By Example processing, by using active mechanisms of biological         vision, such as saccadic eye movements and fixations. In particular, we discuss the way to generate two fixation sequences         from a query image I                     q             and a test image I                     t             of the data set, respectively, and how to compare the two sequences in order to compute a similarity measure between the         two images. Meanwhile, we show how the approach can be used to discover and represent the hidden semantic associations among         images, in terms of categories, which in turn drive the query process.      </content></document><document><year>2007</year><authors>Robert A. Morris1 | Jacob K. Asiedu1| William A. Haber2| Fred SaintOurs1| Robert D. Stevenson1 | Hua Tang1</authors><title>Database-backed decision trees with application to biological informatics      </title><content>We describe a mechanism for the identification of biological organisms through the use of enhanced taxonomic keys-decision         trees with nodes augmented by property lists that can serve as arguments to web or local services that access databases or         other resources about species, specimens, and ecosystems. Authors of these identification schemes can use simple spreadsheet         tools to structure the identification abstractions, and middleware renders the resulting trees into many different forms,         with the databases possibly discovered and queried at the time an identification is proposed.      </content></document><document><year>2007</year><authors>Chung-Wen Cho1| Yi-Hung Wu2 | Arbee L. P. Chen3 </authors><title>Effective database transformation and efficient support computation for mining sequential patterns      </title><content>In this paper, we propose a novel algorithm for mining frequent sequences from transaction databases. The transactions of         the same customers form a set of customer sequences. A sequence (an ordered list of itemsets) is frequent if the number of customer sequences containing it satisfies the user-specified threshold. The 1-sequence is a special type of sequences because it consists of only a single itemset instead of an ordered list, while the k-sequence is a sequence composed of k itemsets. Compared with the cost of mining frequent k-sequences (k&amp;#8201;&amp;#8805;&amp;#8201;2), the cost of mining frequent 1-sequences is negligible. We adopt a two-phase architecture to find the two types of frequent         sequences separately in order that the discovery of frequent k-sequences can be well designed and optimized. For efficient frequent k-sequence mining, every frequent 1-sequence is encoded as a unique symbol and the database is transformed into one constituted         by the symbols. We find that it is unnecessary to encode all the frequent 1-seqences, and make full use of the discovered         frequent 1-sequences to transform the database into one with a smaller size. For every k&amp;#8201;&amp;#8805;&amp;#8201;2, the customer sequences in the transformed database are scanned to find all the frequent k-sequences. We devise the compact representation for a customer sequence and elaborate the method to enumerate all distinct         subsequences from a customer sequence without redundant scans. The soundness of the proposed approach is verified and a number         of experiments are performed. The results show that our approach outperforms the previous works in both scalability and execution         time.      </content></document><document><year>2007</year><authors>Tao Li1 | Shenghuo Zhu2  | Mitsunori Ogihara3 </authors><title>Hierarchical document classification using automatically generated hierarchy      </title><content>Automated text categorization has witnessed a booming interest with the exponential growth of information and the ever-increasing         needs for organizations. The underlying hierarchical structure identifies the relationships of dependence between different         categories and provides valuable sources of information for categorization. Although considerable research has been conducted         in the field of hierarchical document categorization, little has been done on automatic generation of topic hierarchies. In         this paper, we propose the method of using linear discriminant projection to generate more meaningful intermediate levels         of hierarchies in large flat sets of classes. The linear discriminant projection approach first transforms all documents onto         a low-dimensional space and then clusters the categories into hier- archies accordingly. The paper also investigates the effect         of using generated hierarchical structure for text classification. Our experiments show that generated hierarchies improve         classification performance in most cases.      </content></document><document><year>2007</year><authors>Alex Spokoiny1  | Yuval Shahar1 </authors><title>Incremental application of knowledge to continuously arriving time-oriented data      </title><content>In our previous work, we introduced a computational architecture that effectively supports the tasks of continuous monitoring         and of aggregation querying of complex domain meaningful time-oriented concepts and patterns (temporal abstractions), in environments featuring large volumes of continuously arriving and accumulating time-oriented raw data. Examples include         provision of decision support in clinical medicine, making financial decisions, detecting anomalies and potential threats         in communication networks, integrating intelligence information from multiple sources, etc. In this paper, we describe the         general, domain-independent but task-specific problem-solving method underling our computational architecture, which we refer         to as incremental knowledge-based temporal abstraction (IKBTA). The IKBTA method incrementally computes temporal abstractions by maintaining persistence and validity of continuously computed         temporal abstractions from arriving time-stamped data. We focus on the computational framework underlying our reasoning method,         provide well-defined semantic and knowledge requirements for incremental inference, which utilizes a logical model of time,         data, and high-level abstract concepts, and provide a detailed analysis of the computational complexity of our approach.      </content></document><document><year>2007</year><authors>Karen S. Baker1  | Geoffrey C. Bowker2 </authors><title>Information ecology: open system environment for data, memories, and knowing      </title><content>An information ecology provides a conceptual framework to consider data, the creation of knowledge, and the flow of information         within a multidimensional context. This paper, reporting on a 1;year project to study the heterogeneity of information and         its management within the Long Term Ecological Research (LTER) community, presents some manifestations of traditionally unreported         &amp;#8216;invisible work&amp;#8217; and associated elements of informal knowledge and unarticulated information. We draw from a range of ethnographic         materials to understand ways in which data-information-knowledge are viewed within the community and consider some of the         non-linear aspects of data-knowledge-information that relate to the development of a sustained, robust, persistent infrastructure         for data collection in environmental science research. Taking data as the unit of study, the notion of long-term research         and data holdings leads to consideration of types of memory and of knowledge important for design of cyberinfrastructures.         Complexity, ambiguity, and nonlinearity are part of an information ecology and addressed today by exploring multiple types         of knowledge, developing information system vocabularies, and recognizing the need for intermediation.      </content></document><document><year>2007</year><authors>Said Elnaffar1 | Pat Martin2 | Berni Schiefer3  | Sam Lightstone3 </authors><title>Is it DSS or OLTP: automatically identifying DBMS workloads      </title><content>The type of the workload on a database management system (DBMS) is a key consideration in tuning the system. Allocations for         resources such as main memory can be very different depending on whether the workload type is Online Transaction Processing         (OLTP) or Decision Support System (DSS). A DBMS also typically experiences changes in the type of workload it handles during         its normal processing cycle. Database administrators must therefore recognize the significant shifts of workload type that         demand reconfiguring the system in order to maintain acceptable levels of performance. We envision intelligent, autonomic         DBMSs that have the capability to manage their own performance by automatically recognizing the workload type and then reconfiguring         their resources accordingly. In this paper, we present an approach to automatically identifying a DBMS workload as either         OLTP or DSS. Using data mining techniques, we build a classification model based on the most significant workload characteristics         that differentiate OLTP from DSS and then use the model to identify any change in the workload type. We construct and compare         classifiers built from two different sets of workloads, namely the TPC-C and TPC-H benchmarks and the Browsing and Ordering         profiles from the TPC-W benchmark. We demonstrate the feasibility and success of these classifiers with TPC-generated workloads         and with industry-supplied workloads.      </content></document><document><year>2007</year><authors>James Cheng1 | Yiping Ke1  | Wilfred Ng1 </authors><title>Maintaining frequent closed itemsets over a sliding window      </title><content>In this paper, we study the incremental update of Frequent Closed Itemsets (FCIs) over a sliding window in a high-speed data stream. We propose the notion of semi-FCIs, which is to progressively increase the minimum support threshold for an itemset as it is retained longer in the window,         thereby drastically reducing the number of itemsets that need to be maintained and processed. We explore the properties of         semi-FCIs and observe that a majority of the subsets of a semi-FCI are not semi-FCIs and need not be updated. This finding         allows us to devise an efficient algorithm, IncMine, that incrementally updates the set of semi-FCIs over a sliding window. We also develop an inverted index to facilitate the update process. Our empirical results show that IncMine achieves significantly higher throughput and consumes         less memory than the state-of-the-art streaming algorithms for mining FCIs and FIs. IncMine also attains high accuracy of         100% precision and over 93% recall.      </content></document><document><year>2007</year><authors>Elisa Bertino1| Giovanna Guerrini2 | Marco Mesiti3 </authors><title>Measuring the structural similarity among XML documents and DTDs      </title><content>Measuring the structural similarity between an XML document and a DTD has many relevant applications that range from document         classification and approximate structural queries on XML documents to selective dissemination of XML documents and document         protection. The problem is harder than measuring structural similarity among documents, because a DTD can be considered as         a generator of documents. Thus, the problem is to evaluate the similarity between a document and a set of documents. An effective         structural similarity measure should face different requirements that range from considering the presence and absence of required         elements, as well as the structure and level of the missing and extra elements to vocabulary discrepancies due to the use         of synonymous or syntactically similar tags. In the paper, starting from these requirements, we provide a definition of the         measure and present an algorithm for matching a document against a DTD to obtain their structural similarity. Finally, experimental         results to assess the effectiveness of the approach are presented.      </content></document><document><year>2007</year><authors>Giuseppe Manco1 | Elio Masciari1  | Andrea Tagarelli2 </authors><title>Mining categories for emails via clustering and pattern discovery      </title><content>The continuous exchange of information by means of the popular email service has raised the problem of managing the huge amounts         of messages received from users in an effective and efficient way. We deal with the problem of email classification by conceiving         suitable strategies for: (1) organizing messages into homogeneous groups, (2) redirecting further incoming messages according         to an initial organization, and (3) building reliable descriptions of the message groups discovered. We propose a unified         framework for handling and classifying email messages. In our framework, messages sharing similar features are clustered in         a folder organization. Clustering and pattern discovery techniques for mining structured and unstructured information from         email messages are the basis of an overall process of folder creation/maintenance and email redirection. Pattern discovery         is also exploited for generating suitable cluster descriptions that play a leading role in cluster updating. Experimental         evaluation performed on several personal mailboxes shows the effectiveness of our approach.      </content></document><document><year>2007</year><authors>Reda Alhajj1| 3  | Mehmet Kaya2 </authors><title>Multi-objective genetic algorithms based automated clustering for fuzzy association rules mining      </title><content>Researchers realized the importance of integrating fuzziness into association rules mining in databases with binary and quantitative         attributes. However, most of the earlier algorithms proposed for fuzzy association rules mining either assume that fuzzy sets         are given or employ a clustering algorithm, like CURE, to decide on fuzzy sets; for both cases the number of fuzzy sets is         pre-specified. In this paper, we propose an automated method to decide on the number of fuzzy sets and for the autonomous         mining of both fuzzy sets and fuzzy association rules. We achieve this by developing an automated clustering method based         on multi-objective Genetic Algorithms (GA); the aim of the proposed approach is to automatically cluster values of a quantitative         attribute in order to obtain large number of large itemsets in less time. We compare the proposed multi-objective GA based         approach with two other approaches, namely: 1) CURE-based approach, which is known as one of the most efficient clustering         algorithms; 2) Chien et al. clustering approach, which is an automatic interval partition method based on variation of density.         Experimental results on 100;K transactions extracted from the adult data of USA census in year 2000 showed that the proposed         automated clustering method exhibits good performance over both CURE-based approach and Chien et al.&amp;#8217;s work in terms of runtime,         number of large itemsets and number of association rules.      </content></document><document><year>2007</year><authors>M. T. Musavi1 | H. Ressom2| S. Srirangam1| P. Natarajan1| R. W. Virnstein3| L. J. Morris3 | W. Tweedale3</authors><title>Neural network-based light attenuation model for monitoring seagrass population in the Indian river lagoon      </title><content>Seagrasses have been considered one of the most critical marine habitat types of coastal and estuarine ecosystems such as         the Indian River Lagoon. They are an important part of biological productivity, nutrient cycling, habitat stabilization and         species diversity and are the primary focus of restoration efforts in the Indian River Lagoon. The areal extent of seagrasses         has declined within segments of the lagoon over the years. Light availability to seagrasses is a major criterion limiting         their distribution. Decreased water clarity and resulting reduced light penetration have been cited as the major factors responsible         for the decline in seagrasses in the lagoon. Hence, light is a critical factor for the survival of seagrass species. Light         attenuation coefficient is an important parameter that indicates the light attenuated by the water column and can therefore         be used as an indicator of seagrass vigor. A number of region-specific linear light attenuation models have been proposed         in the literature. Though, in practice, linear light attenuation models have been commonly used, there is need for a flexible         and robust model that incorporates the non-linearities present in coastal and estuarine environments. This paper presents         a neural network based model to estimate light attenuation coefficient from water quality parameters and thereby indirectly         monitor seagrass population in the Indian River Lagoon. The proposed neural network models were compared with linear regression         models, step-wise linear regression models, model trees and support vector machines. The neural network models performed fairly         better compared to the other models considered.      </content></document><document><year>2007</year><authors>Guy de Tr1 | Rita de Caluwe1 | Henri Prade2</authors><title>Null values in fuzzy databases      </title><content>Since in the real world, it often occurs that information is missing, database systems clearly need some facilities to deal         with missing data. With respect to traditional database systems, the most commonly adopted approach to this problem is based         on null values and three valued logic. This paper deals with the semantics and the use of null values in fuzzy databases.         In dealing with missing information a distinction is made between incompleteness due to unavailability and incompleteness         due to inapplicability. Both the database modelling and database querying aspects are described. With respect to attribute         values, incompleteness due to unavailability is modelled by possibility distributions, which is a commonly used technique         in the fuzzy databases. Domain specific null values, represented by a bottom symbol, are used to model incompleteness due         to inapplicability. Extended possibilistic truth values are used to formalize the impact of data manipulation and (flexible)         querying operations in the presence of these null values. The different cases of appearances of null values in the handling         of selection conditions of flexible database queries are described in detail.      </content></document><document><year>2007</year><authors>Sheng-Yuan Yang1| 2 | Fang-Chen Chuang3  | Cheng-Seen Ho3| 4 </authors><title>Ontology-supported FAQ processing and ranking techniques      </title><content>This paper describes an FAQ system on the Personal Computer (PC) domain, which employs ontology as the key technique to pre-process         FAQs and process user query. It is also equipped with an enhanced ranking technique to present retrieved, query-relevant results.         Basically, the system bases on the wrapper technique to help clean, retrieve, and transform FAQ information collected from         a heterogeneous environment and stores it in an ontological database. During retrieval of FAQs, the system trims irrelevant         query keywords, employs either full keywords match or partial keywords match to retrieve FAQs, and removes conflicting FAQs         before turning the final results to the user. Ontology plays the key role in all the above activities. To produce a more effective         presentation of the search results, the system employs an enhanced ranking technique, which includes Appearance Probability,         Satisfaction Value, Compatibility Value, and Statistic Similarity Value as four measures properly weighted to rank the FAQs.         Our experiments show the system does improve precision rate and produces better ranking results. The proposed FAQ system manifests         the following interesting features. First, the ontology-supported FAQ extraction from webpages can clean FAQ information by         removing redundant data, restore missing data, and resolve inconsistent data. Second, the FAQs are stored in an ontology-directed         internal format, which supports semantics-constrained retrieval of FAQs. Third, the ontology-supported natural language processing         of user query helps pinpoint user&amp;#8217;s intent. Finally, the partial keywords match-based ranking method helps present user-most-wanted,         conflict-free FAQ solutions for the user.      </content></document><document><year>2007</year><authors>Ling Qiu1 | Yingjiu Li2  | Xintao Wu3 </authors><title>Preserving privacy in association rule mining with bloom filters      </title><content>Privacy preserving association rule mining has been an active research area since recently. To this problem, there have been         two different approaches&amp;#8212;perturbation based and secure multiparty computation based. One drawback of the perturbation based         approach is that it cannot always fully preserve individual&amp;#8217;s privacy while achieving precision of mining results. The secure         multiparty computation based approach works only for distributed environment and needs sophisticated protocols, which constrains         its practical usage. In this paper, we propose a new approach for preserving privacy in association rule mining. The main         idea is to use keyed Bloom filters to represent transactions as well as data items. The proposed approach can fully preserve         privacy while maintaining the precision of mining results. The tradeoff between mining precision and storage requirement is         investigated. We also propose &amp;#948;-folding technique to further reduce the storage requirement without sacrificing mining precision and running time.      </content></document><document><year>2007</year><authors>Henri Avancini1 | Leonardo C|ela1  | Umberto Straccia1 </authors><title>Recommenders in a personalized, collaborative digital library environment      </title><content>We envisage an information source not only as an information resource where users may submit queries to satisfy their daily         information need, but also as a collaborative working and meeting space of people sharing common interests. Indeed, we will         present a highly personalized environment where not only users may organize (and search into) the information space according         to their individual taste and use, but which provides advanced features of collaborative work among the users. It is up to         the system to discover interesting properties about the users&amp;#8217; interests, relationships between users and user communities         and to make recommendations based on preference patterns of the users, which is the main topic of this paper.      </content></document><document><year>2007</year><authors>Francois Barbanon1  | Daniel P. Miranker1 </authors><title>SPHINX: Schema integration by example      </title><content>The Internet has instigated a critical need for automated tools that facilitate integrating countless databases. Since nontechnical         end users are often the ultimate repositories of the domain information required to distinguish differences in data types,         an effective solution must integrate simple GUI based data browsing tools and automatic mapping methods that eliminate the         requirement for a technical user to supervise the process. We develop a metamodel of data integration as the basis for absorbing         feedback from an end user. The schema integration algorithm draws examples from the data and learns integrating view definitions         by asking a user simple yes or no questions. The metamodel enables a search mechanism that is guaranteed to converge to a         correct integrating view definition without the user having to know a view definition language such as SQL or SchemaSQL, or         even having to inspect the final view definition. We show how data catalog statistics, normally used to optimize queries,         can be exploited to parameterize the search heuristics and improve the convergence of the learning algorithm.      </content></document><document><year>2007</year><authors>Ollivier Haemmerl1 | Patrice Buche2  | Rallou Thomopoulos3| 4 </authors><title>The MIEL system: Uniform interrogation of structured and weakly-structured imprecise data      </title><content>We present an information system developed to help assessing the microbiological risk in food. That information system contains         experimental results in microbiology, mainly extracted from scientific publications. The increasing amount of the experimental         results available and the difficulty to integrate them into a classic relational database schema led us to design a system         composed of two distinct subsystems queried through a common interface. The first subsystem is a classic relational database.         The second subsystem is a database containing weakly-structured pieces of information expressed in terms of conceptual graphs.         The data stored in both bases can be fuzzy ones in order to take into account the specificities of the biological information.         The uniform query language used on both relational database and conceptual graph database allows the users to express preferences         by using fuzzy sets in their queries. The MIEL system is now operational and used by the microbiologists involved in the Sym&amp;#8217;Previus         French project.      </content></document><document><year>2007</year><authors>Siwoo Byun1 </authors><title>Transaction Management for Flash Media Databases in Portable Computing Environments      </title><content>Flash memory is becoming a major database storage in building embedded systems or portable devices because of its non-volatile,         shock-resistant, power-economic nature, and fast access time for read operations. Flash memory, however, should be erased         before it can be rewritten and the erase and write operations are very slow as compared to main memory. Due to this drawback,         traditional database management schemes are not easy to apply directly to flash memory database for portable devices. Therefore,         we improve the traditional schemes and propose a new scheme called flash two phase locking (F2PL) scheme for efficient transaction processing in a flash memory database environment. F2PL achieves high transaction performance by exploiting the notion of the alternative version coordination which allows previous version reads and efficiently handles slow write/erase operations in lock management processes. We         also propose a simulation model to show the performance of F2PL. Based on the results of the performance evaluation, we conclude that F2PL scheme outperforms the traditional schemes.      </content></document><document><year>2007</year><authors>Hung-Yi Lin1 </authors><title>Using B+-trees for processing of line segments in large spatial databases      </title><content>Points, lines, and regions are the three basic entities for constituting vector-based objects in spatial databases. Many indexing         methods (G-tree, K-D-B tree, Quad-tree, PMR-tree, Grid-file, R-tree, and so on) have been widely discussed for handling point or region data. These traditional methods can efficiently         organize point or region objects in a space into a hashing or hierarchical directory. They provide efficient access methods         to meet the requirement of accurate retrievals. However, two problems are encountered when their techniques are applied to         deal with line segments. The first is that representing line segments by means of point or region objects cannot exactly and         properly preserve the spatial information about the proximities of line segments. The second problem is derived from the large         dead space and overlapping areas in external and internal nodes of the hierarchical directory caused by the use of rectangles         to enclose line objects. In this paper, we propose an indexing structure for line segments based on B                     +            -tree to remedy these two problems. Through the experimental results, we demonstrate that our approach has significant improvement         over the storage efficiency. In addition, the retrieval efficiency has also been significantly prompted as compared to the         method using R-tree index scheme. These improvements derive mainly from the proposed data processing techniques and the new indexing method.      </content></document><document><year>2007</year><authors>Stergos D. Afantenos1 | Vangelis Karkaletsis2| Panagiotis Stamatopoulos3 | Constantin Halatsis3</authors><title>Using synchronic and diachronic relations for summarizing multiple documents describing evolving events      </title><content>In this paper we present a fresh look at the problem of summarizing evolving events from multiple sources. After a discussion         concerning the nature of evolving events we introduce a distinction between linearly and non-linearly evolving events. We present then a general methodology for the automatic creation of summaries from evolving events. At its         heart lie the notions of Synchronic and Diachronic cross-document Relations (SDRs), whose aim is the identification of similarities and differences between sources, from a         synchronical and diachronical perspective. SDRs do not connect documents or textual elements found therein, but structures         one might call messages. Applying this methodology will yield a set of messages and relations, SDRs, connecting them, that is a graph which we call         grid. We will show how such a grid can be considered as the starting point of a Natural Language Generation System. The methodology         is evaluated in two case-studies, one for linearly evolving events (descriptions of football matches) and another one for         non-linearly evolving events (terrorist incidents involving hostages). In both cases we evaluate the results produced by our         computational systems.      </content></document><document><year>2007</year><authors>Aless|ro D&amp;#8217 Atri1  | Amihai Motro2 </authors><title>VirtuE: a formal model of virtual enterprises for information markets      </title><content>A vital part of a modern economy is an information market. In this market, information products are being traded in countless         ways. Information is bought, modified, integrated, incorporated into other products, and then sold again. Often, the manufacturing         of an information product requires the collaboration of several participants. A virtual enterprise is a community of business         entities that collaborate on the manufacturing of complex products. This collaboration is often ad hoc, for a specific product         only, after which the virtual enterprise may dismantle. The virtual enterprise paradigm is particularly appealing for modeling         collaborations for manufacturing information products, and in this paper we present a new model, called VirtuE, for modeling         such activities. VirtuE has three principal components. First, it defines a distributed infrastructure with concepts such as members, products, inventories, and production plans. Second, it defines transactions among members, to enable collaborative production of complex products. Finally, it provides means for the instrumentation of enterprises, to measure their performance and to govern their behavior.      </content></document><document><year>2005</year><authors>Gregory F. Cooper1</authors><title>A Bayesian method for learning belief networks that contain hidden variables</title><content>This paper presents a Bayesian method for computing the probability of a Bayesian belief-network structure from a database. In particular, the paper focuses on computing the probability of a belief-network structure that contains a hidden (latent) variable. A hidden variable represents a postulated entity that has not been directly measured. After reviewing related techniques, which previously were reported, this paper presents a new, more efficient method for handling hidden variables in belief networks.</content></document><document><year>2005</year><authors>Gilles Fouqu&amp;eacute 1| 2  | Stan Matwin1 </authors><title>A case-based approach to software reuse</title><content>This software reuse system helps a user build programs by reusing modules stored in an existing library. The system, dubbed caesar (Case-basEd SoftwAre Reuse), is conceived in the case-based reasoning framework, where cases consist of program specifications and the corresponding C language code. The case base is initially seeded by decomposing relevant programs into functional slices using algorithms from dataflow analysis. caesar retrieves stored specifications from this base and specializes and/or generalizes them to match the user specification. Testing techniques are applied to the construct assembled by caesar through sequential composition to generate test data which exhibits the behavior of the code. For efficiency, inductive logic programming techniques are used to capture combinations of functions that frequently occur together in specifications. Such combinations may be stored as new functional slices.</content></document><document><year>2005</year><authors>Yves Caseau1 | Pierre -Yves Guillo2  | Eric Levenez3</authors><title>A deductive and object-oriented approach to a complex scheduling problem</title><content>This paper presents an application of combined deductive and object-oriented technologies to a complex scheduling (timetable) problem. This approach emphasizes local propagation of constraints, which we perform with deductive rules, and combines it with global pruning heuristics, which we represent with methods (in a procedural manner) attached to objects. Because both components are essential to ensure success, we see this scheduling application as an interesting demonstration of the synergy between object-oriented and deductive technology. We provide a precise description of the problem, discuss what makes it difficult, and present detailed techniques that we used for its resolution.</content></document><document><year>2005</year><authors>Kayvan Najarian1 </authors><title>A Fixed-Distribution PAC Learning Theory for Neural FIR Models      </title><content>The PAC learning theory creates a framework to assess the learning properties of static models. This theory has been extended         to include learning of modeling tasks with m-dependent data given that the data are distributed according to a uniform distribution.         The extended theory can be applied for learning of nonlinear FIR models with the restriction that the data are unformly distributed.                     In this paper, The PAC learning scheme is extended to deal with any FIR model regardless of the distribution of the data.               This fixed-distribution m-dependent extension of the PAC learning theory is then applied to the learning of FIR three-layer               feedforward sigmoid neural networks.            </content></document><document><year>2005</year><authors>Latifur Khan1 | Dennis McLeod2  | Eduard Hovy3 </authors><title>A Framework for Effective Annotation of Information from Closed Captions Using Ontologies</title><content>To improve the accuracy in terms of precision and recall of an audio information retrieval system we have created a domain-specific ontology (a collection of key concepts and their interrelationships), as well as a novel, pruning algorithm. Given the shortcomings of keyword-based techniques, we have opted to employ a concept-based technique utilizing this ontology. Achieving high precision and high recall is the key problem in the retrieval of audio information. In traditional approaches, high recall is typically achieved at the expense of low precision, and vice versa. Through the use of a domain-specific ontology appropriate concepts can be identified during metadata generation (description of audio) or query generation, thus improving precision.When irrelevant concepts are associated with queries or documents there is a loss of precision. On the other side of the coin, if relevant concepts are discarded, a loss of recall will ensue. In conjunction with the use of a domain specific ontology we have thus proposed a novel, automatic pruning algorithm which prunes as many irrelevant concepts as possible during any case of description and identification of documents, and query generation. To improve recall, A controlled and correct query expansion mechanism is proposed for the improvement of recall, thus guaranteeing that precision will not be lost.</content></document><document><year>2005</year><authors>Wenzhong Zhao1 | Alex Dekhtyar2  | Judy Goldsmith1 </authors><title>A Framework for Management of Semistructured Probabilistic Data      </title><content>This paper describes the theoretical framework and implementation of a database management system for storing and manipulating         diverse probability distributions of discrete random variables with finite domains, and associated information. A formal Semistructured         Probabilistic Object (SPO) data model and a Semistructured Probabilistic Query Algebra (SP-algebra) are proposed. The SP-algebra         supports standard database queries as well as some specific to probabilities, such as conditionalization and marginalization.         Thus, the Semistructured Probabilistic Database may be used as a backend to any application that involves the management of         large quantities of probabilistic information, such as building stochastic models. The implementation uses XML encoding of         SPOs to facilitate communication with diverse applications. The database management system has been implemented on top of         a relational DBMS. The translation of SP-algebra queries into relational queries are discussed here, and the results of initial         experiments evaluating the system are reported.      </content></document><document><year>2005</year><authors>S. Misbah Deen1 </authors><title>A general framework for coherence in a CKBS</title><content>Coherence in a distributed system is meant to offset the disadvantages of distribution. The paper explores four issues under coherence, namely preservation of knowledge consistency across the agents, reliability of the overall system, integration of local solutions and the global performance. It presents some general strategies that can be employed to improve coherence in a CKBS, which include a weak consistency with versions for knowledge revision, and a recovery mechanism based on a hierarchic three-stage coordination, which ensures the correct isolation of potentially hierarchic multiagent actions. The paper goes on to identify the sources and classes of conflicts in global integration, and it suggests remedies, which at worst case would involve negotiation. In global performance, it focusses on planning and result synthesis, as the two most important problem domains, and suggests strategies ameliorate performance.</content></document><document><year>2005</year><authors>William H. Mansfield Jr.1  | Robert M. Fleischman2 </authors><title>A high-performance, ad hoc, fuzzy query processing system</title><content>Database queries involving imprecise or fuzzy predicates are currently an evolving area of academic and industrial research (Buckles and Perty 1987; Bosc et al. 1988; Bosc and Pivert 1991; Kacprzyk et al. 1989; Prade and Testemale, 1987; Tahani, 1977; Umano, 1983; Zemankova and Kandel, 1985). Such queries place severe stress on the indexing and I/O subsystems of conventional database systems since they frequently involve the search of large numbers of records. The Datacycle (Datacycle is a trademark of Bellcore.) architecture and research prototype is a database processing system that uses filtering technology to perform an efficient, exhaustive search of an entire database. It has been modified to include fuzzy predicates in its query processing. The approach obviates the need for complex index structures, provides high-performance query throughput, permits the use of ad hoc fuzzy membership functions and provides deterministic response time largely independent of query complexity and load. This paper describes the Datacycle prototype implementation of fuzzy queries and some recent performance results.</content></document><document><year>2005</year><authors>Xu Wu1 | Nick Cercone1 | Tadao Ichikawa2</authors><title>A knowledge-based system for generating informative responses to indirect database queries</title><content>The objective of this study is to develop a knowledge-base framework for generatingcooperative answers to indirect queries. Anindirect query can be considered as a nonstandard database query in which a user did not specify explicitly the information request. In a cooperative query answering system, a user's indirect query should be answered with an informative response, either anaffirmative response or anegative response, which is generated on the basis of the inference of the user's information request and the reformulation of the users' indirect query.This paper presents methods for inferring users' intended actions, determining users' information requirements, and for automatically reformulating indirect queries into direct queries. The inference process is carried out on the basis of a user model, calluser action model, as well as the query context. Two kinds ofinformative responses, i.e.affirmative responses andnegative responses can be generated by arule-based approach.</content></document><document><year>2005</year><authors>Hung-Chen Chen1 | Arbee L. P. Chen2 </authors><title>A Music Recommendation System Based on Music and User Grouping</title><content>In this paper, we present a music recommendation system, which provides a personalized service of music recommendation. The polyphonic music objects of MIDI format are first analyzed for deriving information for music grouping. For this purpose, the representative track of each polyphonic music object is first determined, and then six features are extracted from this track for proper music grouping. Moreover, the user access histories are analyzed to derive the profiles of user interests and behaviors for user grouping. The content-based, collaborative, and statistics-based recommendation methods are proposed based on the favorite degrees of the users to the music groups, and the user groups they belong to. A series of experiments are carried out to show that our approach performs well.</content></document><document><year>2005</year><authors>Jin Zhang1  | Tien N. Nguyen2 </authors><title>A New Term Significance Weighting Approach</title><content>The authors present a new term significance measure that integrates term frequency retrieval characteristics, term frequency, document collection characteristics, and both the term depth and width distribution characteristics. A new concept, the term depth distribution, is introduced and its impact on the term significance is analyzed. The authors address the features of the new term significance measure from the angles of the impact of the variables (parameters) on it and the iso-significance contour analyses. An experimental study was conducted to compare the newly developed approach with two other popular approaches from the perspectives of both efficiency and effectiveness. The results show that the newly developed approach achieves satisfactory performance. Issues for further research on this topic are suggested.</content></document><document><year>2005</year><authors>Ran Giladi1  | Peretz Shoval1 </authors><title>An architecture of an intelligent system for routing user requests in a network of heterogeneous databases</title><content>We present a general purpose model for routing user requests, e.g. queries, in a network of autonomous heterogeneous databases. The database schemas and other information on the database nodes are used to construct a multi-level knowledge-base (MKB) that resides in various nodes. Access to the databases is not done by creating direct connections between the user and the nodes where the data are presumably located. Rather, the user approaches the network by contents via an intelligent system that utilizes the MKB in order to identify the nodes and databases where the most relevant information resides, and establishes access routes to those nodes.</content></document><document><year>2005</year><authors>Shie-Jue Lee1 </authors><title>An autonomous multistrategy theorem proving system using knowledge-based techniques</title><content>Most general-purpose theorem-proving systems have weak search control. There is no alternative to the use of a large number of heuristics or strategies for search guidance. Choosing appropriate strategies for solving a given problem may require the knowledge of different strategies and may involve a lot of painstaking trial-and-error. To encourage the widespread use of computer reasoning systems, it is important that a theorem prover be usable by those with little knowledge of problem-solving strategies, and that a theorem prover be able to select good strategies for the user. An autonomous multistrategy theorem-proving system is developed, using knowledge-based techniques, to entirely free the user from the necessity of understanding the system or the merits of different strategies. All the user has to do is input his or her problem in first-order logic, and the system solves the problem efficiently for him or her without any manual intervention. The system embodies much of expert knowledge about how to solve problems. The knowledge is represented as metarules in knowledge base which guide a hyperlinking theorem prover to solve problems automatically and efficiently.</content></document><document><year>2005</year><authors>S. M. Deen1 </authors><title>An Engineering Approach to Cooperating Agents for Distributed Information Systems</title><content>This paper presents a multi-agent model of a distributed information system, using what is described as an engineering approach to real world application environment. The objective is to define, using proven ideas in the industrial context, the agent-based behaviour of the distributed system, which must operate correctly and effectively in an error-prone environment. Issues such as stability, robustness and scalability have also been addressed, along with some new ideas on a high-level communication strategies, as distinct from protocol-based communications. The work is being carried out under the DREAM theme at Keele, an earlier version of the approach having been successfully applied to agent-based manufacturing in an international project called HMS, in which some of the world&amp;#x2019;s major manufacturing industries participated.</content></document><document><year>2005</year><authors>Silvia Schiaffino1| 2  | Anala Am|i1| 2 </authors><title>An Interface Agent Approach to Personalize Users' Interaction with Databases      </title><content>Making queries to a database system through a computer application can become a repetitive and time-consuming task for those         users who generally make similar queries to get the information they need to work with. We believe that interface agents could         help these users by personalizing the query-making and information retrieval tasks. Interface agents are characterized by         their ability to learn users' interests in a given domain and to help them by making suggestions or by executing tasks on         their behalf. Having this purpose in mind we have developed an agent, named QueryGuesser, to assist users of computer applications in which retrieving information from a database is a key task. This agent observes         a user's behavior while he is working with the database and builds the user's profile. Then, QueryGuesser uses this profile to suggest the execution of queries according to the user's habits and interests, and to provide the user         information relevant to him by making time-demanding queries in advance or by monitoring the events and operations occurring         in the database system. In this way, the interaction between database users and databases becomes personalized while it is         enhanced.      </content></document><document><year>2005</year><authors>Terry Gaasterl|1| Parke Godfrey1 | Jack Minker2</authors><title>An overview of cooperative answering</title><content>Databases and information systems are often hard to use because they do not explicitly attempt to cooperate with their users. Direct answers to database and knowledge base queries may not always be the best answers. Instead, an answer with extra or alternative information may be more useful and less misleading to a user. This paper surveys foundational work that has been done toward endowing intelligent information systems with the ability to exhibit cooperative behavior. Grice''s maxims of cooperative conversation, which provided a starting point for the field of cooperative answering, are presented along with relevant work in natural language dialogue systems, database query answering systems, and logic programming and deductive databases. The paper gives a detailed account of cooperative techniques that have been developed for considering users'' beliefs and expectations, presuppositions, and misconceptions. Also, work in intensional answering and generalizing queries and answers is covered. Finally, the Cooperative Answering System at Maryland, which is intended to be a general, portable platform for supporting a wide spectrum of cooperative answering techniques, is described.</content></document><document><year>2005</year><authors>Usama M. Fayyad1 | Padhraic Smyth1 | Nicholas Weir2  | S. Djorgovski2 </authors><title>Automated analysis and exploration of image databases: Results, progress, and challenges</title><content>In areas as diverse as earth remote sensing, astronomy, and medical imaging, image acquisition technology has undergone tremendous improvements in recent years. The vast amounts of scientific data are potential treasure-troves for scientific investigation and analysis. Unfortunately, advances in our ability to deal with this volume of data in an effective manner have not paralleled the hardware gains. While special-purpose tools for particular applications exist, there is a dearth of useful general-purpose software tools and algorithms which can assist a scientist in exploring large scientific image databases. This paper presents our recent progress in developing interactive semi-automated image database exploration tools based on pattern recognition and machine learning technology. We first present a completed and successful application that illustrates the basic approach: the SKICAT system used for the reduction and analysis of a 3 terabyte astronomical data set. SKICAT integrates techniques from image processing, data classification, and database management. It represents a system in which machine learning played a powerful and enabling role, and solved a difficult, scientifically significant problem. We then proceed to discuss the general problem of automated image database exploration, the particular aspects of image databases which distinguish them from other databases, and how this impacts the application of off-the-shelf learning algorithms to problems of this nature. A second large image database is used to ground this discussion: Magellan''s images of the surface of the planet Venus. The paper concludes with a discussion of current and future challenges.</content></document><document><year>2005</year><authors>Hsin-Chang Yang1  | Chung-Hong Lee2 </authors><title>Automatic Category Theme Identification and Hierarchy Generation for Chinese Text Categorization</title><content>Recently research on text mining has attracted lots of attention from both industrial and academic fields. Text mining concerns of discovering unknown patterns or knowledge from a large text repository. The problem is not easy to tackle due to the semi-structured or even unstructured nature of those texts under consideration. Many approaches have been devised for mining various kinds of knowledge from texts. One important aspect of text mining is on automatic text categorization, which assigns a text document to some predefined category if the document falls into the theme of the category. Traditionally the categories are arranged in hierarchical manner to achieve effective searching and indexing as well as easy comprehension for human beings. The determination of category themes and their hierarchical structures were most done by human experts. In this work, we developed an approach to automatically generate category themes and reveal the hierarchical structure among them. We also used the generated structure to categorize text documents. The document collection was trained by a self-organizing map to form two feature maps. These maps were then analyzed to obtain the category themes and their structure. Although the test corpus contains documents written in Chinese, the proposed approach can be applied to documents written in any language and such documents can be transformed into a list of separated terms.</content></document><document><year>2005</year><authors>Ian Kaminskyj1  | Tadeusz Czaszejko1 </authors><title>Automatic Recognition of Isolated Monophonic Musical Instrument Sounds using kNNC</title><content>The instrument recognition system described in this paper classifies isolated monophonic musical instrument sounds using six features: cepstral coefficients, constant Q transform frequency spectrum, multidimensional scaling analysis trajectories, RMS amplitude envelope, spectral centroid and vibrato. Sounds from nineteen instruments of definite pitch, covering the note range C3&amp;#x2013;C6 and representing the major musical instrument families and subfamilies were used to test the system. Nearest neighbor classification was utilised and results were evaluated in terms of accuracy and reliability. Using the leave-one-out test strategy yielded an accuracy of 93% in instrument recognition, 97% in instrument family recognition, and 100% for sustain/impulsive instruments.</content></document><document><year>2005</year><authors>Alfons Kemper1 | Peter C. Lockemann2 | Guido Moerkotte2  | Hans-Dirk Walter2 </authors><title>Autonomous objects: A natural model for complex applications</title><content>Object-oriented database systems are emerging as the next generation DBMSs for advanced applications, e.g., VLSI design, mechanical CAD/CAM, software engineering, etc. However, a close analysis indicates that the requirements imposed by these application domains cannot be met by an object-oriented model that relies purely onpassive objects. In this work we go beyond the conventionalsingle-thread-of-control paradigm of passive object models and propose a model ofactive objects which can autonomously initiate responses to environmental changes.Autonomous objects cooperate with each other by synchronous orasynchronous message passing&amp;#x2014;giving rise to themultiple-thread-of-control in such an environment. It is shown howevents&amp;#x2014;to which active objects react&amp;#x2014;can be incorporated into this model. We propose a nondeterministic computational model for the individual active objects that allows the autonomous reaction upon events. We show that this very sparse extension to an object-oriented model gives rise to several high-level features which can be controlled by events. The object-oriented paradigm allows one to isolate the rules according to which events are being raised. This leads to a potentially rather efficient execution model compared to existing relational concepts, which are typically globally-defined event trigger mechanisms.</content></document><document><year>2005</year><authors>Ji Zhang1 | Wynne Hsu1  | Mong Li Lee1 </authors><title>Clustering in Dynamic Spatial Databases</title><content>Efficient clustering in dynamic spatial databases is currently an open problem with many potential applications. Most traditional spatial clustering algorithms are inadequate because they do not have an efficient support for incremental clustering.In this paper, we propose DClust, a novel clustering technique for dynamic spatial databases. DClust is able to provide multi-resolution view of the clusters, generate arbitrary shapes clusters in the presence of noise, generate clusters that are insensitive to ordering of input data and support incremental clustering efficiently. DClust utilizes the density criterion that captures arbitrary cluster shapes and sizes to select a number of representative points, and builds the Minimum Spanning Tree (MST) of these representative points, called R-MST. After the initial clustering, a summary of the cluster structure is built. This summary enables quick localization of the effect of data updates on the current set of clusters. Our experimental results show that DClust outperforms existing spatial clustering methods such as DBSCAN, C2P, DENCLUE, Incremental DBSCAN and BIRCH in terms of clustering time and accuracy of clusters found.</content></document><document><year>2005</year><authors>Shekhar Pradhan1| 2 | Jack Minker3| 1  | V. S. Subrahmanian3| 1 </authors><title>Combining databases with prioritized information</title><content>To solve a problem one may need to combine the knowledge of several different experts. It can happen that some of the claims of one or more experts may be in conflict with the claims of other experts. There may be several such points of conflict and any claim may be involved in several different such points of conflict. In that case, the user of the knowledge of experts may prefer a certain claim to another in one conflict-point without necessarily preferring that statement in another conflict-point.Our work constructs a framework within which the consequences of a set of such preferences (expressed as priorities among sets of statements) can be computed. We give four types of semantics for priorities, three of which are shown to be equivalent to one another. The fourth type of semantics for priorities is shown to be more cautious than the other three. In terms of these semantics for priorities, we give a function for combining knowledge from different sources such that the combined knowledge is conflict-free and satisfies all the priorities.</content></document><document><year>2005</year><authors>Matthias Jarke1 | Rainer Gallersd&amp;ouml rfer1| Manfred A. Jeusfeld1| Martin Staudt1 | Stefan Eherer2 </authors><title>ConceptBase &amp;#x2014; A deductive object base for meta data management</title><content>Deductive object bases attempt to combine the advantages of deductive relational databases with those of object-oriented databases. We review modeling and implementation issues encountered during the development of ConceptBase, a prototype deductive object manager supporting the Telos object model. Significant features include: 1) The symmetric treatment of object-oriented, logic-oriented and graph-oriented perspectives, 2) an infinite metaclass hierarchy as a prerequisite for extensibility and schema evolution, 3) a simple yet powerful formal semantics used as the basis for implementation, 4) a client-server architecture supporting collaborative work in a wide-area setting. Several application experiences demonstrate the value of the approach especially in the field of meta data management.</content></document><document><year>2005</year><authors>Sanjiv K. Bhatia1| Jitender S. Deogun2 | Vijay V. Raghavan3</authors><title>Conceptual query formulation and retrieval</title><content>In this paper, we advance a technique to develop a user profile for information retrieval through knowledge acquisition techniques. The profile bridges the discrepancy between user-expressed keywords and system-recognizable index terms. The approach presented in this paper is based on the application of personal construct theory to determine a user''s vocabulary and his/her view of different documents in a training set. The elicited knowledge is used to develop a model for each phrase/concept given by the user by employing machine learning techniques.Our model correlates the concepts in a user''s vocabulary to the index terms present in the documents in the training set. Computation of dependence between the user phrases also contributes in the development of the user profile and in creating a classification of documents. The resulting system is capable of automatically identifying the user concepts and query translation to index terms computed by the conventional indexing process. The system is evaluated by using the standard measures of precision and recall by comparing its performance against the performance of the smart system for different queries.</content></document><document><year>2005</year><authors>Aris M. Ouksel1  | Channah F. Naiman1</authors><title>Coordinating context building in heterogeneous information systems</title><content>We present an architecture to coordinate the construction of the context within which meaningful information between heterogeneous information systems can be exchanged. We call this coordinator SCOPES (Semantic Coordinator Over Parallel Exploration Spaces). A classification of semantic conflicts we proposed elsewhere is used to build and refine the context, by discovering the semantic mapping rules (inter-schema correspondence assertions) between corresponding elements of the communicating systems. A truth maintenance system is used to manage the multiple intermediate contexts. It provides a mechanism to infer or retract assertions on the basis of the knowledge acquired during the reconciliation process. This nonmonotonic technique is used in conjunction with the Dempster-Shafer theory of belief functions to model the likelihood of alternative contexts. Finally, we propose an algorithm which illustrates how the various components of the architecture interact with one another in order to build context.</content></document><document><year>2005</year><authors>Alon Y. Levy1 | Divesh Srivastava1  | Thomas Kirk1 </authors><title>Data model and query evaluation in global information systems</title><content>Global information systems involve a large number of information sources distributed over computer networks. The variety of information sources and disparity of interfaces makes the task of easily locating and efficiently accessing information over the network very cumbersome. We describe an architecture for global information systems that is especially tailored to address the challenges raised in such an environment, and distinguish our architecture from architectures of multidatabase and distributed database systems. Our architecture is based on presenting a conceptually unified view of the information space to a user, specifying rich descriptions of the contents of the information sources, and using these descriptions for optimizing queries posed in the unified view. The contributions of this paper include: (1) we identify aspects of site descriptions that are useful in query optimization; (2) we describe query optimization techniques that minimize the number of information sources accessed; and (3) we demonstrate the need for interleaving planning and query execution in such a system, and present an algorithm for this purpose.</content></document><document><year>2005</year><authors>Jan M. ytkow1  | Robert Zembowicz1 </authors><title>Database exploration in search of regularities</title><content>Large databases can be a source of useful knowledge. Yet this knowledge is implicit in the data. It must be mined and expressed in a concise, useful form of statistical patterns, equations, rules, conceptual hierarchies, and the like. Automation of knowledge discovery is important because databases are growing in size and number, and standard data analysis techniques are not designed for exploration of huge hypotheses spaces. We concentrate on discovery of regularities, defining a regularity by a pattern and the range in which that pattern holds. We argue that two types of patterns are particularly important: contingency tables and equations, and we present Forty-Niner (49er), a general-purpose database mining system which conducts large-scale search for those patterns in many subsets of data, conducting a more costly search for equations only when data indicate a functional relationship. 49er can refine the initial regularities to yield stronger and more general regularities and more useful concepts. 49er combines several searches, each contributing to a different aspect of a regularity. Correspondence between the components of search and the structure of regularities makes the system easy to understand, use, and expand. Finally, we discuss 49er's performance in four categories of tests: (1) open exploration of new databases; (2) reproduction of human findings (limited because databases which have been extensively explored are very rare); (3) hide- and -seek testing on artificially created data, to evaluate 49er on large scale against known results; (4) exploration of randomly generated databases.</content></document><document><year>2005</year><authors>Doug Fisher1  | Gilford Hapanyengwi1 </authors><title>Database management and analysis tools of machine induction</title><content>This paper surveys machine induction techniques for database management and analysis. Our premise is that machine induction facilitates an evolution from relatively unstructured data stores to efficient and correct database implementations.</content></document><document><year>2005</year><authors>Saso Dzeroski1  | Ljupco Todorovski1 </authors><title>Discovering dynamics: From inductive logic programming to machine discovery</title><content>Machine discovery systems help humans to find natural laws from collections of experimentally collected data. Most of the laws found by existing machine discovery systems describe static situations, where a physical system has reached equilibrium. In this paper, we consider the problem of discovering laws that govern the behavior of dynamical systems, i.e., systems that change their state over time. Based on ideas from inductive logic programming and machine discovery, we present two systems, QMN and LAGRANGE, for discovery of qualitative and quantitative laws from quantitative (numerical) descriptions of dynamical system behavior. We illustrate their use by generating a variety of dynamical system models from example behaviors.</content></document><document><year>2005</year><authors>Olivier Gillet1  | Ga&amp;euml l Richard1 </authors><title>Drum Loops Retrieval from Spoken Queries</title><content>Recent efforts in audio indexing and music information retrieval mostly focus on melody. If this is appropriate for polyphonic music signals, specific approaches are needed for systems dealing with percussive audio signals such as those produced by drums, tabla or djemb&amp;eacute;. In this article, we present a complete system allowing the management of a drum patterns (or drumloops) database. Queries in this database are formulated with spoken onomatopoeias&amp;#x2014;short meaningless words imitating the different sounds of the drumkit. The transcription task necessary to index the database is performed using Hidden Markov Models (HMM) and Support Vector Machines (SVM) and achieves a 86.4% correct recognition rate. The syllables of spoken queries are recognized and a relevant statistical model allows the comparison and alignment of the query with the rythmic sequences stored in the database, in order to provide a set of the most relevant drum loops.</content></document><document><year>2005</year><authors>Beng Chin Ooi1 | Cuie Zhao2  | Hongjun Lu3 </authors><title>DUET &amp;#x2014; A database user interface design environment</title><content>The DUET database user interface management system aims to help database application programmers to create, modify, and maintain interactive graphical user interfaces for different applications. DUET supports the creation of a complete user interface via direct manipulation techniques. It provides a large set of database widgets which are necessary for database applications. DUET provides facilities to step through the validation of a created user interface. A user interface can be saved as C code which can be integrated into a database backend. In this paper, the features and the architecture of DUET are presented.</content></document><document><year>2005</year><authors>Maria Zemankova1 </authors><title>Editorial</title><content>Without Abstract</content></document><document><year>2005</year><authors>Larry Kerschberg1| Zbigniew Ras1 | Maria Zemankova1</authors><title>Editors' preface</title><content>Without Abstract</content></document><document><year>2005</year><authors>C. Faloutsos1 | R. Barber2 | M. Flickner2 | J. Hafner2 | W. Niblack2 | D. Petkovic2  | W. Equitz3 </authors><title>Efficient and effective Querying by Image Content</title><content>In the QBIC (Query By Image Content) project we are studying methods to query large on-line image databases using the images'' content as the basis of the queries. Examples of the content we use include color, texture, shape, position, and dominant edges of image objects and regions. Potential applications include medical (Give me other images that contain a tumor with a texture like this one), photo-journalism (Give me images that have blue at the top and red at the bottom), and many others in art, fashion, cataloging, retailing, and industry. We describe a set of novel features and similarity measures allowing query by image content, together with the QBIC system we implemented. We demonstrate the effectiveness of our system with normalized precision and recall experiments on test databases containing over 1000 images and 1000 objects populated from commercially available photo clip art images, and of images of airplane silhouettes. We also present new methods for efficient processing of QBIC queries that consist of filtering and indexing steps. We specifically address two problems: (a) non Euclidean distance measures; and (b) the high dimensionality of feature vectors. For the first problem, we introduce a new theorem that makes efficient filtering possible by bounding the non-Euclidean, full cross-term quadratic distance expression with a simple Euclidean distance. For the second, we illustrate how orthogonal transforms, such as Karhunen Loeve, can help reduce the dimensionality of the search space. Our methods are general and allow some false hits but no false dismissals. The resulting QBIC system offers effective retrieval using image content, and for large image databases significant speedup over straightforward indexing alternatives. The system is implemented in X/Motif and C running on an RS/6000.</content></document><document><year>2005</year><authors>Willi Kl&amp;ouml sgen1 </authors><title>Efficient discovery of interesting statements in databases</title><content>The Explora system supportsDiscovery in Databases by large scale search for interesting instances of statistical patterns. In this paper we describe how Explora assessesinterestingness and achievescomputational efficiency. These problems arise because of the variety of patterns and the immense combinatorial possibilities of generating instances when studying relations between variables in subsets of data. First, the user must be saved from getting overwhelmed with a deluge of findings. To restrict the search with respect to the analysis goals, the user can focus each discovery task performed during an interactive and iterative exploration process. Some basic organization principles of search can further limit the search effort. One principle is to organize search hierarchically and to evaluate first the statistical or information theoretic evidence of the general hypotheses. Then more special hypotheses can be eliminated from further search, if a more general hypothesis was already verified. But this approach alone has some drawbacks and even in moderately sized data does not prevent large sets of findings. Therefore, in a second evaluation phase, further aspects of interestingness are assessed. A refinement strategy selects the most interesting of the statistically significant statements. A second problem for discovery systems is efficiency. Each hypothesis evaluation requires many data accesses. We describe strategies that reduce data accesses and speed up computation.</content></document><document><year>2005</year><authors>John Grant1 | Tok Wang Ling1  | Mong Li Lee2 </authors><title>ERL: Logic for entity-relationship databases</title><content>We develop a logic for entity-relationship databases, ERL, that is a generalization of database logic. ERL provides advantages to the ER model much as FOL (first-order logic) does to the relational model: a uniform language for expressing database schema, integrity constraints, and database manipulation; clearly defined semantics; the capability to express database transformations; and deductive capabilities. We propose three query languages for ER databases called ERRC, ERSQL, and ERQBE, which are generalizations of the relational calculus, SQL, and QBE, respectively. We use example queries and updates to demonstrate the capabilities of these languages. We apply database transformations to introduce the notion of views and to show that both ERRC and ERSQL are relationally complete.</content></document><document><year>2005</year><authors>Erratum</authors><title/></document></documents>