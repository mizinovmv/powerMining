<?xml version="1.0" encoding="UTF-8" standalone="no"?><documents><document><year>2004</year><authors>Huosheng Hu1 | Michael Brady1</authors><title>A bayesian approach to real-time obstacle avoidance for a mobile robot</title><content>Real-time obstacle avoidance is essential for the safe operation of mobile robots in a dynamically changing environment. This paper investigates how an industrial mobile robot can respond to unexpected static obstacles while following a path planned by a global path planner. The obstacle avoidance problem is formulated using decision theory to determine an optimal response based on inaccurate sensor data. The optimal decision rule minimises the Bayes risk by trading between a sidestep maneuver and backtracking to follow an alternative path. Real-time implementation is emphasised here as part of a framework for real world applications. It has been successfully implemented both in simulation and in reality using a mobile robot.</content></document><document><year>2004</year><authors>Miguel Angel Sotelo | Francisco Javier Rodriguez | Luis Magdalena | Luis Miguel Bergasa  | Luciano Boquete </authors><title>A Color Vision-Based Lane Tracking System for Autonomous Driving on Unmarked Roads</title><content>This work describes a color Vision-based System intended to perform stable autonomous driving on unmarked roads. Accordingly, this implies the development of an accurate road surface detection system that ensures vehicle stability. Although this topic has already been documented in the technical literature by different research groups, the vast majority of the already existing Intelligent Transportation Systems are devoted to assisted driving of vehicles on marked extra urban roads and highways. The complete system was tested on the BABIECA prototype vehicle, which was autonomously driven for hundred of kilometers accomplishing different navigation missions on a private circuit that emulates an urban quarter. During the tests, the navigation system demonstrated its robustness with regard to shadows, road texture, and weather and changing illumination conditions.</content></document><document><year>2004</year><authors>Anthony Stentz1 | Martial Hebert1</authors><title>A complete navigation system for goal acquisition in unknown environments</title><content>Most autonomous outdoor navigation systems tested on actual robots have centered on local navigation tasks such as avoiding obstacles or following roads. Global navigation has been limited to simple wandering, path tracking, straight-line goal seeking behaviors, or executing a sequence of scripted local behaviors. These capabilities are insufficient for unstructured and unknown environments, where replanning may be needed to account for new information discovered in every sensor image. To address these problems, we have developed a complete system that integrates local and global navigation. The local system uses a scanning laser rangefinder to detect obstacles and recommend steering commands to ensure robot safety. These obstacles are passed to the global system which stores them in a map of the environment. With each addition to the map, the global system uses an incremental path planning algorithm to optimally replan the global path and recommend steering commands to reach the goal. An arbiter combines the steering recommendations to achieve the proper balance between safety and goal acquisition. This system was tested on a real robot and successfully drove it 1.4 kilometers to find a goal given no a priori map of the environment.</content></document><document><year>2004</year><authors>Scott McMillan1 | David E. Orin2  | Robert B. McGhee3 </authors><title>A computational framework for simulation of Underwater Robotic Vehicle systems</title><content>This paper presents a computational framework for efficiently simulating the dynamics and hydrodynamics of Underwater Robotic Vehicle (URV) systems. Through the use of object-oriented mechanisms, a very general yet efficient version of the Articulated-Body (AB) algorithm has been implemented. An efficient solution to branching within chains is developed in the paper so that the algorithm can be used to compute the dynamics for the entire class of open-chain, tree-structured mechanisms. By including compliant contacts with the environment, most closed-chain systems can also be modeled. URV systems with an extended set of topologies can be simulated including proposed underwater walking machines with intra-body powered articulations. Using the encapsulation inherent in C++, the hydrodynamics code has been confined to a single class, thereby explicitly defining this framework and providing an environment for readily implementing desired hydrodynamics algorithms. Resulting simulations are very efficient and can be used in a number of applications both in the development and use of URV systems.</content></document><document><year>2004</year><authors>Laci Jalics1 | Hooshang Hemami1 | Bradley Clymer1</authors><title>A Control Strategy for Terrain Adaptive Bipedal Locomotion</title><content>Rhythmic movements of a five-link sagittal biped with muscle-likeactuators are considered. In walking, as the support phases changecontact is periodically made with the environment. The inputs toevery actuator are modeled after the inputs to muscles in mammals. Thesystem possesses intrinsic position and velocity feedback due to theactuator dynamics. A control strategy is articulated that is novelin that it; a) is physiologically viable; b) simplifies the dynamics;and c) adapts to the speed of walking, going up and down stairs,going up or down inclines, maneuvering over obstacles or holes, andthe tempo and stride length of walking. Walking simulations of afive-link sagittal biped are presented.</content></document><document><year>2004</year><authors>T. J. Tarn1| G. A. Shoults1 | S. P. Yang1</authors><title>A dynamic model of an underwater vehicle with a robotic manipulator using Kane's method</title><content>Development of a robust autonomous Underwater Robotic Vehicle (URV) is a key element to the exploitation of marine resources. An accurate dynamic model is important for both controller design and mission simulation, regardless of the control strategy employed. In this paper, a dynamic model for an underwater vehicle with an n-axis robot arm is developed based on Kane's method. The technique provides a direct method for incorporating external environmental forces into the model. The model developed in this paper includes four major hydrodynamic forces: added mass, profile drag, fluid acceleration, and buoyancy. The model derived is a closed form solution which can be utilized in modern model-based control schemes.</content></document><document><year>2004</year><authors>Satoshi Kagami1| Tomonobu Kitagawa2| Koichi Nishiwaki2| Tomomichi Sugihara2| Masayuki Inaba2 | Hirochika Inoue2</authors><title>A Fast Dynamically Equilibrated Walking Trajectory Generation Method of Humanoid Robot</title><content>This paper describes a fast dynamically equilibrated trajectory generation method for a humanoid robot. From a given input motion and the desired ZMP trajectory, the algorithm generates a dynamically equilibrated trajectory using the relationship between the robot's center of gravity and the ZMP. Three key issues are denoted: 1) an enhanced ZMP constraint which enables the calculation of robot stability even if several limbs are contacting the environment, 2) a simplified robot model is introduced that represents the relationship between its center of gravity and ZMP, 3) a convergence method is adopted to eliminate approximation errors arising from the simplified model. Combining these three key issues together with online ZMP compensation method, humanoid robot H5 have succeeded to walk, step down and so on. Experimental results using humanoid robot H5 are described.</content></document><document><year>2004</year><authors>Stefan Waldherr1| Roseli Romero2 | Sebastian Thrun3</authors><title>A Gesture Based Interface for Human-Robot Interaction</title><content>Service robotics is currently a highly active research area in robotics, with enormous societal potential. Since service robots directly interact with people, finding natural and easy-to-use user interfaces is of fundamental importance. While past work has predominately focussed on issues such as navigation and manipulation, relatively few robotic systems are equipped with flexible user interfaces that permit controlling the robot by natural means. This paper describes a gesture interface for the control of a mobile robot equipped with a manipulator. The interface uses a camera to track a person and recognize gestures involving arm motion. A fast, adaptive tracking algorithm enables the robot to track and follow a person reliably through office environments with changing lighting conditions. Two alternative methods for gesture recognition are compared: a template based approach and a neural network approach. Both are combined with the Viterbi algorithm for the recognition of gestures defined through arm motion (in addition to static arm poses). Results are reported in the context of an interactive clean-up task, where a person guides the robot to specific locations that need to be cleaned and instructs the robot to pick up trash.</content></document><document><year>2004</year><authors>Haris Baltzakis1| 2  | Panos Trahanias1| 2 </authors><title>A Hybrid Framework for Mobile Robot Localization: Formulation Using Switching State-Space Models</title><content>In this paper we address one of the most important issues for autonomous mobile robots, namely their ability to localize themselves safely and reliably within their environments. We propose a probabilistic framework for modelling the robot's state and sensory information based on a Switching State-Space Model. The proposed framework generalizes two of the most successful probabilistic model families currently used for this purpose: the Kalman filter Linear models and the Hidden Markov Models. The proposed model combines the advantages of both models, relaxing at the same time inherent assumptions made individually in each of these existing models.</content></document><document><year>2004</year><authors>Edward A. LeMaster1  | Stephen M. Rock1 </authors><title>A Local-Area GPS Pseudolite-Based Navigation System for Mars Rovers</title><content>Tasks envisioned for future generation Mars rovers&amp;#x2014;sample collection, area survey, resource mining, habitat construction, etc.&amp;#x2014;will require greatly enhanced navigational capabilities over those possessed by the Mars Sojourner rover. Many of these tasks will involve cooperative efforts by multiple rovers and other agents, adding further requirements both for accuracy and commonality between users. This paper presents a new navigation system called a Self-Calibrating Pseudolite Array (SCPA) that can provide centimeter-level, drift-free localization to multiple rovers within a local area by utilizing GPS-based transceivers deployed in a ground-based array. Such a system of localized beacons can replace or augment a system based on orbiting satellite transmitters, and is capable of fully autonomous operations and calibration. This paper describes the basic principles of navigation using an SCPA, focusing on the critical issue of array self-calibration. The new algorithm presented herein&amp;#x2014;called Quadratic Iterative Least Squares&amp;#x2014;achieves successful self-calibration 99.80% of the time even under extremely adverse conditions. The paper concludes with a description of the experimental prototype developed to demonstrate these capabilities and presents successful results from field trials which validate both the navigation and self-calibration functions of the SCPA.</content></document><document><year>2004</year><authors>Michael G. Paulin1 | Larry F. Hoffman2| 3 | Christopher Assad4</authors><title>A Model of Cerebellar Computations for Dynamical State Estimation</title><content>The cerebellum is a neural structure that is essential for agility in vertebrate movements. Its contribution to motor control appears to be due to a fundamental role in dynamical state estimation, which also underlies its role in various non-motor tasks. Single spikes in vestibular sensory neurons carry information about head state. We show how computations for optimal dynamical state estimation may be accomplished when signals are encoded in spikes. This provides a novel way to design dynamical state estimators, and a novel way to interpret the structure and function of the cerebellum.</content></document><document><year>2004</year><authors>Cem &amp;Uuml nsal1 | Han Kili&amp;ccedil &amp;ccedil &amp;ouml te1  | Pradeep K. Khosla1 </authors><title>A Modular Self-Reconfigurable Bipartite Robotic System: Implementation and Motion Planning</title><content>In this manuscript, we discuss I-Cubes, a class of modular robotic system thatis capable of reconfiguring itself in 3-D to adapt to its environment. This is abipartite system, i.e., a collection of (i) active elements for actuation, and (ii)passive elements acting as connectors. Active elements (links) are 3-DOFmanipulators that are capable of attaching/detaching from/to the passive elements(cubes), which can be positioned and oriented using links. Self-reconfigurationcapability enables the system to perform locomotion tasks over difficult terrain; theshape and size can be changed according to the task. This paper describes the designof the system, and 3-D reconfiguration properties. Specifics of the hardwareimplementation, results of the experiments with the current prototypes, our approachto motion planning and problems related to 3-D motion planning are given.</content></document><document><year>2004</year><authors>D&amp;iacute dac Busquets1 | Carles Sierra1  | Ramon L&amp;oacute pez de M&amp;agrave ntaras1 </authors><title>A Multiagent Approach to Qualitative Landmark-Based Navigation</title><content>In this paper we present a multiagent system for landmark-based navigation in unknown environments. We propose a bidding mechanism to coordinate the actions requested by the different agents. The navigation system has been tested on a real robot on indoor unstructured environments.</content></document><document><year>2004</year><authors>Gisbert Lawitzky1 </authors><title>A Navigation System for Cleaning Robots</title><content>Free navigation in indoor environments is one of the main enabling technologies for many service robot applications. The SIEMENS navigation system SINAS which is primarily targeted towards cleaning robot applications, has proved its suitability for tough everyday operation since August 1996 on several occasions, e.g., in several chain store supermarkets. This paper discusses the main requirements of a navigation system for cleaning robots, presents the architecture and main modules of the SINAS system, and reports on real-world experiences.</content></document><document><year>2004</year><authors>George A. Bekey</authors><title>A New Century and a Modest Prediction</title><content>Without Abstract</content></document><document><year>2004</year><authors>S. Hern&amp;aacute ndez1 | J.M. Torres1| C.A. Morales1 | L. Acosta1</authors><title>A New Low Cost System for Autonomous Robot Heading and Position Localization in a Closed Area</title><content>A low cost system for the localization of mobile indoor robots is presented. The system is composed of an emitter located on a wall and a receptor on top of the robot. The emitter is a laser pointer acting like a beacon, and the receptor is a cylinder made out of 32 independent photovoltaic cells. The robot's position and orientation are obtained from the moments when the laser crosses each cell.</content></document><document><year>2004</year><authors>G. Rodriguez1  | C.R. Weisbin1</authors><title>A New Method to Evaluate Human-Robot System Performance</title><content>One of the key issues in space exploration is that of deciding what space tasks are best done with humans, with robots, or a suitable combination of each. In general, human and robot skills are complementary. Humans provide as yet unmatched capabilities to perceive, think, and act when faced with anomalies and unforeseen events, but there can be huge potential risks to human safety in getting these benefits. Robots provide complementary skills in being able to work in extremely risky environments, but their ability to perceive, think, and act by themselves is currently not error-free, although these capabilities are continually improving with the emergence of new technologies. Substantial past experience validates these generally qualitative notions. However, there is a need for more rigorously systematic evaluation of human and robot roles, in order to optimize the design and performance of human-robot system architectures using well-defined performance evaluation metrics. This article summarizes a new analytical method to conduct such quantitative evaluations. While the article focuses on evaluating human-robot systems, the method is generally applicable to a much broader class of systems whose performance needs to be evaluated.</content></document><document><year>2004</year><authors>Luiz Chaimowicz | Vijay Kumar  | Mario F. M. Campos </authors><title>A Paradigm for Dynamic Coordination of Multiple Robots</title><content>In this paper, we present a paradigm for coordinating multiple robots in the execution of cooperative tasks. The basic idea in the paper is to assign to each robot in the team, a role that determines its actions during the cooperation. The robots dynamically assume and exchange roles in a synchronized manner in order to perform the task successfully, adapting to unexpected events in the environment. We model this mechanism using a hybrid systems framework and apply it in different cooperative tasks: cooperative manipulation and cooperative search and transportation. Simulations and real experiments demonstrating the effectiveness of the proposed paradigm are presented.</content></document><document><year>2004</year><authors>R. Rajagopalan1 | R. M. H. Cheng1 </authors><title>A predictor-corrector guidance control scheme for AGV navigation</title><content>This paper presents a scheme that employs feedforward control in conjunction with a predictor-corrector scheme for guidance control of Automated Guided Vehicles (AGVs). The predictor-corrector scheme provides the desired values of steering parameters which depend on the geometry of the track and a driving criterion. The geometry of the track/road ahead of the vehicle is obtained by extrapolating the identified (estimated) geometry of the track/road traversed during the elapsed time interval. This real-time identification is carried out by fitting a curve to the path traversed by the vehicle. The coordinates of the path are provided by a transformation formulation which makes use of the motion parameters and a kinematic model of the vehicle. The driving criterion specifies the positioning of the AGV on the track. Several possible criteria are identified in the paper and mathematical formulations are presented for one such criterion. Results of off-line calculations using simulated track profiles and experimental data obtained using a prototype AGV while following various track profiles are provided for illustration.</content></document><document><year>2004</year><authors>Dieter Fox1 | Wolfram Burgard2 | Hannes Kruppa3  | Sebastian Thrun4 </authors><title>A Probabilistic Approach to Collaborative Multi-Robot Localization</title><content>This paper presents a statistical algorithm for collaborative mobile robot localization. Our approach uses a sample-based version of Markov localization, capable of localizing mobile robots in an any-time fashion. When teams of robots localize themselves in the same environment, probabilistic methods are employed to synchronize each robot's belief whenever one robot detects another. As a result, the robots localize themselves faster, maintain higher accuracy, and high-cost sensors are amortized across multiple robot platforms. The technique has been implemented and tested using two mobile robots equipped with cameras and laser range-finders for detecting other robots. The results, obtained with the real robots and in series of simulation runs, illustrate drastic improvements in localization speed and accuracy when compared to conventional single-robot localization. A further experiment demonstrates that under certain conditions, successful localization is only possible if teams of heterogeneous robots collaborate during localization.</content></document><document><year>2004</year><authors>Sebastian Thrun1 | Wolfram Burgard2  | Dieter Fox2</authors><title>A Probabilistic Approach to Concurrent Mapping and Localization for Mobile Robots</title><content>This paper addresses the problem of building large-scale geometric maps of indoor environments with mobile robots. It poses the map building problem as a constrained, probabilistic maximum-likelihood estimation problem. It then devises a practical algorithm for generating the most likely map from data, along with the most likely path taken by the robot. Experimental results in cyclic environments of size up to 80&amp;times;25 m illustrate the appropriateness of the approach.</content></document><document><year>2004</year><authors>Frank Kirchner1| 1  | Joachim Hertzberg1| 1 </authors><title>A Prototype Study of an Autonomous Robot Platform for Sewerage System Maintenance</title><content>The paper describes KURT, an autonomous robot platform prototypethat is able to navigate through a network of sewage pipes. KURT is asix-wheeled vehicle; it has modular, layered hardware and controlarchitectures. Its sensor configuration consists mainly of stationary andone flexible ultrasound transducers and of two inclinometers. In experimentsrun in a test sewerage system consisting of concrete pipes of 600 mmdiameter, it has proven its abilities to travel safely through straightpipes, to recognize different types of pipe crossings, to turn at crossings,and to navigate from a given start point to arbitrary specified goalpoints.</content></document><document><year>2004</year><authors>Anthony Stentz1| John Bares1| Sanjiv Singh1 | Patrick Rowe1</authors><title>A Robotic Excavator for Autonomous Truck Loading</title><content>Excavators are used for the rapid removal of soil and other materials in mines, quarries, and construction sites. The automation of these machines offers promise for increasing productivity and improving safety. To date, most research in this area has focussed on selected parts of the problem. In this paper, we present a system that completely automates the truck loading task. The excavator uses two scanning laser rangefinders to recognize and localize the truck, measure the soil face, and detect obstacles. The excavator's software decides where to dig in the soil, where to dump in the truck, and how to quickly move between these points while detecting and stopping for obstacles. The system was fully implemented and was demonstrated to load trucks as fast as human operators.</content></document><document><year>2004</year><authors>Reid R. Harrison1  | Christof Koch1 </authors><title>A Robust Analog VLSI Motion Sensor Based on the Visual System of the Fly</title><content>Sensing visual motion gives a creature valuable information about its interactions with the environment. Flies in particular use visual motion information to navigate through turbulent air, avoid obstacles, and land safely. Mobile robots are ideal candidates for using this sensory modality to enhance their performance, but so far have been limited by the computational expense of processing video. Also, the complex structure of natural visual scenes poses an algorithmic challenge for extracting useful information in a robust manner. We address both issues by creating a small, low-power visual sensor with integrated analog parallel processing to extract motion in real-time. Because our architecture is based on biological motion detectors, we gain the advantages of this highly evolved system: A design that robustly and continuously extracts relevant information from its visual environment. We show that this sensor is suitable for use in the real world, and demonstrate its ability to compensate for an imperfect motor system in the control of an autonomous robot. The sensor attenuates open-loop rotation by a factor of 31 with less than 1 mW power dissipation.</content></document><document><year>2004</year><authors>M. Di Marco | A. Garulli | A. Giannitrapani  | A. Vicino </authors><title>A Set Theoretic Approach to Dynamic Robot Localization and Mapping</title><content>This paper addresses the localization and mapping problem for a robot moving through a (possibly) unknown environment where indistinguishable landmarks can be detected. A set theoretic approach to the problem is presented. Computationally efficient algorithms for measurement-to-feature matching, estimation of landmark positions, estimation of robot location and heading are derived, in terms of uncertainty regions, under the hypothesis that errors affecting all sensors measurements are unknown-but-bounded. The proposed technique is validated in both simulation and experimental setups.</content></document><document><year>2004</year><authors>Erwin Prassler1 | Arno Ritter2| Christoph Schaeffer3  | Paolo Fiorini4 </authors><title>A Short History of Cleaning Robots</title><content>The definition of the desired functions and the design of an ultimate versatile personal robot is an ongoing debate. Meanwhile, however, precursors of this yet to evolve species are well on their way to become commercial products. Cleaning robots for public environments as well as for private households seem to be able to provide the breakthrough which the designers of non-industrial robot systems have long awaited.This survey describes a selection of 30 different cleaning robots, with the first developments reaching back more than 15 years. With a few exceptions we have focused on floor cleaning, in particular indoor floor cleaning. We describe a variety of scrubbing and vacuuming robots which were developed for this task. The described systems range from heavy, large, and expensive industrial cleaning vehicles to small-size, light-weight, low-cost household devices. Thesurvey does not include, however, systems for cleaning facades of buildings, or windows, or production tools.</content></document><document><year>2004</year><authors>Yasushi Nakauchi1 | Reid Simmons2</authors><title>A Social Robot that Stands in Line</title><content>Recent research in mobile robot navigation make it feasible to utilize autonomous robots in service fields. But, such applications require more than just navigation. To operate in a peopled environment, robots should recognize and act according to human social behavior. In this paper, we present the design and implementation of one such social behavior: a robot that stands in line much as people do. The system employs stereo vision to recognize lines of people, and uses the concept of personal space for modeling the social behavior. Personal space is used both to detect the end of a line and to determine how much space to leave between the robot and the person in front of it. Our model of personal space is based on measurements from people forming lines. We demonstrate our ideas with a mobile robot navigation system that can purchase a cup of coffee, even if people are waiting in line for service.</content></document><document><year>2004</year><authors>Angelo M. Sabatini1</authors><title>A statistical estimation method for segmentation of sonar range data</title><content>In this paper, we describe how to deal with an important sensorial activity that ultrasonic echo-locating systems for mobile robot navigation have often to perform, namely the extraction of straight line segments from range data and the accurate localization of the corresponding planar targets. It is commonplace that range data segmentation starts with using least squares interpolation algorithms for obtaining straight line segments: it is our goal to prove that caution must be called for in order to avoid somewhat misleading results. The case study concerns the use of a linear array formed by three ultrasonic transducers in a 2D specular environment composed of line and point acoustic targets.The segmentation algorithm we propose is subdivided into two functionally distinct modules, namely identification and localization. The identification module is based on a sequential hypothesis testing between alternative hypotheses that explain the sonar range data as originated from line or point targets. With regard to the localization module, we demonstrate that widely used approaches to sensor modeling are, to some extent, deceptively simple: the estimation accuracy for the localization of planar objects may be decreased by the inability of some traditional sonar sensor models to take properly into account the specularity of reflections. A physically based model of acoustic range sensors acting in specular environments allows us to design a localization module which is capable of producing accurate and unbiased estimates of the parameters of a planar geometric feature.</content></document><document><year>2004</year><authors>M. Girone1| G. Burdea1 | M. Bouzit1| V. Popescu1 | J.E. Deutsch2</authors><title>A Stewart Platform-Based System for Ankle Telerehabilitation</title><content>The Rutgers Ankle is a Stewart platform-type haptic interface designed for use in rehabilitation. The system supplies six-DOF resistive forces in response to virtual reality-based exercises running on a host PC. The Stewart platform uses double-acting pneumatic cylinders, linear potentiometers as position sensors, and a six-DOF force sensor. The Rutgers Ankle controller contains an embedded Pentium board, pneumatic solenoid valves, valve controllers, and associated signal conditioning electronics. Communication with the host PC is over a standard RS232 line. The platform movement and output forces are transparently recorded by the host PC in a database. This database can be accessed remotely over the Internet. Thus, the Rutgers Ankle Orthopedic Rehabilitation Interface will allow patients to exercise at home while being monitored remotely by therapists. A prototype was constructed, and proof-of-concept trials were conducted at the University of Medicine and Dentistry of New Jersey. The results indicate that the system works well as a diagnostic tool. The subjective evaluation by patients was very positive. Further medical trials are needed before the system clinical efficacy in rehabilitation can be established.</content></document><document><year>2004</year><authors>Young D. Kwon1  | Jin S. Lee1 </authors><title>A Stochastic Map Building Method for Mobile Robot using 2-D Laser Range Finder</title><content>This paper presents a stochastic map building method for mobile robot using a 2-D laser range finder. Unlike other methods that are based on a set of geometric primitives, the presented method builds a map with a set of obstacle regions. In building a map of the environment, the presented algorithm represents the obstacles with a number of stochastic obstacle regions, each of which is characterized by its own stochastic parameters such as mean and covariance. Whereas the geometric primitives based map sometimes does not fit well to sensor data, the presented method reliably represents various types of obstacles including those of irregular walls and sets of tiny objects. Their shapes and features are easily extracted from the stochastic parameters of their obstacle regions, and are used to develop reliable navigation and obstacle avoidance algorithms. The algorithm updates the world map in real time by detecting the changes of each obstacle region. Consequently, it is adequate for modeling the quasi-static environment, which includes occasional changes in positions of the obstacles rather than constant dynamic moves of the obstacles. The presented map building method has successfully been implemented and tested on the ARES-II mobile robot system equipped with a LADAR 2D-laser range finder.</content></document><document><year>2004</year><authors>Anthony Stentz1| Cristian Dima1| Carl Wellington1| Herman Herman1 | David Stager1</authors><title>A System for Semi-Autonomous Tractor Operations</title><content>Tractors are the workhorses of the modern farm. By automating these machines, we can increase the productivity, improve safety, and reduce costs for many agricultural operations. Many researchers have tested computer-controlled machines for farming, but few have investigated the larger issues such as how humans can supervise machines and work amongst them. In this paper, we present a system for tractor automation. A human programs a task by driving the relevant routes. The task is divided into subtasks and assigned to a fleet of tractors that drive portions of the routes. Each tractor uses on-board sensors to detect people, animals, and other vehicles in the path of the machine, stopping for such obstacles until it receives advice from a supervisor over a wireless link. A first version of the system was implemented on a single tractor. Several features of the system were validated, including accurate path tracking, the detection of obstacles based on both geometric and non-geometric properties, and self-monitoring to determine when human intervention is required. Additionally, the complete system was tested in a Florida orange grove, where it autonomously drove seven kilometers.</content></document><document><year>2004</year><authors>Gregory Dudek1 | Michael R. M. Jenkin2| Evangelos Milios2 | David Wilkes2 </authors><title>A taxonomy for multi-agent robotics</title><content>A key difficulty in the design of multi-agent robotic systems is the size and complexity of the space of possible designs. In order to make principled design decisions, an understanding of the many possible system configurations is essential. To this end, we present a taxonomy that classifies multi-agent systems according to communication, computational and other capabilities. We survey existing efforts involving multi-agent systems according to their positions in the taxonomy. We also present additional results concerning multi-agent systems, with the dual purposes of illustrating the usefulness of the taxonomy in simplifying discourse about robot collective properties, and also demonstrating that a collective can be demonstrably more powerful than a single unit of the collective.</content></document><document><year>2004</year><authors>Susan Hert1| Sanjay Tiwari1 | Vladimir Lumelsky1</authors><title>A terrain-covering algorithm for an AUV</title><content>An efficient, on-line terrain-covering algorithm is presented for a robot (AUV) moving in an unknown three-dimensional underwater environment. Such an algorithm is necessary for producing mosaicked images of the ocean floor. The basis of this three-dimensional motion planning algorithm is a new planar algorithm for nonsimply connected areas with boundaries of arbitrary shape. We show that this algorithm generalizes naturally to complex three-dimensional environments in which the terrain to be covered is projectively planar. This planar algorithm represents an improvement over previous algorithms because it results in a shorter path length for the robot and does not assume a polygonal environment. The path length of our algorithm is shown to be linear in the size of the area to be covered; the amount of memory required by the robot to implement the algorithm is linear in the size of the description of the boundary of the area. An example is provided that demonstrates the algorithm's performance in a nonsimply connected, nonplanar environment.</content></document><document><year>2004</year><authors>Timothy Horiuchi1 | Kai M. Hynna2</authors><title>A VLSI-Based Model of Azimuthal Echolocation in the Big Brown Bat</title><content>The azimuthal localization of objects by echolocating bats is based on the difference of echo intensity received at the two ears, known as the interaural level difference (ILD). Mimicking the neural computation of ILD in bats, we have constructed a spike-based VLSI model of the lateral superior olive (LSO) that can successfully produce direction-dependent responses. This simple algorithm, while studied in the acoustic domain, is applicable to any localization based on direction-dependent signal attenuation differences.</content></document><document><year>2004</year><authors>Koren Ward1  | Alex|er Zelinsky2 </authors><title>Acquiring Mobile Robot Behaviors by Learning Trajectory Velocities</title><content>The development of robots that learn from experience is a relentless challenge confronting artificial intelligence today. This paper describes a robot learning method which enables a mobile robot to simultaneously acquire the ability to avoid objects, follow walls, seek goals and control its velocity as a result of interacting with the environment without human assistance. The robot acquires these behaviors by learning how fast it should move along predefined trajectories with respect to the current state of the input vector. This enables the robot to perform object avoidance, wall following and goal seeking behaviors by choosing to follow fast trajectories near: the forward direction, the closest object or the goal location respectively. Learning trajectory velocities can be done relatively quickly because the required knowledge can be obtained from the robot's interactions with the environment without incurring the credit assignment problem. We provide experimental results to verify our robot learning method by using a mobile robot to simultaneously acquire all three behaviors.</content></document><document><year>2004</year><authors>J.M. Porta1 | J.J. Verbeek1  | B.J.A. Kr&amp;ouml se1 </authors><title>Active Appearance-Based Robot Localization Using Stereo Vision</title><content>A vision-based robot localization system must be robust: able to keep track of the position of the robot at any time even if illumination conditions change and, in the extreme case of a failure, able to efficiently recover the correct position of the robot. With this objective in mind, we enhance the existing appearance-based robot localization framework in two directions by exploiting the use of a stereo camera mounted on a pan-and-tilt device. First, we move from the classical passive appearance-based localization framework to an active one where the robot sometimes executes actions with the only purpose of gaining information about its location in the environment. Along this line, we introduce an entropy-based criterion for action selection that can be efficiently evaluated in our probabilistic localization system. The execution of the actions selected using this criterion allows the robot to quickly find out its position in case it gets lost. Secondly, we introduce the use of depth maps obtained with the stereo cameras. The information provided by depth maps is less sensitive to changes of illumination than that provided by plain images. The main drawback of depth maps is that they include missing values: points for which it is not possible to reliably determine depth information. The presence of missing values makes Principal Component Analysis (the standard method used to compress images in the appearance-based framework) unfeasible. We describe a novel Expectation-Maximization algorithm to determine the principal components of a data set including missing values and we apply it to depth maps. The experiments we present show that the combination of the active localization with the use of depth maps gives an efficient and robust appearance-based robot localization system.</content></document><document><year>2004</year><authors>J. Lang1 | M.R.M. Jenkin2</authors><title>Active Object Modeling with VIRTUE</title><content>This paper presents a vision system for the task of actively acquiring and modeling the geometry of an unknown object. Using an active trinocular stereo head (VIRTUE), sensed 3-D line segments are grouped into a polyhedral volumetric model through the aid of a constrained Delaunay triangulation. Partial models and a viewpoint enumeration scheme are used to guide the image acquisition process and to determine where to look next. Results of the active vision recovery of a number of objects are provided with their associated volumetric and surface errors.</content></document><document><year>2004</year><authors>Mitra J. Hartmann1 </authors><title>Active Sensing Capabilities of the Rat Whisker System</title><content>The rat whisker system may be a good model for approaching the design of robust robotic active sensing and exploratory systems. Here we examine how rats use their whiskers (vibrissae) during free exploratory behavior and during a texture discrimination task. Results show that during free exploration, the rat rhythmically moves its head to place its small (micro) vibrissae on the surfaces it is exploring. These periodic microvibrissal placements are temporally synchronized with the whisking movements of the large (macro) vibrissae. The periodic microvibrissal placements occurred even during a texture discrimination task, in which a smooth, continuous movement might have been equally effective at extracting the required information. Finally, it was found that rats may sometimes use their micro and macro vibrissae consecutively, instead of simultaneously. This suggests that, like humans, rats' exploration consists of a series of movement sequences in which increasingly refined information is gathered about an object. Some implications of these results for the design of artificial exploratory systems are discussed.</content></document><document><year>2004</year><authors>Shuuji Kajita1  | Kazuo Tani2</authors><title>Adaptive Gait Control of a Biped Robot Based on Realtime Sensing of the Ground Profile</title><content>In this paper, realtime control of dynamic biped locomotion usingsensor information is investigated. We used an ultrasonic rangesensor mounted on the robot to measure the distance from the robot tothe ground surface. During the walking control, the sensor data isconverted into a simple representation of the ground profile inrealtime. We also developed a control architecture based on theLinear Inverted Pendulum Mode which we proposed previously fordynamic walking control. Combining the sensory system and thecontrol system enabled our biped robot, Meltran II, to walk overground of unknown profile successfully.</content></document><document><year>2004</year><authors>Felipe Espinosa1 | Elena L&amp;oacute pez1 | Ra&amp;uacute l Mateos1 | Manuel Mazo1  | Ricardo Garc&amp;iacute a1 </authors><title>Advanced and Intelligent Control Techniques Applied to the Drive Control and Path Tracking Systems on a Robotic Wheelchair</title><content>This paper presents the theoretical support and experimental results of the application of advanced and intelligent control techniques to the drive control and trajectory tracking systems on a robotic wheelchair. The adaptive optimal control of the differential drive helps to improve the automatic guidance system's safety and comfort taking into consideration operating conditions such as load and distribution changes or motion actuator limitations. Furthermore, the incorporation of an optimal controller to minimize location errors and a fuzzy controller to adapt the linear velocity to the characteristics of the trajectory, provide the vehicle with a high degree of intelligence and autonomy, even when faced with obstacles. The global control solution implemented increases the features of the wheelchair for handicapped people, especially for those with a high degree of disability.</content></document><document><year>2004</year><authors>Terrence Fong1| 2| Charles Thorpe1 | Charles Baur2</authors><title>Advanced Interfaces for Vehicle Teleoperation: Collaborative Control, Sensor Fusion Displays, and Remote Driving Tools</title><content>We are working to make vehicle teleoperation accessible to all users, novices and experts alike. In our research, we are developing a new control model for teleoperation, sensor-fusion displays and a suite of remote driving tools. Our goal is to build a framework which enables humans and robots to communicate, to exchange ideas and to resolve differences. In short, to develop systems in which humans and robots work together and jointly solve problems.</content></document><document><year>2004</year><authors>J. Corde Lane1| Craig R. Carignan1 | David L. Akin1</authors><title>Advanced Operator Interface Design for Complex Space Telerobots</title><content>With technology advancements in computers and displays, computer interfaces can be used to alleviate the operator workload while controlling a complex robot. A graphical simulation of the robotic system can be used to improve development, train operators, and enhance their performance during actual operations. This paper summarizes the advantages realized using a graphical simulation to visually display telemetry from a multiple arm space telerobot. By displaying the commanded position of a manipulator graphically along with the actual position, the operator becomes more effective in diagnosing anomalies in the system. The negative impact of communication time delay can also be alleviated using this commanded display. The above advantages coupled with the simulation's ability to display multiple synthetic views, to move each view to any virtual location, and to highlight functions to emphasize important information, can ease the operator's workload, making him or her more effective in controlling a complex system.</content></document><document><year>2004</year><authors>Matt Wilson | Chris Melhuish | Ana B. Sendova-Franks  | Samuel Scholes </authors><title>Algorithms for Building Annular Structures with Minimalist Robots Inspired by Brood Sorting in Ant Colonies</title><content>This study shows that a task as complicated as multi-object ant-like annular sorting can be accomplished with minimalist solutions employing simple mechanisms and minimal hardware. It provides an alternative to patch sorting for multi-object sorting. Three different mechanisms, based on hypotheses about the behaviour of Leptothorax ants are investigated and comparisons are made. Mechanism I employs a simple clustering algorithm, with objects of different sizes. The mechanism explores the idea that it is the size difference of the object that promotes segregation. Mechanism II is an extension to our earlier two-object segregation mechanism. We test the ability of this mechanism to segregate an increased number of object types. Mechanism III uses a combined leaky integrator, which allows a greater segregation of object types while retaining the compactness of the structure. Its performance is improved by optimizing the mechanism's parameters using a genetic algorithm. We compare the three mechanisms in terms of sorting performance. Comparisons between the results of these sorting mechanisms and the behaviour of ants should facilitate further insights into both biological and robotic research and make a contribution to the further development of swarm robotics.</content></document><document><year>2004</year><authors>Haoyong Yu1| Matthew Spenko1 | Steven Dubowsky1 </authors><title>An Adaptive Shared Control System for an Intelligent Mobility Aid for the Elderly</title><content>The control system for a personal aid for mobility and health monitoring (PAMM) for the elderly is presented. PAMM is intended to assist the elderly living independently or in senior assisted living facilities. It provides physical support and guidance, as well as monitoring basic vital signs for users that may have both limited physical and cognitive capabilities. This paper presents the design of a bi-level control system for PAMM. The first level is an admittance-based mobility controller that provides a natural and intuitive human machine interface. The second level is an adaptive shared controller that allocates control between the user and the computer based on metrics of the user's performance. Field trials at an eldercare facility show the effectiveness of the design.</content></document><document><year>2004</year><authors>H.R. Everett1 | G.A. Gilbreath1 | D.A. Ciccimaro1</authors><title>An Advanced Telereflexive Tactical Response Robot</title><content>ROBART III is intended as an advanced demonstration platform for non-lethal tactical response, extending the concepts of reflexive teleoperation into the realm of coordinated weapons control (i.e., sensor-aided control of mobility, camera, and weapon functions) in law enforcement and urban warfare scenarios. A rich mix of ultrasonic and optical proximity and range sensors facilitates remote operation in unstructured and unexplored buildings with minimal operator oversight. Supervised autonomous navigation and mapping of interior spaces is significantly enhanced by an innovative algorithm which exploits the fact that the majority of man-made structures are characterized by (but not limited to) parallel and orthogonal walls. This paper presents a brief overview of the advanced telereflexive man-machine interface and its associated human-centered mapping strategy.</content></document><document><year>2004</year><authors>Bj&amp;ouml rn &amp;Aring str|1 | Albert-Jan Baerveldt1</authors><title>An Agricultural Mobile Robot with Vision-Based Perception for Mechanical Weed Control</title><content>This paper presents an autonomous agricultural mobile robot for mechanical weed control in outdoor environments. The robot employs two vision systems: one gray-level vision system that is able to recognize the row structure formed by the crops and to guide the robot along the rows and a second, color-based vision system that is able to identify a single crop among weed plants. This vision system controls a weeding-tool that removes the weed within the row of crops. The row-recognition system is based on a novel algorithm and has been tested extensively in outdoor field tests and proven to be able to guide the robot with an accuracy of &amp;plusmn;2 cm. It has been shown that color vision is feasible for single plant identification, i.e., discriminating between crops and weeds. The system as a whole has been verified, showing that the subsystems are able to work together effectively. A first trial in a greenhouse showed that the robot is able to manage weed control within a row of crops.</content></document><document><year>2004</year><authors>Kyung-Min Jeong1 | Jun-Ho Oh1</authors><title>An aperiodic straight motion planning method for a quadruped walking robot</title><content>In even terrain, wave gait is the periodic gait having the optimal stability. In this paper, we focus on aperiodic forward straight motion having the lifting sequence of wave gait in order for quadruped to adapt to terrain and to have good moving capability. We investigated the condition of support pattern from which such gait motion can be generated. It is proved that from any support pattern satisfying the condition, it is always possible to transform the given support pattern to the support pattern of wave gait. An aperiodic gait planning method that adapt to terrain and maximize moving capability is proposed. A simulation result shows that the proposed method works well in rough terrain having forbidden areas.</content></document><document><year>2004</year><authors>Kyung-Min Jeong1  | Jun-Ho Oh1 </authors><title>An aperiodic Z type spinning gait planning method for a quadruped walking robot</title><content>Spinning gaits are used for altering the direction of body in a narrow space. Previous studies reveal thatz type leg-lifting sequence is suitable for spinning motion. In this paper, we focus on anz type aperiodic spinning gait for a quadruped walking robot. We proposed a condition of support pattern suitable for the aperiodicz type spinning motion. Based on the condition, we proposed an aperiodicz type spinning gait planning method. It is shown that spinning capability can be independent of required stability margin. A simulation shows that good spinning capability and good terrain adaptability are obtained.</content></document><document><year>2004</year><authors>Vivek A. Sujan | Steven Dubowsky| Terry Huntsberger| Hr| Aghazarian| Yang Cheng | Paul Schenker</authors><title>An Architecture for Distributed Environment Sensing with Application to Robotic Cliff Exploration</title><content>Future planetary exploration missions will use cooperative robots to explore and sample rough terrain. To succeed robots will need to cooperatively acquire and share data. Here a cooperative multi-agent sensing architecture is presented and applied to the mapping of a cliff surface. This algorithm efficiently repositions the systems' sensing agents using an information theoretic approach and fuses sensory information using physical models to yield a geometrically consistent environment map. This map is then distributed among the agents using an information based relevant data reduction scheme. Experimental results for cliff face mapping using the JPL Sample Return Rover (SRR) are presented. The method is shown to significantly improve mapping efficiency over conventional methods.</content></document><document><year>2004</year><authors>Nenad M. Kircanski1| 2 | Andrew A. Goldenberg1| 2  | Sheldon K. Dickie3</authors><title>An autonomous cable winding and pay-out system for mobile robots</title><content>A reliable communication link between a supervisory computer and a mobile robot is essential for achieving various mission goals, especially those related to hazardous and explosive material handling. Coaxial copper and fiber-optic cable proved to be much more reliable than aerial communication links. Aerial communication used with explosive disposal robots allows for possible terrorist interference making cable communication a requirement. The main problem in using a cable is that it has to be carefully paid out and stacked on the winding spool when the vehicle is moving from/towards the operator. Clearly, the cable can not be dragged by the robot since the typical radius of a mobile robot is 200 to 500 m. Therefore, building a winding system capable of performing smooth and neat cable winding and pay-out operation is crucial for the overall robot reliability. Unfortunately, research in the area of mobile robotic systems has traditionally been focused elsewhere. For this reason a winding system was developed conceptually, and a prototype was built. Hundreds of experiments were performed before completing the work. The developed prototype has unique features including: (i) full independence of the robot control/sensorial system, that is, it is autonomous with respect to the vehicle carrying it; (ii) handling different cables without any readjustment of the control system; and (iii) a sophisticated safety system to prevent irregular cable handling. The paper includes the description of the developed prototype, the underlying conceptual ideas, as well as experimental results.</content></document><document><year>2004</year><authors>E.J. van Henten1| J. Hemming1| B.A.J. van Tuijl1| J.G. Kornet1| J. Meuleman1| J. Bontsema1 | E.A. van Os1</authors><title>An Autonomous Robot for Harvesting Cucumbers in Greenhouses</title><content>This paper describes the concept of an autonomous robot for harvesting cucumbers in greenhouses. A description is given of the working environment of the robot and the logistics of harvesting. It is stated that for a 2 ha Dutch nursery, 4 harvesting robots and one docking station are needed during the peak season. Based on these preliminaries, the design specifications of the harvest robot are defined. The main requirement is that a single harvest operation may take at most 10 s. Then, the paper focuses on the individual hardware and software components of the robot. These include, the autonomous vehicle, the manipulator, the end-effector, the two computer vision systems for detection and 3D imaging of the fruit and the environment and, finally, a control scheme that generates collision-free motions for the manipulator during harvesting. The manipulator has seven degrees-of-freedom (DOF). This is sufficient for the harvesting task. The end-effector is designed such that it handles the soft fruit without loss of quality. The thermal cutting device included in the end-effector prevents the spreading of viruses through the greenhouse. The computer vision system is able to detect more than 95% of the cucumbers in a greenhouse. Using geometric models the ripeness of the cucumbers is determined. A motion planner based on the A*-search algorithm assures collision-free eye-hand co-ordination. In autumn 2001 system integration took place and the harvesting robot was tested in a greenhouse. With a success rate of 80%, field tests confirmed the ability of the robot to pick cucumbers without human interference. On average the robot needed 45 s to pick one cucumber. Future research focuses on hardware and software solutions to improve the picking speed and accuracy of the eye-hand co-ordination of the robot.</content></document><document><year>2004</year><authors>Barney Pell1 | Douglas E. Bernard2| Steve A. Chien2| Erann Gat2| Nicola Muscettola3| P. P|urang Nayak3| Michael D. Wagner4  | Brian C. Williams5 </authors><title>An Autonomous Spacecraft Agent Prototype</title><content>This paper describes the New Millennium Remote Agent (NMRA) architecture for autonomous spacecraft control systems. The architecture supports challenging requirements of the autonomous spacecraft domain not usually addressed in mobile robot architectures, including highly reliable autonomous operations over extended time periods in the presence of tight resource constraints, hard deadlines, limited observability, and concurrent activity. A hybrid architecture, NMRA integrates traditional real-time monitoring and control with heterogeneous components for constraint-based planning and scheduling, robust multi-threaded execution, and model-based diagnosis and reconfiguration. Novel features of this integrated architecture include support for robust closed-loop generation and execution of concurrent temporal plans and a hybrid procedural/deductive executive.We implemented a prototype autonomous spacecraft agent within the architecture and successfully demonstrated the prototype in the context of a challenging autonomous mission scenario on a simulated spacecraft. As a result of this success, the integrated architecture has been selected to fly as an autonomy experiment on Deep Space One (DS-1), the first flight of NASA';s New Millennium Program (NMP), which will launch in 1998. It will be the first AI system to autonomously control an actual spacecraft.</content></document><document><year>2004</year><authors>S. Kazadi1 | R. Goodman2 | D. Tsikata3 | D. Green4  | H. Lin5</authors><title>An Autonomous Water Vapor Plume Tracking Robot Using Passive Resistive Polymer Sensors</title><content>A simple reactive robot is described which is capable of tracking a water vapor plume to its source. The robot acts completely within the plume and is endowed with no deliberate information about wind direction or speed, yet accurately tracks the plume upstream. The robot's behavior, results from the behavior of simple resistive polymer sensors and their strategic placement on the robot's body.</content></document><document><year>2004</year><authors>Edwardo F. Fukushima1 | Shigeo Hirose1 </authors><title>An efficient steering control formulation for the articulated body mobile robot KR-II</title><content>This paper introduces an efficient steering control method for the articulated body mobile robot Koryu-II (KR-II). KR-II is a real robot, composed of six cylindrical segments linked in series and has a long snake-like appearance. The main issue on KR-II's steering control is, given from a remote human operator the velocity and orientation commands for the foremost segment, to automatically generate joint commands for all the following segments, such that they follow the foremost segment's trajectory. The derived method is based on a trajectory planning scheme in the inertial reference frame, and is feasible for real time computation. It also presents good energy efficiency and trajectory tracking performance characteristics, and can be extended for KR-II's W-Shaped Configuration steering control, which augments the lateral stability of the robot, essential for locomotion over uneven terrain. The validity of these methods are verified by experiments on the mechanical model KR-II.</content></document><document><year>2004</year><authors>Bradley J. Nelson1 | Pradeep K. Khosla2</authors><title>An Expectation-Based Framework of Object Schemas and Port-Based Agents for Disparate Feedback Assimilation</title><content>Robotic manipulation systems that operate in unstructured environments must be responsive to feedback from sensors that are disparate in both location and modality. This paper describes a distributed framework for assimilating the disparate feedback provided by force and vision sensors, including active vision sensors, for robotic manipulation systems. The main components of the expectation-based framework include object schemas and port-based agents. Object schemas represent the manipulation task internally in terms of geometric models with attached sensor mappings. Object schemas are dynamically updated by sensor feedback, and thus provide an ability to perform three dimensional spatial reasoning during task execution. Because object schemas possess knowledge of sensor mappings, they are able to both select appropriate sensors and guide active sensors based on task characteristics. Port-based agents are the executors of reference inputs provided by object schemas and are defined in terms of encapsulated control strategies. Experimental results demonstrate the capabilities of the framework in two ways: the performance of manipulation tasks with active camera-lens systems, and the assimilation of force and vision sensory feedback.</content></document><document><year>2004</year><authors>Ryo Kurazume1  | Shigeo Hirose2</authors><title>An Experimental Study of a Cooperative Positioning System</title><content>Several position identification methods are being used for mobile robots. Dead reckoning is a popular method but due to the error accumulation from wheel slippage, its reliability is low for measurement of long distances especially on uneven surfaces. Another popular method is the landmark method, which estimates current position relative to known landmarks, but the landmark method''s limitation is that it cannot be used in an uncharted environment. Thus, this paper proposes a new method called Cooperative Positioning System (CPS) that is able to overcome these shortcomings. The main concept of CPS is to divide the robots into two groups, A and B where group A remains stationary and acts as a landmark while group B moves and then group B stops and acts as a landmark for group A. This process is repeated until the target position is reached. Compared with dead reckoning, CPS has a far lower accumulation of positioning errors, and can also work in three dimensions. Furthermore, CPS employs inherent landmarks and therefore can be used in uncharted environments unlike the landmark method. In this paper, we introduce the basic concept of CPS and its positioning principle. Next, we outline a second prototype CPS machine model (CPS-II) and discuss the method of position estimation using the variance of positioning error and weighted least squares method. Position identification experiments using the CPS-II model give a positioning accuracy of 0.12% for position and 0.32 degree for attitude after the robots traveled a distance of 21.5 m.</content></document><document><year>2004</year><authors>Christopher Assad1 </authors><title>An Hypothesis for a Novel Learning Mechanism in the Cerebellar Cortex</title><content>The cerebellar cortical circuitry may support a distinct second form of associative learning, complementary to the well-known synaptic plasticity (long term depression, LTD) that has been previously shown. As the granule cell axons ascend to the molecular layer, they make multiple synapses on the overlying Purkinje cells (PC). This ascending branch (AB) input, which has been ignored in models of cerebellar learning, is likely to be functionally distinct from the parallel fiber (PF) synaptic input. We predict that AB-PF correlations lead to Hebbian-type learning at the PF-PC synapse, including long term potentiation (LTP), and allowing the cortical circuit to combine AB-PF LTP for feedforward state prediction with climbing fiber LTD for feedback error correction. The new learning mechanism could therefore add computational capacity to cerebellar models and may explain more of the experimental data.</content></document><document><year>2004</year><authors>Andrew Howard1| Maja J. Matari1 | Gaurav S. Sukhatme1</authors><title>An Incremental Self-Deployment Algorithm for Mobile Sensor Networks</title><content>This paper describes an incremental deployment algorithm for mobile sensor networks. A mobile sensor network is a distributed collection of nodes, each of which has sensing, computation, communication and locomotion capabilities. The algorithm described in this paper will deploy such nodes one-at-a-time into an unknown environment, with each node making use of information gathered by previously deployed nodes to determine its deployment location. The algorithm is designed to maximize network coverage while simultaneously ensuring that nodes retain line-of-sight relationships with one another. This latter constraint arises from the need to localize the nodes in an unknown environment: in our previous work on team localization (A. Howard, M.J. Matari, and G.S. Sukhatme, in Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems, EPFL, Switzerland, 2002; IEEE Transactions on Robotics and Autonomous Systems, 2002) we have shown how nodes can localize themselves by using other nodes as landmarks. This paper describes the incremental deployment algorithm and presents the results from an extensive series of simulation experiments. These experiments serve to both validate the algorithm and illuminate its empirical properties.</content></document><document><year>2004</year><authors>George A. Bekey</authors><title>Another Journal is born</title><content>Without Abstract</content></document><document><year>2004</year><authors>Tom Ziemke1  | Noel E. Sharkey2 </authors><title>Artificial Neural Networks for Robot Learning&amp;#x2014;Guest Editors' Introduction</title><content>Without Abstract</content></document><document><year>2004</year><authors>Illah R. Nourbakhsh1 | Michael R. Genesereth1 </authors><title>Assumptive planning and execution: A simple, working robot architecture</title><content>In this paper, we present a simple approach to interleaving planning and execution based on the idea of making simplifying assumptions. Since assumptions can be tragically wrong, this assumptive approach must ensure both that the robot does not believe it has solved a problem when it has not and that it does not take steps that make a problem unsolvable. We present an assumptive algorithm that preserves goal-reachability and in addition we specify conditions under which the assumptive architecture is sound and complete.We have successfully implemented the assumptive architecture on several real-world robots. Students in an introductory robot lab at Stanford University implement an assumptive system on robots that have incomplete information about their maze world. Dervish, our winning entry in the 1994 AAAI National Robot Competition, implements an assumptive architecture to cope with partially specified environments and unreliable effectors and sensors.</content></document><document><year>2004</year><authors>Ajo Fod1| Maja J. Matari1 | Odest Chadwicke Jenkins1</authors><title>Automated Derivation of Primitives for Movement Classification</title><content>We describe a new method for representing human movement compactly, in terms of a linear super-imposition of simpler movements termed primitives. This method is a part of a larger research project aimed at modeling motor control and imitation using the notion of perceptuo-motor primitives, a basis set of coupled perceptual and motor routines. In our model, the perceptual system is biased by the set of motor behaviors the agent can execute. Thus, an agent can automatically classify observed movements into its executable repertoire. In this paper, we describe a method for automatically deriving a set of primitives directly from human movement data.We used movement data gathered from a psychophysical experiment on human imitation to derive the primitives. The data were first filtered, then segmented, and principal component analysis was applied to the segments. The eigenvectors corresponding to a few of the highest eigenvalues provide us with a basis set of primitives. These are used, through superposition and sequencing, to reconstruct the training movements as well as novel ones. The validation of the method was performed on a humanoid simulation with physical dynamics. The effectiveness of the motion reconstruction was measured through an error metric. We also explored and evaluated a technique of clustering in the space of primitives for generating controllers for executing frequently used movements.</content></document><document><year>2004</year><authors>B. Thuilot1| C. Cariou2| P. Martinet1 | M. Berducat2</authors><title>Automatic Guidance of a Farm Tractor Relying on a Single CP-DGPS</title><content>Precision agriculture involves very accurate farm vehicle control along recorded paths, which are not necessarily straight lines. In this paper, we investigate the possibility of achieving this task with a CP-DGPS as the unique sensor. The vehicle heading is derived according to a Kalman state reconstructor, and a nonlinear velocity independent control law is designed, relying on chained systems properties. Field experiments, demonstrating the capabilities of our guidance system, are reported and discussed.</content></document><document><year>2004</year><authors>P. Martinet1  | C. Thibaud2 </authors><title>Automatic Guided Vehicles: Robust Controller Design in Image Space</title><content>We have been interested in Automatic Guided Vehicles (AGV) for several years. In this paper, we synthesize controllers for AGV applications using monocular vision. In particular, we are interested in road following and direction change tasks, and in analyzing the influence of extrinsic camera parameter perturbations on vehicle behavior. We use the bicycle as the kinematic vehicle model, and we choose the position of the white band on the road as the sensor signal. We define an interaction between the camera, which is mounted inside the vehicle, and the white band detected in the image space. Using this kind of interaction, we present how to use a pole assignment technique to solve the servoing task. We show the simulation and experimental results (1/10 scale demonstrator) with and without perturbations. We then investigate the use of a robust controller to slow down the effect of perturbations on the behavior of the vehicle.</content></document><document><year>2004</year><authors>Clayton Kunz1| Thomas Willeke1 | Illah R. Nourbakhsh2</authors><title>Automatic Mapping of Dynamic Office Environments</title><content>We present a robot, InductoBeast, that greets a new office building by learning the floorplan automatically, with minimal human intervention and a priori knowledge. Our robot architecture is unique because it combines aspects of both abductive and inductive mapping methods to solve this problem. We present experimental results spanning three ofiice environments, mapped and navigated during normal business hours. We hope these results help to establish a performance benchmark against which robust and adaptive mapping robots of the future may be measured.</content></document><document><year>2004</year><authors>F. Javier Rodr&amp;iacute guez1| Manuel Mazo1 | Miguel A. Sotelo1</authors><title>Automation of an Industrial Fork Lift Truck, Guided by Artificial Vision in Open Environments</title><content>Mobile robots capable of moving autonomously in more or less structured environments are being increasingly employed in the automation of certain industrial processes. Along these lines, the authors constructed a platform, on the base of a commercial industrial truck, provided with sufficient autonomy to carry out tasks within an industrial environment (VIA: Autonomous Industrial Vehicle).One of the sensor systems used in the truck is a system of artificial vision which enables it to move on asphalted surfaces both in open environments (roads) and closed ones, seeking the markings which most easily allow it to determine the path marked in the images. The system for following roads is capable of following painted lines, determining the sides of the road by texture analysis or determining the minimum width of the road for the robot to pass, according to the circumstances. A model of the road predicts its situation and enables a decision to be made on whether the information provided by the algorithm is reliable or not. At the same time, a neural network is trained with the results obtained by any of the previous algorithms, in such a way that when the training process converges the network takes over the steering of the truck.</content></document><document><year>2004</year><authors>Barry Brumitt1| Anthony Stentz1| Martial Hebert1 | CMU UGV Group1</authors><title>Autonomous Driving with Concurrent Goals and Multiple Vehicles: Experiments and Mobility Components</title><content>In this paper, we describe a complete system for mission planning and execution for multiple robots in natural terrain. We report on experiments with a system for autonomously driving two vehicles based on complex mission specifications. We show that the system is able to plan local paths in obstacle fields based on sensor data, to plan and update global paths to goals based on frequent obstacle map updates, and to modify mission execution, e.g., the assignment and ordering of the goals, based on the updated paths to the goals.Two recently developed sensors are used for obstacle detection: a high-speed laser range finder, and a video-rate stereo system. An updated version of a dynamic path planner, D*, is used for on-line computation of routes. A new mission planning and execution-monitoring tool, GRAMMPS, is used for managing the allocation and ordering of goals between vehicles.</content></document><document><year>2004</year><authors>Barry Brumitt1| Anthony Stentz1| Martial Hebert1 | The Cmu Ugv Group1</authors><title>Autonomous Driving with Concurrent Goals and Multiple Vehicles: Mission Planning and Architecture</title><content>We introduce a new distributed planning paradigm, which permits optimal execution and dynamic replanning of complex multi-goal missions. In particular, the approach permits dynamic allocation of goals to vehicles based on the current environment model while maintaining information-optimal route planning for each individual vehicle to individual goals. Complex missions can be specified by using a grammar in which ordering of goals, priorities, and multiple alternatives can be described. We show that the system is able to plan local paths in obstacle fields based on sensor data, to plan and update global paths to goals based on frequent obstacle map updates, and to modify mission execution, e.g., the assignment and ordering of the goals, based on the updated paths to the goals.The multi-vehicle planning system is based on the GRAMMPS planner; the on-board dynamic route planner is based on the D* planner. Experiments were conducted with stereo and high-speed ladar as the to sensors used for obstacle detection. This paper focuses on the multi-vehicle planner and the systems architecture. A companion paper (Brumitt et al., 2001) analyzes experiments with the multi-vehicle system and describes in details the other components of the system.</content></document><document><year>2004</year><authors>Katsumi Kimoto1  | Shin'ichi Yuta1 </authors><title>Autonomous mobile robot simulator&amp;#x2014;a programming tool for sensor-based behavior</title><content>An autonomous mobile robot must achieve its goal in very complex environments with uncertainties of sensors and actuators. Due to such uncertainties, the control algorithm of robot behavior must have the ability to cope with various possible environmental situations and robot status. To develop such a control algorithm of robot behavior, the algorithm must be tested under numerous conditions of the robot's environment.Such a process requires a large number of experiments using real robots and because of high experimental cost and environmental complexity, a realistic simulator should be developed for verification of behavior algorithms.</content></document><document><year>2004</year><authors>M. Simoncelli1| G. Zunino1| H.I. Christensen1 | K. Lange2</authors><title>Autonomous Pool Cleaning: Self Localization and Autonomous Navigation for Cleaning</title><content>Cleaning is a major problem associated with pools. Since the manual cleaning is tedious and boring there is an interest in automating the task. This paper presents methods for autonomous localization and navigation for a pool cleaner to enable full coverage of pools. Path following cannot be ensured through use of internal position estimation methods alone; therefore sensing is needed. Sensor based estimation enable automatic correction of slippage. For this application we use ultrasonic sonars. Based on an analysis of the overall task and performance of the system a strategy for cleaning/navigation is developed. For the automatic localization a Kalman filtering technique is proposed: the Kalman filter uses sonar measurements and a dynamic model of the robot to provide estimates of the pose of the pool cleaner. Using this localization method we derive an optimal control strategy for traversal of a pool. The system has been implemented and successfully tested on the WEDAB400 pool cleaner.</content></document><document><year>2004</year><authors>D. B. Marco1| A. J. Healey1 | R. B. McGhee1 </authors><title>Autonomous underwater vehicles: Hybrid control of mission and motion</title><content>This paper provides an experimental implementation and verification of a hybrid (mixed discrete state/ continuous state) controller for semi-autonomous and autonomous underwater vehicles in which the missions imply multiple task robot behavior. An overview of some of the missions being considered for this rapidly developing technology is mentioned including environmental monitoring, underwater inspection, geological survey as well as military missions in mine countermeasures.The functionalities required of such vehicles and their relation to intelligent control technology is discussed. In particular, the use of Prolog as a computer language for the specification of the discrete event system (DES) aspects of the mission control is proposed. The connections between a Prolog specification and the more common Petri Net graphical representation of a DES are made. Links are made between activation commands, transitioning signals, and the continuous state dynamic control system (DCS) responsible for vehicle stabilization.</content></document><document><year>2004</year><authors>K. Schilling1 | J. De Lafontaine2 | H. Roth3</authors><title>Autonomy capabilities of European deep space probes</title><content>The European Space Agency ESA is currently preparing the two deep space missions, Huygens and Rosetta. This paper reviews the related requirements for autonomous operations in a poorly known environment. While for Huygens emphasis is on the control of the descent through Titan's atmosphere, for Rosetta the safe drilling of material samples in the microgravity environment of a comet is analysed. The control concepts to enable adaptive reactions in an uncertain, remote environment are summarized for the crucial mission phases. The performance results obtained from computer simulations are presented and discussed.</content></document><document><year>2004</year><authors>Jeffrey A. Fayman1| Ehud Rivlin1 | Henrik I. Christensen2</authors><title>AV-Shell, an Environment for Autonomous Robotic Applications Using Active Vision</title><content>In this paper, we present a system called the Active Vision Shell (AV-shell) which provides a programming framework for expressing and implementing autonomous robotic tasks using perception and action where perception is provided by active vision. The AV-shell is a system with a powerful interactive C-shell style interface providing many important capabilities including: (1) architectural support; (2) an abstract interface enabling interaction with a wide variety of devices; (3) a rich set of visual routines; and (4) a process composition framework. The utility of the AV-shell is demonstrated in several examples showing the relevance of the AV-shell to meaningful applications in autonomous robotics.</content></document><document><year>2004</year><authors>Maria C. Garcia-Alegre1  | Felicidad Recio1</authors><title>Basic Visual and Motor Agents for Increasingly Complex Behavior Generation on a Mobile Robot</title><content>Present work addresses the guidelines that have been followed to construct basic behavioral agents for visually guided navigation within the framework of a hierarchical architecture. Visual and motor interactions are described within this generic framework that allows for an incremental development of behavior from an initial basis set. Basic locomotion agents as, Stop&amp;amp;Backward, Avoid, and Forward are implemented by means of fuzzy knowledge bases to deal with the uncertainty and imprecision inherent to real systems and environments. Basic visual agents as, Saccadic, Find_Contour, and Center are raised under a space-variant representation pursuing an anthropomorphic approach. We illustrate how a complex behavior results from the combination of lower level agents always connected to the basic motor agents. The proposed methodology is validated on a caterpillar mobile robot in navigation tasks directed by an object description.</content></document><document><year>2004</year><authors>Olivier Lebeltel| Pierre Bessi&amp;egrave re | Julien Diard | Emmanuel Mazer</authors><title>Bayesian Robot Programming</title><content>We propose a new method to program robots based on Bayesian inference and learning. It is called BRP for Bayesian Robot Programming. The capacities of this programming method are demonstrated through a succession of increasingly complex experiments. Starting from the learning of simple reactive behaviors, we present instances of behavior combination, sensor fusion, hierarchical behavior composition, situation recognition and temporal sequencing. This series of experiments comprises the steps in the incremental development of a complex robot program. The advantages and drawbacks of BRP are discussed along with these different experiments and summed up as a conclusion. These different robotics programs may be seen as an illustration of probabilistic programming applicable whenever one must deal with problems based on uncertain or incomplete knowledge. The scope of possible applications is obviously much broader than robotics.</content></document><document><year>2004</year><authors>Blake Hannaford1| Kristen Jaax1 | Glenn Klute1</authors><title>Bio-Inspired Actuation and Sensing</title><content>The superb ability of animals to negotiate rough terrain has caused engineers to focus on mechanical properties of muscle and other unique features in order to design improved robots for exploration. This paper reviews recent work in artificial muscle actuators, as well as a new sensor based on a robotic model of the muscle spindle cell. The actuator contains a pneumatic force generator in parallel with a non-linear damping element and in series with a non-linear elastic tendon. Work loop experiments were performed to characterize this actuator under conditions similar to real locomotion at different speeds. The robotic muscle spindle is an 8 &amp;times; 1 cm device which simulates the response of the physiological muscle spindle to stretch. Its non-linear properties are thought to contribute to stable accurate control over a wide range of motion.</content></document><document><year>2004</year><authors>Terry Huntsberger1 </authors><title>Biologically Inspired Autonomous Rover Control</title><content>Robotic missions beyond 2013 will likely be precursors to a manned habitat deployment on Mars. Such missions require robust control systems for long duration activities. Current single rover missions will evolve into deployment of multiple, heterogeneous cooperating robotic colonies. This paper describes the map-making memory and action selection mechanism of BISMARC (iologically nspired ystem for ap-based utonomous over ontrol) that is currently under development at the Jet Propulsion Laboratory in Pasadena, CA (Huntsberger and Rose, Neutral Networks, 11(7/8):1497&amp;#x2013;1510). BISMARC is an integrated control system for long duration missions involving robots performing cooperative tasks.</content></document><document><year>2004</year><authors>Tyson H. Harty1 | Gene G. Korienek1 | Charles Leddon1  | Abigail B. Bautista1 </authors><title>Biologically-Inspired Collective Control for an Autonomous Robotic Arm</title><content>Biological collective control architectures and simple control principles used in nervous systems provide novel alternative approaches for the design of fault-tolerant, adaptable real-world robotic systems that have traditionally relied on centralized control. In this research, a robotic arm composed of multiple identical segments in a collective computational architecture was tested for its ability to produce adaptive pointing and reaching behavior. The movement rules for these robotic arm segments were derived from reflex arc principles in the human nervous system. These arm segments received no central directions and used no direct informational exchange, but rather the arm was sensor-driven at its leading segment in a way that maximized pointing accuracy of the arm. The remaining non-leading segments in the arm were moved in a sequential order using only sensed locally-available movement information about neighboring segments.Pointing and reaching behavior was observed in experiments with and without obstacles to movement. Because such behavior was not specified within each segment, the overall limb behavior emerged due to the interaction and coordination of all segments, rather than due to any single segment, centrally controlled influence, or explicit inter-segmental method of communication.</content></document><document><year>2004</year><authors>Keon Young Yi1 | Yuan F. Zheng2</authors><title>Biped Locomotion by Reduced Ankle Power</title><content>Power reduction in the ankle joints of a biped robot is considered inthis paper. Ankles of human beings have small torque and are veryflexible within a certain range of motion (very stiff near and beyondthis range). This characteristic makes foot landing soft and gives agood contact between its sole and the ground. This feature can beimplemented in a biped robot by using a small actuator for the anklejoints. A small actuator consumes less energy and reduces the weightof the leg. With less power in the ankle joints, robot walkingbecomes more difficult to control. This problem can be solved byproviding a feedback control mechanism as presented in this paper. Thecontrol mechanism uses the motion of the body and the swinging leg toeliminate instability caused by the weak ankle. Two locomotionexamples, standing and walking, were investigated respectively toshow the validity of the proposed control scheme. In standing, thecontrol input is the displacement of the ankle joint of thesupporting leg. The control mechanism decides the bending angle ofthe body and the position of the swinging leg. For walking, only thebending angle of the body is used to avoid the discontinuity of thecontrol input. Experimental results are presented to show theeffectiveness of the control mechanism.</content></document><document><year>2004</year><authors>M. Caccia1 | G. Bruzzone1  | G. Veruggio1 </authors><title>Bottom-Following for Remotely Operated Vehicles: Algorithms and Experiments</title><content>This paper addresses the problem of designing high precision bottom followers for remotely operated vehicles. In the framework of hierarchical control architectures able to uncouple the robot's kinematics (guidance) and dynamics (velocity control), the task of bottom-following is accomplished by suitable guidance task functions, which enable the system to handle unmodeled, i.e., not measured or estimated, kinematics interactions between the robot and the operating environment. In order to increase the bottom followers' reliability, the paper discusses possible techniques for modeling and handling the environmental and measurement uncertainty in the estimate of the local interactions between the vehicle and the operating environment, i.e., altitude and bottom slope. In particular, according to at-sea operational experience, the problem of managing dropouts due to erroneous tracking of multi-path echoes by the ROV altimeter(s) is addressed. Results of a large set of pool trials carried out with the Romeo ROV are reported and discussed.</content></document><document><year>2004</year><authors>Rodney A. Brooks1 | Lynn Andrea Stein1</authors><title>Building brains for bodies</title><content>We describe a project to capitalize on newly available levels of computational resources in order to understand human cognition. We are building an integrated physical system including vision, sound input and output, and dextrous manipulation, all controlled by a continuously operating large scale parallel MIMD computer. The resulting system will learn to think by building on its bodily experiences to accomplish progressively more abstract tasks. Past experience suggests that in attempting to build such an integrated system we will have to fundamentally change the way artificial intelligence, cognitive science, linguistics, and philosophy think about the organization of intelligence. We expect to be able to better reconcile the theories that will be developed with current work in neuroscience.</content></document><document><year>2004</year><authors>Jonas Svennebring  | Sven Koenig </authors><title>Building Terrain-Covering Ant Robots: A Feasibility Study</title><content>Robotics researchers have studied robots that can follow trails laid by other robots. We, on the other hand, study robots that leave trails in the terrain to cover closed terrain repeatedly. How to design such ant robots has so far been studied only theoretically for gross robot simplifications. In this article, we describe for the first time how to build physical ant robots that cover terrain and test their design both in realistic simulation environments and on a Pebbles III robot. We show that the coverage behavior of our ant robots can be modeled with a modified version of node counting, a real-time search method. We then report on first experiments that we performed to understand their efficiency and robustness in situations where some ant robots fail, they are moved without realizing this, the trails are of uneven quality, and some trails are destroyed. Finally, we report the results of a large-scale simulation experiment where ten ant robots covered a factory floor of 25 by 25 meters repeatedly over 85 hours without getting stuck.</content></document><document><year>2004</year><authors>Call for Papers</authors><title/></document><document><year>2009</year><authors>Ruben Martinez-Cantin1 | N|o de Freitas2 | Eric Brochu2 | Jos Castellanos3 | Arnaud Doucet2 </authors><title>A Bayesian exploration-exploitation approach for optimal online sensing and planning with a visually guided mobile robot      </title><content>We address the problem of online path planning for optimal sensing with a mobile robot. The objective of the robot is to learn         the most about its pose and the environment given time constraints. We use a POMDP with a utility function that depends on         the belief state to model the finite horizon planning problem. We replan as the robot progresses throughout the environment.         The POMDP is high-dimensional, continuous, non-differentiable, nonlinear, non-Gaussian and must be solved in real-time. Most         existing techniques for stochastic planning and reinforcement learning are therefore inapplicable. To solve this extremely         complex problem, we propose a Bayesian optimization method that dynamically trades off exploration (minimizing uncertainty         in unknown parts of the policy space) and exploitation (capitalizing on the current best solution). We demonstrate our approach         with a visually-guide mobile robot. The solution proposed here is also applicable to other closely-related domains, including         active vision, sequential experimental design, dynamic sensing and calibration with mobile sensors.      </content></document><document><year>2009</year><authors>V. Mohan1 | P. Morasso1| 2| G. Metta1| 2 | G. S|ini1| 2</authors><title>A biomimetic, force-field based computational model for motion planning and bimanual coordination in humanoid robots      </title><content>This paper addresses the problem of planning the movement of highly redundant humanoid robots based on non-linear attractor         dynamics, where the attractor landscape is obtained by combining multiple force fields in different reference systems. The         computational process of relaxation in the attractor landscape is similar to coordinating the movements of a puppet by means         of attached strings, the strings in our case being the virtual force fields generated by the intended/attended goal and the         other task dependent combinations of constraints involved in the execution of the task. Hence the name PMP (Passive Motion         Paradigm) was given to the computational model. The method does not require explicit kinematic inversion and the computational         mechanism does not crash near kinematic singularities or when the robot is asked to achieve a final pose that is outside its         intrinsic workspace: what happens, in this case, is the gentle degradation of performance that characterizes humans in the same situations. Further, the measure of inconsistency in the relaxation         in such cases can be directly used to trigger higher level reasoning in terms of breaking the goal into a sequence of subgoals         directed towards searching and perhaps using tools to realize the otherwise unrealizable goal. The basic PMP model has been         further expanded in the present paper by means of (1);a;non-linear dynamical timing mechanism that provides terminal attractor         properties to the relaxation process and (2);branching units that allow to &amp;#8216;compose&amp;#8217; complex PMP-networks to coordinate multiple         kinematic chains in a complex structure, including manipulated tools. A;preliminary evaluation of the approach has been carried         out with the 53;degrees of freedom humanoid robot iCub, with particular reference to trajectory formation and bimanual/whole         upper body coordination under the presence of different structural and task specific constraints.      </content></document><document><year>2009</year><authors>T. Yang1 | E. R. Westervelt2 | A. Serrani3  | J. P. Schmiedeler4 </authors><title>A framework for the control of stable aperiodic walking in;underactuated planar bipeds      </title><content>This paper presents a new definition of stable walking for point-footed planar bipedal robots that is not necessarily periodic.         The inspiration for the definition is the commonly-held notion of stable walking: the biped does not fall. Somewhat more formally,         biped walking is shown to be stable if the trajectory of each step places the robot in a state at the end of the step for         which a controller is known to exist that generates a trajectory for the next step with this same property. To make the definition         useful, an algorithm is given to verify if a given controller induces stable walking in the given sense. Also given is a framework         to synthesize controllers that induce stable walking. The results are illustrated on a 5-link biped ERNIE in simulation and         experiment.      </content></document><document><year>2009</year><authors>Ingmar Posner1 | Mark Cummins1  | Paul Newman1 </authors><title>A generative framework for fast urban labeling using spatial and;temporal context      </title><content>This paper introduces a multi-level classification framework for the semantic annotation of urban maps as provided by a mobile         robot. Environmental cues are considered for classification at different scales. The first stage considers local scene properties         using a probabilistic bag-of-words classifier. The second stage incorporates contextual information across a given scene (spatial         context) and across several consecutive scenes (temporal context) via a Markov Random Field (MRF). Our approach is driven         by data from an onboard camera and 3D;laser scanner and uses a combination of visual and geometric features. By framing the         classification exercise probabilistically we take advantage of an information-theoretic bail-out policy when evaluating class-conditional         likelihoods. This efficiency, combined with low order MRFs resulting from our two-stage approach, allows us to generate scene         labels at speeds suitable for online deployment. We demonstrate the virtue of considering such spatial and temporal context         during the classification task and analyze the performance of our technique on data gathered over almost 17;km of track through         a city.      </content></document><document><year>2009</year><authors>Ashish D. Deshp|e1  | Jonathan E. Luntz2 </authors><title>A methodology for design and analysis of cooperative behaviors with mobile robots      </title><content>New methodologies are needed for modeling of physically cooperating mobile robots to be able to systematically design and         analyze such systems. In this context, we present a method called the &amp;#8216;P-robot Method&amp;#8217; under which we introduce entities called         the p-robots at the environmental contact points and treat the linked mobile robots as a multiple degree-of-freedom object, comprising         an articulated open kinematic chain, which is manipulated by the p-robots. The method is suitable to address three critical aspects of physical cooperation: a) analysis of environmental contacts,         b) utilization of redundancy, and c) exploitation of system dynamics. Dynamics of the open chain are computed independent         of the constraints, thus allowing the same set of equations to be used as the constraint conditions change, and simplifying         the addition of multiple robots to the chain. The decoupling achieved through constraining the p-robots facilitates the analysis of kinematic as well as force constraints. We introduce the idea of a &amp;#8216;tipping cone&amp;#8217;, similar to         a standard friction cone, to test whether forces on the robots cause undesired tipping. We have employed the P-robot Method         for the static as well as dynamic analysis for a cooperative behavior involving two robots. The method is generalizable to         analyze cooperative behaviors with any number of robots. We demonstrate that redundant actuation achieved by an adding a third         robot to cooperation can help in satisfying the contact constraints. The P-robot Method can be useful to analyze other interesting         multi-body robotic systems as well.      </content></document><document><year>2009</year><authors>Matthew Howard1 | Stefan Klanke1| Michael Gienger2| Christian Goerick2 | Sethu Vijayakumar1</authors><title>A novel method for learning policies from variable constraint data      </title><content>Many everyday human skills can be framed in terms of performing some task subject to constraints imposed by the environment.         Constraints are usually unobservable and frequently change between contexts. In this paper, we present a novel approach for         learning (unconstrained) control policies from movement data, where observations come from movements under different constraints.         As a key ingredient, we introduce a small but highly effective modification to the standard risk functional, allowing us to         make a meaningful comparison between the estimated policy and constrained observations. We demonstrate our approach on systems         of varying complexity, including kinematic data from the ASIMO humanoid robot with 27 degrees of freedom, and present results         for learning from human demonstration.      </content></document><document><year>2009</year><authors>Matthias Luber1 | Kai O. Arras1 | Christian Plagemann1  | Wolfram Burgard1 </authors><title>Classifying dynamic objects         An unsupervised learning approach</title><content>For robots operating in real-world environments, the ability to deal with dynamic entities such as humans, animals, vehicles,         or other robots is of fundamental importance. The variability of dynamic objects, however, is large in general, which makes         it hard to manually design suitable models for their appearance and dynamics. In this paper, we present an unsupervised learning         approach to this model-building problem. We describe an exemplar-based model for representing the time-varying appearance         of objects in planar laser scans as well as a clustering procedure that builds a set of object classes from given observation         sequences. Extensive experiments in real environments demonstrate that our system is able to autonomously learn useful models         for, e.g., pedestrians, skaters, or cyclists without being provided with external class information.      </content></document><document><year>2009</year><authors>Philippe Giguere1  | Gregory Dudek1 </authors><title>Clustering sensor data for autonomous terrain identification using time-dependency      </title><content>In this paper we are interested in autonomous vehicles that can automatically develop terrain classifiers without human interaction         or feedback. A key issue is the clustering of time-series data collected by the sensors of a ground-based vehicle moving over         several terrain surfaces (e.g. concrete or soil). In this context, we present a novel off-line windowless clustering algorithm         that exploits time-dependency between samples. In terrain coverage, sets of sensory measurements are returned that are spatially,         and hence temporally, correlated. Our algorithm works by finding a set of parameter values for a user-specified classifier         that minimize a cost function. This cost function is related to the change in classifier probability estimates over time.         The main advantage over other existing methods is its ability to cluster data for fast-switching systems that either have         high process or observation noise, or complex distributions that cannot be properly characterized within the time interval         that the system stays in a single state. The algorithm was evaluated using three different classifiers (linear separator,         mixture of Gaussians and k-Nearest Neighbor), over both synthetic data sets and two different mobile robotic platforms, with success. Comparisons are         provided against a window-based algorithm and against a hidden Markov model trained with Expectation-Maximization, with positive         results.      </content></document><document><year>2009</year><authors>Charles W. Fox1 | Ben Mitchinson1| Martin J. Pearson2| Anthony G. Pipe2  | Tony J. Prescott1</authors><title>Contact type dependency of texture classification in;a;whiskered mobile robot      </title><content>Actuated artificial whiskers modeled on rat macrovibrissae can provide effective tactile sensor systems for autonomous robots.         This article focuses on texture classification using artificial whiskers and addresses a limitation of previous studies, namely,         their use of whisker deflection signals obtained under relatively constrained experimental conditions. Here we consider the         classification of signals obtained from a whiskered robot required to explore different surface textures from a range of orientations         and distances. This procedure resulted in a variety of deflection signals for any given texture. Using a standard Gaussian         classifier we show, using both hand-picked features and ones derived from studies of rat vibrissal processing, that a robust         rough-smooth discrimination is achievable without any knowledge of how the whisker interacts with the investigated object.         On the other hand, finer discriminations appear to require knowledge of the target&amp;#8217;s relative position and/or of the manner         in which the whisker contact its surface.      </content></document><document><year>2009</year><authors>Eric Martinson1  | Alan Schultz1</authors><title>Discovery of sound sources by an autonomous mobile robot      </title><content>In this work, we describe an autonomous mobile robotic system for finding, investigating, and modeling ambient noise sources         in the environment. The system has been fully implemented in two different environments, using two different robotic platforms         and a variety of sound source types. Making use of a two-step approach to autonomous exploration of the auditory scene, the         robot first quickly moves through the environment to find and roughly localize unknown sound sources using the auditory evidence         grid algorithm. Then, using the knowledge gained from the initial exploration, the robot investigates each source in more         depth, improving upon the initial localization accuracy, identifying volume and directivity, and, finally, building a classification         vector useful for detecting the sound source in the future.      </content></document><document><year>2009</year><authors>Albert S. Huang1 | David Moore1| Matthew Antone1| Edwin Olson1 | Seth Teller1</authors><title>Finding multiple lanes in urban road networks with vision and lidar      </title><content>This paper describes a system for detecting and estimating the properties of multiple travel lanes in an urban road network         from calibrated video imagery and laser range data acquired by a moving vehicle. The system operates in real-time in several         stages on multiple processors, fusing detected road markings, obstacles, and curbs into a stable non-parametric estimate of         nearby travel lanes. The system incorporates elements of a provided piecewise-linear road network as a weak prior.                     Our method is notable in several respects: it detects and estimates multiple travel lanes; it fuses asynchronous, heterogeneous               sensor streams; it handles high-curvature roads; and it makes no assumption about the position or orientation of the vehicle               with respect to the road.            </content></document><document><year>2009</year><authors>Jonathan Ko1  | Dieter Fox1 </authors><title>GP-BayesFilters: Bayesian filtering using Gaussian process prediction and observation models      </title><content>Bayesian filtering is a general framework for recursively estimating the state of a dynamical system. Key components of each         Bayes filter are probabilistic prediction and observation models. This paper shows how non-parametric Gaussian process (GP)         regression can be used for learning such models from training data. We also show how Gaussian process models can be integrated         into different versions of Bayes filters, namely particle filters and extended and unscented Kalman filters. The resulting         GP-BayesFilters can have several advantages over standard (parametric) filters. Most importantly, GP-BayesFilters do not require         an accurate, parametric model of the system. Given enough training data, they enable improved tracking accuracy compared to         parametric models, and they degrade gracefully with increased model uncertainty. These advantages stem from the fact that         GPs consider both the noise in the system and the uncertainty in the model. If an approximate parametric model is available,         it can be incorporated into the GP, resulting in further performance improvements. In experiments, we show different properties         of GP-BayesFilters using data collected with an autonomous micro-blimp as well as synthetic data.      </content></document><document><year>2009</year><authors>Jos Neira1  | Jeff Trinkle2 </authors><title>Guest editorial: selected papers from Robotics: Science;and;Systems;2008      </title><content>Without Abstract</content></document><document><year>2009</year><authors>Jan Peters1  | Andrew Y. Ng2 </authors><title>Guest editorial: Special issue on robot learning, Part A      </title><content>Without Abstract</content></document><document><year>2009</year><authors>Jan Peters1  | Andrew Y. Ng2 </authors><title>Guest editorial: Special issue on robot learning, Part;B      </title><content>Without Abstract</content></document><document><year>2009</year><authors>Stphane Viollet1  | Franck Ruffier1 </authors><title>Guest editorial: Visual guidance systems for;small;Unmanned;Aerial;Vehicles      </title><content>Without Abstract</content></document><document><year>2009</year><authors>Cyrill Stachniss1 | Christian Plagemann2  | Achim J. Lilienthal3 </authors><title>Learning gas distribution models using sparse Gaussian process mixtures      </title><content>In this paper, we consider the problem of learning two-dimensional spatial models of gas distributions. To build models of         gas distributions that can be used to accurately predict the gas concentration at query locations is a challenging task due         to the chaotic nature of gas dispersal. We formulate this task as a regression problem. To deal with the specific properties         of gas distributions, we propose a sparse Gaussian process mixture model, which allows us to accurately represent the smooth         background signal and the areas with patches of high concentrations. We furthermore integrate the sparsification of the training         data into an EM procedure that we apply for learning the mixture components and the gating function. Our approach has been         implemented and tested using datasets recorded with a real mobile robot equipped with an electronic nose. The experiments         demonstrate that our technique is well-suited for predicting gas concentrations at new query locations and that it outperforms         alternative and previously proposed methods in robotics.      </content></document><document><year>2009</year><authors>Nikos Vlassis1 | Marc Toussaint2 | Georgios Kontes1  | Savas Piperidis1 </authors><title>Learning model-free robot control by a Monte Carlo EM algorithm      </title><content>We address the problem of learning robot control by model-free reinforcement learning (RL). We adopt the probabilistic model         for model-free RL of Vlassis and Toussaint (Proceedings of the international conference on machine learning, Montreal, Canada,         2009), and we propose a Monte Carlo EM algorithm (MCEM) for control learning that searches directly in the space of controller         parameters using information obtained from randomly generated robot trajectories. MCEM is related to, and generalizes, the         PoWER algorithm of Kober and Peters (Proceedings of the neural information processing systems, 2009). In the finite-horizon case MCEM reduces precisely to PoWER, but MCEM can also handle the discounted infinite-horizon case.         An interesting result is that the infinite-horizon case can be viewed as a &amp;#8216;randomized&amp;#8217; version of the finite-horizon case,         in the sense that the length of each sampled trajectory is a random draw from an appropriately constructed geometric distribution.         We provide some preliminary experiments demonstrating the effects of fixed (PoWER) vs randomized (MCEM) horizon length in         two simulated and one real robot control tasks.      </content></document><document><year>2009</year><authors>Nathan D. Ratliff1 | David Silver1  | J. Andrew Bagnell2 </authors><title>Learning to search: Functional gradient techniques for imitation learning      </title><content>Programming robot behavior remains a challenging task. While it is often easy to abstractly define or even demonstrate a desired         behavior, designing a controller that embodies the same behavior is difficult, time consuming, and ultimately expensive. The         machine learning paradigm offers the promise of enabling &amp;#8220;programming by demonstration&amp;#8221; for developing high-performance robotic         systems. Unfortunately, many &amp;#8220;behavioral cloning&amp;#8221; (Bain and Sammut in Machine intelligence agents. London: Oxford University         Press, 1995; Pomerleau in Advances in neural information processing systems 1, 1989; LeCun et al. in Advances in neural information processing systems 18, 2006) approaches that utilize classical tools of supervised learning (e.g. decision trees, neural networks, or support vector         machines) do not fit the needs of modern robotic systems. These systems are often built atop sophisticated planning algorithms         that efficiently reason far into the future; consequently, ignoring these planning algorithms in lieu of a supervised learning         approach often leads to myopic and poor-quality robot performance.                     While planning algorithms have shown success in many real-world applications ranging from legged locomotion (Chestnutt et               al. in Proceedings of the IEEE-RAS international conference on humanoid robots, 2003) to outdoor unstructured navigation (Kelly et al. in Proceedings of the international symposium on experimental robotics               (ISER), 2004; Stentz et al. in AUVSI&amp;#8217;s unmanned systems, 2007), such algorithms rely on fully specified cost functions that map sensor readings and environment models to quantifiable               costs. Such cost functions are usually manually designed and programmed. Recently, a set of techniques has been developed               that explore learning these functions from expert human demonstration. These algorithms apply an inverse optimal control approach to find a cost function for which planned behavior mimics an expert&amp;#8217;s demonstration.            </content></document><document><year>2009</year><authors>Jenhwa Guo1 </authors><title>Maneuvering and control of a biomimetic autonomous underwater vehicle      </title><content>This work presents an approach for maneuvering and controlling a biomimetic autonomous underwater vehicle (BAUV). The BAUV         swims forward by oscillating its body and caudal fin. It turns by bending its body and caudal fin toward the intended direction         of motion. A body-spline function is specified by a set of parameters. Genetic algorithms are then used to find the values         of the parameter by evaluating a fitness function over several swimming trials in a water tank. The fitness function is defined         as the ratio of the kinetic energy of the forward motion to the required driving power of the joint motors. A control law         that uses the oscillating frequency to control the forward speed, and applies a body-spline offset parameter to control the         yawing rate is proposed. Moving averages of swimming speeds and heading angles are utilized as feedback signals to control         the forward speed and heading angle of the BAUV. The effectiveness of the control algorithm is experimentally confirmed.      </content></document><document><year>2009</year><authors>Anna Petrovskaya1  | Sebastian Thrun1 </authors><title>Model based vehicle detection and tracking for autonomous urban driving      </title><content>Situational awareness is crucial for autonomous driving in urban environments. This paper describes the moving vehicle detection         and tracking module that we developed for our autonomous driving robot Junior. The robot won second place in the Urban Grand         Challenge, an autonomous driving race organized by the U.S. Government in 2007. The module provides reliable detection and         tracking of moving vehicles from a high-speed moving platform using laser range finders. Our approach models both dynamic         and geometric properties of the tracked vehicles and estimates them using a single Bayes filter per vehicle. We present the         notion of motion evidence, which allows us to overcome the low signal-to-noise ratio that arises during rapid detection of         moving vehicles in noisy urban environments. Furthermore, we show how to build consistent and efficient 2D representations         out of 3D range data and how to detect poorly visible black vehicles. Experimental validation includes the most challenging         conditions presented at the Urban Grand Challenge as well as other urban settings.      </content></document><document><year>2009</year><authors>Antoni Burguera1 | Yol|a Gonzlez1  | Gabriel Oliver1 </authors><title>On the use of likelihood fields to perform sonar scan matching localization      </title><content>Scan matching algorithms have been extensively used in the last years to perform mobile robot localization. Although these         algorithms require dense and accurate sets of readings with which to work, such as the ones provided by laser range finders,         different studies have shown that scan matching localization is also possible with sonar sensors. Both sonar and laser scan         matching algorithms are usually based on the ideas introduced in the ICP (Iterative Closest Point) approach. In this paper a different approach to scan matching, the Likelihood Field based approach, is presented. Three         scan matching algorithms based on this concept, the non filtered sNDT (sonar Normal Distributions Transform), the filtered sNDT and the LF/SoG (Likelihood Field/Sum of Gaussians), are introduced and analyzed. These algorithms are experimentally evaluated and compared to previously existing ICP-based         algorithms. The obtained results suggest that the Likelihood Field based approach compares favorably with algorithms from         the ICP family in terms of robustness and accuracy. The convergence speed, as well as the time requirements, are also experimentally         evaluated and discussed.      </content></document><document><year>2009</year><authors>Andrej Gams1 | Auke J. Ijspeert2 | Stefan Schaal3 | Jadran Lenar&amp;#269 i&amp;#269 1</authors><title>On-line learning and modulation of periodic movements with;nonlinear dynamical systems      </title><content>The paper presents a two-layered system for (1) learning and encoding a periodic signal without any knowledge on its frequency         and waveform, and (2) modulating the learned periodic trajectory in response to external events. The system is used to learn         periodic tasks on a humanoid HOAP-2 robot. The first layer of the system is a dynamical system responsible for extracting         the fundamental frequency of the input signal, based on adaptive frequency oscillators. The second layer is a dynamical system         responsible for learning of the waveform based on a built-in learning algorithm. By combining the two dynamical systems into         one system we can rapidly teach new trajectories to robots without any knowledge of the frequency of the demonstration signal.         The system extracts and learns only one period of the demonstration signal. Furthermore, the trajectories are robust to perturbations         and can be modulated to cope with a dynamic environment. The system is computationally inexpensive, works on-line for any         periodic signal, requires no additional signal processing to determine the frequency of the input signal and can be applied         in parallel to multiple dimensions. Additionally, it can adapt to changes in frequency and shape, e.g. to non-stationary signals,         such as hand-generated signals and human demonstrations.      </content></document><document><year>2009</year><authors>Martin Riedmiller1 | Thomas Gabel1| Rol| Hafner1 | Sascha Lange1</authors><title>Reinforcement learning for robot soccer      </title><content>Batch reinforcement learning methods provide a powerful framework for learning efficiently and effectively in autonomous robots.         The paper reviews some recent work of the authors aiming at the successful application of reinforcement learning in a challenging         and complex domain. It discusses several variants of the general batch learning framework, particularly tailored to the use         of multilayer perceptrons to approximate value functions over continuous state spaces. The batch learning framework is successfully         used to learn crucial skills in our soccer-playing robots participating in the RoboCup competitions. This is demonstrated         on three different case studies.      </content></document><document><year>2008</year><authors>Andrea Gasparri1  | Mattia Prosperi1 </authors><title>A bacterial colony growth algorithm for mobile robot localization      </title><content>         Achieving robot autonomy is a fundamental objective in Mobile Robotics. However in order to realize this goal, a robot must         be aware of its location within an environment. Therefore, the localization problem (i.e.,the problem of determining robot         pose relative to a map of its environment) must be addressed. This paper proposes a new biology-inspired approach to this         problem. It takes advantage of models of species reproduction to provide a suitable framework for maintaining the multi-hypothesis.         In addition, various strategies to track robot pose are proposed and investigated through statistical comparisons.                                             The Bacterial Colony Growth Algorithm (BCGA) provides two different levels of modeling: a background level that carries on the multi-hypothesis and a foreground level that identifies the best hypotheses according to an exchangeable strategy. Experiments, carried out on the robot ATRV-Jr               manufactured by iRobot, show the effectiveness of the proposed BCGA.                           </content></document><document><year>2008</year><authors>Andrea Cherubini1 | Giuseppe Oriolo1 | Francesco Macr1| Fabio Aloise2 | Febo Cincotti2  | Donatella Mattia2 </authors><title>A multimode navigation system for an assistive robotics project      </title><content>Assistive technology is an emerging area, where robotic devices can help individuals with motor disabilities to achieve independence         in daily activities. This paper deals with a system that provides remote control of Sony AIBO, a commercial mobile robot,         within the assistive project ASPICE. The robot can be controlled by various input devices, including a Brain-Computer Interface.         AIBO has been chosen for its friendly-looking aspect, in order to ease interaction with the patients. The development of the         project is described by focusing on the design of the robot navigation system. Single step, semi-autonomous and autonomous         navigation modes have been realized to provide different levels of control. Automatic collision avoidance is integrated in         all cases. Other features of the system, such as the video feedback from the robotic platform to the user, and the use of         AIBO as communication aid, are briefly described. The performance of the navigation system is shown by simulations as well         as experiments. The system has been clinically validated, in order to obtain a definitive assessment through patient feedback.      </content></document><document><year>2008</year><authors>SungHwan Ahn1 | Jinwoo Choi1 | Nakju Lett Doh2  | Wan Kyun Chung1 </authors><title>A practical approach for EKF-SLAM in an indoor environment: fusing ultrasonic sensors and stereo camera      </title><content>         Improving the practical capability of SLAM requires effective sensor fusion to cope with the large uncertainties from the         sensors and environment. Fusing ultrasonic and vision sensors possesses advantages of both economical efficiency and complementary         cooperation. In particular, it can resolve the false data association and divergence problem of an ultrasonic sensor-only         algorithm and overcome both the low frequency of SLAM update caused by the computational burden and the weakness to illumination         changes of a vision sensor-only algorithm. In this paper, we propose a VR-SLAM (Vision and Range sensor-SLAM) algorithm to         combine ultrasonic sensors and stereo camera very effectively. It consists of two schemes: (1);extracting robust point and         line features from sonar data and (2);recognizing planar visual objects using a multi-scale Harris corner detector and its         SIFT descriptor from a pre-constructed object database. We show that fusing these schemes through EKF-SLAM frameworks can         achieve correct data association via the object recognition and high frequency update via the sonar features. The performance         of the proposed algorithm was verified by experiments in various real indoor environments.               </content></document><document><year>2008</year><authors>Geoffrey Biggs1  | Bruce A. MacDonald1 </authors><title>A pragmatic approach to dimensional analysis for mobile robotic programming      </title><content>An application-specific approach to the design of a robot programming language may allow for a language better suited to the         unique challenges found when programming in this domain. One area of robotics programming that can be supported by an application-specific         approach is dimensioned data. Robot programs typically manage a substantial amount of dimensioned data. However, existing         robot programming tools do not directly support the description and manipulation of dimensioned quantities. A;new system is         presented for managing dimensioned data in robot software. The design provides a new primitive data type to support dimensioned         data. Its unique syntax improves program readability and writability. Dimensional consistency is automatically checked by         the system and any errors are reported, significantly easing the debugging of dimensioned data and improving the reliability         of robot software. The data type is evaluated by common criteria for evaluating programming languages and a small user study,         and is found to be an improvement.      </content></document><document><year>2008</year><authors>Erik Berglund1 | Joaquin Sitte2  | Gordon Wyeth1 </authors><title>Active audition using the parameter-less self-organising map      </title><content>This paper presents a novel method for enabling a robot to determine the position of a sound source in three dimensions using         just two microphones and interaction with its environment. The method uses the Parameter-Less Self-Organising Map (PLSOM)         algorithm and Reinforcement Learning (RL) to achieve rapid, accurate response. We also introduce a method for directional         filtering using the PLSOM. The presented system is compared to a similar system to evaluate its performance.      </content></document><document><year>2008</year><authors>Ruben Mitnik1 | Miguel Nussbaum1  | Alvaro Soto1 </authors><title>An autonomous educational mobile robot mediator      </title><content>So far, most of the applications of robotic technology to education have mainly focused on supporting the teaching of subjects         that are closely related to the Robotics field, such as robot programming, robot construction, or mechatronics. Moreover,         most of the applications have used the robot as an end or a passive tool of the learning activity, where the robot has been         constructed or programmed. In this paper, we present a novel application of robotic technologies to education, where we use         the real world situatedness of a robot to teach non-robotic related subjects, such as math and physics. Furthermore, we also         provide the robot with a suitable degree of autonomy to actively guide and mediate in the development of the educational activity.         We present our approach as an educational framework based on a collaborative and constructivist learning environment, where         the robot is able to act as an interaction mediator capable of managing the interactions occurring among the working students.         We illustrate the use of this framework by a 4-step methodology that is used to implement two educational activities. These         activities were tested at local schools with encouraging results. Accordingly, the main contributions of this work are: i);A         novel use of a mobile robot to illustrate and teach relevant concepts and properties of the real world; ii);A novel use of         robots as mediators that autonomously guide an educational activity using a collaborative and constructivist learning approach;         iii);The implementation and testing of these ideas in a real scenario, working with students at local schools.      </content></document><document><year>2008</year><authors>Bradley Hamner1 | Sanjiv Singh1 | Stephan Roth1  | Takeshi Takahashi1 </authors><title>An efficient system for combined route traversal and collision avoidance      </title><content>         Here we consider the problem of a robot that must follow a previously designated path outdoors. While the nominal path, a         series of closely spaced via points, is provided with an assurance that it will lead to the destination, we can&amp;#8217;t be guaranteed         that it will be obstacle free. We present an efficient system capable of both following the path as well as being perceptive         and agile enough to avoid obstacles in its way. We present a system that detects obstacles using laser ranging, as well as         a layered system that continuously tracks the path, avoiding obstacles and replanning the route when necessary. The distinction         of this system is that compared to the state of the art, it is minimal in sensing and computation while achieving high speeds.         In this paper, we present an algorithm that is based on models of obstacle avoidance by humans and show variations of the         model to deal with practical considerations. We show how the parameters of this model are automatically learned from observation         of human operation and discuss limitations of the model. We then show how these models can be extended by adding online route         planning and a formulation that allows for operation at varying speeds. We present experimental results from an autonomous         vehicle that has operated several hundred kilometers to validate the methodology.               </content></document><document><year>2008</year><authors>Paul E. Rybski1 | Stergios Roumeliotis2| Maria Gini2 | Nikolaos Papanikopoulos2</authors><title>Appearance-based mapping using minimalistic sensor models      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Ohung Kwon1  | Jong Hyeon Park2 </authors><title>Asymmetric trajectory generation and impedance control for;running of biped robots      </title><content>An online asymmetric trajectory generation method for biped robots is proposed to maintain dynamical postural stability and         increase energy autonomy, based on the running stability criterion defined in phases. In a support phase, an asymmetric trajectories         for the hip and swing leg of the biped robots is obtained from an approximated running model with two springless legs and         a spring-loaded inverted pendulum model so that the zero moment point should exist inside the safety boundary of a supporting         foot, and the supporting leg should absorb large reaction forces, take off and fly through the air. The biped robot is under-actuated         with six degrees of under-actuation during flight. The trajectory generation strategies for the hip and both legs in a flight         phase use the approximated running model and non-holonomic constraints based on the linear and angular momenta at the mass         center. Next, we present an impedance control with a force modulation strategy to guarantee a stable landing on the ground         and simultaneously track the desired trajectories where the desired impedance at the hip link and both legs is specified.         A series of computer simulations for two different types of biped robots show that the proposed running trajectory and impedance         control method satisfy the two conditions for running stability and make the biped robot more robust to variations in the         desired running speed, gait transitions between walking and running, and parametric modeling errors. We have examined the         feasibility of this method with running experiments on a 12-DOF biped robot without arms. The biped robot could run successfully         with average forward speed of about 0.3359 [m/s].      </content></document><document><year>2008</year><authors>Massimo Caccia1 | Marco Bibuli1 | Riccardo Bono1  | Gabriele Bruzzone1 </authors><title>Basic navigation, guidance and control of;an;Unmanned Surface Vehicle      </title><content>This paper discusses the navigation, guidance and control (NGC) system of an Unmanned Surface Vehicle (USV) through extended         at sea trials carried out with the prototype autonomous catamaran Charlie. In particular, experiments demonstrate the effectiveness,         both for precision and power consumption, of extended Kalman filter and simple PID guidance and control laws to perform basic         control tasks such as auto-heading, auto-speed and straight line following with a USV equipped only with GPS and compass.      </content></document><document><year>2008</year><authors>Cecilia Laschi1  | Rol| S. Johansson2</authors><title>Bio-inspired sensory-motor coordination      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Gabriel Recatal1 | Eris Chinellato1 | ngel P. del Pobil1 | Youcef Mezouar2  | Philippe Martinet2 </authors><title>Biologically-inspired 3D grasp synthesis based on;visual exploration      </title><content>Object grasping is a typical human ability which is widely studied from both a biological and an engineering point of view.         This paper presents an approach to grasp synthesis inspired by the human neurophysiology of action-oriented vision. Our grasp         synthesis method is built upon an architecture which, taking into account the differences between robotic and biological systems,         proposes an adaptation of brain models to the peculiarities of robotic setups. The architecture modularity allows for scalability         and integration of complex robotic tasks. The grasp synthesis is designed as integrated with the extraction of a 3D object         description, so that the object visual analysis is actively driven by the needs of the grasp synthesis: visual reconstruction         is performed incrementally and selectively on the regions of the object that are considered more interesting for grasping.      </content></document><document><year>2008</year><authors>Alej|ra Barrera1  | Alfredo Weitzenfeld1 </authors><title>Biologically-inspired robot spatial cognition based;on;rat;neurophysiological studies      </title><content>         This paper presents a robot architecture with spatial cognition and navigation capabilities that captures some properties         of the rat brain structures involved in learning and memory. This architecture relies on the integration of kinesthetic and         visual information derived from artificial landmarks, as well as on Hebbian learning, to build a holistic topological-metric         spatial representation during exploration, and employs reinforcement learning by means of an Actor-Critic architecture to         enable learning and unlearning of goal locations. From a robotics perspective, this work can be placed in the gap between         mapping and map exploitation currently existent in the SLAM literature. The exploitation of the cognitive map allows the robot         to recognize places already visited and to find a target from any given departure location, thus enabling goal-directed navigation.         From a biological perspective, this study aims at initiating a contribution to experimental neuroscience by providing the         system as a tool to test with robots hypotheses concerned with the underlying mechanisms of rats&amp;#8217; spatial cognition. Results         from different experiments with a mobile AIBO robot inspired on classical spatial tasks with rats are described, and a comparative         analysis is provided in reference to the reversal task devised by O&amp;#8217;Keefe in 1983.               </content></document><document><year>2008</year><authors>Karl Iagnemma1  | Chris C. Ward1</authors><title>Classification-based wheel slip detection and detector fusion for;mobile robots on outdoor terrain      </title><content>This paper introduces a signal-recognition based approach for detecting autonomous mobile robot immobilization on outdoor         terrain. The technique utilizes a support vector machine classifier to form class boundaries in a feature space composed of         statistics related to inertial and (optional) wheel speed measurements. The proposed algorithm is validated using experimental         data collected with an autonomous robot operating in an outdoor environment. Additionally, two detector fusion techniques         are proposed to combine the outputs of multiple immobilization detectors. One technique is proposed to minimize false immobilization         detections. A;second technique is proposed to increase overall detection accuracy while maintaining rapid detector response.         The two fusion techniques are demonstrated experimentally using the detection algorithm proposed in this work and a dynamic         model-based algorithm. It is shown that the proposed techniques can be used to rapidly and robustly detect mobile robot immobilization         in outdoor environments, even in the absence of absolute position information.      </content></document><document><year>2008</year><authors>Satoru Sakai1 | Michihisa Iida2| Koichi Osuka3 | Mikio Umeda2</authors><title>Design and control of a heavy material handling manipulator for;agricultural robots      </title><content>In this paper, we propose a manipulation system for agricultural robots that handle heavy materials. The structural systems         of a mobile platform and a manipulator are selected and designed after proposing new knowledge about agricultural robots.         Also, the control systems for these structural systems are designed in the presence of parametric perturbation and uncertainty         while avoiding conservative results. The validity of both the structural and control systems is confirmed by conducting watermelon         harvesting experiments in an open field. Furthermore, an explicit design procedure is confirmed for both the structural and         control systems and three key design tools are clarified.      </content></document><document><year>2008</year><authors>T. Yang1 | E. R. Westervelt2 | J. P. Schmiedeler3  | R. A. Bockbrader4 </authors><title>Design and control of a planar bipedal robot ERNIE with;parallel;knee compliance      </title><content>This paper presents the development of the planar bipedal robot ERNIE as well as numerical and experimental studies of the         influence of parallel knee joint compliance on the energetic efficiency of walking in ERNIE. ERNIE has 5 links&amp;#8212;a torso, two         femurs and two tibias&amp;#8212;and is configured to walk on a treadmill so that it can walk indefinitely in a confined space. Springs         can be attached across the knee joints in parallel with the knee actuators. The hybrid zero dynamics framework serves as the         basis for control of ERNIE&amp;#8217;s walking. In the investigation of the effects of compliance on the energetic efficiency of walking,         four cases were studied: one without springs and three with springs of different stiffnesses and preloads. It was found that         for low-speed walking, the addition of soft springs may be used to increase energetic efficiency, while stiffer springs decrease         the energetic efficiency. For high-speed walking, the addition of either soft or stiff springs increases the energetic efficiency         of walking, while stiffer springs improve the energetic efficiency more than do softer springs.      </content></document><document><year>2008</year><authors>Bram V|erborght1| 2 | Bjrn Verrelst1| Ronald Van Ham1| Michal Van Damme1| Pieter Beyl1 | Dirk Lefeber1</authors><title>Development of a compliance controller to reduce energy consumption for bipedal robots      </title><content>In this paper a strategy is proposed to combine active trajectory tracking for bipedal robots with exploiting the natural         dynamics by simultaneously controlling the torque and stiffness of a compliant actuator. The goal of this research is to preserve         the versatility of actively controlled humanoids, while reducing their energy consumption. The biped Lucy, powered by pleated         pneumatic artificial muscles, has been built and controlled and is able to walk up to a speed of 0.15 m/s. The pressures inside         the muscles are controlled by a joint trajectory tracking controller to track the desired joint trajectories calculated by         a trajectory generator. However, the actuators are set to a fixed stiffness value. In this paper a compliance controller is         presented to reduce the energy consumption by controlling the stiffness. A mathematical formulation has been developed to         find an optimal stiffness setting depending on the desired trajectory and physical properties of the system and the proposed         strategy has been validated on a pendulum structure powered by artificial muscles. This strategy has not been implemented         on the real robot because the walking speed of the robot is currently too slow to benefit already from compliance control.      </content></document><document><year>2008</year><authors>Benjamin Lavis1 | Tomonari Furukawa1 | Hugh F. Durrant Whyte2</authors><title>Dynamic space reconfiguration for Bayesian search and tracking with moving targets      </title><content>         This paper presents a technique for dynamically reconfiguring search spaces in order to enable Bayesian autonomous search         and tracking missions with moving targets. In particular, marine search and rescue scenarios are considered, highlighting         the need for space reconfiguration in situations where moving targets are involved. The proposed technique improves the search         space configuration by maintaining the validity of the recursive Bayesian estimation. The advantage of the technique is that         autonomous search and tracking can be performed indefinitely, without loss of information. Numerical results first show the         effectiveness of the technique with a single search vehicle and a single moving target. The efficacy of the approach for coordinated         autonomous search and tracking is shown through simulation, incorporating multiple search vehicles and multiple targets. The         examples also highlight the added benefit to human mission planners resulting from the technique&amp;#8217;s simplification of the search         space allocation task.               </content></document><document><year>2008</year><authors>Sabine Hauert1 | Jean-Christophe Zufferey1 | Dario Floreano1</authors><title>Evolved swarming without positioning information: an;application in aerial communication relay      </title><content>In most swarm systems, agents are either aware of the position of their direct neighbors or they possess a substrate on which         they can deposit information (stigmergy). However, such resources are not always obtainable in real-world applications because         of hardware and environmental constraints. In this paper we study in 2D simulation the design of a swarm system which does         not make use of positioning information or stigmergy.                     This endeavor is motivated by an application whereby a large number of Swarming Micro Air Vehicles (SMAVs), of fixed-wing               configuration, must organize autonomously to establish a wireless communication network (SMAVNET) between users located on               ground. Rather than relative or absolute positioning, agents must rely only on their own heading measurements and local communication               with neighbors.            </content></document><document><year>2008</year><authors>Stefano Carpin1 </authors><title>Fast and accurate map merging for multi-robot systems      </title><content>We present a new algorithm for merging occupancy grid maps produced by multiple robots exploring the same environment. The         algorithm produces a set of possible transformations needed to merge two maps, i.e translations and rotations. Each transformation         is weighted, thus allowing to distinguish uncertain situations, and enabling to track multiple cases when ambiguities arise.         Transformations are produced extracting some spectral information from the maps. The approach is deterministic, non-iterative, and fast. The algorithm has been tested on public available         datasets, as well as on maps produced by two robots concurrently exploring both indoor and outdoor environments. Throughout         the experimental validation stage the technique we propose consistently merged maps exhibiting very different characteristics.      </content></document><document><year>2008</year><authors>Edmond M. DuPont1 | Carl A. Moore2 | Emmanuel G. Collins Jr.2  | Eric Coyle2 </authors><title>Frequency response method for terrain classification in;autonomous;ground vehicles      </title><content>         Many autonomous ground vehicle (AGV) missions, such as those related to agricultural applications, search and rescue, or reconnaissance         and surveillance, require the vehicle to operate in difficult outdoor terrains such as sand, mud, or snow. To ensure the safety         and performance of AGVs on these terrains, a terrain-dependent driving and control system can be implemented. A key first         step in implementing this system is autonomous terrain classification. It has recently been shown that the magnitude of the         spatial frequency response of the terrain is an effective terrain signature. Furthermore, since the spatial frequency response         is mapped by an AGV&amp;#8217;s vibration transfer function to the frequency response of the vibration measurements, the magnitude of         the latter frequency responses also serve as a terrain signature. Hence, this paper focuses on terrain classification using         vibration measurements. Classification is performed using a probabilistic neural network, which can be implemented online         at relatively high computational speeds. The algorithm is applied experimentally to both an ATRV-Jr and an eXperimental Unmanned         Vehicle (XUV) at multiple speeds. The experimental results show the efficacy of the proposed approach.               </content></document><document><year>2008</year><authors>Masashi Sugiyama1| 2 | Hirotaka Hachiya1 | Christopher Towell2  | Sethu Vijayakumar2 </authors><title>Geodesic Gaussian kernels for value function approximation      </title><content>The least-squares policy iteration approach works efficiently in value function approximation, given appropriate basis functions.         Because of its smoothness, the Gaussian kernel is a popular and useful choice as a basis function. However, it does not allow         for discontinuity which typically arises in real-world reinforcement learning tasks. In this paper, we propose a new basis         function based on geodesic Gaussian kernels, which exploits the non-linear manifold structure induced by the Markov decision processes. The usefulness of the proposed         method is successfully demonstrated in simulated robot arm control and Khepera robot navigation.      </content></document><document><year>2008</year><authors>Jonathan Courbon1| Youcef Mezouar1  | Philippe Martinet1</authors><title>Indoor navigation of a non-holonomic mobile robot using a visual memory      </title><content>When navigating in an unknown environment for the first time, a natural behavior consists on memorizing some key views along         the performed path, in order to use these references as checkpoints for a future navigation mission. The navigation framework         for wheeled mobile robots presented in this paper is based on this assumption. During a human-guided learning step, the robot         performs paths which are sampled and stored as a set of ordered key images, acquired by an embedded camera. The set of these         obtained visual paths is topologically organized and provides a visual memory of the environment. Given an image of one of         the visual paths as a target, the robot navigation mission is defined as a concatenation of visual path subsets, called visual         route. When running autonomously, the robot is controlled by a visual servoing law adapted to its nonholonomic constraint.         Based on the regulation of successive homographies, this control guides the robot along the reference visual route without         explicitly planning any trajectory. The proposed framework has been designed for the entire class of central catadioptric         cameras (including conventional cameras). It has been validated onto two architectures. In the first one, algorithms have         been implemented onto a dedicated hardware and the robot is equipped with a standard perspective camera. In the second one,         they have been implemented on a standard PC and an omnidirectional camera is considered.      </content></document><document><year>2008</year><authors>Luis Montesano1 | Javier Minguez2 | Luis Montano2</authors><title>Modeling dynamic scenarios for local sensor-based motion planning      </title><content>This paper addresses the modeling of the static and dynamic parts of the scenario and how to use this information with a sensor-based         motion planning system. The contribution in the modeling aspect is a formulation of the detection and tracking of mobile objects         and the mapping of the static structure in such a way that the nature (static/dynamic) of the observations is included in         the estimation process. The algorithm provides a set of filters tracking the moving objects and a local map of the static         structure constructed on line. In addition, this paper discusses how this modeling module is integrated in a real sensor-based         motion planning system taking advantage selectively of the dynamic and static information. The experimental results confirm         that the complete navigation system is able to move a vehicle in unknown and dynamic scenarios. Furthermore, the system overcomes         many of the limitations of previous systems associated to the ability to distinguish the nature of the parts of the scenario.      </content></document><document><year>2008</year><authors>Brian Smith1 | Ayanna Howard1 | John-Michael McNew2 | Jiuguang Wang1  | Magnus Egerstedt1 </authors><title>Multi-robot deployment and coordination with Embedded Graph Grammars      </title><content>This paper presents a framework for going from specifications to implementations of decentralized control strategies for multi-robot         systems. In particular, we show how the use of Embedded Graph Grammars (EGGs) provides a tool for characterizing local interaction         and control laws. This paper highlights some key implementation aspects of the EGG formalism, and develops and discusses experimental         results for a hexapod-based multi-robot system, as well as a multi-robot system of wheeled robots.      </content></document><document><year>2008</year><authors>Gustavo Arechavaleta1 | Jean-Paul Laumond1 | Halim Hicheur2  | Alain Berthoz2 </authors><title>On the nonholonomic nature of human locomotion      </title><content>         In the kinematic realm, wheeled robot&amp;#8217;s determining characteristic lies in its nonholonomic constraint. Indeed, the wheels of the robot unequivocally force the robot vehicle to move tangentially to its main axis.         Here we test the hypothesis that human locomotion can also be partly described by such a nonholonomic system. This hypothesis         is inspired by the trivial observation that humans do not walk sideways: some constraints of different natures (anatomical,         mechanical&amp;#8230;) may restrict the way humans generate locomotor trajectories. To model these constraints, we propose a simple         differential system satisfying the so called rolling without sliding constraint. We validated the proposed model by comparing simulated trajectories with actual (recorded) trajectories obtained         from goal-oriented locomotion of human subjects. These subjects had to start from a pre-defined position and direction in         space and cross over to a distant porch so that both initial and final positions and directions were controlled. A comparative         analysis was successfully undertaken by making use of numerical methods to compute the control inputs from actual trajectories.         To achieve this, three body segments were used as local reference frames: head, pelvis and torso. The best simulations were         obtained using the last body segment. We therefore suggest an analogy between the steering wheels and the torso segment, meaning         that for the control of locomotion, the trunk behavior is constrained in a nonholonomic manner. Our approach allowed us to         successfully predict 87 percent of trajectories recorded in seven subjects and might be particularly relevant for future pluridisciplinary         research programs dealing with modeling of biological locomotor behaviors.               </content></document><document><year>2008</year><authors>Micha Hersch1  | Aude G. Billard1 </authors><title>Reaching with multi-referential dynamical systems      </title><content>         We study a reaching movement controller for a redundant serial arm manipulator, based on two principles believed to be central         to biological motion control: multi-referential control and dynamical system control. The resulting controller is based on         two concurrent dynamical systems acting on different, yet redundant variables. The first dynamical system acts on the end-effector         location variables and the second one acts on the joint angle variables. Coherence constraints are enforced between those         two redundant representations of the movement and can be used to modulate the relative influence of each dynamical system.         We illustrate the advantages of such a redundant representation of the movement regarding singularities and joint angle avoidance.               </content></document><document><year>2008</year><authors>Christopher Rasmussen1 </authors><title>RoadCompass: following rural roads with vision + ladar using vanishing point tracking      </title><content>We present a vision- and ladar-based approach to autonomous driving on rural and desert roads that has been tested extensively         in a closed-loop system. The vision component uses Gabor wavelet filters for texture analysis to find ruts and tracks from         which the road vanishing point can be inferred via Hough-style voting, yielding a direction estimate for steering control.         The ladar component projects detected obstacles along the road direction onto the plane of the front of the vehicle and tracks         the 1-D obstacle &amp;#8220;gap&amp;#8221; presumed due to the road to yield a lateral offset estimate. Several image- and state-based tests to         detect failure conditions such as off-road poses (i.e., there is no road to follow) and poor lighting due to sun glare or         distracting shadows are also explained. The system&amp;#8217;s efficacy is demonstrated with analysis of diverse logged data including         from the 2005 DARPA Grand Challenge, as well as tests with full control of a vehicle over 15 km of difficult roads at up to         37 km/h with no waypoints.      </content></document><document><year>2008</year><authors>Jonas Buchli2| 1  | Auke Jan Ijspeert2</authors><title>Self-organized adaptive legged locomotion in a compliant quadruped robot      </title><content>In this contribution we present experiments of an adaptive locomotion controller on a compliant quadruped robot. The adaptive         controller consists of adaptive frequency oscillators in different configurations and produces dynamic gaits such as bounding         and jumping. We show two main results: (1);The adaptive controller is able to track the resonant frequency of the robot which         is a function of different body parameters (2);controllers based on dynamical systems as we present are able to &amp;#8220;recognize&amp;#8221;         mechanically intrinsic modes of locomotion, adapt to them and enforce them. More specifically the main results are supported         by several experiments, showing first that the adaptive controller is constantly tracking body properties and readjusting         to them. Second, that important gait parameters are dependent on the geometry and movement of the robot and the controller         can account for that. Third, that local control is sufficient and the adaptive controller can adapt to the different mechanical         modes. And finally, that key properties of the gaits are not only depending on properties of the body but also the actual         mode of movement that the body is operating in. We show that even if we specify the gait pattern on the level of the CPG the         chosen gait pattern does not necessarily correspond to the CPG&amp;#8217;s pattern. Furthermore, we present the analytical treatment         of adaptive frequency oscillators in closed feedback loops, and compare the results to the data from the robot experiments.      </content></document><document><year>2008</year><authors>Dezhen Song1 | Ni Qin1  | Ken Goldberg2 </authors><title>Systems, control models, and codec for collaborative observation of remote environments with an;autonomous networked robotic         camera      </title><content>Networked robotic cameras are becoming popular in remote observation applications such as natural observation, surveillance,         and distance learning. Equipped with a high optical zoom lens and agile pan-tilt mechanisms, a networked robotic camera can         cover a;large region with various resolutions. The optimal selection of;camera control parameters for competing observation         requests and the on-demand delivery of video content for various spatiotemporal queries are two challenges in the design of         such autonomous systems. For camera control, we introduce memoryless and temporal frame selection models that effectively         enable collaborative control of the camera based on the competing inputs from in-situ sensors and users. For content delivery, we design a patch-based motion panorama representation and coding/decoding algorithms         (codec) to allow efficient storage and computation. We present system architecture, frame selection models, user interface,         and codec algorithms. We have implemented the system and extensively tested our design in real world applications including         natural observation, public surveillance, distance learning, and building construction monitoring. Experiment results show         that our frame selection models are robust and effective and our on-demand content delivery codec can satisfy a variety of         spatiotemporal queries efficiently in terms of computation time communications bandwidth.      </content></document><document><year>2008</year><authors>Richard Alan Peters II1 | Kimberly A. Hambuchen2 | Robert E. Bodenheimer1</authors><title>The sensory ego-sphere: a mediating interface between sensors and cognition      </title><content>The Sensory Ego-Sphere (SES) is an interface for a robot that serves to mediate information between sensors and cognition.         The SES can be visualized as a sphere centered on the coordinate frame of the robot, spatially indexed by polar and azimuthal         angles. Internally, the SES is a graph with a fixed number of edges that partitions surrounding space and contains localized         sensor information from the robot. This paper describes the SES and gives the results of implementing the SES on multiple         robots, both humanoid and mobile, to support essential functions such as a localized short-term memory, spatio-temporal sensory-motor         event detection, attentional processing, data sharing, and ego-centric navigation.      </content></document><document><year>2008</year><authors>Valentino Crespi1 | Aram Galstyan2 | Kristina Lerman2</authors><title>Top-down vs bottom-up methodologies in multi-agent system design      </title><content>         Traditionally, two alternative design approaches have been available to engineers: top-down and bottom-up. In the top-down         approach, the design process starts with specifying the global system state and assuming that each component has global knowledge         of the system, as in a centralized approach. The solution is then decentralized by replacing global knowledge with communication.         In the bottom-up approach, on the other hand, the design starts with specifying requirements and capabilities of individual         components, and the global behavior is said to emerge out of interactions among constituent components and between components         and the environment. In this paper we present a comparative study of both approaches with particular emphasis on applications         to multi-agent system engineering and robotics. We outline the generic characteristics of both approaches from the MAS perspective,         and identify three elements that we believe should serve as criteria for how and when to apply either of the approaches. We         demonstrate our analysis on a specific example of load balancing problem in robotics. We also show that under certain assumptions         on the communication and the external environment, both bottom-up and top-down methodologies produce very similar solutions.               </content></document><document><year>2008</year><authors>Hans de Ruiter1  | Beno Benhabib1 </authors><title>Visual-model-based, real-time 3D pose tracking for autonomous navigation: methodology and experiments      </title><content>This paper presents a novel 3D-model-based computer-vision method for tracking the full six degree-of-freedom (dof) pose (position         and orientation) of a rigid body, in real-time. The methodology has been targeted for autonomous navigation tasks, such as         interception of or rendezvous with mobile targets. Tracking an object&amp;#8217;s complete six-dof pose makes the proposed algorithm         useful even when targets are not restricted to planar motion (e.g., flying or rough-terrain navigation). Tracking is achieved         via a combination of textured model projection and optical flow. The main contribution of our work is the novel combination         of optical flow with z-buffer depth information that is produced during model projection. This allows us to achieve six-dof tracking with a single         camera.                     A localized illumination normalization filter also has been developed in order to improve robustness to shading. Real-time               operation is achieved using GPU-based filters and a new data-reduction algorithm based on colour-gradient redundancy, which was developed within the framework of our project. Colour-gradient redundancy is an important property of colour images,               namely, that the gradients of all colour channels are generally aligned. Exploiting this property provides a threefold increase               in speed. A processing rate of approximately 80 to 100 fps has been obtained in our work when utilizing synthetic and real               target-motion sequences. Sub-pixel accuracies were obtained in tests performed under different lighting conditions.            </content></document><document><year>2006</year><authors>Dominique Martinez1 | Oliver Rochel1 | Etienne Hugues1</authors><title>A biomimetic robot for tracking specific odors in turbulent plumes      </title><content>Two basic tasks must be performed by an olfactory robot tracking a specific odor source: navigate in a turbulent odor plume         and recognize an odor regardless of its concentration. For these two tasks, we propose simple biologically inspired strategies,         well suited for building dedicated circuits and for on-board implementation on real robots. The odor recognition system is         based on a spiking neural network using a synchronization coding scheme. The robot navigation system is based on the use of         bilateral comparison between two spatially separated gas sensors arrays at either side of the robot. We propose binary or         analog navigation laws depending on the nature of the available sensory information extracted from the plume structure (isolated         odor patches or smoother concentration field).      </content></document><document><year>2006</year><authors>Jing Wang1 | Zhihua Qu1 | Curtis M. Ihlefeld2 | Richard A. Hull3</authors><title>A control-design-based solution to robotic ecology: Autonomy of achieving cooperative behavior from a high-level astronaut         command      </title><content>In this paper, we propose a cooperative control strategy for a group of robotic vehicles to achieve the specified task issued         from a high-level astronaut command. The problem is mathematically formulated as designing the cooperative control for a general         class of multiple-input-multiple-output (MIMO) dynamical systems in canonical form with arbitrary but finite relative degrees         such that the outputs of the overall system converge to the explicitly given steady state. The proposed cooperative control for individual vehicle only need to use the sensed and communicated         outputs information from its local neighboring vehicles. No fixed leader and time-invariant communication networks are assumed         among vehicles. Particularly, a set of less-restrictive conditions on the connectivity of the sensor/communication networks         are established, under which it is rigorously proven by using the newly found nice properties of the convergence of sequences         of row stochastic matrices that the cooperative objective of the overall system can be achieved. Simulation results for a         group of vehicles achieving a target and surrounding a specified object in formation are provided to support the proposed         approach in this paper.      </content></document><document><year>2006</year><authors>Udo Frese1 </authors><title>A Discussion of Simultaneous Localization and Mapping      </title><content>This paper aims at a discussion of the structure of the SLAM problem. The analysis is not strictly formal but based both on         informal studies and mathematical derivation. The first part highlights the structure of uncertainty of an estimated map with         the key result being &amp;#8220;Certainty of Relations despite Uncertainty of Positions&amp;#8221;. A formal proof for approximate sparsity of         so-called information matrices occurring in SLAM is sketched. It supports the above mentioned characterization and provides         a foundation for algorithms based on sparse information matrices.                     Further, issues of nonlinearity and the duality between information and covariance matrices are discussed and related to common               methods for solving SLAM.            </content></document><document><year>2006</year><authors>Takayuki K|a1 | Masayuki Kamasima1| 2| Michita Imai1| 2| Tetsuo Ono1| 3| Daisuke Sakamoto1| 3| Hiroshi Ishiguro1| 4 | Yuichiro Anzai2</authors><title>A humanoid robot that pretends to listen to route guidance from a human      </title><content>This paper reports the findings for a humanoid robot that expresses its listening attitude and understanding to humans by         effectively using its body properties in a route guidance situation. A human teaches a route to the robot, and the developed         robot behaves similar to a human listener by utilizing both temporal and spatial cooperative behaviors to demonstrate that         it is indeed listening to its human counterpart. The robot's software consists of many communicative units and rules for selecting         appropriate communicative units. A communicative unit realizes a particular cooperative behavior such as eye-contact and nodding,         found through previous research in HRI. The rules for selecting communicative units were retrieved through our preliminary         experiments with a WOZ method. An experiment was conducted to verify the effectiveness of the robot, with the results revealing         that a robot displaying cooperative behavior received the highest subjective evaluation, which is rather similar to a human         listener. A detailed analysis showed that this evaluation was mainly due to body movements as well as utterances. On the other         hand, subjects' utterance to the robot was encouraged by the robot's utterances but not by its body movements.      </content></document><document><year>2006</year><authors>M. Abou-Samah1 | C. P. Tang2 | R. M. Bhatt2  | V. Krovi2 </authors><title>A kinematically compatible framework for cooperative payload transport by nonholonomic mobile manipulators      </title><content>In this paper, we examine the development of a kinematically compatible control framework for a modular system of wheeled         mobile manipulators that can team up to cooperatively transport a common payload. Each individually autonomous mobile manipulator         consists of a differentially-driven Wheeled Mobile Robot (WMR) with a mounted two degree-of-freedom (d.o.f) revolute-jointed,         planar and passive manipulator arm. The composite wheeled vehicle, formed by placing a payload at the end-effectors of two         (or more) such mobile manipulators, has the capability to accommodate, detect and correct both instantaneous and finite relative         configuration errors.                     The kinematically-compatible motion-planning/control framework developed here is intended to facilitate maintenance of all               kinematic (holonomic and nonholonomic) constraints within such systems. Given an arbitrary end-effector trajectory, each individual               mobile-manipulator's bi-level hierarchical controller first generates a kinematically-feasible desired trajectory for the WMR base, which is then tracked by a suitable lower-level posture stabilizing controller. Two variants of system-level               cooperative control schemes&amp;#8212;leader-follower and decentralized control&amp;#8212;are then created based on the individual mobile-manipulator               control scheme. Both methods are evaluated within an implementation framework that emphasizes both virtual prototyping (VP)               and hardware-in-the-loop (HIL) experimentation. Simulation and experimental results of an example of a two-module system are               used to highlight the capabilities of a real-time local sensor-based controller for accommodation, detection and corection               of relative formation errors.            </content></document><document><year>2006</year><authors>C. Urdiales1 | E. J. Perez1| J. Vzquez-Salceda2| M. Snchez-Marr2 | F. S|oval1</authors><title>A purely reactive navigation scheme for dynamic environments using Case-Based Reasoning      </title><content>This paper presents a new sonar based purely reactive navigation technique for mobile platforms. The method relies on Case-Based         Reasoning to adapt itself to any robot and environment through learning, both by observation and self experience. Thus, unlike         in other reactive techniques, kinematics or dynamics do not need to be explicitly taken into account. Also, learning from         different sources allows combination of their advantages into a safe and smooth path to the goal. The method has been succesfully         implemented on a Pioneer robot wielding 8 Polaroid sonar sensors.      </content></document><document><year>2006</year><authors>Rafael Murrieta-Cid1 | Benjamn Tovar2  | Seth Hutchinson3 </authors><title>A Sampling-Based Motion Planning Approach to Maintain Visibility of Unpredictable Targets      </title><content>This paper deals with the surveillance problem of computing the motions of one or more robot observers in order to maintain         visibility of one or several moving targets. The targets are assumed to move unpredictably, and the distribution of obstacles         in the workspace is assumed to be known in advance. Our algorithm computes a motion strategy by maximizing the shortest distance to escape&amp;#8212;the shortest distance the target must move to escape an observer's visibility region. Since this optimization problem is         intractable, we use randomized methods to generate candidate surveillance paths for the observers. We have implemented our         algorithms, and we provide experimental results using real mobile robots for the single target case, and simulation results         for the case of two targets-two observers.      </content></document><document><year>2006</year><authors>S. Micera1 | M. C. Carrozza1| E. Guglielmelli2| G. Cappiello3| F. Zaccone3| C. Freschi3| R. Colombo4| A. Mazzone4| C. Delconte5| F. Pisano5| G. Minuco6 | P. Dario7</authors><title>A Simple Robotic System for Neurorehabilitation      </title><content>In the recent past, several researchers have shown that important variables in relearning motor skills and in changing the         underlying neural architecture after stroke are the quantity, duration, content, and intensity of training sessions. Unfortunately,         when traditional therapy is provided in a hospital or rehabilitation center, the patient is usually seen for few hours a week.         Robot-mediated therapies could improve this situation but even if interesting results have been achieved by several groups,         the use of robot-mediated therapy has not become very common in clinical practice. This is due to many different reasons (e.g.,         the &amp;#8220;technophobia&amp;#8221; of some clinicians, the need for more extensive clinical trials) but one of the more important is the cost         and the complexity of these devices which make them difficult to be purchased and used in all the clinical centers.                     The aim of this work was to verify the possibility of improving motor recovery of hemiparetic subjects by using a simple mechatronic               system. To achieve this goal, our system (named &amp;#8220;MEchatronic system for MOtor recovery after Stroke&amp;#8221; (MEMOS)) has been designed               with the aim of using mainly &amp;#8220;off-the-shelf products&amp;#8221; with only few parts simply manufactured with standard technology, when               commercial parts were not available. Moreover, the prototype has been developed taking into account the requirements related               to the clinical applicability such as robustness and safety.            </content></document><document><year>2006</year><authors>C. Lytridis1| E. E. Kadar2  | G. S. Virk3 </authors><title>A systematic approach to the problem of odour source localisation      </title><content>Although chemical sensing is far simpler than vision or hearing, navigation in a chemical diffusion field is still not well         understood. Biological studies have already demonstrated the use of various search methods (e.g., chemotaxis and biased random         walk), but robotics research could provide new ways to investigate principles of olfactory-based search skills (Webb, 2000;         Grasso, 2001). In previous studies on odour source localisation, we have tested three biologically inspired search strategies:         chemotaxis, biased random walk, and a combination of these methods (Kadar and Virk, 1998; Lytridis et;al., 2001). The main         objective of the present paper is to demonstrate how simulation and robot experiments could be used conjointly to systematically         study these search strategies. Specifically, simulation studies are used to calibrate and test our three strategies in concentric         diffusion fields with various noise levels. An experiment with a mobile robot was also conducted to assess these strategies         in a real diffusion field. The results of this experiment are similar to those of simulation studies showing that chemotaxis         is a more efficient but less robust strategy than biased random walk. Overall, the combined strategy seems to be superior         to chemotaxis and biased random walk in both simulation and robot experiment.      </content></document><document><year>2006</year><authors>Javier Minguez1 | Luis Montano1  | Jos Santos-Victor2 </authors><title>Abstracting Vehicle Shape and Kinematic Constraints from Obstacle Avoidance Methods      </title><content>Most obstacle avoidance techniques do not take into account vehicle shape and kinematic constraints. They assume a punctual         and omnidirectional vehicle and thus they are doomed to rely on approximations when used on real vehicles. Our main contribution         is a framework to consider shape and kinematics together in an exact manner in the obstacle avoidance process, by abstracting         these constraints from the avoidance method usage. Our approach can be applied to many non-holonomic vehicles with arbitrary         shape.                     For these vehicles, the configuration space is three-dimensional, while the control space is two-dimensional. The main idea               is to construct (centred on the robot at any time) the two-dimensional manifold of the configuration space that is defined               by elementary circular paths. This manifold contains all the configurations that can be attained at each step of the obstacle               avoidance and is thus general for all methods. Another important contribution of the paper is the exact calculus of the obstacle               representation in this manifold for any robot shape (i.e. the configuration regions in collision). Finally, we propose a change               of coordinates of this manifold so that the elementary paths become straight lines. Therefore, the three-dimensional obstacle               avoidance problem with kinematic constraints is transformed into the simple obstacle avoidance problem for a point moving               in a two-dimensional space without any kinematic restriction (the usual approximation in obstacle avoidance). Thus, existing               avoidance techniques become applicable.            </content></document><document><year>2006</year><authors>Pawel Pyk1 | Sergi Bermdez i Badia1| Ulysses Bernardet1| Philipp Knsel1| Mikael Carlsson3| Jing Gu4| Eric Chanie5| Bill S. Hansson3| Tim C. Pearce4 | Paul F. M. J. Verschure1| 2</authors><title>An artificial moth: Chemical source localization using a robot based neuronal model of moth optomotor anemotactic search      </title><content>Robots have been used to model nature, while nature in turn can contribute to the real-world artifacts we construct. One particular         domain of interest is chemical search where a number of efforts are underway to construct mobile chemical search and localization         systems. We report on a project that aims at constructing such a system based on our understanding of the pheromone communication         system of the moth. Based on an overview of the peripheral processing of chemical cues by the moth and its role in the organization         of behavior we emphasize the multimodal aspects of chemical search, i.e. optomotor anemotactic chemical search. We present         a model of this behavior that we test in combination with a novel thin metal oxide sensor and custom build mobile robots.         We show that the sensor is able to detect the odor cue, ethanol, under varying flow conditions. Subsequently we show that         the standard model of insect chemical search, consisting of a surge and cast phases, provides for robust search and localization         performance. The same holds when it is augmented with an optomotor collision avoidance model based on the Lobula Giant Movement         Detector (LGMD) neuron of the locust. We compare our results to others who have used the moth as inspiration for the construction         of odor robots.      </content></document><document><year>2006</year><authors>Shugen Ma1| 2  | Naoki Tadokoro3 </authors><title>Analysis of Creeping Locomotion of a Snake-like Robot on a Slope      </title><content>The diverse locomotion modes and physiology of biological snakes make them supremely adapted for their environment. To model         the noteworthy features of these snakes we have developed a snake-like robot that has no forward direction driving force.         In order to enhance the ability of our robot to adapt to the environment, in this study we investigate the creeping locomotion         of a snake-like robot on a slope. A computer simulator is presented for analysis of the creeping locomotion of the snake-like         robot on a slope, and the environmentally-adaptable body shape for our robot is also derived through this simulator.      </content></document><document><year>2006</year><authors> Soyer1| 2 | H. I. Bozma2  | Y. &amp;#304 stefanopulos1| 2 </authors><title>APES: Attentively Perceiving Robot      </title><content>Robot vision systems&amp;#8212;inspired by human-like vision&amp;#8212;are required to employ mechanisms similar to those that have proven to         be crucial in human visual performance. One of these mechanisms is attentive perception. Findings from vision science research         suggest that attentive perception requires a multitude of properties: A retina with fovea-periphery distinction, an attention         mechanism that can be manipulated both mechanically and internally, an extensive set of visual primitives that enable different         representation modes, an integration mechanism that can infer the appropriate visual information in spite of eye, head, body         and target motion, and finally memory for guiding eye movements and modeling the environment. In this paper we present an         attentively &amp;#8220;perceiving&amp;#8221; robot called APES. The novelty of this system stems from the fact that it incorporates all of these         properties simultaneously. As is explained, original approaches have to be taken to realize each of the properties so that         they can be integrated together in an attentive perception framework.      </content></document><document><year>2006</year><authors>Erol &amp;#350 ahin1 | Sertan Girgin1 | Emre U&amp;#287 ur1</authors><title>Area measurement of large closed regions with a mobile robot      </title><content>How can a mobile robot measure the area of a closed region that is beyond its immediate sensing range? This problem, which         we name as blind area measurement, is inspired from scout worker ants who assess potential nest cavities. We first review the insect studies that have shown         that these scouts, who work in dark, seem to assess arbitrary closed spaces and reliably reject nest sites that are small         for the colony. We briefly describe the hypothesis that these scouts use &amp;#8220;Buffon&amp;#8217;s needle method&amp;#8221; to measure the area of the         nest. Then we evaluate and analyze this method for mobile robots to measure large closed regions. We use a simulated mobile         robot system to evaluate the performance of the method through systematic experiments. The results showed that the method         can reliably measure the area of large and rather open, closed regions regardless of their shape and compactness. Moreover,         the method&amp;#8217;s performance seems to be undisturbed by the existence of objects and by partial barriers placed inside these regions.         Finally, at a smaller scale, we partially verified some of these results on a real mobile robot platform.      </content></document><document><year>2006</year><authors>Andrew G. Brooks1  | Ronald C. Arkin2 </authors><title>Behavioral overlays for non-verbal communication expression on a humanoid robot      </title><content>This research details the application of non-verbal communication display behaviors to an autonomous humanoid robot, including         the use of proxemics, which to date has been seldom explored in the field of human-robot interaction. In order to allow the         robot to communicate information non-verbally while simultaneously fulfilling its existing instrumental behavior, a &amp;#8220;behavioral         overlay&amp;#8221; model that encodes this data onto the robot's pre-existing motor expression is developed and presented. The state         of the robot's system of internal emotions and motivational drives is used as the principal data source for non-verbal expression,         but in order for the robot to display this information in a natural and nuanced fashion, an additional para-emotional framework         has been developed to support the individuality of the robot's interpersonal relationships with humans and of the robot itself.         An implementation on the Sony QRIO is described which overlays QRIO's existing EGO architecture and situated schema-based         behaviors with a mechanism for communicating this framework through modalities that encompass posture, gesture and the management         of interpersonal distance.      </content></document><document><year>2006</year><authors>Thomas Bock1 </authors><title>Construction robotics      </title><content>First construction robots had been designed in the beginning seventies in order to increase the quality in prefabrication         of modular homes in Japan and the late 70ies planning started for use of robots in construction sites. In the 80ies the first         construction robots appeared on sites and in the 90ies integrated automated building construction sites had been developped         and implemented about 20 times. Furthermore maintenance robots for cleaning and inspection of buildings, infrastructure and         real estate and safety robots guarding buildings had been developped. In the first decade of this century humanoid construction         robots had been tested. In the future service robots will be a big market in the built envrironment.      </content></document><document><year>2006</year><authors>Esben Hallundbk &#152;stergaard1 | Kristian Kassow1| Richard Beck1 | Henrik Hautop Lund1</authors><title>Design of the ATRON lattice-based self-reconfigurable robot      </title><content>Self-reconfigurable robots are robots that can change their shape in order to better suit their given task in their immediate         environment. Related work on around fifteen such robots is presented, compared and discussed. Based on this survey, design         considerations leading to a novel design for a self-reconfigurable robot, called &amp;#8220;ATRON&amp;#8221;, is described. The ATRON robot is         a lattice-based self-reconfigurable robot with modules composed of two hemispheres joined by a single revolute joint. Mechanical         design and resulting system properties are described and discussed, based on FEM analyses as well as real-world experiments.         It is concluded that the ATRON design is both competent and novel. Even though the ATRON modules are minimalistic, in the         sense that they have only one actuated degree of freedom, the collective of modules is capable of self-reconfiguring in three         dimensions. Also, a question is raised on how to compare and evaluate designs for self-reconfigurable robots, with a focus         on lattice-based systems.      </content></document><document><year>2006</year><authors>Rafael Muoz-Salinas1 | Eugenio Aguirre1  | Miguel Garca-Silvente1 </authors><title>Detection of doors using a genetic visual fuzzy system for mobile robots      </title><content>Doors are common objects in indoor environments and their detection can be used in robotic tasks such as map-building, navigation         and positioning. This work presents a new approach to door-detection in indoor environments using computer vision. Doors are         found in gray-level images by detecting the borders of their architraves. A variation of the Hough Transform is used in order         to extract the segments in the image after applying the Canny edge detector. Features like length, direction, or distance         between segments are used by a fuzzy system to analyze whether the relationship between them reveals the existence of doors.         The system has been designed to detect rectangular doors typical of many indoor environments by the use of expert knowledge.         Besides, a tuning mechanism based on a genetic algorithm is proposed to improve the performance of the system according to         the particularities of the environment in which it is going to be employed. A large database of images containing doors of         our building, seen from different angles and distances, has been created to test the performance of the system before and         after the tuning process. The system has shown the ability to detect rectangular doors under heavy perspective deformations         and it is fast enough to be used for real-time applications in a mobile robot.      </content></document><document><year>2006</year><authors>James Kramer1  | Matthias Scheutz1 </authors><title>Development environments for autonomous mobile robots: A survey      </title><content>         Robotic Development Environments (RDEs) have come to play an increasingly important role in robotics research in general, and for the development of architectures         for mobile robots in particular. Yet, no systematic evaluation of available RDEs has been performed; establishing a comprehensive         list of evaluation criteria targeted at robotics applications is desirable that can subsequently be used to compare their         strengths and weaknesses. Moreover, there are no practical evaluations of the usability and impact of a large selection of         RDEs that provides researchers with the information necessary to select an RDE most suited to their needs, nor identifies         trends in RDE research that suggest directions for future RDE development.                     This survey addresses the above by selecting and describing nine open source, freely available RDEs for mobile robots, evaluating               and comparing them from various points of view. First, based on previous work concerning agent systems, a conceptual framework               of four broad categories is established, encompassing the characteristics and capabilities that an RDE supports. Then, a practical               evaluation of RDE; usability in designing, implementing, and executing robot architectures is presented. Finally, the impact of specific RDEs on the field of robotics is addressed by providing a list of published applications and research projects               that give concrete examples of areas in which systems have been used. The comprehensive evaluation and comparison of the nine               RDEs concludes with suggestions of how to use the results of this survey and a brief discussion of future trends in RDE design.            </content></document><document><year>2006</year><authors>Arianna Menciassi1| 2 | Dino Accoto1| Samuele Gorini1 | Paolo Dario1</authors><title>Development of a biomimetic miniature robotic crawler      </title><content>The paper presents the development of segmented artificial crawlers endowed with passive hook-shaped frictional microstructures.         The goal is to find design rules for fabricating biomimetic, adaptable and mobile machines mimicking segmented animals with         hydrostatic skeleton, and intended to move effectively along unstructured substrates.                     The paper describes the mechanical model, the design and the fabrication of a SMA-actuated segmented microrobot, whose locomotion               is inspired by the peristaltic motion of Annelids, and in particular of earthworms (Lumbricus Terrestris). Experimental locomotion performance are compared with theoretical performance predicted by a purposely developed friction               model -taking into account design parameters such as number of segments, body mass, special friction enhancement appendixes&amp;#8212;and               with locomotion performance of real earthworms as presented in literature.            </content></document><document><year>2006</year><authors>Seung Nam Yu1 | Seung Yeol Lee1| Chang Soo Han1 | Kye Young Lee2  | Sang Heon Lee2</authors><title>Development of the curtain wall installation robot: Performance and efficiency tests at a construction site      </title><content>The development of automation in construction has been restricted by the variety of construction materials used, variable         circumstances, and difficulties in the quantitative management of the construction process. Curtain wall, however, can be         considered a relatively standard material compared to other construction materials. In this study, we analyze the current         process of curtain wall installation and investigate the potential of an automated system combining a commercial excavator         and a 3-DOF manipulator. This automated system has the adaptations necessary to work with any type of commercialized excavator.         Therefore, workers need to transfer only the 3-DOF manipulator portion of the system when they move to other construction         sites. This paper investigates experimental trials involving the proposed system at construction sites to determine its performance,         working time, and efficiency. Results from this study were analyzed and further research options outlined.      </content></document><document><year>2006</year><authors>Woojin Chung1 | Gunhee Kim2  | Munsang Kim2 </authors><title>Development of the multi-functional indoor service robot PSR systems      </title><content>This paper discusses the development of the multi-functional indoor service robot PSR (Public Service Robots) systems. We have built three versions of PSR systems, which are the mobile manipulator PSR-1 and PSR-2, and the guide robot Jinny. The PSR robots successfully accomplished four target service tasks including a delivery, a patrol, a guide, and a floor         cleaning task. These applications were defined from our investigation on service requirements of various indoor public environments.         This paper shows how mobile-manipulator typed service robots were developed towards intelligent agents in a real environment.         We identified system integration, multi-functionality, and autonomy considering environmental uncertainties as key research issues. Our research focused on solving these issues, and the solutions can be considered as the distinct         features of our systems. Several key technologies were developed to satisfy technological consistency through the proposed         integration scheme.      </content></document><document><year>2006</year><authors>Shahab Kalantar1  | Uwe R. Zimmer1 </authors><title>Distributed shape control of homogeneous swarms of autonomous underwater vehicles      </title><content>In this paper, we study large formations of underwater autonomous vehicles for the purposes of exploration and sampling the         ocean surface. The formations or aggregates we consider are composed of up to hundreds of robots with the capability of forming         various complex shapes dictated by the shape of the region to be explored, as well as special shapes suitable for migration.         The shapes are determined through bathymetric maps and described with reduced-dimensional representation techniques. The approach         we propose is that of breaking up the control and coordination strategy into two decoupled problems, i.e., partitioning the         aggregate into two non-overlapping sets: its boundary and its interior. The boundary uses general theory of curve evolution         to form shapes while the interior passively complies, using attraction-repulsion forces, to form a uniform distribution inside         the boundary. This makes the problem much more tractable than previous methods. Decision making by individual robots is entirely         based on local information, autonomous underwater vehicles, formation control, swarm control.      </content></document><document><year>2006</year><authors>Thomas Bock1 | Masahiru Nohmi2 | Miros&amp;#322 aw Skibniewski3</authors><title>Enabling e-business tools and robotics technology for teleconstruction      </title><content>This paper proposes a comprehensive approach to the development of technology infrastructure for the application of information         techology (IT) based solutions in teleconstruction&amp;#8212;the performance of on-site construction and related tasks through the use         of IT and robotics by a remotely located team of project participants: general contractor, subcontractors, equipment operators,         materials suppliers, and project office professionals. The paper proposes that technologies exist that enable both terrestrial         and extraterrestrial teleconstruction.      </content></document><document><year>2006</year><authors>Chris Melhuish1 | Ioannis Ieropoulos1| 2| John Greenman1| 2 | Ian Horsfield1</authors><title>Energetically autonomous robots: Food for thought      </title><content>This paper reports on the robot EcoBot-II, which is designed to power itself solely by converting unrefined insect biomass         into useful energy using on-board microbial fuel cells with oxygen cathodes. In bench experiments different &amp;#8216;fuels&amp;#8217; (sugar,         fruit and dead flies) were explored in the microbial fuel cell system and their efficiency of conversion to electricity is         compared with the maximum available energy calculated from bomb calorimetry trials. In endurance tests EcoBot-II was able         to run for 12 days while carrying out phototaxis, temperature sensing and radio transmission of sensed data approximately         every 14;min.      </content></document><document><year>2006</year><authors>Matteo Zoppi1  | Rezia Molfino1 </authors><title>Equilibrium analysis of multi-limbs walking and climbing robots      </title><content>The paper discusses the complex problem of assessing online the static equilibrium of statically-indeterminate climbing and         walking robots (CLAWARs) with quasi-static locomotion. The method proposed is general and works for whatever number of legs         and ropes operated by actuated winches connecting the robot to the environment. The configuration of the robot is assigned.         First, the compliance of the robot body, of the legs and the compliances of the ground and the ropes are modeled as localized         elasticities. The static equilibrium problem of the resulting model is statically-determinate under the hypothesis that the         foot and rope points (where the ropes are fixed to the robot body) are joined to the ground by bilateral constraints. Since         these constraints are unilateral (the feet are contact points and can detach from the ground, and the ropes can become slack),         it is necessary to apply an iterative solving procedure in order to solve the static equilibrium problem. The method presented         in the paper is a fast and effective alternative to nonlinear analysis of a finite element model of the robot at any assigned         configuration. As an example, we consider the case of the heavy-duty CLAWAR Roboclimber.      </content></document><document><year>2006</year><authors>Finding Narrow Passages with Probabilistic Roadmaps: The Small-Step Retraction Method      </authors><title>Probabilistic Roadmaps (PRM) have been successfully used to plan complex robot motions in configuration spaces of small and         large dimensionalities. However, their efficiency decreases dramatically in spaces with narrow passages. This paper presents         a new method&amp;#8212;small-step retraction&amp;#8212;that helps PRM planners find paths through such passages. This method consists of slightly         &amp;#8220;fattening&amp;#8221; robot's free space, constructing a roadmap in fattened free space, and finally repairing portions of this roadmap         by retracting them out of collision into actual free space. Fattened free space is not explicitly computed. Instead, the geometric         models of workspace objects (robot links and/or obstacles) are &amp;#8220;thinned&amp;#8221; around their medial axis. A robot configuration lies         in fattened free space if the thinned objects do not collide at this configuration. Two repair strategies are proposed. The         &amp;#8220;optimist&amp;#8221; strategy waits until a complete path has been found in fattened free space before repairing it. Instead, the &amp;#8220;pessimist&amp;#8221;         strategy repairs the roadmap as it is being built. The former is usually very fast, but may fail in some pathological cases.         The latter is more reliable, but not as fast. A simple combination of the two strategies yields an integrated planner that         is both fast and reliable. This planner was implemented as an extension of a pre-existing single-query PRM planner. Comparative         tests show that it is significantly faster (sometimes by several orders of magnitude) than the pre-existing planner.      </title><content/></document><document><year>2007</year><authors>Cecilia Laschi1 | Gioel Asuni1| Eugenio Guglielmelli3| Giancarlo Teti1| 2| Rol| Johansson4| Hitoshi Konosu5| Zbigniew Wasik5| Maria Chiara Carrozza1 | Paolo Dario1</authors><title>A bio-inspired predictive sensory-motor coordination scheme for;robot reaching and preshaping      </title><content>         This paper presents a sensory-motor coordination scheme for a robot hand-arm-head system that provides the robot with the         capability to reach an object while pre-shaping the fingers to the required grasp configuration and while predicting the tactile         image that will be perceived after grasping. A model for sensory-motor coordination derived from studies in humans inspired         the development of this scheme. A peculiar feature of this model is the prediction of the tactile image.                                             The implementation of the proposed scheme is based on a neuro-fuzzy module that, after a learning phase, starting from visual               data, calculates the position and orientation of the hand for reaching, selects the best-suited hand configuration, and predicts               the tactile feedback. The implementation of the scheme on a humanoid robot allowed experimental validation of its effectiveness               in robotics and provided perspectives on applications of sensory predictions in robot motor control.                           </content></document><document><year>2007</year><authors>Viet Nguyen1 | Stefan Gchter1 | Agostino Martinelli1 | Nicola Tomatis2  | Rol| Siegwart1 </authors><title>A comparison of line extraction algorithms using 2D range data for indoor mobile robotics      </title><content>         This paper presents an experimental evaluation of different line extraction algorithms applied to 2D laser scans for indoor         environments. Six popular algorithms in mobile robotics and computer vision are selected and tested. Real scan data collected         from two office environments by using different platforms are used in the experiments in order to evaluate the algorithms.         Several comparison criteria are proposed and discussed to highlight the advantages and drawbacks of each algorithm, including         speed, complexity, correctness and precision. The results of the algorithms are compared with ground truth using standard         statistical methods. An extended case study is performed to further evaluate the algorithms in a SLAM application.               </content></document><document><year>2007</year><authors>Young-Ho Choi1 | Tae-Kyeong Lee1  | Se-Young Oh1 </authors><title>A line feature based SLAM with low grade range sensors using geometric constraints and active exploration for mobile robot      </title><content>         This paper describes a geometrically constrained Extended Kalman Filter (EKF) framework for a line feature based SLAM, which         is applicable to a rectangular indoor environment. Its focus is on how to handle sparse and noisy sensor data, such as PSD         infrared sensors with limited range and limited number, in order to develop a low-cost navigation system. It has been applied         to a vacuum cleaning robot in our research. In order to meet the real-time objective with low computing power, we develop         an efficient line feature extraction algorithm based upon an iterative end point fit (IEPF) technique assisted by our constrained         version of the Hough transform. It uses a geometric constraint that every line is orthogonal or parallel to each other because         in a general indoor setting, most furniture and walls satisfy this constraint. By adding this constraint to the measurement         model of EKF, we build a geometrically constrained EKF framework which can estimate line feature positions more accurately         as well as allow their covariance matrices to converge more rapidly when compared to the case of an unconstrained EKF. The         experimental results demonstrate the accuracy and robustness to the presence of sensor noise and errors in an actual indoor         environment.               </content></document><document><year>2007</year><authors>Jan Peters1| 4 | Michael Mistry1| Firdaus Udwadia1| Jun Nakanishi2| 3 | Stefan Schaal1| 2</authors><title>A unifying framework for robot control with redundant DOFs      </title><content>         Recently, Udwadia (Proc. R. Soc. Lond. A 2003:1783&amp;#8211;1800, 2003) suggested to derive tracking controllers for mechanical systems with redundant degrees-of-freedom (DOFs) using a generalization         of Gauss&amp;#8217; principle of least constraint. This method allows reformulating control problems as a special class of optimal controllers.         In this paper, we take this line of reasoning one step further and demonstrate that several well-known and also novel nonlinear         robot control laws can be derived from this generic methodology. We show experimental verifications on a Sarcos Master Arm         robot for some of the derived controllers. The suggested approach offers a promising unification and simplification of nonlinear         control law design for robots obeying rigid body dynamics equations, both with or without external constraints, with over-actuation         or underactuation, as well as open-chain and closed-chain kinematics.               </content></document><document><year>2007</year><authors>J. Serres1 | D. Dray1| F. Ruffier1  | N. Franceschini1 </authors><title>A vision-based autopilot for a miniature air vehicle: joint speed control and lateral obstacle avoidance      </title><content>         In our project on the autonomous guidance of Micro-Air Vehicles (MAVs) in confined indoor and outdoor environments, we have         developed a vision based autopilot, with which a miniature hovercraft travels along a corridor by automatically controlling         both its speed and its clearance from the walls. A hovercraft is an air vehicle endowed with natural roll and pitch stabilization         characteristics, in which planar flight control systems can be developed conveniently. Our hovercraft is fully actuated by         two rear and two lateral thrusters. It travels at a constant altitude (&amp;#8764;2 mm) and senses the environment by means of two lateral         eyes that measure the right and left optic flows (OFs). The visuo-motor control system, which is called LORA III (Lateral         Optic flow Regulation Autopilot, Mark III), is a dual OF regulator consisting of two intertwined feedback loops, each of which has its own OF set-point and controls the vehicle&amp;#8217;s translation         in one degree of freedom (surge or sway). Our computer-simulated experiments show that the hovercraft can navigate along a         straight or tapered corridor at a relatively high speed (up to 1 m/s). It also reacts to any major step perturbations in the         lateral OF (provided by a moving wall) and to any disturbances caused by a tapered corridor. The minimalistic visual system         (comprised of only 4 pixels) suffices for the hovercraft to be able to control both its clearance from the walls and its forward         speed jointly, without ever measuring speed and distance. The non-emissive visual sensors and the simple control system developed         here are suitable for use on MAVs with a permissible avionic payload of only a few grams. This study also accounts quantitatively for previous ethological findings on honeybees flying freely in a straight or tapered corridor.               </content></document><document><year>2007</year><authors>Shinya Aoi1  | Kazuo Tsuchiya1 </authors><title>Adaptive behavior in turning of an oscillator-driven biped robot      </title><content>         This paper concentrates on a biped robot&amp;#8217;s turning behavior that consists of straight and curved walking and the transition         between these two patterns. We investigate how a robot achieves adaptive walking during such turning by focusing on rhythm         control and propose a locomotion control system that generates robot motions by rhythmic signals from internal oscillators         and modulates signal generation based on touch sensor signals. First, we verify that the robot attains limit cycles of straight         and curved walking by numerical simulations and hardware experiments. Second, we examine the transition between these walking         patterns based on the basin of attraction of the limit cycles in numerical simulations. Finally, we verify whether the robot         actually accomplishes transition and turning by hardware experiments. This paper clarifies that the robot establishes such         turning motions by adequate modulation of walking rhythm and phase through interactions between the dynamics of its mechanical         system, oscillators, and environment.               </content></document><document><year>2007</year><authors>Roozbeh Mottaghi1  | Richard Vaughan1 </authors><title>An integrated particle filter and potential field method applied to cooperative multi-robot target tracking      </title><content>         We describe a;novel method whereby a;particle filter is used to create a potential field for robot control without prior clustering.         We show an application of this technique to control a;team of mobile robots to cooperatively locate and track a;moving target.         The particle filter models a;probability distribution over the estimated location of the target, providing robust tracking         despite frequent target occlusion. This method extends previous work in particle-filter-based tracking in two important ways.         First, the particle cloud is never clustered to find a;single estimate of target location. Instead, robot motion is guided         by a;potential field generated directly from the particle cloud. Secondly, effective coordinated multi-robot searching and         tracking can be achieved by simply assigning a;subset of the particles to each robot.                                             Simulation trials and real robot experiments demonstrate the method successfully locating and tracking targets, and experiments               show that multiple coordinated robots outperform a;similar but uncoordinated team.                           </content></document><document><year>2007</year><authors>Mustafa Suphi Erden1  | Kemal Leblebicio&amp;#287 lu1 </authors><title>Analysis of wave gaits for energy efficiency      </title><content>         In this paper an energy efficiency analysis of wave gaits is performed for a six-legged walking robot. A;simulation model         of the robot is used to obtain the data demonstrating the energy consumption while walking in different modes and with varying         parameters. Based on the analysis of this data some strategies are derived in order to minimize the search effort for determining         the parameters of the gaits for an energy efficient walk. Then, similar data is obtained from an actual experimental setup,         in which the Robot-EA308 is used as the walking machine. The strategies are justified based on this realistic data. The analysis         concludes the following: a phase modified version of wave gaits is more efficient than the (conventional) wave gaits, using         the possible minimum protraction time results in more energy efficient gaits and higher velocity results in less energy consumption         per traveled distance. A stability analysis is performed for the phase modification of the wave gaits, and the stability loss         due to the modification is calculated. It is concluded that the loss in stability is insignificant.               </content></document><document><year>2007</year><authors>E. R. Westervelt1 | B. Morris2  | K. D. Farrell1 </authors><title>Analysis results and tools for the control of planar bipedal gaits using hybrid zero dynamics      </title><content>         New analysis and tools are presented that extend the hybrid zero dynamics (HZD) framework for the control of planar bipedal         walkers. Results include (i) analysis of walking on a;slope, (ii) analysis of dynamic (decoupling matrix) singularities, and         (iii) an alternative method for choosing virtual constraints. A key application of the new tools is the design of controllers         that render a;passive bipedal gait robust to disturbances without the use of full actuation&amp;#8212;while still requiring zero control         effort at steady-state. The new tools can also be used to design controllers for gaits having an arbitrary steady-state torque         profile. Five examples are given that illustrate these and other results.               </content></document><document><year>2007</year><authors>Paul E. Rybski1 | Stergios Roumeliotis2| Maria Gini2 | Nikolaos Papanikopoulos2</authors><title>Appearance-based mapping using minimalistic sensor models      </title><content>         This paper addresses the problem of localization and map construction by a mobile robot in an indoor environment. Instead         of trying to build high-fidelity geometric maps, we focus on constructing topological maps as they are less sensitive to poor         odometry estimates and position errors. We propose a modification to the standard SLAM algorithm in which the assumption that         the robots can obtain metric distance/bearing information to landmarks is relaxed. Instead, the robot registers a distinctive         sensor &amp;#8220;signature&amp;#8221;, based on its current location, which is used to match robot positions. In our formulation of this non-linear         estimation problem, we infer implicit position measurements from an image recognition algorithm. We propose a method for incrementally         building topological maps for a robot which uses a panoramic camera to obtain images at various locations along its path and         uses the features it tracks in the images to update the topological map. The method is very general and does not require the         environment to have uniquely distinctive features. Two algorithms are implemented to address this problem. The Iterated form         of the Extended Kalman Filter (IEKF) and a batch-processed linearized ML estimator are compared under various odometric noise         models.               </content></document><document><year>2007</year><authors>Michael L. Walters1 | Dag S. Syrdal1 | Kerstin Dautenhahn1 | Ren te Boekhorst1  | Kheng Lee Koay1 </authors><title>Avoiding the uncanny valley: robot appearance, personality and consistency of behavior in an attention-seeking home scenario         for a robot companion      </title><content>         This article presents the results of video-based Human Robot Interaction (HRI) trials which investigated people&amp;#8217;s perceptions         of different robot appearances and associated attention-seeking features and behaviors displayed by robots with different         appearance and behaviors. The HRI trials studied the participants&amp;#8217; preferences for various features of robot appearance and         behavior, as well as their personality attributions towards the robots compared to their own personalities. Overall, participants         tended to prefer robots with more human-like appearance and attributes. However, systematic individual differences in the         dynamic appearance ratings are not consistent with a universal effect. Introverts and participants with lower emotional stability         tended to prefer the mechanical looking appearance to a greater degree than other participants. It is also shown that it is         possible to rate individual elements of a particular robot&amp;#8217;s behavior and then assess the contribution, or otherwise, of that         element to the overall perception of the robot by people. Relating participants&amp;#8217; dynamic appearance ratings of individual         robots to independent static appearance ratings provided evidence that could be taken to support a portion of the left hand         side of Mori&amp;#8217;s theoretically proposed &amp;#8216;uncanny valley&amp;#8217; diagram. Suggestions for future work are outlined.               </content></document><document><year>2007</year><authors>Ashish Deshp|e1  | Jonathan Luntz1 </authors><title>Behaviors for physical cooperation between robots for mobility improvement      </title><content>         A team of small, low-cost robots instead of a single large, complex robot is useful in operations such as search and rescue,         urban exploration etc. However, performance of such a team is limited due to restricted mobility of the team members. We propose         solutions based on physical cooperation among mobile robots to improve the overall mobility. Our focus is on the development         of the low level system components. Recognizing that small robots need to overcome discrete obstacles, we develop specific         analytical maneuvers to negotiate each obstacle where a maneuver is built from a sequence of fundamental cooperative behaviors. In this paper we present cooperative behaviors that are achieved by interactions among robots via un-actuated links thus         avoiding the need for additional actuation.                                             We analyze the cooperative lift behavior and demonstrate that useful maneuvers such a gap crossing can be built using this behavior. We prove that the requirements               on ground friction and wheel torques set fundamental limits for physical cooperation. Using the design guidelines based on               static analysis we have developed simple and low cost hardware to illustrate cooperative gap crossing with two robots. We               have developed a complete dynamic model of two-robot cooperation which leads to control design. A novel connecting link design               is proposed that can change the system configuration with no additional actuators. A decentralized control architecture is               designed for the two-robot system, where each robot controls its own state with no information about the state of the other               robot thus avoiding the need of continuous communication between the two robots. Simulation and hardware results demonstrate               a successful implementation with the gap crossing example. We have analytically proved that robot dynamics can be used to               reduce the friction requirements and have demonstrated, with simulations, the implementation of this idea for the cooperative               lifting behavior.                           </content></document><document><year>2007</year><authors>Eliseo Stefano Maini1 | Luigi Manfredi1| Cecilia Laschi1 | Paolo Dario1</authors><title>Bioinspired velocity control of fast gaze shifts on a robotic anthropomorphic head      </title><content>         In this paper we address the problem of executing fast gaze shifts toward a visual target with a robotic platform. The robotic         platform is an anthropomorphic head with seven degrees of freedom (DOFs) that was designed to mimic the physical dimensions         (i.e. geometry and masses), the performances (i.e. angles and velocities) and the functional abilities (i.e. neck-movements         and eyes vergence) of the human head. In our approach the &amp;#8220;gold performance&amp;#8221; of the robotic head is represented by the accurate         eye-head coordination that is observed during head-free gaze saccades in humans.                                             To this aim, we implemented and tested on the robotic head a well-characterized, biologically inspired model of gaze control               and we investigate the effectiveness of the bioinspired paradigm to achieve an appropriate control of the multi-DOF robotic               head. Moreover, in order to verify if the proposed model can reproduce the typical patterns of actual human movements, we               performed a quantitative investigation of the relation between movement amplitude, duration and peak velocity. In the latter               case, we compared the actual robot performances with existing data on human main sequence which is known to provide a general               method for quantifying the dynamic of oculomotor control. The obtained results confirmed (1) the ability of the proposed bioinspired               control to achieve and maintain and stable fixation of the target which was always well-positioned within the fovea and (2) the ability to reproduce the               typical human main sequence diagrams which were never been successfully implemented on a fully anthropomorphic head.                           </content></document><document><year>2007</year><authors>Bruce A. Maxwell1 </authors><title>Building robot systems to interact with people in real environments      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Eric Meisner1 | Volkan Isler1  | Jeff Trinkle1 </authors><title>Controller design for human-robot interaction      </title><content>         Many robotics tasks require a robot to share the same workspace with humans. In such settings, it is important that the robot         performs in such a way that does not cause distress to humans in the workspace. In this paper, we address the problem of designing         robot controllers which minimize the stress caused by the robot while performing a given task. We present a novel, data-driven         algorithm which computes human-friendly trajectories. The algorithm utilizes biofeedback measurements and combines a set of         geometric controllers to achieve human friendliness. We evaluate the comfort level of the human using a Galvanic Skin Response         (GSR) sensor. We present results from a human tracking task, in which the robot is required to stay within a specified distance         without causing high stress values.               </content></document><document><year>2007</year><authors>Aless|ro Crespi1 | Daisy Lachat1 | Ariane Pasquier1  | Auke Jan Ijspeert1 </authors><title>Controlling swimming and crawling in a fish robot using a central pattern generator      </title><content>         Online trajectory generation for robots with multiple degrees of freedom is still a difficult and unsolved problem, in particular         for non-steady state locomotion, that is, when the robot has to move in a complex environment with continuous variations of         the speed, direction, and type of locomotor behavior. In this article we address the problem of controlling the non-steady         state swimming and crawling of a novel fish robot. For this, we have designed a control architecture based on a central pattern         generator (CPG) implemented as a system of coupled nonlinear oscillators. The CPG, like its biological counterpart, can produce         coordinated patterns of rhythmic activity while being modulated by simple control parameters.                                             To test our controller, we designed BoxyBot, a simple fish robot with three actuated fins capable of swimming in water and               crawling on firm ground. Using the CPG model, the robot is capable of performing and switching between a variety of different               locomotor behaviors such as swimming forwards, swimming backwards, turning, rolling, moving upwards/downwards, and crawling.               These behaviors are triggered and modulated by sensory input provided by light, water, and touch sensors. Results are presented               demonstrating the agility of the robot and interesting properties of a CPG-based control approach such as stability of the               rhythmic patterns due to limit cycle behavior, and the production of smooth trajectories despite abrupt changes of control               parameters.                           </content></document><document><year>2007</year><authors>E. Garcia1 | P. Gonzalez de Santos1 | F. Matia2</authors><title>Dealing with internal and external perturbations on walking robots      </title><content>         Up to now, walking robots have been working outdoors under favorable conditions and using very large stability margins to         cope with natural environments and intrinsic robot dynamics that can cause instability in these machines when they use statically-stable         gaits. The result has been very slow robots prone to tumble down in the presence of perturbations. This paper proposes a novel         gait-adaptation method based on the maximization of the Normalized Dynamic Energy Stability Margin. This method enables walking-machine         gaits to adapt to internal (robot dynamics) and external (environmental) perturbations, including the slope of the terrain,         by finding the gait parameters that maximize robot stability. The adaptation method is inspired in the natural gait adaptation         carried out by humans and animals to balance external forces or the effect of sloping terrain. Experiments with the SILO4         quadruped robot are presented and show how robot stability is more robust when the proposed approach is used for different         external forces and sloping terrains. Using the proposed gait-adaptation approach the robot is able to withstand external         forces up to 58% the robot weight and 25-degree slopes.               </content></document><document><year>2007</year><authors>Jonathan Evans1 | Pedro Patrn1 | Ben Smith1  | David M. Lane1 </authors><title>Design and evaluation of a reactive and deliberative collision avoidance and escape architecture for autonomous robots      </title><content>         We present the design and evaluation of an architecture for collision avoidance and escape of mobile autonomous robots operating         in unstructured environments. The approach mixes both reactive and deliberative components. This provides the vehicle&amp;#8217;s behavior         designers with an explicit means to design-in avoidance strategies that match system requirements in concepts of operations         and for robot certification. The now traditional three layer architecture is extended to include a fourth Scenario layer,         where scripts describing specific responses are selected and parameterized on the fly. A;local map is maintained using available         sensor data, and adjacent objects are combined as they are observed. This has been observed to create safer trajectories.         Objects have persistence and fade if not re-observed over time. In common with behavior based approaches, a;reactive layer         is maintained containing pre-defined knee jerk responses for extreme situations. The reactive layer can inhibit outputs from         above. Path planning of updated goal point outputs from the Scenario layer is performed using a fast marching method made         more efficient through lifelong planning techniques. The architecture is applied to applications with Autonomous Underwater         Vehicles. Both simulated and open water tests are carried out to establish the performance and usefulness of the approach.               </content></document><document><year>2007</year><authors>David S. Touretzky1 | Neil S. Halelamien2 | Ethan J. Tira-Thompson3 | Jordan J. Wales4  | Kei Usui3 </authors><title>Dual-coding representations for robot vision programming in Tekkotsu      </title><content>We describe complementary iconic and symbolic representations for parsing the visual world. The iconic pixmap representation         is operated on by an extensible set of &amp;#8220;visual routines&amp;#8221; (Ullman, 1984; Forbus et al., 2001). A symbolic representation, in         terms of lines, ellipses, blobs, etc., is extracted from the iconic encoding, manipulated algebraically, and re-rendered iconically.         The two representations are therefore duals, and iconic operations can be freely intermixed with symbolic ones. The dual-coding         approach offers robot programmers a versatile collection of primitives from which to construct application-specific vision         software. We describe some sample applications implemented on the Sony AIBO.      </content></document><document><year>2007</year><authors>Gaurav S. Sukhatme1 </authors><title>Editorial         Looking back, looking forward</title><content>Without Abstract</content></document><document><year>2007</year><authors>George A. Bekey1 </authors><title>Editorial         It is time for a change</title><content>Without Abstract</content></document><document><year>2007</year><authors>R. Morales1 | A. Gonzalez1| V. Feliu1 | P. Pintado1</authors><title>Environment adaptation of a new staircase-climbing wheelchair      </title><content>         This paper describes the mechanical devices conforming a novel wheelchair prototype capable of climbing staircases. The key         feature of the mechanical design is the use of two decoupled mechanisms in each axle, one to negotiate steps, and the other         to position the axle with respect to the chair to accommodate the overall slope. This design simplifies the control task substantially.         Kinematic models are necessary to describe the behavior of the system and to control the actuated degrees of freedom of the         wheelchair to ensure the passenger&amp;#8217;s comfort. The choice of a good climbing strategy simplifies the control and decreases         the power consumption of the wheelchair. In particular, we demonstrate that if the movement of the wheelchair has the same         slope as the racks or the same slope as the terrain that supports the wheel axles (depending on the configuration mechanism),         control is easier and power consumption is less. Experimental results are reported which show the behavior of the prototype         as it moves over different situations: (a) ascending a single step of different heights using different climbing strategies;         and (b) climbing a staircase using an appropriate climbing strategy that simplifies the control and reduces the power consumption         of the wheelchair.               </content></document><document><year>2007</year><authors>Audrey Duquette1 | Franois Michaud2  | Henri Mercier1 </authors><title>Exploring the use of a mobile robot as an imitation agent with;children with low-functioning autism      </title><content>         Unpredictability and complexity of social interactions are important challenges for a low functioning autistic child. The         objective of this research is to study how a mobile robot can, by appearing more predictable, appealing and simple than a         human being, facilitate reciprocal interaction such as imitative play. By conducting an exploratory study involving four children,         we found that forms of shared conventions such as imitation of body movements and of familiar actions are higher with two         children paired with a human mediator, compared to two children paired with a robot mediator. However, the two children paired         with the robot mediator demonstrated increased shared attention (visual contact, physical proximity) and imitate facial expressions         (smile) more than the children paired with the human mediator.               </content></document><document><year>2007</year><authors>Jose-Luis Blanco1 | Javier Gonzlez1  | Juan-Antonio Fernndez-Madrigal1 </authors><title>Extending obstacle avoidance methods through multiple parameter-space transformations      </title><content>         Obstacle avoidance methods approach the problem of mobile robot autonomous navigation by steering the robot in real-time according         to the most recent sensor readings, being suitable to dynamic or unknown environments. However, real-time performance is commonly         gained by ignoring the robot shape and some or all of its kinematic restrictions which may lead to poor navigation performance         in many practical situations.                                             In this paper we propose a framework where a kinematically constrained and any-shape robot is transformed in real-time into               a free-flying point in a new space where well-known obstacle avoidance methods are applicable. Our contribution with this               framework is twofold: the definition of generalized space transformations that cover most of the existing transformational               approaches, and a reactive navigation system where multiple transformations can be applied concurrently in order to optimize               robot motion decisions. As a result, these transformations allow existing obstacle avoidance methods to perform better detection               of the surrounding free-space, through &amp;#8220;sampling&amp;#8221; the space with paths compatible with the robot kinematics.                           </content></document><document><year>2007</year><authors>Anders Lyhne Christensen1 | Rehan O&amp;#8217 Grady1 | Mauro Birattari1  | Marco Dorigo1 </authors><title>Fault detection in autonomous robots based on fault injection and;learning      </title><content>         In this paper, we study a new approach to fault detection for autonomous robots. Our hypothesis is that hardware faults change         the flow of sensory data and the actions performed by the control program. By detecting these changes, the presence of faults         can be inferred. In order to test our hypothesis, we collect data from three different tasks performed by real robots. During         a number of training runs, we record sensory data from the robots while they are operating normally and after a fault has         been injected. We use back-propagation neural networks to synthesize fault detection components based on the data collected         in the training runs. We evaluate the performance of the trained fault detectors in terms of number of false positives and         time it takes to detect a fault. The results show that good fault detectors can be obtained. We extend the set of possible         faults and go on to show that a single fault detector can be trained to detect several faults in both a robot&amp;#8217;s sensors and         actuators. We show that fault detectors can be synthesized that are robust to variations in the task, and we show how a fault         detector can be trained to allow one robot to detect faults that occur in another robot.               </content></document><document><year>2007</year><authors>Thomas Bock1 </authors><title>Guest editorial      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Adriana Tapus1  | Maja J. Matari&amp;#263 1 </authors><title>Guest editorial: special issue on socially assistive robotics      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Ignacio Fernndez1 | Manuel Mazo1 | Jos L. Lzaro1 | Daniel Pizarro1 | Enrique Santiso1 | Pedro Martn1  | Cristina Losada1 </authors><title>Guidance of a mobile robot using an array of static cameras located in the environment      </title><content>         This paper presents a new proposal for positioning and guiding mobile robots in indoor environments. The proposal is based         on the information provided by static cameras located in the movement environment. This proposal falls within the scope of         what are known as intelligent environments; in this case, the environment is provided with cameras that, once calibrated,         allow the position of the robots to be obtained. Based on this information, control orders for the robots can be generated         using a radio frequency link. In order to facilitate identification of the robots, even under extremely adverse ambient lighting         conditions, a beacon consisting of four circular elements constructed from infrared diodes is mounted on board the robots.         In order to identify the beacon, an edge detection process is carried out. This is followed by a process that, based on the         algebraic distance, obtains the estimated ellipses associated with each element of the beacon. Once the beacon has been identified,         the coordinates of the centroids for the elements that make up the beacon are obtained on the various image planes. Based         on these coordinates, an algorithm is proposed that takes into account the standard deviation of the error produced in the         various cameras in ascertaining the coordinates of the beacon&amp;#8217;s elements. An odometric system is also used in guidance that,         in conjunction with a Kalman Filter, allows the position of the robot to be estimated during the time intervals required to         process the visual information provided by the cameras.               </content></document><document><year>2007</year><authors>Anies Hannawati Purnamadjaja1  | R. Andrew Russell1 </authors><title>Guiding robots&amp;#8217; behaviors using pheromone communication      </title><content>         This paper describes an ongoing project to investigate the uses of pheromones as a means of communication in robotics. The         particular example of pheromone communication considered here was inspired by queen bee pheromones that have a number of crucial         functions in a bee colony, such as keeping together and stabilizing the colony. In the context of a robotic system, one of         the proposed applications for robot pheromones is to allow a group of robots to be guided by a robot leader. The robot leader         could release different chemicals to elicit a range of behaviors from other members of the group. A change of the operating         temperature of tin oxide gas sensors has been implemented in order to differentiate different chemicals. This paper provides         details of the robots used in the project and their behaviors. The sensors, especially the method of using the tin oxide gas         sensors, the robot control algorithms and experimental results are presented. In this project, pheromones were used to trigger         congregating behavior and light seeking in a group of robots.               </content></document><document><year>2007</year><authors>Takayuki K|a1 | Shogo Nabe1| 2| Kazuo Hiraki1| 2| Hiroshi Ishiguro1| 3 | Norihiro Hagita1</authors><title>Human friendship estimation model for communication robots      </title><content>         Based on the analysis of non-verbal inter-human interaction, this paper proposes a model for estimating human friendships         in the presence of a humanoid robot. Our previous study in an elementary school provided rich video data of two months of         inter-human interaction in the presence of a humanoid robot. Such data are particularly useful for developing a robot&amp;#8217;s social         ability: a friendship estimation capability. We analyzed the video based on an observation method to analyze the interaction         among children and the robot. From their non-verbal interactions, several important factors for friendship estimation were         retrieved, including touch, gaze, co-presence, and distance. Gender was also considered a factor in the model, since gender         differences were observed in non-verbal interactions. The model discriminated between friendly and non-friendly relationships         among the children with 74.5% accuracy for boys and 83.8% for girls.               </content></document><document><year>2007</year><authors>Matthew L. Robinson1 | Eric T. Baumgartner2| Kevin M. Nickels3 | Todd E. Litwin1</authors><title>Hybrid image plane/stereo (HIPS) manipulation for;robotic space applications      </title><content>         Manipulation systems for planetary exploration operate under severe limitations due to power and weight restrictions and extreme         environmental conditions. Typically such systems employ carefully calibrated stereo cameras and carefully calibrated manipulators         to achieve precision on the order of ten millimeters with respect to instrument placement activities. The environmental and         functional restrictions under which these systems are used limit the operational accuracy of these approaches. This paper         presents a;novel approach to stereo-based manipulation designed to robustly achieve high precision levels despite the aforementioned         limitations. The basic principle of the approach, known as Hybrid Image Plane/Stereo (HIPS) Manipulation, is the generation         of camera models through direct visual sensing of the manipulator&amp;#8217;s end-effector. The HIPS method estimates and subsequently         uses these models to position the manipulator at a;target location specified in the image-planes of a;stereo camera pair using         stereo correlation and triangulation. In-situ estimation and adaptation of the manipulator/camera models in this method accounts         for changes in the system configuration, thus ensuring consistent precision for the life of the mission. The end result is         a;increase in positioning precision by a;factor of approximately two for a;limited version of HIPS, and an order of magnitude         increase in positioning precision for the full on-line version of HIPS.               </content></document><document><year>2007</year><authors>P. Gonzalez de Santos1 | E. Garcia1 | J. Estremera1</authors><title>Improving walking-robot performances by optimizing leg distribution      </title><content>         Walking-robot technology has already achieved an important stage of development, as demonstrated in a few real applications.         However, walking robots still need further improvement if they are to compete with traditional vehicles. A potential improvement         could be made through optimization at design time. A better distribution of the legs around a robot&amp;#8217;s body can help decrease         actuator size in the design procedure and reduce power consumption during walking as well, which is of vital importance in         autonomous robots. This paper, thus, presents a method focused on the distribution of legs around the body to decrease maximum         foot forces against the ground, which play heavily in determining robot shape and actuator size. Some experiments have been         performed with the SILO6 walking robot to validate the theoretical results.               </content></document><document><year>2007</year><authors>Hyun Do Choi1 | Chun Kyu Woo1 | Soohyun Kim1 | Yoon Keun Kwak1  | Sukjune Yoon2 </authors><title>Independent traction control for uneven terrain using stick-slip phenomenon: application to a stair climbing robot      </title><content>         Mobile robots are being developed for building inspection and security, military reconnaissance, and planetary exploration.         In such applications, the robot is expected to encounter rough terrain. In rough terrain, it is important for mobile robots         to maintain adequate traction as excessive wheel slip causes the robot to lose mobility or even be trapped. This paper proposes         a traction control algorithm that can be independently implemented to each wheel without requiring extra sensors and devices         compared with standard velocity control methods. The algorithm estimates the stick-slip of the wheels based on estimation         of angular acceleration. Thus, the traction force induced by torque of wheel converses between the maximum static friction         and kinetic friction. Simulations and experiments are performed to validate the algorithm. The proposed traction control algorithm         yielded a 40.5% reduction of total slip distance and 25.6% reduction of power consumption compared with the standard velocity         control method. Furthermore, the algorithm does not require a complex wheel-soil interaction model or optimization of robot         kinematics.               </content></document><document><year>2007</year><authors>Paul E. Rybski1  | William D. Smart2</authors><title>Introduction to the special issue on the science behind embodied AI : The robots of the AAAI competition and exhibition      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Michael Shneier1 | Tommy Chang1 | Tsai Hong1 | Will Shackleford1 | Roger Bostelman1  | James S. Albus1 </authors><title>Learning traversability models for autonomous mobile vehicles      </title><content>         Autonomous mobile robots need to adapt their behavior to the terrain over which they drive, and to predict the traversability         of the terrain so that they can effectively plan their paths. Such robots usually make use of a set of sensors to investigate         the terrain around them and build up an internal representation that enables them to navigate. This paper addresses the question         of how to use sensor data to learn properties of the environment and use this knowledge to predict which regions of the environment         are traversable. The approach makes use of sensed information from range sensors (stereo or ladar), color cameras, and the         vehicle&amp;#8217;s navigation sensors. Models of terrain regions are learned from subsets of pixels that are selected by projection         into a local occupancy grid. The models include color and texture as well as traversability information obtained from an analysis         of the range data associated with the pixels. The models are learned without supervision, deriving their properties from the         geometry and the appearance of the scene.                                             The models are used to classify color images and assign traversability costs to regions. The classification does not use the               range or position information, but only color images. Traversability determined during the model-building phase is stored               in the models. This enables classification of regions beyond the range of stereo or ladar using the information in the color               images. The paper describes how the models are constructed and maintained, how they are used to classify image regions, and               how the system adapts to changing environments. Examples are shown from the implementation of this algorithm in the DARPA               Learning Applied to Ground Robots (LAGR) program, and an evaluation of the algorithm against human-provided ground truth is               presented.                           </content></document><document><year>2007</year><authors>Gianluca Antonelli1  | Stefano Chiaverini1 </authors><title>Linear estimation of the physical odometric parameters for differential-drive mobile robots      </title><content>         In this paper a calibration technique aimed at identifying the odometric parameters of differential-drive mobile robots is         proposed. The algorithm is based on two successive least-squares estimations based on the continuous-time kinematic equations         of motion; the time-discretization error, thus, is avoided. The use of the least-squares technique is made possible by working         on a linear mapping between the unknowns and the measurements and is not the result of a linearization. Another advantage         of the proposed technique is that no specific path is required. The basic technique makes use of video-camera measurements         and absolute position readings of the wheels&amp;#8217; encoders; the use of different sensors and measurements of the wheels velocities         is also discussed. Experimental results with a mobile robot Khepera;II confirm the effectiveness of the proposed technique.               </content></document><document><year>2007</year><authors>Young-Ho Choi1  | Se-Young Oh1 </authors><title>Map building through pseudo dense scan matching using visual sonar data      </title><content>         This paper presents a novel approach to the vision based grid map building and localization problem that works in a complex         indoor environment with a single forward viewing camera. Most existing visual SLAM has been limited to the feature-based method         and only a few researchers have proposed visual SLAM methods for building a grid map using a stereo vision system which has         not been popular in practical application. In this paper, we estimate the planar depth by applying a simple visual sonar ranging         technique to the single camera image and then associating sequential scans through our own pseudo dense adaptive scan matching         algorithm reducing the processing time compared to the standard point-to-point correspondence based algorithm and finally         produce a grid map. To this end, we construct a Pseudo Dense Scan (PDS) which is an odometry based temporal accumulation of         the visual sonar readings emulating omni-directional sensing in order to overcome the sparseness of the visual sonar. Moreover,         in order to obtain a much more refined map, we further correct the slight trajectory error incurred in the PDS construction         step using Sequential Quadratic Programming (SQP) which is a well-known optimization scheme. Experimental results show that         our method can obtain an accurate grid map using a single camera without the need for a high price range sensors or stereo         camera.               </content></document><document><year>2007</year><authors>Wolfgang Hbner1  | Hanspeter A. Mallot1 </authors><title>Metric embedding of view-graphs         A vision and odometry-based approach to cognitive mapping</title><content>         Most recent robotic systems, capable of exploring unknown environments, use topological structures (graphs) as a spatial representation.         Localization can be done by deriving an estimate of the global pose from landmark information. In this case navigation is         tightly coupled to metric knowledge, and hence the derived control method is mainly pose-based. Alternative to continuous         metric localization, it is also possible to base localization methods on weaker constraints, e.g. the similarity between images         capturing the appearance of places or landmarks. In this case navigation can be controlled by a homing algorithm. Similarity         based localization can be scaled to continuous metric localization by adding additional constraints, such as alignment of         depth estimates.                                             We present a method to scale a similarity based navigation system (the view-graph-model) to continuous metric localization.               Instead of changing the landmark model, we embed the graph into the three dimensional pose space. Therefore, recalibration               of the path integrator is only possible at discrete locations in the environment. The navigation behavior of the robot is               controlled by a homing algorithm which combines three local navigation capabilities, obstacle avoidance, path integration,               and scene based homing. This homing scheme allows automated adaptation to the environment. It is further used to compensate               for path integration errors, and therefore allows to derive globally consistent pose estimates based on &amp;#8220;weak&amp;#8221; metric knowledge,               i.e. knowledge solely derived from odometry. The system performance is tested with a robotic setup and with a simulated agent               which explores a large, open, and cluttered environment.                           </content></document><document><year>2007</year><authors>Seung Yeol Lee1 | Yong Seok Lee2 | Bum Seok Park3 | Sang Heon Lee4  | Chang Soo Han1 </authors><title>MFR (Multipurpose Field Robot) for installing construction materials      </title><content>The objective of the study was to propose a MFR (Multipurpose Field Robot) in hazardous operation environments. This system         combines a basic system composed of a multi-DOF (Degree Of Freedom) manipulator and a mobile platform with an additional module         for construction, national defense and emergency-rescue. According to an additional module type combined with a basic system,         it can be used in a various fields. In this study, we describe a prototype of construction robot which helps a human operator         handle easily construction materials in case of using the cooperation system on construction site. This study introduces an         additional module for construction and a robot control algorithm for a HRC (Human-Robot Cooperation). In addition, it proposes         a novel construction method to install construction materials with robot on construction site.      </content></document><document><year>2007</year><authors>Patrick Deegan1| Roderic Grupen1| Allen Hanson1| Emily Horrell1| Shichao Ou1 | Edward Riseman1| Shiraj Sen1| Bryan Thibodeau1| Adam Williams1 | Dan Xie1</authors><title>Mobile manipulators for assisted living in residential settings      </title><content>         We describe a methodology for creating new technologies for assisted living in residential environments. The number of eldercare         clients is expected to grow dramatically over the next decade as the baby boom generation approaches 65 years of age. The         UMass/Smith ASSIST framework aims to alleviate the strain on centralized medical providers and community services as their         clientele grow, reduce the delays in service, support independent living, and therefore, improve the quality of life for the         up-coming elder population. We propose a closed loop methodology wherein innovative technical systems are field tested in         assisted care facilities and analyzed by social scientists to create and refine residential systems for independent living.         Our goal is to create technology that is embraced by clients, supports efficient delivery of support services, and facilitates         social interactions with family and friends. We introduce a series of technologies that are currently under evaluation based         on a distributed sensor network and a unique mobile manipulator (MM) concept. The mobile manipulator provides client services         and serves as an embodied interface for remote service providers. As a result, a wide range of cost-effective eldercare applications         can be devised, several of which are introduced in this paper. We illustrate tools for social interfaces, interfaces for community         service and medical providers, and the capacity for autonomous assistance in the activities of daily living. These projects         and others are being considered for field testing in the next cycle of ASSIST technology development.               </content></document><document><year>2007</year><authors>Yuan F. Zheng1  | Weidong Chen2</authors><title>Mobile robot team forming for crystallization of proteins      </title><content>         The process of protein crystallization is explained using the theory of robotics, particularly path planning of mobile robots. Path planning is a procedure which specifies motion trajectories of multiple mobile robots to form a robotic         team with a desired pattern. Since protein crystals consist of a large number of protein molecules which come together to         form a 3D lattice of uniform structure, it is hypothesized that each protein behaves like a mobile robot and takes adequate         path to form a robotic team (crystal). Based on this hypothesis, it is shown that trajectories of the proteins should be simple and local, which generates three rules of motion for the protein robots, i.e., (a) each protein searches for its nearest neighbor,         (b) each protein takes the shortest path to approach the nearest neighbor, and (c) multiple proteins may form a sub-team of         proteins. It is then proven mathematically that the planned path according to the three rules is stable and able to crystallize         the proteins. Interaction forces at the molecular level are analyzed to show that the simple and local motion of the proteins         is physically warranted. Computer simulation and experimental results are presented to validate the new theory.               </content></document><document><year>2007</year><authors>Andrea Carbone1 | Alberto Finzi2 | Andrea Orl|ini3  | Fiora Pirri1 </authors><title>Model-based control architecture for attentive robots in;rescue;scenarios      </title><content>         In this paper we present a control architecture for an autonomous rescue robot specialized in victim finding in an unknown         and unstructured environment. The reference domain for rescue robots is the rescue-world arenas purposefully arranged for         the Robocup competitions. The main task of a rescue mobile robot is to explore the environment and report to the rescue-operators         the map of visited areas annotated with its finding. In this context all the attentional activities play a major role in decision         processes: salient elements in the environment yield utilities and objectives. A;model-based executive controller is proposed         to coordinate, integrate, and monitor the distributed decisions and initiatives emerging from the modules involved in the         control loop. We show how this architecture integrates the reactive model-based control of a rescue mission, with an attentive         perceptual activity processing the sensor and visual stimuli. The architecture has been implemented and tested in real-world         experiments by comparing the performances of metric exploration and attentive exploration. The results obtained demonstrate         that the attentive behavior significantly focus the exploration time in salient areas enhancing the overall victim finding         effectiveness.               </content></document><document><year>2007</year><authors>Joaqun Gutirrez1 | Dimitrios Apostolopoulos2  | Jos Luis Gordillo3 </authors><title>Numerical comparison of steering geometries for robotic vehicles by modeling positioning error      </title><content>         This paper describes an analytical method for modeling the positioning error of a robotic vehicle and examines how the metric         of this error can be used to compare the geometries of various steering configuration. Positioning error can be caused by         many factors stemming from the robot&amp;#8217;s hardware and software configurations and the interaction between the robot and its         environment. A slip motion model that captures the effects of key factors that contribute to positioning error is presented.         Robot kinematic models with and without slippage are reformulated and used to perform an in-depth assessment and characterization         of positioning error. The method is applied to three characteristic advance and steering configurations: Ackermann, articulated,         and explicitly steered. This analysis serves as a quantitative evaluation of the properties of the steering geometries for         path tracking under identical slippage conditions. The method can also be used as a tool for comparing robot configurations         to make trade-off decisions early in the design process, as it allows for derivation of predicted performance values of alternative         steering geometries.               </content></document><document><year>2007</year><authors>Paolo Arena1 | Sebastiano De Fiore1 | Luigi Fortuna1 | Mattia Frasca1 | Luca Patan1  | Guido Vagliasindi1 </authors><title>Reactive navigation through multiscroll systems: from theory to;real-time implementation      </title><content>         In this paper a new reactive layer for multi-sensory integration applied to robot navigation is proposed. The new robot navigation         technique exploits the use of a chaotic system able to be controlled in real-time towards less complex orbits, like periodic         orbits or equilibrium points, considered as perceptive orbits. These are subject to real-time modifications on the basis of environment changes acquired through a distributed sensory         system. The strategy is inspired to the olfactory bulb neural activity observed in rabbits subject to external stimuli. The         mathematical details of the approach are given including simulation results in a virtual environment. Furthermore the proposed         strategy has been tested on an experimental environment consisting of an FPGA-based hardware driving an autonomous roving         robot. The obtained results demonstrate the capability to perform a real-time navigation control.               </content></document><document><year>2007</year><authors>Ofir Avni1 | Francesco Borrelli2 | Gadi Katzir3| Ehud Rivlin1  | Hector Rotstein4</authors><title>Scanning and tracking with independent cameras&amp;#8212;a biologically motivated approach based on model predictive control      </title><content>         This paper presents a framework for visual scanning and target tracking with a set of independent pan-tilt cameras. The approach         is systematic and based on Model Predictive Control (MPC), and was inspired by our understanding of the chameleon visual system.                                             We make use of the most advanced results in the MPC theory in order to design the scanning and tracking controllers. The scanning               algorithm combines information about the environment and a model for the motion of the target to perform optimal scanning               based on stochastic MPC. The target tracking controller is a switched control combining smooth pursuit and saccades. Min-Max               and minimum-time MPC theory is used for the design of the tracking control laws.                           </content></document><document><year>2007</year><authors>David P. Miller1 | Jacob Q. Milstein2 | Cathryne Stein3</authors><title>Scarecrow: If I only had AI      </title><content>At the 1992 AAAI robot contest one of the top finishers was Scarecrow&amp;#8212;a robot that had no computer in the traditional sense,         was built out of less than $200 of parts, and was explained and operated by a five year old. The designers sought to demonstrate         the capabilities and competence that can be accomplished by using a strictly reactive architecture for well defined tasks         such as that contest. This paper reexamines the Scarecrow robot and puts it into historical context. With fifteen years of         perspective, we can also see what Scarecrow has to say about the perception of intelligence.      </content></document><document><year>2007</year><authors>Marek P. Michalowski1 | Selma &amp;#352 abanovi&amp;#263 2| Carl DiSalvo1| Ddac Busquets1| Laura M. Hiatt1| Nik A. Melchior1 | Reid Simmons1</authors><title>Socially distributed perception: GRACE plays social tag at;AAAI;2005      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Mohan Sridharan1  | Peter Stone1</authors><title>Structure-based color learning on a;mobile robot under changing illumination      </title><content>         A;central goal of robotics and AI is to be able to deploy an agent to act autonomously in the real world over an extended         period of time. To operate in the real world, autonomous robots rely on sensory information. Despite the potential richness         of visual information from on-board cameras, many mobile robots continue to rely on non-visual sensors such as tactile sensors,         sonar, and laser. This preference for relatively low-fidelity sensors can be attributed to, among other things, the characteristic         requirement of real-time operation under limited computational resources. Illumination changes pose another big challenge.         For true extended autonomy, an agent must be able to recognize for itself when to abandon its current model in favor of learning a;new one; and how to learn in its current situation. We describe a;self-contained vision system that works on-board a;vision-based autonomous         robot under varying illumination conditions. First, we present a;baseline system capable of color segmentation and object         recognition within the computational and memory constraints of the robot. This relies on manually labeled data and operates under constant and reasonably uniform illumination conditions. We then relax these limitations by introducing algorithms for (i);Autonomous         planned color learning, where the robot uses the knowledge of its environment (position, size and shape of objects) to automatically         generate a;suitable motion sequence and learn the desired colors, and (ii);Illumination change detection and adaptation, where         the robot recognizes for itself when the illumination conditions have changed sufficiently to warrant revising its knowledge         of colors. Our algorithms are fully implemented and tested on the Sony ERS-7 Aibo robots.               </content></document><document><year>2007</year><authors>T. Schmickl1  | K. Crailsheim1 </authors><title>Trophallaxis within a robotic swarm: bio-inspired communication among robots in a swarm      </title><content>         This article presents a bio-inspired communication strategy for large-scale robotic swarms. The strategy is based purely on         robot-to-robot interactions without any central unit of communication. Thus, the emerging swarm regulates itself in a purely         self-organized way. The strategy is biologically inspired by the trophallactic behavior (mouth-to-mouth feedings) performed         by social insects. We show how this strategy can be used in a collective foraging scenario and how the efficiency of this         strategy can be shaped by evolutionary computation. Although the algorithm works stable enough that it can be easily parameterized         by hand, we found that artificial evolution could further increase the efficiency of the swarm&amp;#8217;s behavior. We investigated         the suggested communication strategy by simulation of robotic swarms in several arena scenarios and studied the properties         of some of the emergent collective decisions made by the robots. We found that our control algorithm led to a nonlinear, but         graduated path selection of the emerging trail of loaded robots. They favored the shortest path, but not all robots converged         to this trail, except in arena setups with extreme differences in the length of the two possible paths. Finally, we demonstrate         how the flexibility of collective decisions that arise through this new strategy can be used in changing environments. We         furthermore show the importance of a negative feedback in an environment with changing foraging targets. Such feedback loops         allow outdated information to decay over time. We found that task efficiency is constrained by a lower and an upper boundary         concerning the strength of this negative feedback.               </content></document><document><year>2007</year><authors>Arnaud Clrentin1 | Mlanie Delafosse1 | Laurent Delahoche1 | Bruno Marhic1  | Anne-Marie Jolly-Desodt2 </authors><title>Uncertainty and imprecision modeling for the mobile robot localization;problem      </title><content>         This article deals with uncertainty and imprecision treatment during the mobile robot localization process. The imprecision         determination is based on the use of the interval formalism. Indeed, the mobile robot is equipped with an exteroceptive sensor         and odometers. The imprecise data given by these two sensors are fused by constraint propagation on intervals. At the end         of the algorithm, we get 3D localization subpaving which is supposed to contain the robot&amp;#8217;s position in a guaranteed way.         Concerning the uncertainty, it is managed through a propagation architecture based on the use of the Transferable Belief Model         of Smets. This architecture enables to propagate uncertainty from low level data (sensor data) in order to quantify the global         uncertainty of the robot localization estimation.               </content></document><document><year>2007</year><authors>Eric Demeester1 | Alex|er Hntemann1| Dirk Vanhooydonck1| Gerolf Vanacker1| Hendrik Van Brussel1 | Marnix Nuttin1</authors><title>User-adapted plan recognition and user-adapted shared control: A Bayesian approach to semi-autonomous wheelchair driving      </title><content>         Many elderly and physically impaired people experience difficulties when maneuvering a powered wheelchair. In order to ease         maneuvering, powered wheelchairs have been equipped with sensors, additional computing power and intelligence by various research         groups.                                             This paper presents a Bayesian approach to maneuvering assistance for wheelchair driving, which can be adapted to a specific               user. The proposed framework is able to model and estimate even complex user intents, i.e. wheelchair maneuvers that the driver               has in mind. Furthermore, it explicitly takes the uncertainty on the user&amp;#8217;s intent into account. Besides during intent estimation,               user-specific properties and uncertainty on the user&amp;#8217;s intent are incorporated when taking assistive actions, such that assistance               is tailored to the user&amp;#8217;s driving skills. This decision making is modeled as a greedy Partially Observable Markov Decision               Process (POMDP).                           </content></document><document><year>2007</year><authors>Marjorie Skubic1 | Derek Anderson1| Samuel Blisard1| Dennis Perzanowski2 | Alan Schultz2</authors><title>Using a hand-drawn sketch to control a team of robots      </title><content>In this paper, we describe a prototype interface that facilitates the control of a mobile robot team by a single operator,         using a sketch interface on a Tablet PC. The user draws a sketch map of the scene and includes the robots in approximate starting         positions. Both path and target position commands are supported as well as editing capabilities. Sensor feedback from the         robots is included in the display such that the sketch interface acts as a two-way communication device between the user and         the robots. The paper also includes results of a usability study, in which users were asked to perform a series of tasks.      </content></document><document><year>2007</year><authors>Dezhen Song1 | Hyun Nam Lee2| Jingang Yi3 | Anthony Lev|owski4</authors><title>Vision-based motion planning for an autonomous motorcycle on;ill-structured roads      </title><content>         We report our development of a vision-based motion planning system for an autonomous motorcycle designed for desert terrain,         where uniform road surface and lane markings are not present. The motion planning is based on a vision vector space (V2-Space), which is a unitary vector set that represents local collision-free directions in the image coordinate system. The         V2-Space is constructed by extracting the vectors based on the similarity of adjacent pixels, which captures both the color         information and the directional information from prior vehicle tire tracks and pedestrian footsteps. We report how the V2-Space is constructed to reduce the impact of varying lighting conditions in outdoor environments. We also show how the V2-Space can be used to incorporate vehicle kinematic, dynamic, and time-delay constraints in motion planning to fit the highly         dynamic requirements of the motorcycle. The combined algorithm of the V2-Space construction and the motion planning runs in O(n) time, where n is the number of pixels in the captured image. Experiments show that our algorithm outputs correct robot motion commands         more than 90% of the time.               </content></document><document><year>2007</year><authors>Ralf Mller1 | Andrew Vardy2| Sven Kreft1 | Sebastian Ruwisch1</authors><title>Visual homing in environments with anisotropic landmark distribution      </title><content>         Gradient descent in image distances can lead a navigating agent to the goal location, but in environments with an anisotropic         distribution of landmarks, gradient home vectors deviate from the true home direction. These deviations can be reduced by         applying Newton&amp;#8217;s method to matched-filter descent in image distances (MFDID). Based on several image databases we demonstrate         that the home vectors produced by the Newton-based MFDID method are usually closer to the true home direction than those obtained         from the original MFDID method. The greater accuracy of Newton-MFDID home vectors in the vicinity of the goal location would         allow a navigating agent to approach the goal on straighter trajectories, improve the results of triangulation procedures,         and enhance a robot&amp;#8217;s ability to detect its arrival at a goal.               </content></document><document><year>2007</year><authors>Xiuli Zhang1  | Haojun Zheng2 </authors><title>Walking up and down hill with a biologically-inspired postural reflex in a quadrupedal robot      </title><content>         This paper presents a control strategy, biologically-inspired postural reflex, based directly on animal behaviors, which allows         a quadrupedal robot to walk up and down hill smoothly. A central pattern generator (CPG) and a hip-to-knee mapping function         are employed to realize the basic rhythmic motion for the quadrupedal robot. The trunk pitch angle feedback of the robot is         linearly introduced to the CPG, spontaneously changing the mid-positions of the four legs to make postural adjustments as         the way a cat does. Thereby, slipping and falling-down are greatly reduced. Numerical simulations and experimental implementation         on a physical quadrupedal robot demonstrate the effectiveness of the proposed approach.               </content></document><document><year>2005</year><authors>tefan Havl&amp;Iacute k1 </authors><title>A Modular Concept of the Robotic Vehicle for Demining Operations</title><content>Paper analyses some most important characteristics that should be taken into consideration in building the robotic demining vehicle. Based on previous experiences from the development of demining technology the modular concept of the multipurpose vehicle and some its main functional parts are discussed. Such robotic vehicle can be used as general porter of various detection systems, tools for cleaning terrain as well as neutralization equipment. Further development towards partially autonomous system and some principal tasks of positioning in dangerous terrain are analyzed. The real construction of the vehicle equipped by the flailing mechanisms for mechanical activation of explosions is briefly presented.</content></document><document><year>2005</year><authors>Mar&amp;iacute a Elena L&amp;oacute pez1 | Luis Miguel Bergasa1 | Rafael Barea1  | Mar&amp;iacute a Soledad Escudero1 </authors><title>A Navigation System for Assistant Robots Using Visually Augmented POMDPs</title><content>Assistant robots have received special attention from the research community in the last years. One of the main applications of these robots is to perform care tasks in indoor environments such as houses, nursing homes or hospitals, and therefore they need to be able to navigate robustly for long periods of time. This paper focuses on the navigation system of SIRA, a robotic assistant for elderly and/or blind people based on a Partially Observable Markov Decision Process (POMDP) to global localize the robot and to direct its goal-oriented actions. The main novel feature of our approach is that it combines sonar and visual information in a natural way to produce state transitions and observations in the framework of Markov Decision Processes. Besides this multisensorial fusion, a two-level layered planning architecture that combines several planning objectives (such as guiding to a goal room and reducing locational uncertainty) improves the robustness of the navigation system, as its shown in our experiments with SIRA navigating corridors.</content></document><document><year>2005</year><authors>A New Approach to Humanitarian DeminingPart 1: Mobile Platform for Operation on Unstructured Terrain</authors><title>Landmines can deprive whole areas of valuable resources, and continue to kill and cause injuries years after the end of armed conflicts. Armored vehicles are used for mine clearance, but with limited reliability. The final inspection of minefields is still performed by human deminers exposed to potentially fatal accidents. The aim of this research is to introduce automation as a way to improve the final level of humanitarian demining. This paper addresses mobility and manipulation, while sensing, communication and visualization shall be discussed in detail in a subsequent paper. After analyzing the merits and limitations of previous works, a new approach to tele-operated demining is considered, using off-road buggies equipped with combustion engines, and taking into account actual field requirements. Control of the automated buggies on rough terrain is also discussed, as well as the development of a new weight-balanced manipulator for landmine clearance operations.</title><content/></document></documents>