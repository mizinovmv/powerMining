<?xml version="1.0" encoding="UTF-8" standalone="no"?><documents><document><year>2004</year><authors>Springer London Ltd.1</authors><title>2004 KAIS Reviewers</title><content>Without Abstract</content></document><document><year>2004</year><authors>David Genest1  | Michel Chein2</authors><title>A content-search information retrieval process based on conceptual graphs</title><content>An intelligent information retrieval system is presented in this paper. In our approach, which complies with the logical view of information retrieval, queries, document contents and other knowledge are represented by expressions in a knowledge representation language based on the conceptual graphs introduced by Sowa. In order to take the intrinsic vagueness of information retrieval into account, i.e. to search documents imprecisely and incompletely represented in order to answer a vague query, different kinds of probabilistic logic are often used. The search process described in this paper uses graph transformations instead of probabilistic notions. This paper is focused on the content-based retrieval process, and the cognitive facet of information retrieval is not directly addressed. However, our approach, involving the use of a knowledge representation language for representing data and a search process based on a combinatorial implementation of van Rijsbergen&amp;#x2019;s logical uncertainty principle, also allows the representation of retrieval situations. Hence, we believe that it could be implemented at the core of an operational information retrieval system. Two applications, one dealing with academic libraries and the other concerning audiovisual documents, are briefly presented. </content></document><document><year>2004</year><authors>A General Approach to Clustering in Large Databases with Noise</authors><title>Abstract.;;Several clustering algorithms can be applied to clustering in large multimedia databases. The effectiveness and efficiency of the existing algorithms, however, are somewhat limited, since clustering in multimedia databases requires clustering of high-dimensional feature vectors and because multimedia databases often contain large amounts of noise. In this paper, we therefore introduce a new Kernel Density Estimation-based algorithm for clustering in large multimedia databases called DENCLUE (DENsity-based CLUstEring). Kernel Density Estimation (KDE) models the overall point density analytically as the sum of kernel (or influence) functions of the data points. Clusters can then be identified by determining density attractors and clusters of arbitrary shape can be easily described by a simple equation of the overall density function. The advantages of our KDE-based DENCLUE approach are: (1) it has a firm mathematical basis; (2) it has good clustering properties in data sets with large amounts of noise; (3) it allows a compact mathematical description of arbitrarily shaped clusters in high-dimensional data sets; and (4) it is significantly faster than existing algorithms. To demonstrate the effectiveness and efficiency of DENCLUE, we perform a series of experiments on a number of different data sets from CAD and molecular biology. A comparison with k-Means, DBSCAN, and BIRCH shows the superiority of our new algorithm.</title><content/></document><document><year>2009</year><authors>Vo Thi Ngoc Chau1  | Suphamit Chittayasothorn2 </authors><title>A conceptual schema-based temporal meta database schemas generation technique for 3D objects      </title><content>In this paper, a temporal meta database for three-dimensional (3D) objects whose properties and relationships are supported         by valid time is introduced. Based on our proposed temporal object-oriented conceptual schema model, a conceptual schema of         the temporal meta database can be generated from a 3D graphical data source and other particular application requirements.         Based on our proposed temporal object relational data model with attribute timestamping, logical schemas of the temporal meta         database can be systematically and automatically generated from the conceptual schema. From the temporal meta database, non-temporal/temporal         metadata about temporal 3D objects are available for temporal information system users. Convenient access using database languages         such as SQL can be performed. Queries over 3D objects using a temporal object relational SQL are demonstrated.      </content></document><document><year>2009</year><authors>Behrooz Safarinejadian1 | Mohammad B. Menhaj1  | Mehdi Karrari1 </authors><title>A distributed EM algorithm to estimate the parameters of a finite mixture of components      </title><content>In this paper, a distributed expectation maximization (DEM) algorithm is first introduced in a general form for estimating         the parameters of a finite mixture of components. This algorithm is used for density estimation and clustering of data distributed         over nodes of a network. Then, a distributed incremental EM algorithm (DIEM) with a higher convergence rate is proposed. After         a full derivation of distributed EM algorithms, convergence of these algorithms is analyzed based on the negative free energy         concept used in statistical physics. An analytical approach is also developed for evaluating the convergence rate of both         incremental and distributed incremental EM algorithms. It is analytically shown that the convergence rate of DIEM is much         faster than that of the DEM algorithm. Finally, simulation results approve that DIEM remarkably outperforms DEM for both synthetic         and real data sets.      </content></document><document><year>2009</year><authors>Furu Wei1| 2 | Wenjie Li1 | Qin Lu1  | Yanxiang He2 </authors><title>A document-sensitive graph model for multi-document summarization      </title><content>In recent years, graph-based models and ranking algorithms have drawn considerable attention from the extractive document         summarization community. Most existing approaches take into account sentence-level relations (e.g. sentence similarity) but         neglect the difference among documents and the influence of documents on sentences. In this paper, we present a novel document-sensitive         graph model that emphasizes the influence of global document set information on local sentence evaluation. By exploiting document&amp;#8211;document         and document&amp;#8211;sentence relations, we distinguish intra-document sentence relations from inter-document sentence relations.         In such a way, we move towards the goal of truly summarizing multiple documents rather than a single combined document. Based         on this model, we develop an iterative sentence ranking algorithm, namely DsR (Document-Sensitive Ranking). Automatic ROUGE         evaluations on the DUC data sets show that DsR outperforms previous graph-based models in both generic and query-oriented         summarization tasks.      </content></document><document><year>2009</year><authors>Yang Yu1 | Zhi-Hua Zhou1 </authors><title>A framework for modeling positive class expansion with single snapshot      </title><content>In many real-world data mining tasks, the connotation of the target concept may change as time goes by. For example, the connotation         of &amp;#8220;learned knowledge&amp;#8221; of a student today may be different from his/her &amp;#8220;learned knowledge&amp;#8221; tomorrow, since the &amp;#8220;learned knowledge&amp;#8221;         of the student is expanding everyday. In order to learn a model capable of making accurate predictions, the evolution of the         concept must be considered, and thus, a series of data sets collected at different time is needed. In many tasks, however,         there is only a single data set instead of a series of data sets. In other words, only a single snapshot of the data along the time axis is available. In this paper, we formulate the Positive Class Expansion with single Snapshot         (PCES) problem and discuss its difference with existing problem settings. To show that this new problem is addressable, we         propose a framework which involves the incorporation of desirable biases based on user preferences. The resulting optimization         problem is solved by the Stochastic Gradient Boosting with Double Target approach, which achieves encouraging performance         on PCES problems in experiments.      </content></document><document><year>2009</year><authors>Javier Andrade1 | Juan Ares1 | MarГ­a A. MartГ­nez2 | Juan Pazos3 | Santiago RodrГ­guez1  | Sonia M. SuГЎrez1 </authors><title>A fuzzy approach for solving a critical benchmarking problem      </title><content>Nowadays, benchmarking is a widespread technique for evaluating an aspect&amp;#8212;process, product, service, etc.&amp;#8212;by comparing it         against the best in class with the aim of improving this aspect or identifying the best alternative. There have been numerous         attempts at defining a rigorous benchmarking process by specifying steps that should be taken to put benchmarking into practice.         All these proposals use a method of calculation that treats the weights and ratings of each criterion as numerical variables,         even if they are not. This means that the binary and linguistic variables have to be artificially translated to numerical         variables, misleading us into thinking that the concepts we are dealing with are quantitative when they really are not. In         this paper, we propose a new method of calculation based on fuzzy logic to rectify this key methodological error. Its definition         is based on: (i) a new division operator for fuzzy numbers representing conjugated variables, as in the case outlined here;         (ii) a new aggregation operator that can integrate binary, numerical and/or linguistic variables; and, finally, (iii) an operator         that can translate the final fuzzy rating into the linguistic variable that best represents it. Therefore, the resulting method         is: (i) closer to the user since it manages more human-understandable values and (ii) not dependent on the above artificial         translation process, which could lead to sizeable variations in the benchmarking result.      </content></document><document><year>2009</year><authors>Abdellali Kelil1| 4 | Shengrui Wang1| 4 | Qingshan Jiang2  | Ryszard Brzezinski3| 4 </authors><title>A general measure of similarity for categorical sequences      </title><content>Measuring the similarity between categorical sequences is a fundamental process in many data mining applications. A key issue         is extracting and making use of significant features hidden behind the chronological and structural dependencies found in         these sequences. Almost all existing algorithms designed to perform this task are based on the matching of patterns in chronological         order, but such sequences often have similar structural features in chronologically different order. In this paper we propose         SCS, a novel, effective and domain-independent method for measuring the similarity between categorical sequences, based on         an original pattern matching scheme that makes it possible to capture chronological and non-chronological dependencies. SCS         captures significant patterns that represent the natural structure of sequences, and reduces the influence of those which         are merely noise. It constitutes an effective approach to measuring the similarity between data in the form of categorical         sequences, such as biological sequences, natural language texts, speech recognition data, certain types of network transactions,         and retail transactions. To show its effectiveness, we have tested SCS extensively on a range of data sets from different         application fields, and compared the results with those obtained by various mainstream algorithms. The results obtained show         that SCS produces results that are often competitive with domain-specific similarity approaches.      </content></document><document><year>2009</year><authors>Ayyaz Hussain1 | M. Arfan Jaffar1  | Anwar M. Mirza1 </authors><title>A hybrid image restoration approach: fuzzy logic and directional weighted median based uniform impulse noise removal      </title><content>In this paper, a hybrid image restoration technique based on fuzzy logic and directional weighted median is presented. The         proposed technique consists of noise detection and fuzzy filtering processes to detect and remove uniform (random-valued)         impulse noise while preserving the image details efficiently. In order to preserve image details such as edges and texture         information, a two-stage robust noise detection is presented in this paper. Pixels detected as noisy by both the noise detection         stages are considered for noise removal by the fuzzy filtering process, which utilizes the direction based weighted median         to construct fuzzy membership function, which is the main contributing factor in noise removal and detail preservation. Extensive         experimentation shows that the proposed technique performs significantly better than state-of-the-art filters based on peak         signal-to-noise ratio, structural similarity index measure and subjective evaluation criteria.      </content></document><document><year>2009</year><authors>Rosa Karimi Adl1  | Seyed Mohammad Taghi Rouhani Rankoohi1 </authors><title>A new ant colony optimization based algorithm for data allocation problem in distributed databases      </title><content>The Performance and the efficiency of a distributed database system depend highly on the way data are allocated to the sites.         The NP-completeness of the data allocation problem and the large size of its real occurrence, call for employing a fast and         scalable heuristic algorithm. In this paper, we address the data allocation problem in terms of minimizing two different types         of data transmission across the network, i.e., data transmissions due to site-fragment dependencies and those caused by inter-fragment         dependencies. We propose a new heuristic algorithm which is based on the ant colony optimization meta-heuristic, with regards         to the applied strategies for query optimization and integrity enforcement. The goal is to design an efficient data allocation         scheme to minimize the total transaction response time under memory capacity constraints of the sites. Experimental tests         indicate that our algorithm is capable of producing near- optimal solutions within a reasonable time. The results also reveal         the flexibility and scalability of the proposed algorithm.      </content></document><document><year>2009</year><authors>Sriparna Saha1  | Sanghamitra B|yopadhyay1 </authors><title>A new multiobjective clustering technique based on the concepts of stability and symmetry      </title><content>Most clustering algorithms operate by optimizing (either implicitly or explicitly) a single measure of cluster solution quality.         Such methods may perform well on some data sets but lack robustness with respect to variations in cluster shape, proximity,         evenness and so forth. In this paper, we have proposed a multiobjective clustering technique which optimizes simultaneously         two objectives, one reflecting the total cluster symmetry and the other reflecting the stability of the obtained partitions         over different bootstrap samples of the data set. The proposed algorithm uses a recently developed simulated annealing-based         multiobjective optimization technique, named AMOSA, as the underlying optimization strategy. Here, points are assigned to         different clusters based on a newly defined point symmetry-based distance rather than the Euclidean distance. Results on several         artificial and real-life data sets in comparison with another multiobjective clustering technique, MOCK, three single objective         genetic algorithm-based automatic clustering techniques, VGAPS clustering, GCUK clustering and HNGA clustering, and several         hybrid methods of determining the appropriate number of clusters from data sets show that the proposed technique is well suited         to detect automatically the appropriate number of clusters as well as the appropriate partitioning from data sets having point         symmetric clusters. The performance of AMOSA as the underlying optimization technique in the proposed clustering algorithm         is also compared with PESA-II, another evolutionary multiobjective optimization technique.      </content></document><document><year>2009</year><authors>Alexis Bondu1 | Marc BoullГ©2 | Vincent Lemaire2 </authors><title>A non-parametric semi-supervised discretization method      </title><content>Semi-supervised classification methods aim to exploit labeled and unlabeled examples to train a predictive model. Most of         these approaches make assumptions on the distribution of classes. This article first proposes a new semi-supervised discretization         method, which adopts very low informative prior on data. This method discretizes the numerical domain of a continuous input         variable, while keeping the information relative to the prediction of classes. Then, an in-depth comparison of this semi-supervised         method with the original supervised MODL approach is presented. We demonstrate that the semi-supervised approach is asymptotically         equivalent to the supervised approach, improved with a post-optimization of the intervals bounds location.      </content></document><document><year>2009</year><authors>Seunghyun Im1 | Zbigniew Ra&amp;#347 2| 3 | Hanna Wasyluk4</authors><title>Action rule discovery from incomplete data      </title><content>Action rule is an implication rule that shows the expected change in a decision value of an object as a result of changes         made to some of its conditional values. An example of an action rule is &amp;#8216;credit card holders of young age are expected to         keep their cards for an extended period of time if they receive a movie ticket once a year&amp;#8217;. In this case, the decision value         is the account status, and the condition value is whether the movie ticket is sent to the customer. The type of action that         can be taken by the company is to send out movie tickets to young customers. The conventional action rule discovery algorithms         build action rules from existing classification rules. This paper discusses an agglomerative strategy that generates the shortest         action rules directly from a decision system. In particular, the algorithm can be used to discover rules from an incomplete         decision system where attribute values are partially incomplete. As one of the testing domains for our research we take HEPAR         system that was built through a collaboration between the Institute of Biocybernetics and Biomedical Engineering of the Polish         Academy of Sciences and physicians at the Medical Center of Postgraduate Education in Warsaw, Poland. HEPAR was designed for         gathering and processing clinical data on patients with liver disorders. Action rules will be used to construct the decision-support         module for HEPAR.      </content></document><document><year>2009</year><authors>Mario Boley1  | Henrik Grosskreutz1 </authors><title>Approximating the number of frequent sets in dense data      </title><content>We investigate the problem of counting the number of frequent (item)sets&amp;#8212;a problem known to be intractable in terms of an         exact polynomial time computation. In this paper, we show that it is in general also hard to approximate. Subsequently, a         randomized counting algorithm is developed using the Markov chain Monte Carlo method. While for general inputs an exponential         running time is needed in order to guarantee a certain approximation bound, we show that the algorithm still has the desired         accuracy on several real-world datasets when its running time is capped polynomially.      </content></document><document><year>2009</year><authors>Wei Lee Woon1  | Stuart Madnick2</authors><title>Asymmetric information distances for automated taxonomy construction      </title><content>A novel method for automatically constructing taxonomies for specific research domains is presented. The proposed methodology         uses term co-occurrence frequencies as an indicator of the semantic closeness between terms. To support the automated creation         of taxonomies or subject classifications we present a simple modification to the basic distance measure, and describe a set         of procedures by which these measures may be converted into estimates of the desired taxonomy. To demonstrate the viability         of this approach, a pilot study on renewable energy technologies is conducted, where the proposed method is used to construct         a hierarchy of terms related to alternative energy. These techniques have many potential applications, but one activity in         which we are particularly interested is the mapping and subsequent prediction of future developments in the technology and         research.      </content></document><document><year>2009</year><authors>Benjamin X. Wang1 | Nathalie Japkowicz2 </authors><title>Boosting support vector machines for imbalanced data sets      </title><content>Real world data mining applications must address the issue of learning from imbalanced data sets. The problem occurs when         the number of instances in one class greatly outnumbers the number of instances in the other class. Such data sets often cause         a default classifier to be built due to skewed vector spaces or lack of information. Common approaches for dealing with the         class imbalance problem involve modifying the data distribution or modifying the classifier. In this work, we choose to use         a combination of both approaches. We use support vector machines with soft margins as the base classifier to solve the skewed         vector spaces problem. We then counter the excessive bias introduced by this approach with a boosting algorithm. We found         that this ensemble of SVMs makes an impressive improvement in prediction performance, not only for the majority class, but         also for the minority class.      </content></document><document><year>2009</year><authors>Lior Aronovich1  | Israel Spiegler1 </authors><title>Bulk construction of dynamic clustered metric trees      </title><content>Repositories of complex data types, such as images, audio, video and free text, are becoming increasingly frequent in various         fields. A general searching approach for such data types is that of similarity search, where the search is for similar objects         and similarity is modeled by a metric distance function. An important class of access methods for similarity search in metric         data is that of dynamic clustered metric trees, where the index is structured as a paged and balanced tree and the space is         partitioned hierarchically into compact regions. While access methods of this class allow dynamic insertions typically of         single objects, the problem of efficiently inserting a given data set into the index in bulk is largely open. In this article         we address this problem and propose novel algorithms corresponding to its two cases, where the index is initially empty (i.e.         bulk loading), and where the index is initially non empty (i.e. bulk insertion). The proposed bulk loading algorithm builds         the index bottom-up layer by layer, using a new sampling based clustering method, which improves clustering results by improving         the quality of the selected sample sets. The proposed bulk insertion algorithm employs the bulk loading algorithm to load         the given data into a new index structure, and then merges the new and the existing structures into a unified high quality         index, using a novel decomposition method to reduce overlaps between the structures. Both algorithms yield significantly improved         construction and search performance, and are applicable to all dynamic clustered metric trees. Results from an extensive experimental         study show that the proposed algorithms outperform alternative methods, reducing construction costs by up to 47% for CPU costs         and 99% for I/O costs, and search costs by up to 48% for CPU costs and 30% for I/O costs.      </content></document><document><year>2009</year><authors>Kamal Taha1  | Ramez Elmasri1 </authors><title>BusSEngine: a business search engine      </title><content>With the emergence of World Wide Web, business&amp;#8217; databases are increasingly being queried directly by customers. The customers         may not be aware of the underlying data and its structure, and might have never learned a query language that enables them         to issue structured queries. Some of the business&amp;#8217; employees who query the databases may also not be aware of the structure         of the data, but they are likely to be aware of some labels of elements containing data. We propose in this article: (1) an         XML Keyword-Based search engine for answering business&amp;#8217; customers called BusSEngine-K, and (2) an XML loosely Structured-Based         search engine for answering business&amp;#8217; employees called BusSEngine-L. The two engines employ novel context-driven search techniques         and are built on top of XQuery search engine. The two engines were evaluated experimentally and compared with three recently         proposed XML search engines. The results showed marked improvement.      </content></document><document><year>2009</year><authors>Hyun-Ho Lee1  | Won-Suk Lee2 </authors><title>Consistent collective evaluation of multiple continuous queries for filtering heterogeneous data streams      </title><content>Query processing for a data stream should also be continuous and rapid. This article proposes a novel approach for consistent         collective evaluation of multiple continuous queries for filtering two different types of data streams: a relational stream         and an XML stream. The proposed approach commonly provides region-based selection constructs: an attribute selection construct for relational queries and a path selection construct for XPath queries. Both collectively evaluate the selection predicates of the same attribute (path), based on the precomputed         matching results of the queries in each of the disjoint regions divided by the selection predicates. The performance experiments         show that the proposed approach is practically more efficient and stable than other approaches at run-time.      </content></document><document><year>2009</year><authors>Vijay G|hi1 | James M. Kang1 | Shashi Shekhar1 | Junchang Ju2 | Eric D. Kolaczyk2  | Sucharita Gopal2 </authors><title>Context inclusive function evaluation: a case study with EM-based multi-scale multi-granular image classification      </title><content>Many statistical queries such as maximum likelihood estimation involve finding the best candidate model given a set of candidate         models and a quality estimation function. This problem is common in important applications like land-use classification at         multiple spatial resolutions from remote sensing raster data. Such a problem is computationally challenging due to the significant         computation cost to evaluate the quality estimation function for each candidate model. For example, a recently proposed method         of multi-scale, multi-granular classification has high computational overhead of function evaluation for various candidate         models independently before comparison. In contrast, we propose an upper bound based context-inclusive approach that reduces         computational overhead based on the context, i.e. the value of the quality estimation function for the best candidate model         so far. We also prove that an upper bound exists for each candidate model and the proposed algorithm is correct. Experimental         results using land-use classification at multiple spatial resolutions from satellite imagery show that the proposed approach         reduces the computational cost significantly.      </content></document><document><year>2009</year><authors>Anne M. Denton1  | Jianfei Wu1 </authors><title>Data mining of vector&amp;#8211;item patterns using neighborhood histograms      </title><content>The representation of multiple continuous attributes as dimensions in a vector space has been among the most influential concepts         in machine learning and data mining. We consider sets of related continuous attributes as vector data and search for patterns         that relate a vector attribute to one or more items. The presence of an item set defines a subset of vectors that may or may         not show unexpected density fluctuations. We test for fluctuations by studying density histograms. A vector&amp;#8211;item pattern is         considered significant if its density histogram significantly differs from what is expected for a random subset of transactions.         Using two different density measures, we evaluate the algorithm on two real data sets and one that was artificially constructed         from time series data.      </content></document><document><year>2009</year><authors>Ilija Suba&amp;#353 i&amp;#263 1  | Bettina Berendt1 </authors><title>Discovery of interactive graphs for understanding and searching time-indexed corpora      </title><content>Rich information spaces (like the Web or scientific publications) are full of &amp;#8220;stories&amp;#8221;: sets of statements that evolve over         time, manifested as, for example, collections of news articles reporting events that relate to an evolving crime investigation,         sets of news articles and blog posts accompanying the development of a political election campaign, or sequences of scientific         papers on a topic. In this paper, we formulate the problem of discovering such stories as Evolutionary Theme Pattern Discovery,         Summary and Exploration (ETP3). We propose a method and a visualisation tool for solving ETP3 by understanding, searching         and interacting with such stories and their underlying documents. In contrast to existing approaches, our method concentrates         on relational information and on local patterns rather than on the occurrence of individual concepts and global models. In addition, it relies on interactive graphs         rather than natural language as the abstracted story representations. Furthermore, we present an evaluation framework. Two         real-life case studies are used to illustrate and evaluate the method and tool.      </content></document><document><year>2009</year><authors>Zhi-Hua Zhou1 | Hang Li2 | Qiang Yang3</authors><title>Editorial: special issue on selected papers of PAKDD 2007      </title><content>Without Abstract</content></document><document><year>2009</year><authors>Ming Zhang1 | Reda Alhajj1| 2 </authors><title>Effectiveness of NAQ-tree as index structure for similarity search in high-dimensional metric space      </title><content>Similarity search (e.g., k-nearest neighbor search) in high-dimensional metric space is the key operation in many applications,         such as multimedia databases, image retrieval and object recognition, among others. The high dimensionality and the huge size         of the data set require an index structure to facilitate the search. State-of-the-art index structures are built by partitioning         the data set based on distances to certain reference point(s). Using the index, search is confined to a small number of partitions.         However, these methods either ignore the property of the data distribution (e.g., VP-tree and its variants) or produce non-disjoint         partitions (e.g., M-tree and its variants, DBM-tree); these greatly affect the search efficiency. In this paper, we study         the effectiveness of a new index structure, called Nested-Approximate-eQuivalence-class tree (NAQ-tree), which overcomes the         above disadvantages. NAQ-tree is constructed by recursively dividing the data set into nested approximate equivalence classes.         The conducted analysis and the reported comparative test results demonstrate the effectiveness of NAQ-tree in significantly         improving the search efficiency.      </content></document><document><year>2009</year><authors>Zhenhua Huang1 | Shengli Sun2  | Wei Wang3 </authors><title>Efficient mining of skyline objects in subspaces over data streams      </title><content>Given a set of k-dimensional objects, the skyline query finds the objects that are not dominated by others. In practice, different users may         be interested in different dimensions of the data, and issue queries on any subset of k dimensions in stream environments.         This paper focuses on supporting concurrent and unpredictable subspace skyline queries over data streams. Simply to compute         and store the skyline objects of every subspace in stream environments will incur expensive update cost. To balance the query         cost and update cost, we only maintain the full space skyline in this paper. We first propose an efficient maintenance algorithm         and several novel pruning techniques. Then, an efficient and scalable two-phase algorithm is proposed to process the skyline         queries in different subspaces based on the full space skyline. Furthermore, we present the theoretical analyses and extensive         experiments that demonstrate our method is both efficient and effective.      </content></document><document><year>2009</year><authors>Dimitrios Mavroeidis1  | Ella Bingham2 </authors><title>Enhancing the stability and efficiency of spectral ordering with partial supervision and feature selection      </title><content>Several studies have demonstrated the prospects of spectral ordering for data mining. One successful application is seriation         of paleontological findings, i.e. ordering the sites of excavation, using data on mammal co-occurrences only. However, spectral         ordering ignores the background knowledge that is naturally present in the domain: paleontologists can derive the ages of         the sites within some accuracy. On the other hand, the age information is uncertain, so the best approach would be to combine         the background knowledge with the information on mammal co-occurrences. Motivated by this kind of partial supervision we propose         a novel semi-supervised spectral ordering algorithm that modifies the Laplacian matrix such that domain knowledge is taken         into account. Also, it performs feature selection by discarding features that contribute most to the unwanted variability         of the data in bootstrap sampling. Moreover, we demonstrate the effectiveness of the proposed framework on the seriation of         Usenet newsgroup messages, where the task is to find out the underlying flow of discussion. The theoretical properties of         our algorithm are thoroughly analyzed and it is demonstrated that the proposed framework enhances the stability of the spectral         ordering output and induces computational gains.      </content></document><document><year>2009</year><authors>Lynda Tamine-Lechani1 | Moh| Boughanem1  | Mariam Daoud1 </authors><title>Evaluation of contextual information retrieval effectiveness: overview of issues and research      </title><content>The increasing prominence of information arising from a wide range of sources delivered over electronic media has made traditional         information retrieval systems less effective. Indeed, users are overwhelmed by the information delivered by such systems in         response to their queries, particularly when the latter are ambiguous. In order to tackle this problem, the state-of-the-art         reveals that there is a growing interest towards contextual information retrieval which relies on various sources of evidence         issued from the user&amp;#8217;s search background and environment like interests, preferences, time and location, in order to improve         the retrieval accuracy. Contextual information retrieval systems are based on different definitions of the core concept of         user&amp;#8217;s context, various user&amp;#8217;s context modeling approaches and several techniques of document relevance measurement, but all         share the goal of providing the most useful information to the users in accordance with their context. However, the evaluation         methodologies conceived in the past several years for traditional information retrieval and widely used in the evaluation         campaigns have been challenged by the consideration of user&amp;#8217;s context in the information retrieval process. Thus, we recognize         that a critical review of existing evaluation methodologies in contextual information retrieval area is needed in order to         design and develop standard evaluation frameworks. We present in this paper a comprehensive survey of contextual information         retrieval evaluation methodologies and provide insights into how and why they are appropriate to measure the retrieval effectiveness.         We also highlight some of the research challenges ahead that would constitute substantive research area for future research.      </content></document><document><year>2009</year><authors>Keivan Kianmehr1| Mohammed Alshalalfa1 | Reda Alhajj1| 2 </authors><title>Fuzzy clustering-based discretization for gene expression classification      </title><content>This paper presents a novel classification approach that integrates fuzzy class association rules and support vector machines.         A fuzzy discretization technique based on fuzzy c-means clustering algorithm is employed to transform the training set, particularly         quantitative attributes, to a format appropriate for association rule mining. A hill-climbing procedure is adapted for automatic         thresholds adjustment and fuzzy class association rules are mined accordingly. The compatibility between the generated rules         and fuzzy patterns is considered to construct a set of feature vectors, which are used to generate a classifier. The reported         test results show that compatibility rule-based feature vectors present a highly- qualified source of discrimination knowledge         that can substantially impact the prediction power of the final classifier. In order to evaluate the applicability of the         proposed method to a variety of domains, it is also utilized for the popular task of gene expression classification. Further,         we show how this method provide biologists with an accurate and more understandable classifier model compared to other machine         learning techniques.      </content></document><document><year>2009</year><authors>M. Arfan Jaffar1 | Ayyaz Hussain1  | Anwar Majid Mirza1 </authors><title>Fuzzy entropy based optimization of clusters for the segmentation of lungs in CT scanned images      </title><content>In this paper, we have proposed a method for segmentation of lungs from Computed Tomography (CT)-scanned images using spatial         Fuzzy C-Mean and morphological techniques known as Fuzzy Entropy and Morphology based Segmentation. To determine dynamic and         adaptive optimal threshold, we have incorporated Fuzzy Entropy. We have proposed a novel histogram-based background removal         operator. The proposed system is capable to perform fully automatic segmentation of CT Scan Lung images, based solely on information         contained by the image itself. We have used different cluster validity functions to find out optimal number of clusters. The         proposed system can be used as a basic building block for Computer-Aided Diagnosis. The technique was tested against the 25         datasets of different patients received from Aga Khan Medical University, Pakistan. The results confirm the validity of technique         as well as enhanced performance.      </content></document><document><year>2009</year><authors>Chen Chen1 | Xifeng Yan2| Feida Zhu1| Jiawei Han1 | Philip S. Yu3</authors><title>Graph OLAP: a multi-dimensional framework for graph data analysis      </title><content>Databases and data warehouse systems have been evolving from handling normalized spreadsheets stored in relational databases,         to managing and analyzing diverse application-oriented data with complex interconnecting structures. Responding to this emerging         trend, graphs have been growing rapidly and showing their critical importance in many applications, such as the analysis of         XML, social networks, Web, biological data, multimedia data and spatiotemporal data. Can we extend useful functions of databases         and data warehouse systems to handle graph structured data? In particular, OLAP (On-Line Analytical Processing) has been a         popular tool for fast and user-friendly multi-dimensional analysis of data warehouses. Can we OLAP graphs? Unfortunately, to our best knowledge, there are no OLAP tools available that can interactively view and analyze graph data         from different perspectives and with multiple granularities. In this paper, we argue that it is critically important to OLAP         graph structured data and propose a novel Graph OLAP framework. According to this framework, given a graph dataset with its nodes and edges associated with respective attributes,         a multi-dimensional model can be built to enable efficient on-line analytical processing so that any portions of the graphs can be generalized/specialized dynamically, offering multiple, versatile views of the data. The contributions of this work are three-fold. First, starting         from basic definitions, i.e., what are dimensions and measures in the Graph OLAP scenario, we develop a conceptual framework for data cubes on graphs. We also look into different semantics         of OLAP operations, and classify the framework into two major subcases: informational OLAP and topological OLAP. Second, we show how a graph cube can be materialized by calculating a special kind of measure called aggregated graph and how to implement it efficiently. This includes both full materialization and partial materialization where constraints         are enforced to obtain an iceberg cube. As we can see, due to the increased structural complexity of data, aggregated graphs that depend on the underlying &amp;#8220;network&amp;#8221;         properties of the graph dataset are much harder to compute than their traditional OLAP counterparts. Third, to provide more         flexible, interesting and informative OLAP of graphs, we further propose a discovery-driven multi-dimensional analysis model to ensure that OLAP is performed in an intelligent manner, guided by expert rules and knowledge         discovery processes. We outline such a framework and discuss some challenging research issues for discovery-driven Graph OLAP.      </content></document><document><year>2009</year><authors>Jianjiang Lu1 | Ran Li1| Yafei Zhang1| Tianzhong Zhao1 | Zining Lu1</authors><title>Image annotation techniques based on feature selection for class-pairs      </title><content>Image annotation technique can be formulated as a multi-class classification problem, which can be solved by the ensemble         of multiple class-pair classifiers. Support vector machine (SVM) classifiers based on optimal class-pair feature subsets from         the multimedia content description interface (MPEG-7) standard are used as the class-pair classifiers. We use a binary-coded         chromosome genetic algorithm (GA) to select optimal class-pair feature subsets, and a bi-coded chromosome GA to simultaneously         select optimal class-pair feature subsets and corresponding optimal weight subsets, i.e. optimal class-pair weighted feature         subsets. We consider two kinds of methods for class-pair feature selection: a common optimal (or weighted) feature subset         is selected for all the class-pairs, and an individual optimal (or weighted) feature subset is selected for each class-pair         respectively. Majority voting scheme is used to combine the class-pair SVM classifiers. The experiments are performed on two         different image sets to validate the performance of our image annotation techniques.      </content></document><document><year>2009</year><authors>Jianhan Zhu1 | Xiangji Huang2 | Dawei Song3  | Stefan RГјger4 </authors><title>Integrating multiple document features in language models for expert finding      </title><content>We argue that expert finding is sensitive to multiple document features in an organizational intranet. These document features         include multiple levels of associations between experts and a query topic from sentence, paragraph, up to document levels,         document authority information such as the PageRank, indegree, and URL length of documents, and internal document structures         that indicate the experts&amp;#8217; relationship with the content of documents. Our assumption is that expert finding can largely benefit         from the incorporation of these document features. However, existing language modeling approaches for expert finding have         not sufficiently taken into account these document features. We propose a novel language modeling approach, which integrates         multiple document features, for expert finding. Our experiments on two large scale TREC Enterprise Track datasets, i.e., the         W3C and CSIRO datasets, demonstrate that the natures of the two organizational intranets and two types of expert finding tasks,         i.e., key contact finding for CSIRO and knowledgeable person finding for W3C, influence the effectiveness of different document         features. Our work provides insights into which document features work for certain types of expert finding tasks, and helps         design expert finding strategies that are effective for different scenarios. Our main contribution is to develop an effective         formal method for modeling multiple document features in expert finding, and conduct a systematic investigation of their effects.         It is worth noting that our novel approach achieves better results in terms of MAP than previous language model based approaches         and the best automatic runs in both the TREC2006 and TREC2007 expert search tasks, respectively.      </content></document><document><year>2009</year><authors>Shaofeng Liu1 | Alex H. B. Duffy1 | Robert Ian Whitfield1  | Iain M. Boyle1 </authors><title>Integration of decision support systems to improve decision support performance      </title><content>Decision support system (DSS) is a well-established research and development area. Traditional isolated, stand-alone DSS has         been recently facing new challenges. In order to improve the performance of DSS to meet the challenges, research has been         actively carried out to develop integrated decision support systems (IDSS). This paper reviews the current research efforts         with regard to the development of IDSS. The focus of the paper is on the integration aspect for IDSS through multiple perspectives,         and the technologies that support this integration. More than 100 papers and software systems are discussed. Current research         efforts and the development status of IDSS are explained, compared and classified. In addition, future trends and challenges         in integration are outlined. The paper concludes that by addressing integration, better support will be provided to decision         makers, with the expectation of both better decisions and improved decision making processes.      </content></document><document><year>2009</year><authors>Jana Schmidt1 | Andreas Hapfelmeier1| Marianne Mueller1 | Robert Perneczky2| Alex|er Kurz2| Alex|er Drzezga3 | Stefan Kramer1 </authors><title>Interpreting PET scans by structured patient data: a data mining case study in dementia research      </title><content>One of the goals of medical research in the area of dementia is to correlate images of the brain with clinical tests. Our         approach is to start with the images and explain the differences and commonalities in terms of the other variables. First,         we cluster Positron emission tomography (PET) scans of patients to form groups sharing similar features in brain metabolism.         To the best of our knowledge, it is the first time ever that clustering is applied to whole PET scans. Second, we explain         the clusters by relating them to non-image variables. To do so, we employ RSD, an algorithm for relational subgroup discovery,         with the cluster membership of patients as target variable. Our results enable interesting interpretations of differences         in brain metabolism in terms of demographic and clinical variables. The approach was implemented and tested on an exceptionally         large data collection of patients with different types of dementia. It comprises 10;GB of image data from 454 PET scans, and         42 variables from psychological and demographical data organized in 11 relations of a relational database. We believe that         explaining medical images in terms of other variables (patient records, demographic information, etc.) is a challenging new         and rewarding area for data mining research.      </content></document><document><year>2009</year><authors>Wei Song1  | Soon Cheol Park1 </authors><title>Latent semantic analysis for vector space expansion and fuzzy logic-based genetic clustering      </title><content>This paper proposes an improved latent semantic analysis (LSA) model to represent textual document and takes advantage of         a fuzzy logic based genetic algorithm (FLGA) for clustering. The standard genetic algorithm (GA) in conventional vector space         model is rather difficult to deal with because the high dimensional encoding of GA makes it explore the optimal solution in         a complicated space which is prone to cause an overflow problem. The LSA-based corpus model not only reduces the dimensions         drastically, but also creates an underlying semantic structure which enhances its ability of distinguishing documents in terms         of concepts and indirectly improves the ability of GA for clustering (genetic clustering). A novel FLGA is proposed in conjunction         with this semantic model in this study. According to the nature of biological evolution, several fuzzy controllers are given         to adaptively adjust and optimize the behaviors of the GA which can effectively prevent the premature convergence to a suboptimum         solution. The experiment results show that the fuzzy logic controllers enhance the ability of the GA to explore the global         optimum solution, and the utilization of the LSA-based text representation method to FLGA further improves its clustering         performance.      </content></document><document><year>2009</year><authors>Neil V. Murray1  | Erik Rosenthal2 </authors><title>Linear response time for implicate and implicant queries      </title><content>Knowledge bases can be represented as propositional Formulas. A query of such a theory typically has the form, Is a clause an implicate of the theory? Answering such queries can require exponential time. In Kautz and Selman (Proceedings of the international workshop on processing         declarative knowledge (PDK), Kaiserslautern, Germany, 1991), knowledge compilation was proposed as a solution to this problem: Pay the exponential penalty once by compiling the knowledge base into a target language that would guarantee fast response to queries. The reduced implicate trie (ri-trie), introduced in Murray and Rosenthal (Proceedings of the international conference TABLEAUX 2005&amp;#8212;analytic tableaux and         related methods, Koblenz, Germany. Lecture notes in artificial intelligence, vol 3702. Springer, Berlin, pp 231&amp;#8211;244, 2005),         may be used as a target language for knowledge compilation. It has the property that a query is processed in time linear in the size of the query, regardless of the size of the compiled knowledge base. In this paper, structures dual to ri-tries, the reduced implicant tries are investigated, and the dual problem&amp;#8212;determining the implicants of a formula&amp;#8212;is considered. The main result is that, for         a given formula, the two structures can be merged into a single reduced implicate/implicant trie that can serve dual roles, representing both implicates and implicants. Furthermore, rii-tries can be computed directly, without separately computing and then merging the dual structures.      </content></document><document><year>2009</year><authors>Bin Shen1| 2 | Min Yao1 | Zhaohui Wu1 | Yunjun Gao1| 3</authors><title>Mining dynamic association rules with comments      </title><content>In this paper, we study a new problem of mining dynamic association rules with comments (DAR-C for short). A DAR-C contains         not only rule itself, but also its comments that specify when to apply the rule. In order to formalize this problem, we first         present the expression method of candidate effective time slots, and then propose several definitions concerning DAR-C. Subsequently,         two algorithms, namely ITS2 and EFP-Growth2, are developed for handling the problem of mining DAR-C. In particular, ITS2 is         an improved two-stage dynamic association rule mining algorithm, while EFP-Growth2 is based on the EFP-tree structure and         is suitable for mining high-density mass data. Extensive experimental results demonstrate that the efficiency and scalability         of our proposed two algorithms (i.e., ITS2 and EFP-Growth2) on DAR-C mining tasks, and their practicability on real retail         dataset.      </content></document><document><year>2009</year><authors>Panagiotis Papapetrou1 | George Kollios1| Stan Sclaroff1 | Dimitrios Gunopulos2</authors><title>Mining frequent arrangements of temporal intervals      </title><content>The problem of discovering frequent arrangements of temporal intervals is studied. It is assumed that the database consists         of sequences of events, where an event occurs during a time-interval. The goal is to mine temporal arrangements of event intervals         that appear frequently in the database. The motivation of this work is the observation that in practice most events are not         instantaneous but occur over a period of time and different events may occur concurrently. Thus, there are many practical         applications that require mining such temporal correlations between intervals including the linguistic analysis of annotated         data from American Sign Language as well as network and biological data. Three efficient methods to find frequent arrangements         of temporal intervals are described; the first two are tree-based and use breadth and depth first search to mine the set of         frequent arrangements, whereas the third one is prefix-based. The above methods apply efficient pruning techniques that include         a set of constraints that add user-controlled focus into the mining process. Moreover, based on the extracted patterns a standard         method for mining association rules is employed that applies different interestingness measures to evaluate the significance         of the discovered patterns and rules. The performance of the proposed algorithms is evaluated and compared with other approaches         on real (American Sign Language annotations and network data) and large synthetic datasets.      </content></document><document><year>2009</year><authors>Cheng-Hsiung Weng1  | Yen-Liang Chen2</authors><title>Mining fuzzy association rules from uncertain data      </title><content>Association rule mining is an important data analysis method that can discover associations within data. There are numerous         previous studies that focus on finding fuzzy association rules from precise and certain data. Unfortunately, real-world data         tends to be uncertain due to human errors, instrument errors, recording errors, and so on. Therefore, a question arising immediately         is how we can mine fuzzy association rules from uncertain data. To this end, this paper proposes a representation scheme to         represent uncertain data. This representation is based on possibility distributions because the possibility theory establishes         a close connection between the concepts of similarity and uncertainty, providing an excellent framework for handling uncertain         data. Then, we develop an algorithm to mine fuzzy association rules from uncertain data represented by possibility distributions.         Experimental results from the survey data show that the proposed approach can discover interesting and valuable patterns with         high certainty.      </content></document><document><year>2009</year><authors>MartГ­n LГіpez-Nores1 | JosГ© J. Pazos-Arias1| Jorge GarcГ­a-Duque1| Yol|a Blanco-FernГЎndez1| Manuela I. MartГ­n-Vicente1| Ana FernГЎndez-Vilas1| Manuel Ramos-Cabrer1 | Alberto Gil-Solla1</authors><title>MiSPOT: dynamic product placement for digital TV through MPEG-4 processing and semantic reasoning      </title><content>In an increasingly competitive market, stakeholders of the television industry strive to exploit all the possibilities to         get revenues from advertising, but their practices are usually at odds with the comfort of the TV viewers. This paper presents         the proof of concept of MiSPOT, a system that brings a non-invasive and fully personalized form of advertising to Interactive         Digital TV, targeting both domestic and mobile receivers. MiSPOT employs semantic reasoning techniques to select advertisements         suited to the preferences, interests and needs of each individual viewer, and then relies on multimedia composition abilities         to blend the advertising material with the TV program he/she is viewing at any time. The advertisements can be set to launch         interactive commercials, thus enabling means for the provision of t-commerce services. Evaluation experiments are described         to show the technical viability of the proposal, and also to gauge the opinions of end users. Questions about the potential         impact and exploitation of this new form of advertising are addressed too.      </content></document><document><year>2009</year><authors>Charu C. Aggarwal1  | Philip S. Yu2</authors><title>On clustering massive text and categorical data streams      </title><content>In this paper, we will study the data stream clustering problem in the context of text and categorical data domains. While         the clustering problem has been studied recently for numeric data streams, the problems of text and categorical data present different challenges because of the large and un-ordered         nature of the corresponding attributes. Therefore, we will propose algorithms for text and categorical data stream clustering.         We will propose a condensation based approach for stream clustering which summarizes the stream into a number of fine grained         cluster droplets. These summarized droplets can be used in conjunction with a variety of user queries to construct the clusters         for different input parameters. Thus, this provides an online analytical processing approach to stream clustering. We also         study the problem of detecting noisy and outlier records in real time. We will test the approach for a number of real and         synthetic data sets, and show the effectiveness of the method over the baseline OSKM algorithm for stream clustering.      </content></document><document><year>2009</year><authors>Nadim Obeid1  | Raj B. K. N. Rao2</authors><title>On integrating event definition and event detection      </title><content>We develop, in this paper, a representation of time and events that supports a range of reasoning tasks such as monitoring         and detection of event patterns which may facilitate the explanation of root cause(s) of faults. We shall compare two approaches         to event definition: the active database approach in which events are defined in terms of the conditions for their detection         at an instant, and the knowledge representation approach in which events are defined in terms of the conditions for their         occurrence over an interval. We shall show the shortcomings of the former definition and employ a three-valued temporal first         order nonmonotonic logic, extended with events, in order to integrate both definitions.      </content></document><document><year>2009</year><authors>FlГЎvia Linhalis1 | Renata Pontin de Mattos Fortes1 | Dilvan de Abreu Moreira1</authors><title>OntoMap: an ontology-based architecture to perform the semantic mapping between an interlingua and software components      </title><content>This paper is about the use of natural language to communicate with computers. Most researches that have pursued this goal         consider only requests expressed in English. A way to facilitate the use of several languages in natural language systems         is by using an interlingua. An interlingua is an intermediary representation for natural language information that can be         processed by machines. We propose to convert natural language requests into an interlingua [universal networking language         (UNL)] and to execute these requests using software components. In order to achieve this goal, we propose OntoMap, an ontology-based         architecture to perform the semantic mapping between UNL sentences and software components. OntoMap also performs component         search and retrieval based on semantic information formalized in ontologies and rules.      </content></document><document><year>2009</year><authors>Lamine M. Aouad1 | Nhien-An Le-Khac1 | Tahar M. Kechadi1</authors><title>Performance study of distributed Apriori-like frequent itemsets mining      </title><content>In this article, we focus on distributed Apriori-based frequent itemsets mining. We present a new distributed approach which         takes into account inherent characteristics of this algorithm. We study the distribution aspect of this algorithm and give         a comparison of the proposed approach with a classical Apriori-like distributed algorithm, using both analytical and experimental         studies. We find that under a wide range of conditions and datasets, the performance of a distributed Apriori-like algorithm         is not related to global strategies of pruning since the performance of the local Apriori generation is usually characterized         by relatively high success rates of candidate sets frequency at low levels which switch to very low rates at some stage, and         often drops to zero. This means that the intermediate communication steps and remote support counts computation and collection         in classical distributed schemes are computationally inefficient locally, and then constrains the global performance. Our         performance evaluation is done on a large cluster of workstations using the Condor system and its workflow manager DAGMan.         The results show that the presented approach greatly enhances the performance and achieves good scalability compared to a         typical distributed Apriori founded algorithm.      </content></document><document><year>2009</year><authors>AГ­da JimГ©nez1 | Fern|o Berzal2  | Juan-Carlos Cubero1 </authors><title>POTMiner: mining ordered, unordered, and partially-ordered trees      </title><content>Non-linear data structures are becoming more and more common in data mining problems. Trees, in particular, are amenable to         efficient mining techniques. In this paper, we introduce a scalable and parallelizable algorithm to mine partially-ordered         trees. Our algorithm, POTMiner, is able to identify both induced and embedded subtrees in such trees. As special cases, it         can also handle both completely ordered and completely unordered trees.      </content></document><document><year>2009</year><authors>Artak Amirbekyan1 | Vladimir Estivill-Castro2 </authors><title>Practical protocol for Yao&amp;#8217;s millionaires problem enables secure multi-party computation of metrics and efficient privacy-preserving         k-NN for large data sets      </title><content>Finding the nearest k objects to a query object is a fundamental operation for many data mining algorithms. With the recent interest in privacy,         it is not surprising that there is strong interest in k-NN queries to enable clustering, classification and outlier-detection tasks. However, previous approaches to privacy-preserving         k-NN have been costly and can only be realistically applied to small data sets. In this paper, we provide efficient solutions         for k-NN queries for vertically partitioned data. We provide the first solution for the L         &amp;#8734; (or Chessboard) metric as well as detailed privacy-preserving computation of all other Minkowski metrics. We enable privacy-preserving         L         &amp;#8734; by providing a practical approach to the Yao&amp;#8217;s millionaires problem with more than two parties. This is based on a pragmatic         and implementable solution to Yao&amp;#8217;s millionaires problem with shares. We also provide privacy-preserving algorithms for combinations         of local metrics into a global metric that handles the large dimensionality and diversity of attributes common in vertically         partitioned data. To manage very large data sets, we provide a privacy-preserving SASH (a very successful data structure for associative queries in high dimensions). Besides providing a theoretical analysis,         we illustrate the efficiency of our approach with an empirical evaluation.      </content></document><document><year>2009</year><authors>Thomas Tran1 </authors><title>Protecting buying agents in e-marketplaces by direct experience trust modelling      </title><content>In this paper, we describe a framework for modelling the trustworthiness of sellers in the context of an electronic marketplace         where multiple selling agents may offer the same good with different qualities and selling agents may alter the quality of         their goods. We consider that there may be dishonest sellers in the market (for example, agents who offer goods with high         quality and later offer the same goods with very low quality). In our approach, buying agents use a combination of reinforcement         learning and trust modelling to enhance their knowledge about selling agents and hence their opportunities to purchase high         value goods in the marketplace. This paper focuses on presenting the theoretical results demonstrating how the modelling of         trust can protect buying agents from dishonest selling agents. The results show that our proposed buying agents will not be         harmed infinitely by dishonest selling agents and therefore will not incur infinite loss, if they are cautious in setting         their penalty factor. We also discuss the value of our particular model for trust, in contrast with related work and conclude         with directions for future research.      </content></document><document><year>2009</year><authors>Laura M&amp;#259 ru&amp;#351 ter1  | Nick R. T. P. van Beest1 </authors><title>Redesigning business processes: a methodology based on simulation and process mining techniques      </title><content>Nowadays, organizations have to adjust their business processes along with the changing environment in order to maintain a         competitive advantage. Changing a part of the system to support the business process implies changing the entire system, which         leads to complex redesign activities. In this paper, a bottom-up process mining and simulation-based methodology is proposed to be employed in redesign activities. The methodology starts         with identifying relevant performance issues, which are used as basis for redesign. A process model is &amp;#8220;mined&amp;#8221; and simulated         as a representation of the existing situation, followed by the simulation of the redesigned process model as prediction of         the future scenario. Finally, the performance criteria of the current business process model and the redesigned business process         model are compared such that the potential performance gains of the redesign can be predicted. We illustrate the methodology         with three case studies from three different domains: gas industry, government institution and agriculture.      </content></document><document><year>2009</year><authors>Ozgul Unal1  | Hamideh Afsarmanesh1 </authors><title>Semi-automated schema integration with SASMINT      </title><content>The emergence of increasing number of collaborating organizations has made clear the need for supporting interoperability         infrastructures, enabling sharing and exchange of data among organizations. Schema matching and schema integration are the         crucial components of the interoperability infrastructures, and their semi-automation to interrelate or integrate heterogeneous         and autonomous databases in collaborative networks is desired. The Semi-Automatic Schema Matching and INTegration (SASMINT)         System introduced in this paper identifies and resolves several important syntactic, semantic, and structural conflicts among         schemas of relational databases to find their likely matches automatically. Furthermore, after getting the user validation         on the matched results, it proposes an integrated schema. SASMINT uses a combination of a variety of metrics and algorithms         from the Natural Language Processing and Graph Theory domains for its schema matching. For the schema integration, it utilizes         a number of derivation rules defined in the scope of the research work explained in this paper. Furthermore, a derivation         language called SASMINT Derivation Markup Language (SDML) is defined for capturing and formulating both the results of matching         and the integration that can be further used, for example for federated query processing from independent databases. In summary,         the paper focuses on addressing: (1) conflicts among schemas that make automatic schema matching and integration difficult,         (2) the main components of the SASMINT approach and system, (3) in-depth exploration of SDML, (4) heuristic rules designed         and implemented as part of the schema integration component of the SASMINT system, and (5) experimental evaluation of SASMINT.      </content></document><document><year>2009</year><authors>Zhi-Hua Zhou1  | Ming Li1</authors><title>Semi-supervised learning by disagreement      </title><content>In many real-world tasks, there are abundant unlabeled examples but the number of labeled training examples is limited, because         labeling the examples requires human efforts and expertise. So, semi-supervised learning which tries to exploit unlabeled         examples to improve learning performance has become a hot topic. Disagreement-based semi-supervised learning is an interesting paradigm, where multiple learners are trained for the task and the disagreements among the learners are         exploited during the semi-supervised learning process. This survey article provides an introduction to research advances in         this paradigm.      </content></document><document><year>2009</year><authors>Teng-Kai Fan1  | Chia-Hui Chang1 </authors><title>Sentiment-oriented contextual advertising      </title><content>Web advertising (Online advertising), a form of advertising that uses the World Wide Web to attract customers, has become         one of the world&amp;#8217;s most important marketing channels. This paper addresses the mechanism of Content-based advertising (Contextual advertising), which refers to the assignment of relevant ads to a generic web page, e.g., a blog post. As blogs become a platform for         expressing personal opinion, they naturally contain various kinds of expressions, including both facts and comments of both         a positive and negative nature. Besides, in line with the major tenet of Web 2.0 (i.e., user-centric), we believe that the web-site owners would be willing to be in charge of the ads which are positively related to their contents.         Hence, in this paper, we propose the utilization of sentiment detection to improve Web-based contextual advertising. The proposed         sentiment-oriented contextual advertising (SOCA) framework aims to combine contextual advertising matching with sentiment         analysis to select ads that are related to the positive (and neutral) aspects of a blog and rank them according to their relevance.         We experimentally validate our approach using a set of data that includes both real ads and actual blog pages. The results         indicate that our proposed method can effectively identify those ads that are positively correlated with the given blog pages.      </content></document><document><year>2009</year><authors>Vineet Chaoji1 | Mohammad Al Hasan1| Saeed Salem1 | Mohammed J. Zaki1</authors><title>SPARCL: an effective and efficient algorithm for mining arbitrary shape-based clusters      </title><content>Clustering is one of the fundamental data mining tasks. Many different clustering paradigms have been developed over the years,         which include partitional, hierarchical, mixture model based, density-based, spectral, subspace, and so on. The focus of this         paper is on full-dimensional, arbitrary shaped clusters. Existing methods for this problem suffer either in terms of the memory         or time complexity (quadratic or even cubic). This shortcoming has restricted these algorithms to datasets of moderate sizes.         In this paper we propose SPARCL, a simple and scalable algorithm for finding clusters with arbitrary shapes and sizes, and         it has linear space and time complexity. SPARCL consists of two stages&amp;#8212;the first stage runs a carefully initialized version         of the Kmeans algorithm to generate many small seed clusters. The second stage iteratively merges the generated clusters to         obtain the final shape-based clusters. Experiments were conducted on a variety of datasets to highlight the effectiveness,         efficiency, and scalability of our approach. On the large datasets SPARCL is an order of magnitude faster than the best existing         approaches.      </content></document><document><year>2009</year><authors>Wilhelmiina HГ¤mГ¤lГ¤inen1 </authors><title>StatApriori: an efficient algorithm for searching statistically significant association rules      </title><content>Searching statistically significant association rules is an important but neglected problem. Traditional association rules         do not capture the idea of statistical dependence and the resulting rules can be spurious, while the most significant rules         may be missing. This leads to erroneous models and predictions which often become expensive. The problem is computationally         very difficult, because the significance is not a monotonic property. However, in this paper, we prove several other properties,         which can be used for pruning the search space. The properties are implemented in the StatApriori algorithm, which searches         statistically significant, non-redundant association rules. Empirical experiments have shown that StatApriori is very efficient,         but in the same time it finds good quality rules.      </content></document><document><year>2009</year><authors>Gabriela Moise1 | Arthur Zimek2| Peer KrГ¶ger2| Hans-Peter Kriegel2 | JГ¶rg S|er1</authors><title>Subspace and projected clustering: experimental evaluation and analysis      </title><content>Subspace and projected clustering have emerged as a possible solution to the challenges associated with clustering in high-dimensional         data. Numerous subspace and projected clustering techniques have been proposed in the literature. A comprehensive evaluation         of their advantages and disadvantages is urgently needed. In this paper, we evaluate systematically state-of-the-art subspace         and projected clustering techniques under a wide range of experimental settings. We discuss the observed performance of the         compared techniques, and we make recommendations regarding what type of techniques are suitable for what kind of problems.      </content></document><document><year>2009</year><authors>Xiao-Liang Tang1 | Min Han1 </authors><title>Ternary reversible extreme learning machines: the incremental tri-training method for semi-supervised classification      </title><content>Tri-training method proposed by Zhou et;al., is an excellent method for semi-supervised classification; nevertheless, the         heavy computational burden caused by the retraining strategy prevents the further application of tri-training method. To address         this problem, this paper proposes the ternary reversible extreme learning machines (TRELM) which is an incremental tri-training         method without relying on the retraining strategy. TRELM employs three reversible extreme learning machines (RELM) as its         base learners and trains the RELM with extended (or detected) samples in each learning round. RELM is an incremental learning         method with reversible derivation capability. RELM can overcome the difficulty for most incremental learning methods in removing         the influence of previously learned mistaken samples. Experimental results indicate that TRELM significantly improves the         learning speed of tri-training method. In addition, TRELM achieves comparable (or even better) classification performance         to other effective semi-supervised learning methods. TRELM is an appropriate choice for semi-supervised classification tasks         with large amounts of data sets or with strict demands for learning speed and classification accuracy.      </content></document><document><year>2009</year><authors>Ebrahim Bagheri1  | Ali A. Ghorbani1</authors><title>The analysis and management of non-canonical requirement specifications through a belief integration game      </title><content>Non-canonical requirement specifications refer to a set of software requirements that is either inconsistent, vague or incomplete.         In this paper, we provide a correspondence between requirement specifications and annotated propositional belief bases. Through         this analogy, we are able to analyze the contents of a given set of requirement collections known as viewpoints and specify         whether they are incomplete, incoherent, or inconsistent under a closed-world reasoning assumption. Based on the requirement         collections&amp;#8217; properties introduced in this paper, we define a viewpoint integration game through which the inconsistencies         of non-canonical requirement specifications are resolved. The game consists of several rounds of negotiation and is performed         by two main functions, namely choice and enhancement functions. The outcome of this game is a set of inconsistency-free requirement         collections that can be integrated to form a unique fair representative of the given requirement collections.      </content></document><document><year>2009</year><authors>Bei Yang1| 2  | Houkuan Huang2</authors><title>TOPSIL-Miner: an efficient algorithm for mining top-K significant itemsets over data streams      </title><content>Frequent itemset mining over data streams becomes a hot topic in data mining and knowledge discovery in recent years, and         has been applied to different areas. However, the setting of a minimum support threshold needs some domain knowledge. It will         bring a lot of difficulties or much burden to users if the support threshold is not set reasonably. It is interesting for         users to find top-K frequent itemsets over data streams. In this paper, a dynamical incremental approximate algorithm TOPSIL-Miner is presented         to mine top-K significant itemsets in landmark windows. A new data structure, TOPSIL-Tree, is designed to store the potential significant         itemsets and other data structures of maximum support list, ordered item list, TOPSET and minimum support list are devised         to maintain information about mining results. Moreover, three optimal strategies are exploited to reduce time and space cost         of the algorithm: (1) pruning trivial nodes in the current data stream, (2) promoting mining support threshold during mining         process adaptively and heuristically, and (3) promoting pruning threshold dynamically. The accuracy of the algorithm is also         analyzed. Extensive experiments are performed to evaluate the good effectiveness and the high efficiency and precision of         the algorithm.      </content></document><document><year>2009</year><authors>Mariam Daoud1 | Lynda-Tamine Lechani1 | Moh| Boughanem1</authors><title>Towards a graph-based user profile modeling for a session-based personalized search      </title><content>Most Web search engines use the content of the Web documents and their link structures to assess the relevance of the document         to the user&amp;#8217;s query. With the growth of the information available on the web, it becomes difficult for such Web search engines         to satisfy the user information need expressed by few keywords. First, personalized information retrieval is a promising way         to resolve this problem by modeling the user profile by his general interests and then integrating it in a personalized document         ranking model. In this paper, we present a personalized search approach that involves a graph-based representation of the         user profile. The user profile refers to the user interest in a specific search session defined as a sequence of related queries.         It is built by means of score propagation that allows activating a set of semantically related concepts of reference ontology,         namely the ODP. The user profile is maintained across related search activities using a graph-based merging strategy. For the purpose of         detecting related search activities, we define a session boundary recognition mechanism based on the Kendall rank correlation measure that tracks changes in the dominant concepts held by the user profile relatively to a new submitted         query. Personalization is performed by re-ranking the search results of related queries using the user profile. Our experimental         evaluation is carried out using the HARD 2003 TREC collection and showed that our session boundary recognition mechanism based         on the Kendall measure provides a significant precision comparatively to other non-ranking based measures like the cosine and the WebJaccard similarity measures. Moreover, results proved that the graph-based search personalization is effective for improving the         search accuracy.      </content></document><document><year>2009</year><authors>Ioannis Katakis1 | Grigorios Tsoumakas1  | Ioannis Vlahavas1 </authors><title>Tracking recurring contexts using ensemble classifiers: an application to email filtering      </title><content>Concept drift constitutes a challenging problem for the machine learning and data mining community that frequently appears         in real world stream classification problems. It is usually defined as the unforeseeable concept change of the target variable         in a prediction task. In this paper, we focus on the problem of recurring contexts, a special sub-type of concept drift, that has not yet met the proper attention from the research community. In the case         of recurring contexts, concepts may re-appear in future and thus older classification models might be beneficial for future         classifications. We propose a general framework for classifying data streams by exploiting stream clustering in order to dynamically         build and update an ensemble of incremental classifiers. To achieve this, a transformation function that maps batches of examples         into a new conceptual representation model is proposed. The clustering algorithm is then applied in order to group batches of examples into concepts         and identify recurring contexts. The ensemble is produced by creating and maintaining an incremental classifier for every         concept discovered in the data stream. An experimental study is performed using (a) two new real-world concept drifting datasets         from the email domain, (b) an instantiation of the proposed framework and (c) five methods for dealing with drifting concepts.         Results indicate the effectiveness of the proposed representation and the suitability of the concept-specific classifiers         for problems with recurring contexts.      </content></document><document><year>2008</year><authors>Hangzai Luo1| Jianping Fan2 | Xiaodong Lin3| Aoying Zhou1 | Elisa Bertino4</authors><title>A distributed approach to enabling privacy-preserving model-based classifier training      </title><content>This paper proposes a novel approach for privacy-preserving distributed model-based classifier training. Our approach is an         important step towards supporting customizable privacy modeling and protection. It consists of three major steps. First, each         data site independently learns a weak concept model (i.e., local classifier) for a given data pattern or concept by using         its own training samples. An adaptive EM algorithm is proposed to select the model structure and estimate the model parameters simultaneously. The second step deals with combined classifier training by integrating the weak concept models that are shared from multiple data sites. To reduce the data transmission costs and         the potential privacy breaches, only the weak concept models are sent to the central site and synthetic samples are directly generated from these shared weak concept models at the central site. Both the shared weak concept models and         the synthetic samples are then incorporated to learn a reliable and complete global concept model. A computational approach is developed to automatically achieve a good trade off between the privacy disclosure risk, the         sharing benefit and the data utility. The third step deals with validating the combined classifier by distributing the global         concept model to all these data sites in the collaboration network while at the same time limiting the potential privacy breaches.         Our approach has been validated through extensive experiments carried out on four UCI machine learning data sets and two image         data sets.      </content></document><document><year>2008</year><authors>Amit Ahuja1  | Yiu-Kai Ng1 </authors><title>A dynamic attribute-based data filtering and recovery scheme for web information processing      </title><content>Web data being transmitted over a network channel on the Internet with excessive amount of data causes data processing problems,         which include selectively choosing useful information to be retained for various data applications. In this paper, we present         an approach for filtering less-informative attribute data from a source Website. A scheme for filtering attributes, instead         of tuples (records), from a Website becomes imperative, since filtering a complete tuple would lead to filtering some informative, as well as less-informative, attribute data in the tuple. Since filtered data at the source Website may be of interest to         the user at the destination Website, we design a data recovery approach that maintains the minimal amount of information for         data recovery purpose while imposing minimal overhead for data recovery at the source Website. Our data filtering and recovery         approach (1) handles a wide range of Web data in different application domains (such as weather, stock exchanges, Internet         traffic, etc.), (2) is dynamic in nature, since each filtering scheme adjusts the amount of data to be filtered as needed,         and (3) is adaptive, which is appealing in an ever-changing Internet environment.      </content></document><document><year>2008</year><authors>David A. Cieslak1 | Nitesh V. Chawla1 </authors><title>A framework for monitoring classifiers&amp;#8217; performance: when and why failure occurs?      </title><content>Classifier error is the product of model bias and data variance. While understanding the bias involved when selecting a given         learning algorithm, it is similarly important to understand the variability in data over time, since even the One True Model might perform poorly when training and evaluation samples diverge. Thus, it becomes the ability to identify distributional         divergence is critical towards pinpointing when fracture points in classifier performance will occur, particularly since contemporary         methods such as tenfolds and hold-out are poor predictors in divergent circumstances. This article implement a comprehensive         evaluation framework to proactively detect breakpoints in classifiers&amp;#8217; predictions and shifts in data distributions through         a series of statistical tests. We outline and utilize three scenarios under which data changes: sample selection bias, covariate         shift, and shifting class priors. We evaluate the framework with a variety of classifiers and datasets.      </content></document><document><year>2008</year><authors>Chris Micacchi1 | Robin Cohen1 </authors><title>A framework for simulating real-time multi-agent systems      </title><content>In this paper, we describe an implementation of use in demonstrating the effectiveness of architectures for real-time multi-agent         systems. The implementation provides a simulation of a simplified RoboCup Search and Rescue environment, with unexpected events,         and includes a simulator for both a real-time operating system and a CPU. We present experimental evidence to demonstrate         the benefit of the implementation in the context of a particular hybrid architecture for multi-agent systems that allows certain         agents to remain fully autonomous, while others are fully controlled by a coordinating agent. In addition, we discuss the         value of the implementation for testing any models for the construction of real-time multi-agent systems and include a comparison         to related work.      </content></document><document><year>2008</year><authors>Ali Khoshgozaran1 | Ali Khodaei1 | Mehdi Sharifzadeh1  | Cyrus Shahabi1 </authors><title>A hybrid aggregation and compression technique for road network databases      </title><content>Vector data and in particular road networks are being queried, hosted and processed in many application domains such as in         mobile computing. Many client systems such as PDAs would prefer to receive the query results in unrasterized format without         introducing an overhead on overall system performance and result size. While several general vector data compression schemes         have been studied by different communities, we propose a novel approach in vector data compression which is easily integrated         within a geospatial query processing system. It uses line aggregation to reduce the number of relevant tuples and Huffman         compression to achieve a multi-resolution compressed representation of a road network database. Our experiments performed         on an end-to-end prototype verify that our approach exhibits fast query processing on both client and server sides as well         as high compression ratio.      </content></document><document><year>2008</year><authors>Zhouxuan Teng1  | Wenliang Du1 </authors><title>A hybrid multi-group approach for privacy-preserving data mining      </title><content>In this paper, we propose a hybrid multi-group approach for privacy preserving data mining. We make two contributions in this         paper. First, we propose a hybrid approach. Previous work has used either the randomization approach or the secure multi-party         computation (SMC) approach. However, these two approaches have complementary features: the randomization approach is much         more efficient but less accurate, while the SMC approach is less efficient but more accurate. We propose a novel hybrid approach, which takes advantage of the strength of both approaches to balance the accuracy and efficiency constraints. Compared         to the two existing approaches, our proposed approach can achieve much better accuracy than randomization approach and much         reduced computation cost than SMC approach. We also propose a multi-group scheme that makes it flexible for the data miner         to control the balance between data mining accuracy and privacy. This scheme is motivated by the fact that existing randomization         schemes that randomize data at individual attribute level can produce insufficient accuracy when the number of dimensions         is high. We partition attributes into groups, and develop a scheme to conduct group-based randomization to achieve better         data mining accuracy. To demonstrate the effectiveness of the proposed general schemes, we have implemented them for the ID3         decision tree algorithm and association rule mining problem and we also present experimental results.      </content></document><document><year>2008</year><authors>Peyman Kouchakpour1 | Anthony Zaknich1 | Thomas BrГ¤unl1</authors><title>A survey and taxonomy of performance improvement of canonical genetic programming      </title><content>The genetic programming (GP) paradigm, which applies the Darwinian principle of evolution to hierarchical computer programs,         has been applied with breakthrough success in various scientific and engineering applications. However, one of the main drawbacks         of GP has been the often large amount of computational effort required to solve complex problems. Much disparate research         has been conducted over the past 25;years to devise innovative methods to improve the efficiency and performance of GP. This         paper attempts to provide a comprehensive overview of this work related to Canonical Genetic Programming based on parse trees         and originally championed by Koza (Genetic programming: on the programming of computers by means of natural selection. MIT,         Cambridge, 1992). Existing approaches that address various techniques for performance improvement are identified and discussed         with the aim to classify them into logical categories that may assist with advancing further research in this area. Finally,         possible future trends in this discipline and some of the open areas of research are also addressed.      </content></document><document><year>2008</year><authors>Guojie Song1 | Bin Cui2| Baihua Zheng3| Kunqing Xie1 | Dongqing Yang2</authors><title>Accelerating sequence searching: dimensionality reduction method      </title><content>Similarity search over long sequence dataset becomes increasingly popular in many emerging applications, such as text retrieval,         genetic sequences exploring, etc. In this paper, a novel index structure, namely Sequence Embedding Multiset tree (SEM;&amp;#8722;;tree), has been proposed to speed up the searching process over long sequences. The SEM-tree is a multi-level structure where         each level represents the sequence data with different compression level of multiset, and the length of multiset increases         towards the leaf level which contains original sequences. The multisets, obtained using sequence embedding algorithms, have         the desirable property that they do not need to keep the character order in the sequence, i.e. shorter representation, but         can reserve the majority of distance information of sequences. Each level of the tree serves to prune the search space more         efficiently as the multisets utilize the predicability to finish the searching process beforehand and reduce the computational         cost greatly. A set of comprehensive experiments are conducted to evaluate the performance of the SEM-tree, and the experimental         results show that the proposed method is much more efficient than existing representative methods.      </content></document><document><year>2008</year><authors>Ngoc Thanh Nguyen1  | Rados&amp;#322 aw P. Katarzyniak1 </authors><title>Actions and social interactions in multi-agent systems      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Kaijun Wang1 | Junying Zhang1| Fengshan Shen1 | Lingfeng Shi2</authors><title>Adaptive learning of dynamic Bayesian networks with changing structures by detecting geometric structures of time series      </title><content>A dynamic Bayesian network (DBN) is one of popular approaches for relational knowledge discovery such as modeling relations         or dependencies, which change over time, between variables of a dynamic system. In this paper, we propose an adaptive learning         method (autoDBN) to learn DBNs with changing structures from multivariate time series. In autoDBN, segmentation of time series         is achieved first through detecting geometric structures transformed from time series, and then model regions are found from         the segmentation by designed finding strategies; in each found model region, a DBN model is established by existing structure         learning methods; finally, model revisiting is developed to refine model regions and improve DBN models. These techniques         provide a special mechanism to find accurate model regions and discover a sequence of DBNs with changing structures, which         are adaptive to changing relations between multivariate time series. Experimental results on simulated and real time series         show that autoDBN is very effective in finding accurate/reasonable model regions and gives lower error rates, outperforming         the switching linear dynamic system method and moving window method.      </content></document><document><year>2008</year><authors>Kaijun Wang1 | Junying Zhang1| Fengshan Shen1 | Lingfeng Shi2</authors><title>Adaptive learning of dynamic Bayesian networks with changing structures by detecting geometric structures of time series      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Germano Resconi1  | Boris Kovalerchuk2 </authors><title>Agents&amp;#8217; model of uncertainty      </title><content>Multi-agent systems play an increasing role in sensor networks, software engineering, web design, e-commerce, robotics, and         many others areas. Uncertainty is a fundamental property of these areas. Agent-based systems use probabilistic and other uncertainty         models developed earlier without explicit consideration of agents. This paper explores the impact of agents on uncertainty         models and theories. We compare two methods of introducing agents to uncertainty theories and propose a new theory called         the agent-based uncertainty theory (AUT). We show advantages of AUT for advancing multi-agent systems and for solving an internal         fundamental question of uncertainty theories, that is identifying coherent approaches to uncertainty. The advantages of AUT         are that it provides a uniform agent-based representation and an operational empirical interpretation for several uncertainty         theories such as rough set theory, fuzzy sets theory, evidence theory, and probability theory. We show also that the introduction         of agents to intuitionist uncertainty formalisms can reduce their conceptual complexity. To build such uniformity the AUT         exploits the fact that agents as independent entities can give conflicting evaluations of the same attribute. The AUT is based         on complex aggregations of crisp (non-fuzzy) conflicting judgments of agents. The generality of AUT is derived from the logical         classification of types (orders) of conflicts in the agent populations. At the first order of conflict, the two agent populations         are disjoint and there is no interference of logic values assigned to any statement p and its negation by agents. The second order of conflict models superposition (interference) of logic values for overlapping         agent populations where an agent assigns conflicting logic values (true, false) to the same attribute simultaneously.      </content></document><document><year>2008</year><authors>Themis P. Exarchos1| Markos G. Tsipouras2| Costas Papaloukas3 | Dimitrios I. Fotiadis2 </authors><title>An optimized sequential pattern matching methodology for sequence classification      </title><content>In this paper we present a novel methodology for sequence classification, based on sequential pattern mining and optimization         algorithms. The proposed methodology automatically generates a sequence classification model, based on a two stage process.         In the first stage, a sequential pattern mining algorithm is applied to a set of sequences and the sequential patterns are         extracted. Then, the score of every pattern with respect to each sequence is calculated using a scoring function and the score         of each class under consideration is estimated by summing the specific pattern scores. Each score is updated, multiplied by         a weight and the output of the first stage is the classification confusion matrix of the sequences. In the second stage an         optimization technique, aims to finding a set of weights which minimize an objective function, defined using the classification         confusion matrix. The set of the extracted sequential patterns and the optimal weights of the classes comprise the sequence         classification model. Extensive evaluation of the methodology was carried out in the protein classification domain, by varying         the number of training and test sequences, the number of patterns and the number of classes. The methodology is compared with         other similar sequence classification approaches. The proposed methodology exhibits several advantages, such as automated         weight assignment to classes using optimization techniques and knowledge discovery in the domain of application.      </content></document><document><year>2008</year><authors>Gang Luo1 | Kun-Lung Wu1  | Philip S. Yu1 </authors><title>Answering linear optimization queries with an approximate stream index      </title><content>We propose a SAO index to approximately answer arbitrary linear optimization queries in a sliding window of a data stream.         It uses limited memory to maintain the most &amp;#8220;important&amp;#8221; tuples. At any time, for any linear optimization query, we can retrieve         the approximate top-K tuples in the sliding window almost instantly. The larger the amount of available memory, the better the quality of the answers         is. More importantly, for a given amount of memory, the quality of the answers can be further improved by dynamically allocating         a larger portion of the memory to the outer layers of the SAO index.      </content></document><document><year>2008</year><authors>Matteo Cristani1  | Elisa Burato1 </authors><title>Approximate solutions of moral dilemmas in multiple agent system      </title><content>Moral dilemmas are one of the major issues of current research in ethical reasoning. In particular, it is well known that         admitting moral dilemmas in Standard Deontic Logic generates a family of inconsistencies that are intrinsically unsolvable.         Since managing dilemmas means performing preferential reasoning, we argue that one simple approach to both types of problems         is by ordering actions. We notice that in general, more than local orderings between two actions, agents have intrinsic preferences         based on classification issues, like the action type, and that, once we have discharged the dilemma as it is intrinsically,         preferential reasoning is performed by using a second-level choice approach. Decision theory has dealt with the problem of         making decisions in presence of conflicting decision criteria, and some researcher has pointed out that this is the case of         moral dilemmas as well. In practice, the choice of preferences in presence of conflicting criteria can be seen as a form of         preferential-ethical reasoning. Although this is certainly an important topic in multiple agent investigations, it is definitely         neglected in the current investigations. It is well known that humans are quite clever in solving moral dilemmas, and the         usage they make of preferential reasoning is very complex. In this paper we address the problems of preferential-ethical reasoning         in a combinatorial fashion and provide an algorithm for making decisions on moral dilemmas in presence of conflicting decision         criteria. We then evaluate the complexity of the algorithm and prove that this approach can be applied in practice.      </content></document><document><year>2008</year><authors>Zheng Wang1 | Qing Wang1 | Ding-Wei Wang1</authors><title>Bayesian network based business information retrieval model      </title><content>The quality of business information can significantly affect the operation level of enterprise. This paper analyses the problem         of business information retrieval (BIR). A Bayesian Network Based business information retrieval model (BN-BIRM) is proposed         by means of Bayesian network (BN) and information retrieval (IR) theory and a method for query adaptation is presented. In         this model the customized query requirement of enterprise (CQR) is expressed in terms of the predefined illustrative documents         related to business domain. The similarities between the documents and the query are evaluated with the conditional probabilities         among the nodes in the BN. In the experiments, BN-BIRM is compared with the Belief Network model based on vector space model         (VSM) ranking strategy and the Inference Network model based on TF-IDF ranking strategy. The experimental results show that         BN-BIRM is effective for collecting business information on a large scale.      </content></document><document><year>2008</year><authors>Darius Pfitzner1 | Richard Leibbr|t1 | David Powers1</authors><title>Characterization and evaluation of similarity measures for pairs of clusterings      </title><content>In evaluating the results of cluster analysis, it is common practice to make use of a number of fixed heuristics rather than         to compare a data clustering directly against an empirically derived standard, such as a clustering empirically obtained from         human informants. Given the dearth of research into techniques to express the similarity between clusterings, there is broad         scope for fundamental research in this area. In defining the comparative problem, we identify two types of worst-case matches         between pairs of clusterings, characterised as independently codistributed clustering pairs and conjugate partition pairs. Desirable behaviour for a similarity measure in either of the two worst cases is discussed, giving rise to five test scenarios         in which characteristics of one of a pair of clusterings was manipulated in order to compare and contrast the behaviour of         different clustering similarity measures. This comparison is carried out for previously-proposed clustering similarity measures,         as well as a number of established similarity measures that have not previously been applied to clustering comparison. We         introduce a paradigm apparatus for the evaluation of clustering comparison techniques and distinguish between the goodness of clusterings and the similarity of clusterings by clarifying the degree to which different measures confuse the two. Accompanying this is the proposal of         a novel clustering similarity measure, the Measure of Concordance (MoC). We show that only MoC, Powers&amp;#8217;s measure, Lopez and Rajski&amp;#8217;s measure and various forms of Normalised Mutual Information         exhibit the desired behaviour under each of the test scenarios.      </content></document><document><year>2008</year><authors>Hui Xiong1 | Michael Steinbach2 | Arifin Ruslim2  | Vipin Kumar2 </authors><title>Characterizing pattern preserving clustering      </title><content>This paper describes a new approach for clustering&amp;#8212;pattern preserving clustering&amp;#8212;which produces more easily interpretable         and usable clusters. This approach is motivated by the following observation: while there are usually strong patterns in the         data&amp;#8212;patterns that may be key for the analysis and description of the data&amp;#8212;these patterns are often split among different         clusters by current clustering approaches. This is, perhaps, not surprising, since clustering algorithms have no built-in         knowledge of these patterns and may often have goals that are in conflict with preserving patterns, e.g., minimize the distance         of points to their nearest cluster centroids. In this paper, our focus is to characterize (1) the benefits of pattern preserving         clustering and (2) the most effective way of performing pattern preserving clustering. To that end, we propose and evaluate         two clustering algorithms, HIerarchical Clustering with pAttern Preservation (HICAP) and bisecting K-means Clustering with         pAttern Preservation (K-CAP). Experimental results on document data show that HICAP can produce overlapping clusters that         preserve useful patterns, but has relatively worse clustering performance than bisecting K-means with respect to the clustering         evaluation criterion of entropy. By contrast, in terms of entropy, K-CAP can perform substantially better than the bisecting         K-means algorithm when data sets contain clusters of widely different sizes&amp;#8212;a common situation in the real-world. Most importantly,         we also illustrate how patterns, if preserved, can aid cluster interpretation.      </content></document><document><year>2008</year><authors>Tao Li1 </authors><title>Clustering based on matrix approximation: a unifying view      </title><content>Clustering is the problem of identifying the distribution of patterns and intrinsic correlations in large data sets by partitioning         the data points into similarity classes. Recently, a number of methods have been proposed and demonstrated good performance         based on matrix approximation. Despite significant research on these methods, few attempts have been made to establish the         connections between them while highlighting their differences. In this paper, we present a unified view of these methods within         a general clustering framework where the problem of clustering is formulated as matrix approximations and the clustering objective         is minimizing the approximation error between the original data matrix and the reconstructed matrix based on the cluster structures.         The general framework provides an elegant base to compare and understand various clustering methods. We provide characterizations         of different clustering methods within the general framework including traditional one-side clustering, subspace clustering         and two-side clustering. We also establish the connections between our general clustering framework with existing frameworks.      </content></document><document><year>2008</year><authors>Ira Assent1 | Ralph Krieger1 | Boris Glavic1  | Thomas Seidl1 </authors><title>Clustering multidimensional sequences in spatial and temporal databases      </title><content>Many environmental, scientific, technical or medical database applications require effective and efficient mining of time         series, sequences or trajectories of measurements taken at different time points and positions forming large temporal or spatial         databases. Particularly the analysis of concurrent and multidimensional sequences poses new challenges in finding clusters         of arbitrary length and varying number of attributes. We present a novel algorithm capable of finding parallel clusters in         different subspaces and demonstrate our results for temporal and spatial applications. Our analysis of structural quality         parameters in rivers is successfully used by hydrologists to develop measures for river quality improvements.      </content></document><document><year>2008</year><authors>Zhaohui Wu1 | Shuiguang Deng1 | Ying Li1  | Jian Wu1 </authors><title>Computing compatibility in dynamic service composition      </title><content>Dynamically composing services requires mechanisms to ensure component services compatible with each other both at all of         the syntax, semantic and behavioral level. This paper focuses on the issue of behavioral compatibility in a service composition.         It adopts the &amp;#960;-calculus to model service behaviors and interactions in a formal way. Based on the formalization, it proposes a method to         automatically check the behavioral compatibility in a qualitative way. Furthermore, it presents an algorithm to compute the         compatibility degree in a quantitative way. The algorithm is implemented in a prototype and its performance analysis is also         carried out to show that it can help composing services on the fly and ensure the services compatible with each other to provide         functions with newly-added values.      </content></document><document><year>2008</year><authors>Jason J. Jung1 </authors><title>Consensus-based evaluation framework for distributed information retrieval systems      </title><content>Multi-agent systems have been attacking the challenges of information retrieval tasks on distributed environment. In this         paper, we propose a consensus choice selection method based framework to evaluate the performance of cooperative information         retrieval tasks of the multiple agents. Thereby, two well-known measurements, precision and recall, are extended to handle consensual closeness (i.e., local and global consensus) between the sets of retrieved results. We         show that in a motivating example the proposed criteria are prone to solve the rigidity problem of classical precision and recall. More importantly, the retrieved results can be ranked with respect to the consensual score, and the ranking mechanism has         been verified to be more reasonable.      </content></document><document><year>2008</year><authors>Anna Formica1  | Elaheh Pourabbas1 </authors><title>Content based similarity of geographic classes organized as partition hierarchies      </title><content>In this paper we propose a method to measure the semantic similarity of geographic classes organized as partition hierarchies         within Naive Geography. The contribution of this work consists in extending and integrating the information content approach, and the method for comparing concept attributes in the ontology management system SymOntos developed at IASI. As a result, this proposal allows us to address both the concept similarity within the partition hierarchy,         and the attribute similarity of geographic classes and, therefore, to reduce the gap among the different similarity approaches         defined in the literature.      </content></document><document><year>2008</year><authors>Ruoming Jin1 | Yuri Breitbart1  | Chibuike Muoh1 </authors><title>Data discretization unification      </title><content>Data discretization is defined as a process of converting continuous data attribute values into a finite set of intervals         with minimal loss of information. In this paper, we prove that discretization methods based on informational theoretical complexity         and the methods based on statistical measures of data dependency are asymptotically equivalent. Furthermore, we define a notion         of generalized entropy and prove that discretization methods based on Minimal description length principle, Gini index, AIC,         BIC, and Pearson&amp;#8217;s X         2 and G         2 statistics are all derivable from the generalized entropy function. We design a dynamic programming algorithm that guarantees         the best discretization based on the generalized entropy notion. Furthermore, we conducted an extensive performance evaluation         of our method for several publicly available data sets. Our results show that our method delivers on the average 31% less         classification errors than many previously known discretization methods.      </content></document><document><year>2008</year><authors>Kaibo Xu1| 2 | Junkang Feng1| 2  | Malcolm Crowe2 </authors><title>Defining the notion of &amp;#8216;Information Content&amp;#8217; and reasoning about it in a database      </title><content>The problem of &amp;#8216;information content&amp;#8217; of an information system appears elusive. In the field of databases, the information         content of a database has been taken as the instance of a database. We argue that this view misses two fundamental points.         One is a convincing conception of the phenomenon concerning information in databases, especially a properly defined notion         of &amp;#8216;information content&amp;#8217;. The other is a framework for reasoning about information content. In this paper, we suggest a modification         of the well known definition of &amp;#8216;information content&amp;#8217; given by Dretske(Knowledge and the flow of information,1981). We then         define what we call the &amp;#8216;information content inclusion&amp;#8217; relation (IIR for short) between two random events. We present a set         of inference rules for reasoning about information content, which we call the IIR Rules. Then we explore how these ideas and         the rules may be used in a database setting to look at databases and to derive otherwise hidden information by deriving new         relations from a given set of IIR. A prototype is presented, which shows how the idea of IIR-Reasoning might be exploited         in a database setting including the relationship between real world events and database values.      </content></document><document><year>2008</year><authors>Songtao Guo1| Xintao Wu1  | Yingjiu Li2</authors><title>Determining error bounds for spectral filtering based reconstruction methods in privacy preserving data mining      </title><content>Additive randomization has been a primary tool for hiding sensitive private information. Previous work empirically showed         that individual data values can be approximately reconstructed from the perturbed values, using spectral filtering techniques.         This poses a serious threat of privacy breaches. In this paper we conduct a theoretical study on how the reconstruction error         varies, for different types of additive noise. In particular, we first derive an upper bound for the reconstruction error         using matrix perturbation theory. Attackers who use spectral filtering techniques to estimate the true data values may leverage         this bound to determine how close their estimates are to the original data. We then derive a lower bound for the reconstruction         error, which can help data owners decide how much noise should be added to satisfy a given threshold of the tolerated privacy         breach.      </content></document><document><year>2008</year><authors>Longbing Cao1  | Tony He2</authors><title>Developing actionable trading agents      </title><content>Trading agents are useful for developing and back-testing quality trading strategies to support smart trading actions in the         market. However, most of the existing trading agent research oversimplifies trading strategies, and focuses on simulated ones.         As a result, there exists a big gap between the deliverables and business needs when the developed strategies are deployed         into the real life. Therefore, the actionable capability of developed trading agents is often very limited. This paper for         the first time introduces effective approaches for optimizing and integrating multiple classes of strategies through trading         agent collaboration. An integration and optimization approach is proposed to identify optimal trading strategy in each category,         and further integrate optimal strategies crossing classes. Positions associated with these optimal strategies are recommended         for trading agents to take actions in the market. Extensive experiments on a large quantity of real-life market data show         that trading agents following the recommended strategies have great potential to obtain high benefits while low costs. This         verifies that it is promising to develop trading agents toward workable and satisfying business needs.      </content></document><document><year>2008</year><authors>Kazuko Takahashi1 | Hiroya Takamura2  | Manabu Okumura2 </authors><title>Direct estimation of class membership probabilities for multiclass classification using multiple scores      </title><content>Accurate estimation of class membership probability is needed for many applications in data mining and decision-making, to         which multiclass classification is often applied. Since existing methods for estimation of class membership probability are         designed for binary classification, in which only a single score outputted from a classifier can be used, an approach for         multiclass classification requires both a decomposition of a multiclass classifier into binary classifiers and a combination         of estimates obtained from each binary classifier to a target estimate. We propose a simple and general method for directly         estimating class membership probability for any class in multiclass classification without decomposition and combination,         using multiple scores not only for a predicted class but also for other proper classes. To make it possible to use multiple         scores, we propose to modify or extend representative existing methods. As a non-parametric method, which refers to the idea         of a binning method as proposed by Zadrozny et al., we create an &amp;#8220;accuracy table&amp;#8221; by a different method. Moreover we smooth         accuracies on the table with methods such as the moving average to yield reliable probabilities (accuracies). As a parametric         method, we extend Platt&amp;#8217;s method to apply a multiple logistic regression. On two different datasets (open-ended data from         Japanese social surveys and the 20 Newsgroups) both with Support Vector Machines and naive Bayes classifiers, we empirically         show that the use of multiple scores is effective in the estimation of class membership probabilities in multiclass classification         in terms of cross entropy, the reliability diagram, the ROC curve and AUC (area under the ROC curve), and that the proposed         smoothing method for the accuracy table works quite well. Finally, we show empirically that in terms of MSE (mean squared         error), our best proposed method is superior to an expansion for multiclass classification of a PAV method proposed by Zadrozny         et al., in both the 20 Newsgroups dataset and the Pendigits dataset, but is slightly worse than the state-of-the-art method,         which is an expansion for multiclass classification of a combination of boosting and a PAV method, on the Pendigits dataset.      </content></document><document><year>2008</year><authors>Jeffrey Chan1 | James Bailey1 | Christopher Leckie1</authors><title>Discovering correlated spatio-temporal changes in evolving graphs      </title><content>Graphs provide powerful abstractions of relational data, and are widely used in fields such as network management, web page         analysis and sociology. While many graph representations of data describe dynamic and time evolving relationships, most graph         mining work treats graphs as static entities. Our focus in this paper is to discover regions of a graph that are evolving         in a similar manner. To discover regions of correlated spatio-temporal change in graphs, we propose an algorithm called cSTAG.         Whereas most clustering techniques are designed to find clusters that optimise a single distance measure, cSTAG addresses         the problem of finding clusters that optimise both temporal and spatial distance measures simultaneously. We show the effectiveness         of cSTAG using a quantitative analysis of accuracy on synthetic data sets, as well as demonstrating its utility on two large,         real-life data sets, where one is the routing topology of the Internet, and the other is the dynamic graph of files accessed         together on the 1998 World Cup official website.      </content></document><document><year>2008</year><authors>Dragomir Yankov1 | Eamonn Keogh1 | Umaa Rebbapragada2</authors><title>Disk aware discord discovery: finding unusual time series in terabyte sized datasets      </title><content>The problem of finding unusual time series has recently attracted much attention, and several promising methods are now in         the literature. However, virtually all proposed methods assume that the data reside in main memory. For many real-world problems         this is not be the case. For example, in astronomy, multi-terabyte time series datasets are the norm. Most current algorithms         faced with data which cannot fit in main memory resort to multiple scans of the disk /tape and are thus intractable. In this         work we show how one particular definition of unusual time series, the time series discord, can be discovered with a disk         aware algorithm. The proposed algorithm is exact and requires only two linear scans of the disk with a tiny buffer of main         memory. Furthermore, it is very simple to implement. We use the algorithm to provide further evidence of the effectiveness         of the discord definition in areas as diverse as astronomy, web query mining, video surveillance, etc., and show the efficiency         of our method on datasets which are many orders of magnitude larger than anything else attempted in the literature.      </content></document><document><year>2008</year><authors>Hua-Fu Li1 | Man-Kwan Shan2  | Suh-Yin Lee3 </authors><title>DSM-FI: an efficient algorithm for mining frequent itemsets in data streams      </title><content>Online mining of data streams is an important data mining problem with broad applications. However, it is also a difficult         problem since the streaming data possess some inherent characteristics. In this paper, we propose a new single-pass algorithm,         called DSM-FI (data stream mining for frequent itemsets), for online incremental mining of frequent itemsets over a continuous         stream of online transactions. According to the proposed algorithm, each transaction of the stream is projected into a set         of sub-transactions, and these sub-transactions are inserted into a new in-memory summary data structure, called SFI-forest         (summary frequent itemset forest) for maintaining the set of all frequent itemsets embedded in the transaction data stream         generated so far. Finally, the set of all frequent itemsets is determined from the current SFI-forest. Theoretical analysis         and experimental studies show that the proposed DSM-FI algorithm uses stable memory, makes only one pass over an online transactional         data stream, and outperforms the existing algorithms of one-pass mining of frequent itemsets.      </content></document><document><year>2008</year><authors>Tianyi Jiang1  | Alex|er Tuzhilin1 </authors><title>Dynamic micro-targeting: fitness-based approach to predicting individual preferences      </title><content>It is crucial to segment customers intelligently in order to offer more targeted and personalized products and services. Traditionally,         customer segmentation is achieved using statistics-based methods that compute a set of statistics from the customer data and         group customers into segments by applying clustering algorithms. Recent research proposed a direct grouping-based approach         that combines customers into segments by optimally combining transactional data of several customers and building a data mining         model of customer behavior for each group. This paper proposes a new micro-targeting method that builds predictive models         of customer behavior not on the segments of customers but rather on the customer-product groups. This micro-targeting method         is more general than the previously considered direct grouping method. We empirically show that it outperforms the direct         grouping and statistics-based segmentation methods across multiple experimental conditions and that it generates predominately         small-sized segments, thus providing additional support for the micro-targeting approach to personalization.      </content></document><document><year>2008</year><authors>Shiming Xiang1 | Feiping Nie1| Yangqiu Song1| Changshui Zhang1 | Chunxia Zhang2</authors><title>Embedding new data points for manifold learning via coordinate propagation      </title><content>In recent years, a series of manifold learning algorithms have been proposed for nonlinear dimensionality reduction. Most         of them can run in a batch mode for a set of given data points, but lack a mechanism to deal with new data points. Here we         propose an extension approach, i.e., mapping new data points into the previously learned manifold. The core idea of our approach         is to propagate the known coordinates to each of the new data points. We first formulate this task as a quadratic programming,         and then develop an iterative algorithm for coordinate propagation. Tangent space projection and smooth splines are used to         yield an initial coordinate for each new data point, according to their local geometrical relations. Experimental results         and applications to camera direction estimation and face pose estimation illustrate the validity of our approach.      </content></document><document><year>2008</year><authors>Shaoning Pang1  | Nikola Kasabov1 </authors><title>Encoding and decoding the knowledge of association rules over SVM classification trees      </title><content>This paper presents a constructive method for association rule extraction, where the knowledge of data is encoded into an         SVM classification tree (SVMT), and linguistic association rule is extracted by decoding of the trained SVMT. The method of         rule extraction over the SVMT (SVMT-rule), in the spirit of decision-tree rule extraction, achieves rule extraction not only         from SVM, but also over the decision-tree structure of SVMT. Thus, the obtained rules from SVMT-rule have the better comprehensibility         of decision-tree rule, meanwhile retains the good classification accuracy of SVM. Moreover, profiting from the super generalization         ability of SVMT owing to the aggregation of a group of SVMs, the SVMT-rule is capable of performing a very robust classification         on such datasets that have seriously, even overwhelmingly, class-imbalanced data distribution. Experiments with a Gaussian         synthetic data, seven benchmark cancers diagnosis, and one application of cell-phone fraud detection have highlighted the         utility of SVMT and SVMT-rule on comprehensible and effective knowledge discovery, as well as the superior properties of SVMT-rule         as compared to a purely support-vector based rule extraction. (A version of SVMT Matlab software is available online at http://kcir.kedri.info)      </content></document><document><year>2008</year><authors>Gisele L. Pappa1  | Alex A. Freitas2</authors><title>Evolving rule induction algorithms with multi-objective grammar-based genetic programming      </title><content>Multi-objective optimization has played a major role in solving problems where two or more conflicting objectives need to         be simultaneously optimized. This paper presents a Multi-Objective grammar-based genetic programming (MOGGP) system that automatically         evolves complete rule induction algorithms, which in turn produce both accurate and compact rule models. The system was compared         with a single objective GGP and three other rule induction algorithms. In total, 20 UCI data sets were used to generate and         test generic rule induction algorithms, which can be now applied to any classification data set. Experiments showed that,         in general, the proposed MOGGP finds rule induction algorithms with competitive predictive accuracies and more compact models         than the algorithms it was compared with.      </content></document><document><year>2008</year><authors>Bin Cao1  | Antonio Badia2 </authors><title>Exploiting maximal redundancy to optimize SQL queries      </title><content>Detecting and dealing with redundancy is an ubiquitous problem in query optimization, which manifests itself in many areas         of research such as materialized views, multi-query optimization, and query-containment algorithms. In this paper, we focus         on the issue of intra-query redundancy, redundancy present within a query. We present a method to detect the maximal redundancy present between a main (outer) query block and a subquery block.         We then use the method for query optimization, introducing query plans and a new operator that take full advantage of the         redundancy discovered. Our approach can deal with redundancy in a wider spectrum of queries than existing techniques. We show         experimental evidence that our approach works under certain conditions, and compares favorably to existing optimization techniques         when applicable.      </content></document><document><year>2008</year><authors>Vasileios K|ylas1 | S. Phineas Upham2 | Lyle H. Ungar1 </authors><title>Finding cohesive clusters for analyzing knowledge communities      </title><content>Documents and authors can be clustered into &amp;#8220;knowledge communities&amp;#8221; based on the overlap in the papers they cite. We introduce         a new clustering algorithm, Streemer, which finds cohesive foreground clusters embedded in a diffuse background, and use it         to identify knowledge communities as foreground clusters of papers which share common citations. To analyze the evolution         of these communities over time, we build predictive models with features based on the citation structure, the vocabulary of         the papers, and the affiliations and prestige of the authors. Findings include that scientific knowledge communities tend         to grow more rapidly if their publications build on diverse information and if they use a narrow vocabulary.      </content></document><document><year>2008</year><authors>Aris Gkoulalas-Divanis1  | Vassilios S. Verykios1 </authors><title>Hiding sensitive knowledge without side effects      </title><content>Sensitive knowledge hiding in large transactional databases is one of the major goals of privacy preserving data mining. However,         it is only recently that researchers were able to identify exact solutions for the hiding of knowledge, depicted in the form         of sensitive frequent itemsets and their related association rules. Exact solutions allow for the hiding of vulnerable knowledge         without any critical compromises, such as the hiding of nonsensitive patterns or the accidental uncovering of infrequent itemsets,         amongst the frequent ones, in the sanitized outcome. In this paper, we highlight the process of border revision, which plays         a significant role towards the identification of exact hiding solutions, and we provide efficient algorithms for the computation         of the revised borders. Furthermore, we review two algorithms that identify exact hiding solutions, and we extend the functionality         of one of them to effectively identify exact solutions for a wider range of problems (than its original counterpart). Following         that, we introduce a novel framework for decomposition and parallel solving of hiding problems, which are handled by each         of these approaches. This framework improves to a substantial degree the size of the problems that both algorithms can handle         and significantly decreases their runtime. Through experimentation, we demonstrate the effectiveness of these approaches toward         providing high quality knowledge hiding solutions.      </content></document><document><year>2008</year><authors>Zhiguo Gong1  | Qian Liu1 </authors><title>Improving keyword based web image search with visual feature distribution and term expansion      </title><content>This paper discusses techniques for improving the performance of keyword-based web image queries. Firstly, a web page is segmented         into several text blocks based on semantic cohesion. The text blocks which contain web images are taken as the associated         texts of corresponding images and TF*IDF model is initially used to index those web images. Then, for each keyword, both relevant web image set and irrelevant web         image set are selected according to their TF*IDF values. And visual feature distributions of both positive image and negative image are modeled using Gaussian Mixture Model.         An image&amp;#8217;s relevance to the keyword with respect to visual feature is thus defined as the ratio of positive distribution density         over negative distribution density. We combine the text-based relevance model with visual feature relevance model to improve         the performance. Thirdly, a query expansion model is used to improve the performance further. Expansion terms are selected         according to their cooccurrences with the query terms in the top-relevant set of the original query. Our experiments show         that our approach yield significant improvement over the traditional keyword based query model.      </content></document><document><year>2008</year><authors>Yan Zhuang1 | Simon Fong1  | Meilin Shi2</authors><title>Knowledge-empowered automated negotiation system for e-Commerce      </title><content>This paper focuses on knowledge empowered automated negotiation systems for buyer-centric multi-bilateral multi-attribute         e-Procurement. We propose two knowledge empowered models, namely KERM and KACM. KERM is used for the buyer to determine a         list of suppliers which are the best qualified candidates to negotiate with. The use of knowledge features largely in the         model, which incorporates both the buyer&amp;#8217;s and supplier&amp;#8217;s profiles in evaluating a quote. Historical trade records of a supplier         contribute to the supplier&amp;#8217;s profile credit and therefore the rank of the supplier&amp;#8217;s quote. KERM also allows the flexibility         to assign appropriate weights, based on buyer&amp;#8217;s interests, to each knowledge factor affecting the overall evaluation result         of a quote. The resulted list of quotes of high rank is believed to produce satisfactory negotiation result for the buyer.         KACM enables an automated concession process, while at the same time facilitates a flexible negotiation via the use of concept         switch and tagged rules. Different from other negotiation models, KACM emphasizes the utilization of knowledge originated         from the historical negotiation data in estimating and fine-tuning the negotiation parameters, for improving the performance         of automated negotiation. Graph results show that our software prototype system makes significant improvement in the satisfaction         level of negotiation results.      </content></document><document><year>2008</year><authors>Liangxiao Jiang1 | Chaoqun Li2 | Zhihua Cai1</authors><title>Learning decision tree for ranking      </title><content>Decision tree is one of the most effective and widely used methods for classification. However, many real-world applications         require instances to be ranked by the probability of class membership. The area under the receiver operating characteristics         curve, simply AUC, has been recently used as a measure for ranking performance of learning algorithms. In this paper, we present         two novel class probability estimation algorithms to improve the ranking performance of decision tree. Instead of estimating         the probability of class membership using simple voting at the leaf where the test instance falls into, our algorithms use         similarity-weighted voting and naive Bayes. We design empirical experiments to verify that our new algorithms significantly         outperform the recent decision tree ranking algorithm C4.4 in terms of AUC.      </content></document><document><year>2008</year><authors>Jianwu Yang1 | William K. Cheung2  | Xiaoou Chen1 </authors><title>Learning element similarity matrix for semi-structured document analysis      </title><content>Capturing latent structural and semantic properties in semi-structured documents (e.g., XML documents) is crucial for improving         the performance of related document analysis tasks. Structured Link Vector Mode (SLVM) is a representation recently proposed         for modeling semi-structured documents. It uses an element similarity matrix to capture the latent relationships between XML         elements&amp;#8212;the constructing components of an XML document. In this paper, instead of applying heuristics to define the element         similarity matrix, we propose to compute the matrix using the machine learning approach. In addition, we incorporate term         semantics into SLVM using latent semantic indexing to enhance the model accuracy, with the element similarity learnability         property preserved. For performance evaluation, we applied the similarity learning to k-nearest neighbors search and similarity-based clustering, and tested the performance using two different XML document collections.         The SLVM obtained via learning was found to outperform significantly the conventional Vector Space Model and the edit-distance-based         methods. Also, the similarity matrix, obtained as a by-product, can provide higher-level knowledge on the semantic relationships         between the XML elements.      </content></document><document><year>2008</year><authors>Chih-Yung Tsai1| Shuo-Yan Chou2 | Shih-Wei Lin3 | Wei-Hao Wang2</authors><title>Location determination of mobile devices for an indoor WLAN application using a neural network      </title><content>Due to the popularity of location-based services, determining the location of a device at all times has become a subject of         great interests. Although many GPS-based applications have been developed and successfully deployed in various fields, their         applicabilities are hindered by the obstruction of the objects in the environment. Essentially, as satellite signals cannot         penetrate the walls of buildings, the coverage of GPS systems is limited to outdoor environments. To fully exploit the benefit         of location-based services, approaches that determine the location of a device in indoor environments need to be established.         This study presents a novel location determination mechanism that uses an indoor WLAN and back-propagation neural network         (BPN). A museum is taken as the context of the example indoor environment. Location determination is achieved using the combined         strengths of 802.11b wireless access signals. With a significant number of access points (APs) installed in the museum, hand-held         devices can sense the strengths of the signals from all APs to which the devices can connect. Using a back-propagation network,         device locations can be estimated with sufficient accuracy. A novel adaptive algorithm is implemented for enhancing the accuracy         of the estimation.      </content></document><document><year>2008</year><authors>Nikolaj Tatti1 </authors><title>Maximum entropy based significance of itemsets      </title><content>We consider the problem of defining the significance of an itemset. We say that the itemset is significant if we are surprised         by its frequency when compared to the frequencies of its sub-itemsets. In other words, we estimate the frequency of the itemset         from the frequencies of its sub-itemsets and compute the deviation between the real value and the estimate. For the estimation         we use Maximum Entropy and for measuring the deviation we use Kullback&amp;#8211;Leibler divergence. A major advantage compared to the         previous methods is that we are able to use richer models whereas the previous approaches only measure the deviation from         the independence model. We show that our measure of significance goes to zero for derivable itemsets and that we can use the         rank as a statistical test. Our empirical results demonstrate that for our real datasets the independence assumption is too         strong but applying more flexible models leads to good results.      </content></document><document><year>2008</year><authors>Thirunavukkarasu Ramkumar1  | Rengaramanujam Srinivasan2 </authors><title>Modified algorithms for synthesizing high-frequency rules from different data sources      </title><content>Because of the rapid growth in information and communication technologies, a company&amp;#8217;s data may be spread over several continents.         For an effective decision-making process, knowledge workers need data, which may be geographically spread in different locations.         In such circumstances, multi-database mining plays a major role in the process of extracting knowledge from different data         sources. In this paper, we have proposed a new methodology for synthesizing high-frequency rules from different data sources,         where data source weight has been calculated on the basis of their transaction population. We have also proposed a new method         for calculating global confidence. Our goal in synthesizing local patterns to obtain global patterns is that, the support         and confidence of synthesized patterns must be very nearly same if all the databases are integrated and mono-mining has been         done. Experiments conducted clearly establish that the proposed method of synthesizing high-frequency rules fairly meets the         stipulation.      </content></document><document><year>2008</year><authors>Hongyu Guo1  | Herna L. Viktor1 </authors><title>Multirelational classification: a multiple view approach      </title><content>Multirelational classification aims at discovering useful patterns across multiple inter-connected tables (relations) in a         relational database. Many traditional learning techniques, however, assume a single table or a flat file as input (the so-called         propositional algorithms). Existing multirelational classification approaches either &amp;#8220;upgrade&amp;#8221; mature propositional learning         methods to deal with relational presentation or extensively &amp;#8220;flatten&amp;#8221; multiple tables into a single flat file, which is then         solved by propositional algorithms. This article reports a multiple view strategy&amp;#8212;where neither &amp;#8220;upgrading&amp;#8221; nor &amp;#8220;flattening&amp;#8221;         is required&amp;#8212;for mining in relational databases. Our approach learns from multiple views (feature set) of a relational databases,         and then integrates the information acquired by individual view learners to construct a final model. Our empirical studies         show that the method compares well in comparison with the classifiers induced by the majority of multirelational mining systems,         in terms of accuracy obtained and running time needed. The paper explores the implications of this finding for multirelational         research and applications. In addition, the method has practical significance: it is appropriate for directly mining many         real-world databases.      </content></document><document><year>2008</year><authors>Dong-Chul Park1 </authors><title>Multiresolution-based bilinear recurrent neural network      </title><content>A multiresolution-based bilinear recurrent neural network (MBLRNN) is proposed in this paper. The proposed MBLRNN is based         on the BLRNN that has robust abilities in modeling and predicting time series. The learning process is further improved by         using a multiresolution-based learning algorithm for training the BLRNN so as to make it more robust for the prediction of         time series data. The proposed MBLRNN is applied to the problems of network traffic prediction and electric load forecasting.         Experiments and results on both practical problems show that the proposed MBLRNN outperforms both the traditional multilayer         perceptron type neural network (MLPNN) and the BLRNN in the prediction accuracy.      </content></document><document><year>2008</year><authors>Yanhua Chen1| Manjeet Rege1| Ming Dong1  | Jing Hua1</authors><title>Non-negative matrix factorization for semi-supervised data clustering      </title><content>Traditional clustering algorithms are inapplicable to many real-world problems where limited knowledge from domain experts         is available. Incorporating the domain knowledge can guide a clustering algorithm, consequently improving the quality of clustering.         In this paper, we propose SS-NMF: a semi-supervised non-negative matrix factorization framework for data clustering. In SS-NMF,         users are able to provide supervision for clustering in terms of pairwise constraints on a few data objects specifying whether         they &amp;#8220;must&amp;#8221; or &amp;#8220;cannot&amp;#8221; be clustered together. Through an iterative algorithm, we perform symmetric tri-factorization of the         data similarity matrix to infer the clusters. Theoretically, we show the correctness and convergence of SS-NMF. Moveover,         we show that SS-NMF provides a general framework for semi-supervised clustering. Existing approaches can be considered as         special cases of it. Through extensive experiments conducted on publicly available datasets, we demonstrate the superior performance         of SS-NMF for clustering.      </content></document><document><year>2008</year><authors>Guido Boella1| Patrice Caire2 | Leendert van der Torre2 </authors><title>Norm negotiation in online multi-player games      </title><content>In this paper, we introduce an agent communication protocol and speech acts for norm negotiation. The protocol creates individual         or contractual obligations to fulfill goals of the agents based on the so-called social delegation cycle. First, agents communicate         their individual goals and powers. Second, they propose social goals which can be accepted or rejected by other agents. Third,         they propose obligations and sanctions to achieve the social goal, which can again be accepted or rejected. Finally, the agents         accept the new norm by indicating which of their communicated individual goals the norm achieves. The semantics of the speech         acts is based on a commitment to public mental attitudes. The norm negotiation model is illustrated by an example of norm         negotiation in multi-player online gaming.      </content></document><document><year>2008</year><authors>Dihua Guo1| Hui Xiong1 | Vijayalakshmi Atluri1 | Nabil R. Adam1</authors><title>Object discovery in high-resolution remote sensing images: a semantic perspective      </title><content>Given its importance, the problem of object discovery in high-resolution remote-sensing (HRRS) imagery has received a lot         of attention in the literature. Despite the vast amount of expert endeavor spent on this problem, more efforts have been expected         to discover and utilize hidden semantics of images for object detection. To that end, in this paper, we address this problem         from two semantic perspectives. First, we propose a semantic-aware two-stage image segmentation approach, which preserves         the semantics of real-world objects during the segmentation process. Second, to better capture semantic features for object         discovery, we exploit a hyperclique pattern discovery method to find complex objects that consist of several co-existing individual         objects that usually form a unique semantic concept. We consider the identified groups of co-existing objects as new feature         sets and feed them into the learning model for better performance of image retrieval. Experiments with real-world datasets         show that, with reliable segmentation and new semantic features as starting points, we can improve the performance of object         discovery in terms of various external criteria.      </content></document><document><year>2008</year><authors>Charu C. Aggarwal1 </authors><title>On classification and segmentation of massive audio data streams      </title><content>In recent years, the proliferation of VOIP data has created a number of applications in which it is desirable to perform quick         online classification and recognition of massive voice streams. Typically such applications are encountered in real time intelligence         and surveillance. In many cases, the data streams can be in compressed format, and the rate of data processing can often run         at the rate of Gigabits per second. All known techniques for speaker voice analysis require the use of an offline training         phase in which the system is trained with known segments of speech. The state-of-the-art method for text-independent speaker         recognition is known as Gaussian mixture modeling (GMM), and it requires an iterative expectation maximization procedure for         training, which cannot be implemented in real time. In many real applications (such as surveillance) it is desirable to perform         the recognition process in online time, so that the system can be quickly adapted to new segments of the data. In many cases,         it may also be desirable to quickly create databases of training profiles for speakers of interest. In this paper, we discuss         the details of such an online voice recognition system. For this purpose, we use our micro-clustering algorithms to design         concise signatures of the target speakers. One of the surprising and insightful observations from our experiences with such         a system is that while it was originally designed only for efficiency, we later discovered that it was also more accurate         than the widely used GMM. This was because of the conciseness of the micro-cluster model, which made it less prone to over training. This is evidence of the fact that it is often possible         to get the best of both worlds and do better than complex models both from an efficiency and accuracy perspective. We present         experimental results illustrating the effectiveness and efficiency of the method.      </content></document><document><year>2008</year><authors>Wei-Guo Zhang1  | Wei-Lin Xiao1</authors><title>On weighted lower and upper possibilistic means and variances of fuzzy numbers and its application in decision      </title><content>In this paper, we define the weighted lower and upper possibilistic variances and covariances of fuzzy numbers. We also obtain         their many properties similar to variance and covariance in probability theory. On the basis of the weighted lower and upper         possibilistic means and variances, we present two new possibilistic portfolio selection models with tolerated risk level and         holdings of assets constraints. The conventional probabilistic mean&amp;#8211;variance model can be transformed to a linear programming         problem under possibility distributions. Finally, an estimation method of possibility distribution is offered and a real example         for portfolio selection problem is given to illustrate the usability of the approach and the effectiveness of our methods.      </content></document><document><year>2008</year><authors>BjГ¶rn Bringmann1  | Albrecht Zimmermann1 </authors><title>One in a million: picking the right patterns      </title><content>Constrained pattern mining extracts patterns based on their individual merit. Usually this results in far more patterns than         a human expert or a machine leaning technique could make use of. Often different patterns or combinations of patterns cover         a similar subset of the examples, thus being redundant and not carrying any new information. To remove the redundant information         contained in such pattern sets, we propose two general heuristic algorithms&amp;#8212;Bouncer and Picker&amp;#8212;for selecting a small subset         of patterns. We identify several selection techniques for use in this general algorithm and evaluate those on several data         sets. The results show that both techniques succeed in severely reducing the number of patterns, while at the same time apparently         retaining much of the original information. Additionally, the experiments show that reducing the pattern set indeed improves         the quality of classification results. Both results show that the developed solutions are very well suited for the goals we         aim at.      </content></document><document><year>2008</year><authors>Sven Groppe1 | Jinghua Groppe1 | Stefan BГ¶ttcher2 | Thomas Wycisk2  | Le Gruenwald3 </authors><title>Optimizing the execution of XSLT stylesheets for querying transformed XML data      </title><content>We have to deal with different data formats whenever data formats evolve or data must be integrated from heterogeneous systems.         These data when implemented in XML for data exchange cannot be shared freely among applications without data transformation.         A common approach to solve this problem is to convert the entire XML data from their source format to the applications&amp;#8217; target         formats using the transformations rules specified in XSLT stylesheets. However, in many cases, not all XML data are required         to be transformed except for a smaller part described by a user&amp;#8217;s query (application). In this paper, we present an approach         that optimizes the execution time of an XSLT stylesheet for answering a given XPath query by modifying the XSLT stylesheet         in such a way that it would (a) capture only the parts in the XML data that are relevant to the query and (b) process only         those XSLT instructions that are relevant to the query. We prove the correctness of our optimization approach, analyze its         complexity and present experimental results. The experimental results show that our approach performs the best in terms of         execution time, especially when many cost-intensive XSLT instructions can be excluded in the XSLT stylesheet.      </content></document><document><year>2008</year><authors>Xinghuo Zeng1 | Jian Pei1 | Ke Wang1  | Jinyan Li2 </authors><title>PADS: a simple yet effective pattern-aware dynamic search method for fast maximal frequent pattern mining      </title><content>While frequent pattern mining is fundamental for many data mining tasks, mining maximal frequent patterns efficiently is important         in both theory and applications of frequent pattern mining. The fundamental challenge is how to search a large space of item         combinations. Most of the existing methods search an enumeration tree of item combinations in a depth-first manner. In this         paper, we develop a new technique for more efficient max-pattern mining. Our method is pattern-aware: it uses the patterns         already found to schedule its future search so that many search subspaces can be pruned. We present efficient techniques to         implement the new approach. As indicated by a systematic empirical study using the benchmark data sets, our new approach outperforms         the currently fastest max-pattern mining algorithms FPMax* and LCM2 clearly. The source code and the executable code (on both         Windows and Linux platforms) are publicly available at http://www.cs.sfu.ca/~jpei/Software/PADS.zip.      </content></document><document><year>2008</year><authors>Anne M. Denton1 | Christopher A. Besemann1  | Dietmar H. Dorr1 </authors><title>Pattern-based time-series subsequence clustering using radial distribution functions      </title><content>Clustering of time series subsequence data commonly produces results that are unspecific to the data set. This paper introduces         a clustering algorithm, that creates clusters exclusively from those subsequences that occur more frequently in a data set         than would be expected by random chance. As such, it partially adopts a pattern mining perspective into clustering. When subsequences         are being labeled based on such clusters, they may remain without label. In fact, if the clustering was done on an unrelated         time series it is expected that the subsequences should not receive a label. We show that pattern-based clusters are indeed         specific to the data set for 7 out of 10 real-world sets we tested, and for window-lengths up to 128 time points. While kernel-density-based         clustering can be used to find clusters with similar properties for window sizes of 8&amp;#8211;16 time points, its performance degrades         fast for increasing window sizes.      </content></document><document><year>2008</year><authors>Fen Xia1 | Wensheng Zhang1| Fuxin Li1 | Yanwu Yang1</authors><title>Ranking with decision tree      </title><content>Ranking problems have recently become an important research topic in the joint field of machine learning and information retrieval.         This paper presented a new splitting rule that introduces a metric, i.e., an impurity measure, to construct decision trees         for ranking tasks. We provided a theoretical basis and some intuitive explanations for the splitting rule. Our approach is         also meaningful to collaborative filtering in the sense of dealing with categorical data and selecting relative features.         Some experiments were made to illustrate our ranking approach, whose results showed that our algorithm outperforms both perceptron-based         ranking and the classification tree algorithms in term of accuracy as well as speed.      </content></document><document><year>2008</year><authors>Hongqin Fan1| Osmar R. ZaГЇane2 | Andrew Foss2 | Junfeng Wu2</authors><title>Resolution-based outlier factor: detecting the top-n most outlying data points in engineering data      </title><content>One of the common endeavours in engineering applications is outlier detection, which aims to identify inconsistent records         from large amounts of data. Although outlier detection schemes in data mining discipline are acknowledged as a more viable         solution to efficient identification of anomalies from these data repository, current outlier mining algorithms require the         input of domain parameters. These parameters are often unknown, difficult to determine and vary across different datasets         containing different cluster features. This paper presents a novel resolution-based outlier notion and a nonparametric outlier-mining         algorithm, which can efficiently identify and rank top listed outliers from a wide variety of datasets. The algorithm generates         reasonable outlier results by taking both local and global features of a dataset into account. Experiments are conducted using         both synthetic datasets and a real life construction equipment dataset from a large road building contractor. Comparison with         the current outlier mining algorithms indicates that the proposed algorithm is more effective and can be integrated into a         decision support system to serve as a universal detector of potentially inconsistent records.      </content></document><document><year>2008</year><authors>Patrick K. L. Ng1  | Vincent T. Y. Ng1 </authors><title>RRSi: indexing XML data for proximity twig queries      </title><content>Twig query pattern matching is a core operation in XML query processing. Indexing XML documents for twig query processing         is of fundamental importance to supporting effective information retrieval. In practice, many XML documents on the web are         heterogeneous and have their own formats; documents describing relevant information can possess different structures. Therefore         some &amp;#8220;user-interesting&amp;#8221; documents having similar but non-exact structures against a user query are often missed out. In this         paper, we propose the RRSi, a novel structural index designed for structure-based query lookup on heterogeneous sources of XML documents supporting         proximate query answers. The index avoids the unnecessary processing of structurally irrelevant candidates that might show         good content relevance. An optimized version of the index, oRRSi, is also developed to further reduce both space requirements and computational complexity. To our knowledge, these structural         indexes are the first to support proximity twig queries on XML documents. The results of our preliminary experiments show         that RRSi and oRRSi based query processing significantly outperform previously proposed techniques in XML repositories with structural heterogeneity.      </content></document><document><year>2008</year><authors>Axel Blumenstock1 | Franz Schweiggert1| Markus MГјller2 | Carsten Lanquillon3</authors><title>Rule cubes for causal investigations      </title><content>With the complexity of modern vehicles tremendously increasing, quality engineers play a key role within today&amp;#8217;s automotive         industry. Field data analysis supports corrective actions in development, production and after sales support. We decompose         the requirements and show that association rules, being a popular approach to generating explanative models, still exhibit         shortcomings. Interactive rule cubes, which have been proposed recently, are a promising alternative. We extend this work         by introducing a way of intuitively visualizing and meaningfully ranking them. Moreover, we present methods to interactively         factorize a problem and validate hypotheses by ranking patterns based on expectations, and by browsing a cube-based network         of related influences. All this is currently in use as an interactive tool for warranty data analysis in the automotive industry.         A real-world case study shows how engineers successfully use it in identifying root causes of quality issues.      </content></document><document><year>2008</year><authors>Yixin Jing1 | Dongwon Jeong2  | Doo-Kwon Baik1 </authors><title>SPARQL graph pattern rewriting for OWL-DL inference queries      </title><content>This paper focuses on the issue of OWL-DL ontology queries implemented in SPARQL. Currently, ontology repositories construct         inference ontology models, and match SPARQL queries to the models, to derive inference results. Because an inference model         uses much more storage space than the original model, and cannot be reused as inference requirements vary, this method is         not suitable for large-scale deployment. To solve this problem, this paper proposes a novel method that passes rewritten SPARQL         queries to the original ontology model, to retrieve inference results. We define OWL-DL inference rules and apply them to         rewriting Graph Patterns in queries. The paper classifies the inference rules and discusses how these rules affect query rewriting.         To illustrate the advantages of our proposal, we present a prototype system based on Jena, and address query optimization,         to eliminate the disadvantages of augmented query sentences. We perform a set of query tests and compare the results with         related works. The results show that the proposed method results in significantly improved query efficiency, without compromising         completeness or soundness.      </content></document><document><year>2008</year><authors>Wei Lee Woon1  | Kuok-Shoong Daniel Wong2</authors><title>String alignment for automated document versioning      </title><content>The automated analysis of documents is an important task given the rapid increase in availability of digital texts. Automatic         text processing systems often encode documents as vectors of term occurrence frequencies, a representation which facilitates         the classification and clustering of documents. Historically, this approach derives from the related field of data mining,         where database entries are commonly represented as points in a vector space. While this lineage has certainly contributed         to the development of text processing, there are situations where document collections do not conform to this clustered structure,         and where the vector representation may be unsuitable for text analysis. As a proof-of-concept, we had previously presented         a framework where the optimal alignments of documents could be used for visualising the relationships within small sets of         documents. In this paper we develop this approach further by using it to automatically generate the version histories of various         document collections. For comparison, version histories generated using conventional methods of document representation are         also produced. To facilitate this comparison, a simple procedure for evaluating the accuracy of the version histories thus         generated is proposed.      </content></document><document><year>2008</year><authors>Anne M. Denton1 </authors><title>Subspace sums for extracting non-random data from massive noise      </title><content>An algorithm is introduced that distinguishes relevant data points from randomly distributed noise. The algorithm is related         to subspace clustering based on axis-parallel projections, but considers membership in any projected cluster of a given side         length, as opposed to a particular cluster. An aggregate measure is introduced that is based on the total number of points         that are close to the given point in all possible 2            d             projections of a d-dimensional hypercube. No explicit summation over subspaces is required for evaluating this measure. Attribute values are         normalized based on rank order to avoid making assumptions on the distribution of random data. Effectiveness of the algorithm         is demonstrated through comparison with conventional outlier detection on a real microarray data set as well as on time series         subsequence data.      </content></document><document><year>2008</year><authors>Olivier Motelet1 | Nelson Baloian1 | JosГ© A. Pino1</authors><title>Taking advantage of metadata semantics: the case of learning-object-based lesson graphs      </title><content>Learning objects (LOs) are pieces of educational material characterized with a valuable amount of information about their         content and usage. This additional information is defined as a set of metadata generally following the IEEE LOM specification.         This specification also serves to characterize the relations existing between LOs. LOs whose relations are explicit are regarded         as the nodes of a lesson graph. Link types and LO metadata constitute the lesson graph semantics. This article proposes to         take advantage of lesson graph semantics using a context diffusion approach. It consists in diffusing the metadata-based processes         along the edges of the lesson graph. This technique aims at coping with the metadata processing issues arising when some graph         metadata are missing, incorrect, or incomplete. This article also presents a three-layer extensible framework for easing the         use of context diffusion in a graph. As part of the framework, two original types of metadata processes are introduced. The         first one takes advantage of the metadata attribute similarities between related LOs. The second one focuses on the lesson         graph consistency. The framework and the application examples were implemented as an open-source Java library used in the         lesson graph authoring tool LessonMapper2. During the lesson authoring process, we show that the framework can bring support         not only for generating and validating metadata, but also for retrieving LOs.      </content></document><document><year>2008</year><authors>Pu Wang1 | Jian Hu2| Hua-Jun Zeng2 | Zheng Chen2</authors><title>Using Wikipedia knowledge to improve text classification      </title><content>Text classification has been widely used to assist users with the discovery of useful information from the Internet. However,         traditional classification methods are based on the &amp;#8220;Bag of Words&amp;#8221; (BOW) representation, which only accounts for term frequency         in the documents, and ignores important semantic relationships between key terms. To overcome this problem, previous work         attempted to enrich text representation by means of manual intervention or automatic document expansion. The achieved improvement         is unfortunately very limited, due to the poor coverage capability of the dictionary, and to the ineffectiveness of term expansion.         In this paper, we automatically construct a thesaurus of concepts from Wikipedia. We then introduce a unified framework to         expand the BOW representation with semantic relations (synonymy, hyponymy, and associative relations), and demonstrate its         efficacy in enhancing previous approaches for text classification. Experimental results on several data sets show that the         proposed approach, integrated with the thesaurus built from Wikipedia, can achieve significant improvements with respect to         the baseline algorithm.      </content></document><document><year>2008</year><authors>Xuxian Jiang1 | Xingquan Zhu2 </authors><title>vEye: behavioral footprinting for self-propagating worm detection and profiling      </title><content>With unprecedented speed, virulence, and sophistication, self-propagating worms remain as one of the most severe threats to         information systems and Internet in general. In order to mitigate the threat, efficient mechanisms are needed to accurately         profile and detect the worms before or during their outbreaks. Particularly, deriving a worm&amp;#8217;s unique signatures, or fingerprints,         is of the first priority to achieve this goal. One of the most popular approaches is to use content-based signatures, which         characterize a worm by extracting its unique information payload. In practice, such content-based signatures, unfortunately,         suffer from numerous disadvantages, such as vulnerable to content mutation attacks or not applicable for polymorphic worms.         In this paper, we propose a new behavioral footprinting (BF) approach that nicely complements the state-of-the-art content-based         signature approaches and allows users to detect and profile self-propagating worms from the unique worm behavioral perspective.         More specifically, our behavioral footprinting method uniquely captures a worm&amp;#8217;s dynamic infection sequences (e.g., probing,         exploitation, and replication) by modeling each interaction step as a behavior phenotype and denoting a complete infection         process as a chained sequence. We argue that a self-propagating worm&amp;#8217;s inherent behaviors or infection patterns can be detected         and characterized by using sequence alignment tools, where patterns shared by the infection sequences will imply the behavioral         footprints of the worm. A systematic platform called vEye has been built to validate the proposed design with either &amp;#8220;live&amp;#8221;         or historical worms, where a number of real-world infection sequences are used to build worm behavioral footprints. Experimental         comparisons with existing content-based fingerprints will demonstrate the uniqueness and effectiveness of the proposed behavior         footprints in self-propagating worm detection and profiling.      </content></document><document><year>2008</year><authors>Keke Chen1  | Ling Liu2</authors><title>&amp;#8220;Best K&amp;#8221;: critical clustering structures in categorical datasets      </title><content>The demand on cluster analysis for categorical data continues to grow over the last decade. A well-known problem in categorical         clustering is to determine the best K number of clusters. Although several categorical clustering algorithms have been developed,         surprisingly, none has satisfactorily addressed the problem of best K for categorical clustering. Since categorical data does         not have an inherent distance function as the similarity measure, traditional cluster validation techniques based on geometric         shapes and density distributions are not appropriate for categorical data. In this paper, we study the entropy property between         the clustering results of categorical data with different K number of clusters, and propose the BKPlot method to address the three important cluster validation problems: (1) How can we determine whether there is significant         clustering structure in a categorical dataset? (2) If there is significant clustering structure, what is the set of candidate         &amp;#8220;best Ks&amp;#8221;? (3) If the dataset is large, how can we efficiently and reliably determine the best Ks?      </content></document><document><year>2001</year><authors>Takuya Wada1| Tadashi Horiuchi1| Hiroshi Motoda1 | Takashi Washio1</authors><title>A Description Length-Based Decision Criterion for Default Knowledge in the Ripple Down Rules Method      </title><content>Abstract.;; The &amp;#8216;Ripple Down Rules (RDR)&amp;#8217; method is a promising approach to directly acquiring and encoding knowledge from human experts.         It requires data to be supplied incrementally to the knowledge base being constructed, each new piece of knowledge being added         as an exception to the existing knowledge base. Because of this patching principle, the knowledge acquired depends strongly         on what is given as the default knowledge, used as an implicit outcome when inference fails. Therefore, it is important to         choose good default knowledge for constructing an accurate and compact knowledge base. Further, real-world data are often         noisy and we want the RDR to be noise resistant. This paper reports experimental results about the effect of the selection         of default knowledge and the amount of noise in data on the performance of RDR, using a simulated expert in place of a human         expert. The best default knowledge is characterized as the class knowledge that maximizes the description length of encoding         rules and misclassified cases. We confirmed by extensive experimentation that this criterion is indeed valid and useful in         constructing an accurate and compact knowledge base. We also ascertained that the same criterion holds when the data are noisy.      </content></document><document><year>2001</year><authors>Seung-Jin Lim1 | Yiu-Kai Ng1</authors><title>A Hybrid Fragmentation Approach for Distributed Deductive Database Systems      </title><content>Abstract.;; Fragmentation of base relations in distributed database management systems increases the level of concurrency and therefore         system throughput for query processing. Algorithms for horizontal and vertical fragmentation of relations in relational, object-oriented         and deductive databases exist; however, hybrid fragmentation techniques based on variable bindings appearing in user queries         and query-access-rule dependency are lacking for deductive database systems. In this paper, we propose a hybrid fragmentation         approach for distributed deductive database systems. Our approach first considers the horizontal partition of base relations         according to the bindings imposed on user queries, and then generates vertical fragments of the horizontally partitioned relations         and clusters rules using affinity of attributes and access frequency of queries and rules. The proposed fragmentation technique         facilitates the design of distributed deductive database systems.      </content></document><document><year>2001</year><authors>Sheng Li1 | Qiang Yang1</authors><title>         ActiveCBR: An Agent System That Integrates Case-Based Reasoning and Active Databases      </title><content>Abstract.;; Case-based reasoning (CBR) is an artificial intelligence (AI) technique for problem solving that uses previous similar examples         to solve a current problem. Despite its success, most current CBR systems are passive: they require human users to activate         them manually and to provide information about the incoming problem explicitly. In this paper, we present an integrated agent         system that integrates CBR systems with an active database system. Active databases, with the support of active rules, can         perform event detection, condition monitoring, and event handling (action execution) in an automatic manner. The integrated         ActiveCBR system consists of two layers. In the lower layer, the active database is rule-driven; in the higher layer, the result of         action execution of active rules is transformed into feature&amp;#8211;value pairs required by the CBR subsystem. The layered architecture         separates CBR from sophisticated rule-based reasoning, and improves the traditional passive CBR system with the active property. The system has both real-time response and is highly exible in knowledge management as well as autonomously in         response to events that a passive CBR system cannot handle. We demonstrate the system efficiency and effectiveness through         empirical tests.      </content></document><document><year>2001</year><authors>L. B. Romdhane1| B. Ayeb1 | S. Wang1</authors><title>An Artificial Network Simulating Cause-to-Effect Reasoning: Cancellation Interactions and Numerical Studies      </title><content>Abstract.;; During the last few decades, a variety of models have been proposed to address causal reasoning (known also as abduction);         most of these dealt with a probabilistic or a logical framework. Recently, a few models have been proposed within a neural         framework. The investigation of neural approaches is mainly motivated by the computational burden of the causal reasoning         task and by the satisfactory results given by neural networks in solving hard problems in general. A particular class of causal         reasoning that raises several difficulties is the cancellation class. From an abstract point of view, cancellation occurs         when two causes (hypotheses) cancel each other's explanation capabilities with respect to a given effect (observation). The         present work is twofold. First, we extend an existing neural model to handle cancellation interactions. Second, we test the         model on a large database and propose objective criteria to quantitatively evaluate the scenarios (explanations) produced.         Simulation results show good performance and stability of the model.      </content></document><document><year>2001</year><authors>Julio Ortega1| Moshe Koppel2 | Shlomo Argamon2</authors><title>Arbitrating Among Competing Classifiers Using Learned Referees      </title><content>Abstract.;; The situation in which the results of several different classifiers and learning algorithms are obtainable for a single classification         problem is common. In this paper, we propose a method that takes a collection of existing classifiers and learning algorithms,         together with a set of available data, and creates a combined classifier that takes advantage of all of these sources of knowledge.         The basic idea is that each classifier has a particular subdomain for which it is most reliable. Therefore, we induce a referee for each classifier, which describes its area of expertise. Given such a description, we arbitrate between the component classifiers by using the most reliable classifier for the examples in each subdomain. In experiments         in several domains, we found such arbitration to be significantly more effective than various voting techniques which do not         seek out subdomains of expertise. Our results further suggest that the more fine grained the analysis of the areas of expertise         of the competing classifiers, the more effectively they can be combined. In particular, we find that classification accuracy         increases greatly when using intermediate subconcepts from the classifiers themselves as features for the induction of referees.      </content></document><document><year>2001</year><authors>H. A. Abbass1| M. Towsey2 | G. Finn2</authors><title>C-Net: A Method for Generating Non-deterministic and Dynamic Multivariate Decision Trees      </title><content>Abstract.;; Despite the fact that artificial neural networks (ANNs) are universal function approximators, their black box nature (that         is, their lack of direct interpretability or expressive power) limits their utility. In contrast, univariate decision trees         (UDTs) have expressive power, although usually they are not as accurate as ANNs. We propose an improvement, C-Net, for both         the expressiveness of ANNs and the accuracy of UDTs by consolidating both technologies for generating multivariate decision         trees (MDTs). In addition, we introduce a new concept, recurrent decision trees, where C-Net uses recurrent neural networks         to generate an MDT with a recurrent feature. That is, a memory is associated with each node in the tree with a recursive condition         which replaces the conventional linear one. Furthermore, we show empirically that, in our test cases, our proposed method         achieves a balance of comprehensibility and accuracy intermediate between ANNs and UDTs. MDTs are found to be intermediate         since they are more expressive than ANNs and more accurate than UDTs. Moreover, in all cases MDTs are more compact (i.e.,         smaller tree size) than UDTs.      </content></document><document><year>2001</year><authors>Andreas L. Prodromidis1 | Salvatore J. Stolfo1</authors><title>Cost Complexity-Based Pruning of Ensemble Classifiers      </title><content>Abstract.;; In this paper we study methods that combine multiple classification models learned over separate data sets. Numerous studies         posit that such approaches provide the means to efficiently scale learning to large data sets, while also boosting the accuracy         of individual classifiers. These gains, however, come at the expense of an increased demand for run-time system resources.         The final ensemble meta-classifier may consist of a large collection of base classifiers that require increased memory resources         while also slowing down classification throughput. Here, we describe an algorithm for pruning (i.e., discarding a subset of         the available base classifiers) the ensemble meta-classifier as a means to reduce its size while preserving its accuracy and         we present a technique for measuring the trade-off between predictive performance and available run-time system resources.         The algorithm is independent of the method used initially when computing the meta-classifier. It is based on decision tree         pruning methods and relies on the mapping of an arbitrary ensemble meta-classifier to a decision tree model. Through an extensive         empirical study on meta-classifiers computed over two real data sets, we illustrate our pruning algorithm to be a robust and         competitive approach to discarding classification models without degrading the overall predictive performance of the smaller         ensemble computed over those that remain after pruning.      </content></document><document><year>2001</year><authors>Eamonn Keogh1| Kaushik Chakrabarti2| Michael Pazzani1 | Sharad Mehrotra1</authors><title>Dimensionality Reduction for Fast Similarity Search in Large Time Series Databases      </title><content>Abstract.;; The problem of similarity search in large time series databases has attracted much attention recently. It is a non-trivial         problem because of the inherent high dimensionality of the data. The most promising solutions involve first performing dimensionality         reduction on the data, and then indexing the reduced data with a spatial access method. Three major dimensionality reduction         techniques have been proposed: Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and more recently         the Discrete Wavelet Transform (DWT). In this work we introduce a new dimensionality reduction technique which we call Piecewise         Aggregate Approximation (PAA). We theoretically and empirically compare it to the other techniques and demonstrate its superiority.         In addition to being competitive with or faster than the other methods, our approach has numerous other advantages. It is         simple to understand and to implement, it allows more flexible distance measures, including weighted Euclidean queries, and         the index can be built in linear time.      </content></document><document><year>2001</year><authors>Yong S. Choi1</authors><title>Discovering Text Databases with Neural Nets      </title><content>Abstract.;; Since documents on the Web are naturally partitioned into many text databases, the efficient document retrieval process requires         identifying the text databases that are most likely to provide relevant documents to the query and then searching for the         identified text databases. In this paper, we propose a neural net based approach to such an efficient document retrieval.         First, we present a neural net agent that learns about underlying text databases from the user's relevance feedback. For a         given query, the neural net agent, which is sufficiently trained on the basis of the BPN learning mechanism, discovers the         text databases associated with the relevant documents and retrieves those documents effectively. In order to scale our approach         with the large number of text databases, we also propose the hierarchical organization of neural net agents which reduces         the total training cost at the acceptable level. Finally, we evaluate the performance of our approach by comparing it to those         of the conventional well-known approaches.      </content></document><document><year>2001</year><authors>Hillol Kargupta1| Weiyun Huang2| Krishnamoorthy Sivakumar2 | Erik Johnson2</authors><title>Distributed Clustering Using Collective Principal Component Analysis      </title><content>Abstract.;; This paper considers distributed clustering of high-dimensional heterogeneous data using a distributed principal component         analysis (PCA) technique called the collective PCA. It presents the collective PCA technique, which can be used independent         of the clustering application. It shows a way to integrate the Collective PCA with a given off-the-shelf clustering algorithm         in order to develop a distributed clustering technique. It also presents experimental results using different test data sets         including an application for web mining.      </content></document><document><year>2001</year><authors>Mehmet Sayal1 | Peter Scheuermann2</authors><title>Distributed Web Log Mining Using Maximal         Large Itemsets      </title><content>Abstract.;; We introduce a partitioning-based distributed document-clustering algorithm using user access patterns from multi-server         web sites. Our algorithm makes it possible to exploit simultaneously adaptive document replication and persistent connections,         two techniques that are most effective in decreasing the response time that is observed by web users. The algorithm first         distributes the user access data evenly among the servers by using a hash function. Then, each server generates a local clustering         on its fair share of the user sessions records by employing a traditional single-machine document-clustering algorithm. Finally,         those local clustering results are combined together by using a novel procedure that generates maximal large itemsets of web         documents. We present preliminary experimental results and discuss alternative approaches to be pursued in the future.      </content></document><document><year>2001</year><authors>Guanling Lee1| K. L. Lee1 | Arbee L. P. Chen1</authors><title>Efficient Graph-Based Algorithms for Discovering and Maintaining Association Rules in Large Databases      </title><content>Abstract.;; In this paper, we study the issues of mining and maintaining association rules in a large database of customer transactions.         The problem of mining association rules can be mapped into the problems of finding large itemsets which are sets of items brought together in a sufficient number of transactions. We revise a graph-based algorithm to further         speed up the process of itemset generation. In addition, we extend our revised algorithm to maintain discovered association         rules when incremental or decremental updates are made to the databases. Experimental results show the efficiency of our algorithms.         The revised algorithm is a significant improvement over the original one on mining association rules. The algorithms for maintaining         association rules are more efficient than re-running the mining algorithms for the whole updated database and outperform previously         proposed algorithms that need multiple passes over the database.      </content></document><document><year>2001</year><authors>R. I. John1 | G. J. Mooney1</authors><title>Fuzzy User Modeling for Information Retrieval on the World Wide Web      </title><content>Abstract.;; Information retrieval from the World Wide Web through the use of search engines is known to be unable to capture effectively         the information needs of users. The approach taken in this paper is to add intelligence to information retrieval from the         World Wide Web, by the modeling of users to improve the interaction between the user and information retrieval systems. In         other words, to improve the performance of the user in retrieving information from the information source. To effect such         an improvement, it is necessary that any retrieval system should somehow make inferences concerning the information the user         might want. The system then can aid the user, for instance by giving suggestions or by adapting any query based on predictions         furnished by the model. So, by a combination of user modeling and fuzzy logic a prototype system has been developed (the Fuzzy         Modeling Query Assistant (FMQA)) which modifies a user's query based on a fuzzy user model. The FMQA was tested via a user         study which clearly indicated that, for the limited domain chosen, the modified queries are better than those that are left         unmodified.      </content></document><document><year>2001</year><authors>Mei-Ling Shyu1| Shu-Ching Chen2 | R. L. Kashyap3</authors><title>Generalized Affinity-Based Association Rule Mining for Multimedia Database Queries      </title><content>Abstract.;; The recent progress in high-speed communication networks and large-capacity storage devices has led to a tremendous increase         in the number of databases and the volume of data in them. This has created a need to discover structural equivalence relationships         from the databases since queries tend to access information from structurally equivalent media objects residing in different         databases. The more databases there are, the more query-processing performance improvement can be achieved when the structural         equivalence relationships are automatically discovered. In response to such a demand, association rule mining has emerged         and proven to be a highly successful technique for discovering knowledge from large databases. In this paper, we explore a         generalized affinity-based association rule mining approach to discover the quasi-equivalence relationships from a network         of databases. The algorithm is implemented and two empirical studies on real databases are conducted. The results show that         the proposed generalized affinity-based association rule mining approach not only correctly exploits the set of quasi-equivalent         media objects from the databases, but also outperforms the basic association rule mining approach in the discovery of the         quasi-equivalent media object pairs.      </content></document><document><year>2001</year><authors>Jinyan Li1| Guozhu Dong2 | Kotagiri Ramamohanarao1</authors><title>Making Use of the Most Expressive Jumping Emerging Patterns for Classification      </title><content>Abstract.;; Classification aims to discover a model from training data that can be used to predict the class of test instances. In this         paper, we propose the use of jumping emerging patterns (JEPs) as the basis for a new classifier called the JEP-Classifier. Each JEP can capture some crucial difference between a pair of datasets. Then, aggregating all JEPs of large supports can         produce a more potent classification power. Procedurally, the JEP-Classifier learns the pair-wise features (sets of JEPs) contained in the training data, and uses the collective impacts contributed by the most expressive pair-wise features to determine the class labels of the test data. Using only the most expressive JEPs in the JEP-Classifier         strengthens its resistance to noise in the training data, and reduces its complexity (as there are usually a very large number         of JEPs). We use two algorithms for constructing the JEP-Classifier which are both scalable and efficient. These algorithms         make use of the border representation to efficiently store and manipulate JEPs. We also present experimental results which show that the JEP-Classifier achieves         much higher testing accuracies than the association-based classifier of (Liu et al, 1998), which was reported to outperform         C4.5 in general.      </content></document><document><year>2001</year><authors>John D. Holt1 | Soon M. Chung1</authors><title>Multipass Algorithms for Mining Association Rules in Text Databases      </title><content>Abstract.;; In this paper, we propose two new algorithms for mining association rules between words in text databases. The characteristics         of text databases are quite different from those of retail transaction databases, and existing mining algorithms cannot handle         text databases efficiently because of the large number of itemsets (i.e., words) that need to be counted. Two well-known mining         algorithms, Apriori algorithm and Direct Hashing and Pruning (DHP) algorithm, are evaluated in the context of mining text         databases, and are compared with the new proposed algorithms named Multipass-Apriori (M-Apriori) and Multipass-DHP (M-DHP).         It has been shown that the proposed algorithms have better performance for large text databases.      </content></document><document><year>2001</year><authors>Stephen D. Bay1</authors><title>Multivariate Discretization for Set Mining      </title><content>Abstract.;; Many algorithms in data mining can be formulated as a set-mining problem where the goal is to find conjunctions (or disjunctions)         of terms that meet user-specified constraints. Set-mining techniques have been largely designed for categorical or discrete         data where variables can only take on a fixed number of values. However, many datasets also contain continuous variables and         a common method of dealing with these is to discretize them by breaking them into ranges. Most discretization methods are         univariate and consider only a single feature at a time (sometimes in conjunction with a class variable). We argue that this         is a suboptimal approach for knowledge discovery as univariate discretization can destroy hidden patterns in data. Discretization         should consider the effects on all variables in the analysis and that two regions X and Y should only be in the same interval         after discretization if the instances in those regions have similar multivariate distributions (F                     x            &amp;#8764;F                     y            ) across all variables and combinations of variables. We present a bottom-up merging algorithm to discretize continuous variables         based on this rule. Our experiments indicate that the approach is feasible, that it will not destroy hidden patterns and that         it will generate meaningful intervals.      </content></document><document><year>2001</year><authors>K. SelГ§uk C|an1 | Wen-Syan Li1</authors><title>On Similarity Measures for Multimedia Database Applications      </title><content>Abstract.;; A multimedia database query consists of a set of fuzzy and boolean (or crisp) predicates, constants, variables, and conjunction,         disjunction, and negation operators. The fuzzy predicates are evaluated based on different media criteria, such as color,         shape, layout, keyword. Since media-based evaluation yields similarity values, results to such a query is defined as an ordered         set. Since many multimedia applications require partial matches, query results also include tuples which do not satisfy all         predicates. Hence, any fuzzy semantics which extends the boolean semantics of conjunction in a straight forward manner may         not be desirable for multimedia databases. In this paper, we focus on the problem of &amp;#8216;given a multimedia query which consists of multiple fuzzy and crisp predicates, how to provide the user with a meaningful            overall ranking.&amp;#8217; More specifically, we study the problem of merging similarity values in queries with multiple fuzzy predicates. We describe         the essential multimedia retrieval semantics, compare these with the known approaches, and propose a semantics which captures         the retrieval requirements in multimedia databases.      </content></document><document><year>2001</year><authors>David B. Skillicorn1 | Yu Wang2</authors><title>Parallel and Sequential Algorithms for Data Mining Using Inductive Logic      </title><content>Abstract.;; Inductive logic is a research area in the intersection of machine learning and logic programming, and has been increasingly         applied to data mining. Inductive logic studies learning from examples, within the framework provided by clausal logic. It         provides a uniform and expressive means of representation: examples, background knowledge, and induced theories are all expressed         in first-order logic. Such an expressive representation is computationally expensive, so it is natural to consider improving         the performance of inductive logic data mining using parallelism. We present a parallelization technique for inductive logic,         and implement a parallel version of a core inductive logic programming system: Progol. The technique provides perfect partitioning         of computation and data access and communication requirements are small, so almost linear speedup is readily achieved. However,         we also show why the information flow of the technique permits superlinear speedup over the standard sequential algorithm.         Performance results on several datasets and platforms are reported. The results have wider implications for the design on         parallel and sequential data-mining algorithms.      </content></document><document><year>2001</year><authors>S. Parthasarathy1| M. J. Zaki2| M. Ogihara3 | W. Li4</authors><title>Parallel Data Mining for Association Rules on Shared-Memory Systems      </title><content>Abstract.;; In this paper we present a new parallel algorithm for data mining of association rules on shared-memory multiprocessors.         We study the degree of parallelism, synchronization, and data locality issues, and present optimizations for fast frequency         computation. Experiments show that a significant improvement of performance is achieved using our proposed optimizations.         We also achieved good speed-up for the parallel algorithm.                     A lot of data-mining tasks (e.g. association rules, sequential patterns) use complex pointer-based data structures (e.g. hash               trees) that typically suffer from suboptimal data locality. In the multiprocessor case shared access to these data structures may also result in false sharing. For these tasks it is commonly observed that the recursive data structure is built once and accessed multiple times during               each iteration. Furthermore, the access patterns after the build phase are highly ordered. In such cases locality and false               sharing sensitive memory placement of these structures can enhance performance significantly. We evaluate a set of placement               policies for parallel association discovery, and show that simple placement schemes can improve execution time by more than               a factor of two. More complex schemes yield additional gains.            </content></document><document><year>2001</year><authors>M.-S. Hacid1| J.-M. Petit2 | F. Toumani2</authors><title>Representing and Reasoning on Database Conceptual Schemas      </title><content>Abstract.;; In this paper we address the problem of reasoning about database conceptual schemas by exploiting the possibility of using         a description logic. We develop an approach by using as a foundation an entity-relationship model that displays features such         as ISA, disjointness and cardinality constraints. We propose an equivalence-preserving transformation of entity relationship schemas         into terminologies in a description logic. This equivalence, based on the measure of information capacity, ensures that the         semantics of entity-relationship schemas is accurately captured by the corresponding terminologies. As a consequence, reasoning         on entity-relationship schemas is appropriately reduced to reasoning on terminologies in a description logic.      </content></document><document><year>2001</year><authors>Ujjwal Maulik1| Sanghamitra B|yopadhyay2 | John C. Trinder3</authors><title>SAFE: An Efficient Feature Extraction Technique      </title><content>Abstract.;; This paper proposes an efficient window-based semi-automatic feature extraction technique which uses simulated annealing         for minimizing the energy of an active contour within a specified image region. The energy is computed based on a chamfer         image, in which pixel values are a function of distance to image edges. A user places a number of control points close to         the feature of interest. B-spline fitted to these points provides an initial approximation of the contour. A window containing         both the initial contour and the feature of interest is considered. The contour with minimum energy inside the window provides         the final delineation. Comparison of the performance of the proposed algorithm with traditional snake, a popular feature extraction technique based on energy minimization, demonstrates the superiority of the SAFE technique.      </content></document><document><year>2001</year><authors>Juan C. Augusto1 | Guillermo R. Simari1</authors><title>Temporal Defeasible Reasoning      </title><content>Abstract.;; An argumentation system that allows temporal reasoning using the notions of instant and interval is presented. Previous proposals just considered either instants or intervals. A many-sorted logic is used to represent temporal         knowledge at the monotonic level. The logic considers how to formalize knowledge about explicit temporal references, events,         properties and actions. The argumentation system provides a non-monotonic layer in which to reason about the justification         of truths in the system. The proposal is illustrated showing how to solve well-known problems of the literature.      </content></document><document><year>2001</year><authors>Dongwook Shin1</authors><title>XML Indexing and Retrieval with a Hybrid Storage Model      </title><content>Abstract.;; XML DTD (Document Type Declaration) puts two distinctive entities (attribute and element content) together into one framework         for representing different document features. The notion of attribute in the XML DTD is similar to the field representation         in the database, whereas the element content corresponds to the full text. In this paper, we view these two entities as different,         each of which requires a different model for storage and retrieval. Attributes are stored in a database system, whereas the         element contents and their indices are saved in files. We present a technique that puts together those two in an efficient         way and builds an XML retrieval system on top of that. Such a system can achieve a reasonable trade-off between performance         and cost in indexing and retrieval.      </content></document><document><year>2001</year><authors>Mark Levene1| JosГ© Borges1 | George Loizou2</authors><title>Zipf's Law for Web Surfers      </title><content>Abstract.;; One of the main activities of Web users, known as &amp;#8216;surfing&amp;#8217;, is to follow links. Lengthy navigation often leads to disorientation         when users lose track of the context in which they are navigating and are unsure how to proceed in terms of the goal of their         original query. Studying navigation patterns of Web users is thus important, since it can lead us to a better understanding         of the problems users face when they are surfing. We derive Zipf's rank frequency law (i.e., an inverse power law) from an         absorbing Markov chain model of surfers' behavior assuming that less probable navigation trails are, on average, longer than         more probable ones. In our model the probability of a trail is interpreted as the relevance (or &amp;#8216;value&amp;#8217;) of the trail. We         apply our model to two scenarios: in the first the probability of a user terminating the navigation session is independent         of the number of links he has followed so far, and in the second the probability of a user terminating the navigation session         increases by a constant each time the user follows a link. We analyze these scenarios using two sets of experimental data         sets showing that, although the first scenario is only a rough approximation of surfers' behavior, the data is consistent         with the second scenario and can thus provide an explanation of surfers' behavior.      </content></document><document><year>2006</year><authors>Cane Wing-ki Leung1 | Stephen Chi-fai Chan1 | Fu-lai Chung1</authors><title>A collaborative filtering framework based on fuzzy association rules and multiple-level similarity</title><content>The rapid development of Internet technologies in recent decades has imposed a heavy information burden on users. This has led to the popularity of recommender systems, which provide advice to users about items they may like to examine. Collaborative Filtering (CF) is the most promising technique in recommender systems, providing personalized recommendations to users based on their previously expressed preferences and those of other similar users. This paper introduces a CF framework based on Fuzzy Association Rules and Multiple-level Similarity (FARAMS). FARAMS extended existing techniques by using fuzzy association rule mining, and takes advantage of product similarities in taxonomies to address data sparseness and nontransitive associations. Experimental results show that FARAMS improves prediction quality, as compared to similar approaches.</content></document><document><year>2006</year><authors>Martin Hepp1| 2 | Joerg Leukel3  | Volker Schmitz4 </authors><title>A quantitative analysis of product categorization standards: content, coverage, and maintenance of eCl@ss, UNSPSC, eOTD, and         the RosettaNet Technical Dictionary      </title><content>Many e-business scenarios require the integration of product-related data into target applications or target documents at         the recipient&amp;#8217;s side. Such tasks can be automated much better if the textual descriptions are augmented by a machine-feasible         representation of the product semantics. For this purpose, categorization standards for products and services, like UNSPSC,         eCl@ss, the ECCMA Open Technical Dictionary (eOTD), or the RosettaNet Technical Dictionary (RNTD) are available, but they         vary in terms of structural properties and content. In this paper, we present metrics for assessing the content quality and         maturity of such standards and apply these metrics to eCl@ss, UNSPSC, eOTD, and RNTD. Our analysis shows that (1) the amount         of content is very unevenly spread over top-level categories, which contradicts the promise of a broad scope implicitly made         by the existence of a large number of top-level categories, and that (2) more expressive structural features exist only for         parts of these standards. Additionally, we (3) measure the amount of maintenance in the various top-level categories, which         helps identify the actively maintained subject areas as compared to those which ones are rather dead branches. Finally, we         show how our approach can be used (4) by enterprises for selecting an appropriate standard, and (5) by standards bodies for         monitoring the maintenance of a standard as a whole.      </content></document><document><year>2006</year><authors>Niina Haiminen1 | Aristides Gionis1 | Kari Laasonen1</authors><title>Algorithms for unimodal segmentation with applications to unimodality detection      </title><content>We study the problem of segmenting a sequence into k pieces so that the resulting segmentation satisfies monotonicity or unimodality constraints. Unimodal functions can be used         to model phenomena in which a measured variable first increases to a certain level and then decreases. We combine a well-known         unimodal regression algorithm with a simple dynamic-programming approach to obtain an optimal quadratic-time algorithm for         the problem of unimodal k-segmentation. In addition, we describe a more efficient greedy-merging heuristic that is experimentally shown to give solutions         very close to the optimal. As a concrete application of our algorithms, we describe methods for testing if a sequence behaves         unimodally or not. The methods include segmentation error comparisons, permutation testing, and a BIC-based scoring scheme.         Our experimental evaluation shows that our algorithms and the proposed unimodality tests give very intuitive results, for         both real-valued and binary data.      </content></document><document><year>2006</year><authors>Jia-Wen Wang1  | Ching-Hsue Cheng1</authors><title>An efficient method for estimating null values in relational databases      </title><content>Generally, a database system containing null value attributes will not operate properly. This study proposes an efficient         and systematic approach for estimating null values in a relational database which utilizes clustering algorithms to cluster         data, and a regression coefficient to determine the degree of influence between different attributes. Two databases are used         to verify the proposed method: (1) Human resource database; and (2) Waugh's database. Furthermore, the mean of absolute error         rate (MAER) and average error are used as evaluation criteria to compare the proposed method with other methods. It demonstrates         that the proposed method is superior to existing methods for estimating null values in relational database systems.      </content></document><document><year>2006</year><authors>Waralak Vongdoiwang1  | Dencho N. Batanov1 </authors><title>An ontology-based procedure for generating object model from text description</title><content>The main objective of the procedure proposed in this paper is to use ontologies to convert a problem domain text description into an object model. The object model of a system consists of objects, identified from the text description and structural linkages corresponding to existing or established relationships. The ontologies provide metadata schemas, offering a controlled vocabulary of concepts. At the center of both object models and ontologies are objects within a given problem domain. The difference is that while the object model should contain explicitly shown structural dependencies between objects in a system, including their properties, relationships, events, and processes, the ontologies are based on related terms only. On the other hand, the object model refers to the collections of concepts used to describe the generic characteristics of objects in object-oriented languages. Because ontology is accepted as a formal, explicit specification of a shared conceptualization, we can naturally link ontologies with object models, which represent a system-oriented map of related objects, described as Abstract Data Types (ADTs). This paper addresses ontologies as a basis of a complete methodology for object modeling, including available tools, particularly CORPORUM OntoExtract and VisualText, which can help the conversion process. This paper describes how the developers can implement this methodology on the basis of an illustrative example.</content></document><document><year>2006</year><authors>Moonjung Cho1 | Jian Pei2  | Ke Wang2 </authors><title>Answering ad hoc aggregate queries from data streams using prefix aggregate trees      </title><content>In some business applications such as trading management in financial institutions, it is required to accurately answer ad         hoc aggregate queries over data streams. Materializing and incrementally maintaining a full data cube or even its compression         or approximation over a data stream is often computationally prohibitive. On the other hand, although previous studies proposed         approximate methods for continuous aggregate queries, they cannot provide accurate answers. In this paper, we develop a novel         prefix aggregate tree (PAT) structure for online warehousing data streams and answering ad hoc aggregate queries. Often, a data stream can be partitioned         into the historical segment, which is stored in a traditional data warehouse, and the transient segment, which can be stored in a PAT to answer ad hoc aggregate queries. The size of a PAT is linear in the size of the transient         segment, and only one scan          of the data stream is needed to create and incrementally maintain a PAT. Although the query answering using PAT costs more         than the case of a fully materialized data cube, the query answering time is still kept linear in the size of the transient         segment. Our extensive experimental results on both synthetic and real data sets illustrate the efficiency and the scalability         of our design.      </content></document><document><year>2006</year><authors>Xiangji Huang1 | Qingsong Yao2 | Aijun An2</authors><title>Applying language modeling to session identification from database trace logs</title><content>A database session is a sequence of requests presented to the database system by a user or an application to achieve a certain task. Session identification is an important step in discovering useful patterns from database trace logs. The discovered patterns can be used to improve the performance of database systems by prefetching predicted queries, rewriting the current query or conducting effective cache replacement.In this paper, we present an application of a new session identification method based on statistical language modeling to database trace logs. Several problems of the language modeling based method are revealed in the application, which include how to select values for the parameters of the language model, how to evaluate the accuracy of the session identification result and how to learn a language model without well-labeled training data. All of these issues are important in the successful application of the language modeling based method for session identification. We propose solutions to these open issues. In particular, new methods for determining an entropy threshold and the order of the language model are proposed. New performance measures are presented to better evaluate the accuracy of the identified sessions. Furthermore, three types of learning methods, namely, learning from labeled data, learning from semi-labeled data and learning from unlabeled data, are introduced to learn language models from different types of training data. Finally, we report experimental results that show the effectiveness of the language model based method for identifying sessions from the trace logs of an OLTP database application and the TPC-C Benchmark.</content></document><document><year>2006</year><authors>Sabyasachi Basu1  | Martin Meckesheimer1 </authors><title>Automatic outlier detection for time series: an application to sensor data      </title><content>In this article we consider the problem of detecting unusual values or outliers from time series data where the process by         which the data are created is difficult to model. The main consideration is the fact that data closer in time are more correlated         to each other than those farther apart. We propose two variations of a method that uses the median from a neighborhood of         a data point and a threshold value to compare the difference between the median and the observed data value. Both variations         of the method are fast and can be used for data streams that occur in quick succession such as sensor data on an airplane.      </content></document><document><year>2006</year><authors>Xiaojun Wan1 </authors><title>Beyond topical similarity: a structural similarity measure for retrieving highly similar documents      </title><content>Accurately measuring document similarity is important for many text applications, e.g. document similarity search, document         recommendation, etc. Most traditional similarity measures are based only on &amp;#8220;bag of words&amp;#8221; of documents and can well evaluate         document topical similarity. In this paper, we propose the notion of document structural similarity, which is expected to         further evaluate document similarity by comparing document subtopic structures. Three related factors (i.e. the optimal matching         factor, the text order factor and the disturbing factor) are proposed and combined to evaluate document structural similarity,         among which the optimal matching factor plays the key role and the other two factors rely on its results. The experimental         results demonstrate the high performance of the optimal matching factor for evaluating document topical similarity, which         is as well as or better than most popular measures. The user study shows the good ability of the proposed overall measure         with all three factors to further find highly similar documents from those topically similar documents, which is much better         than that of the popular measures and other baseline structural similarity measures.      </content></document><document><year>2006</year><authors>Carson Kai-Sang Leung1 | Quamrul I. Khan1| Zhan Li1 | Tariqul Hoque1</authors><title>CanTree: a canonical-order tree for incremental frequent-pattern mining      </title><content>Since its introduction, frequent-pattern mining has been the subject of numerous studies, including incremental updating.         Many existing incremental mining algorithms are Apriori-based, which are not easily adoptable to FP-tree-based frequent-pattern         mining. In this paper, we propose a novel tree structure, called CanTree (canonical-order tree), that captures the content of the transaction database and orders tree nodes according to some canonical order. By exploiting         its nice properties, the CanTree can be easily maintained when database transactions are inserted, deleted, and/or modified.         For example, the CanTree does not require adjustment, merging, and/or splitting of tree nodes during maintenance. No rescan         of the entire updated database or reconstruction of a new tree is needed for incremental updating. Experimental results show         the effectiveness of our CanTree in the incremental mining of frequent patterns. Moreover, the applicability of CanTrees is         not confined to incremental mining; CanTrees can also be applicable to other frequent-pattern mining tasks including constrained         mining and interactive mining.      </content></document><document><year>2006</year><authors>Jian Tang1 | Zhixiang Chen2 | Ada Waichee Fu3  | David W. Cheung4 </authors><title>Capabilities of outlier detection schemes in large datasets, framework and methodologies</title><content>Outlier detection is concerned with discovering exceptional behaviors of objects. Its theoretical principle and practical implementation lay a foundation for some important applications such as credit card fraud detection, discovering criminal behaviors in e-commerce, discovering computer intrusion, etc. In this paper, we first present a unified model for several existing outlier detection schemes, and propose a compatibility theory, which establishes a framework for describing the capabilities for various outlier formulation schemes in terms of matching users'intuitions. Under this framework, we show that the density-based scheme is more powerful than the distance-based scheme when a dataset contains patterns with diverse characteristics. The density-based scheme, however, is less effective when the patterns are of comparable densities with the outliers. We then introduce a connectivity-based scheme that improves the effectiveness of the density-based scheme when a pattern itself is of similar density as an outlier. We compare density-based and connectivity-based schemes in terms of their strengths and weaknesses, and demonstrate applications with different features where each of them is more effective than the other. Finally, connectivity-based and density-based schemes are comparatively evaluated on both real-life and synthetic datasets in terms of recall, precision, rank power and implementation-free metrics.</content></document><document><year>2006</year><authors>Yun Chi1 | Haixun Wang2 | Philip S. Yu2  | Richard R. Muntz1 </authors><title>Catch the moment: maintaining closed frequent itemsets over a data stream sliding window</title><content>This paper considers the problem of mining closed frequent itemsets over a data stream sliding window using limited memory space. We design a synopsis data structure to monitor transactions in the sliding window so that we can output the current closed frequent itemsets at any time. Due to time and memory constraints, the synopsis data structure cannot monitor all possible itemsets. However, monitoring only frequent itemsets will make it impossible to detect new itemsets when they become frequent. In this paper, we introduce a compact data structure, the closed enumeration tree (CET), to maintain a dynamically selected set of itemsets over a sliding window. The selected itemsets contain a boundary between closed frequent itemsets and the rest of the itemsets. Concept drifts in a data stream are reflected by boundary movements in the CET. In other words, a status change of any itemset (e.g., from non-frequent to frequent) must occur through the boundary. Because the boundary is relatively stable, the cost of mining closed frequent itemsets over a sliding window is dramatically reduced to that of mining transactions that can possibly cause boundary movements in the CET. Our experiments show that our algorithm performs much better than representative algorithms for the sate-of-the-art approaches.</content></document><document><year>2006</year><authors>Ankur M. Teredesai1 | Muhammad A. Ahmad1| Juveria Kanodia1 | Roger S. Gaborski1</authors><title>CoMMA: a framework for integrated multimedia mining using multi-relational associations</title><content>Generating captions or annotations automatically for still images is a challenging task. Traditionally, techniques involving higher-level (semantic) object detection and complex feature extraction have been employed for scene understanding. On the basis of this understanding, corresponding text descriptions are generated for a given image. In this paper, we pose the auto-annotation problem as that of multi-relational association rule mining where the relations exist between image-based features, and textual annotations. The central idea is to combine low-level image features such as color, orientation, intensity, etc. and corresponding text annotations to generate association rules across multiple tables using multi-relational association mining. Subsequently, we use these association rules to auto-annotate test images.In this paper we also present a multi-relational extension to the FP-tree algorithm to accomplish the association rule mining task effectively. The motivation for using multi-relational association rule mining for multimedia data mining is to exhibit the potential accorded by multiple descriptions for the same image (such as multiple people labeling the same image differently). Moreover, multi-relational association rule mining can also benefit the auto-annotation process by pruning the number of trivial associations that are generated if text and image features were combined in a single table through a join. In this paper, we discuss these issues and the results of our auto-annotation experiments on different test sets. Another contribution of this paper is highlighting a need to develop robust evaluation metrics for the image annotation task. We propose several applicable scoring techniques and then evaluate the performance of the different algorithms to study the utility of these techniques. A detailed analysis of the datasets used and the performance results are presented to conclude the paper.</content></document><document><year>2006</year><authors>Bugra Gedik1| 2 | Kun-Lung Wu1| Philip S. Yu1 | Ling Liu2</authors><title>CPU load shedding for binary stream joins      </title><content>We present an adaptive load shedding approach for windowed stream joins. In contrast to the conventional approach of dropping         tuples from the input streams, we explore the concept ofselective processing for load shedding. We allow stream tuples to be stored in the windows and shed excessive CPU load by performing the join         operations, not on the entire set of tuples within the windows, but on a dynamically changing subset of tuples that are learned         to be highly beneficial. We support such dynamic selective processing through three forms of runtimeadaptations: adaptation to input stream rates, adaptation to time correlation between the streams and adaptation to join directions.         Our load shedding approach enables us to integrateutility-based load shedding withtime correlation-based load shedding. Indexes are used to further speed up the execution of stream joins. Experiments are conducted to evaluate         our adaptive load shedding in terms of output rate and utility. The results show that our selective processing approach to         load shedding is very effective and significantly outperforms the approach that drops tuples from the input streams.      </content></document><document><year>2006</year><authors>Laure Berti-Г‰quille1 </authors><title>Data quality awareness: a case study for cost optimal association rule mining      </title><content>The quality of discovered association rules is commonly evaluated by interestingness measures (commonly support and confidence)         with the purpose of supplying indicators to the user in the understanding and use of the new discovered knowledge. Low-quality         datasets have a very bad impact over the quality of the discovered association rules, and one might legitimately wonder if         a so-called &amp;#8220;interesting&amp;#8221; rule noted LHS&amp;#8594; RHS is meaningful when 30% of the LHS data are not up-to-date anymore, 20% of the RHS data are not accurate, and 15% of the LHS data come from a data source that is well-known for its bad credibility. This paper presents an overview of data quality         characterization and management techniques that can be advantageously employed for improving the quality awareness of the         knowledge discovery and data mining processes. We propose to integrate data quality indicators for quality aware association         rule mining. We propose a cost-based probabilistic model for selecting legitimately interesting rules. Experiments on the challenging KDD-Cup-98 datasets show that variations on data quality have a great impact on the         cost and quality of discovered association rules and confirm our approach for the integrated management of data quality indicators         into the KDD process that ensure the quality of data mining results.      </content></document><document><year>2006</year><authors>Deepak Agarwal1 </authors><title>Detecting anomalies in cross-classified streams: a Bayesian approach</title><content>We consider the problem of detecting anomalies in data that arise as multidimensional arrays with each dimension corresponding to the levels of a categorical variable. In typical data mining applications, the number of cells in such arrays are usually large. Our primary focus is detecting anomalies by comparing information at the current time to historical data. Naive approaches advocated in the process control literature do not work well in this scenario due to the multiple testing problem&amp;#8212;performing multiple statistical tests on the same data produce excessive number of false positives. We use an empirical Bayes method which works by fitting a two-component Gaussian mixture to deviations at current time. The approach is scalable to problems that involve monitoring massive number of cells and fast enough to be potentially useful in many streaming scenarios. We show the superiority of the method relative to a naive &amp;#8220;per component error rate&amp;#8221; procedure through simulation. A novel feature of our technique is the ability to suppress deviations that are merely the consequence of sharp changes in the marginal distributions. This research was motivated by the need to extract critical application information and business intelligence from the daily logs that accompany large-scale spoken dialog systems. We illustrate our method on one such system.</content></document><document><year>2006</year><authors>Ji Zhang1  | Hai Wang2</authors><title>Detecting outlying subspaces for high-dimensional data: the new task, algorithms, and performance</title><content>In this paper, we identify a new task for studying the outlying degree (OD) of high-dimensional data, i.e. finding the subspaces (subsets of features) in which the given points are outliers, which are called their outlying subspaces. Since the state-of-the-art outlier detection techniques fail to handle this new problem, we propose a novel detection algorithm, called High-Dimension Outlying subspace Detection (HighDOD), to detect the outlying subspaces of high-dimensional data efficiently. The intuitive idea of HighDOD is that we measure the OD of the point using the sum of distances between this point and itsknearest neighbors. Two heuristic pruning strategies are proposed to realize fast pruning in the subspace search and an efficient dynamic subspace search method with a sample-based learning process has been implemented. Experimental results show that HighDOD is efficient and outperforms other searching alternatives such as the naive top&amp;#8211;down, bottom&amp;#8211;up and random search methods, and the existing outlier detection methods cannot fulfill this new task effectively.</content></document><document><year>2006</year><authors>K. SelГ§uk C|an1 | Jong Wook Kim1 | Huan Liu1  | Reshma Suvarna1 </authors><title>Discovering mappings in hierarchical data from multiple sources using the inherent structure</title><content>Unprecedented amounts of media data are publicly accessible. However, it is increasingly difficult to integrate relevant media from multiple and diverse sources for effective applications. The functioning of a multimodal integration system requires metadata, such as ontologies, that describe media resources and media components. Such metadata are generally application-dependent and this can cause difficulties when media needs to be shared across application domains. There is a need for a mechanism that can relate the common and uncommon terms and media components. In this paper, we develop an algorithm to mine and automatically discover mappings in hierarchical media data, metadata, and ontologies, using the structural information inherent in these types of data. We evaluate the performance of this algorithm for various parameters using both synthetic and real-world data collections and show that the structure-based mining of relationships provides high degrees of precision.</content></document><document><year>2006</year><authors>Li Wei1 | Eamonn Keogh1| Helga Van Herle2| Agenor Mafra-Neto3 | Russell J. Abbott4</authors><title>Efficient query filtering for streaming time series with applications to semisupervised learning of time series classifiers      </title><content>In this paper, we define time series query filtering, the problem of monitoring the streaming time series for a set of predefined patterns. This problem is of great practical         importance given the massive volume of streaming time series available through sensors, medical patient records, financial         indices and space telemetry. Since the data may arrive at a high rate and the number of predefined patterns can be relatively         large, it may be impossible for the comparison algorithm to keep up. We propose a novel technique that exploits the commonality         among the predefined patterns to allow monitoring at higher bandwidths, while maintaining a guarantee of no false dismissals.         Our approach is based on the widely used envelope-based lower-bounding technique. As we will demonstrate on extensive experiments         in diverse domains, our approach achieves tremendous improvements in performance in the offline case, and significant improvements         in the fastest possible arrival rate of the data stream that can be processed with guaranteed no false dismissals. As a further         demonstration of the utility of our approach, we demonstrate that it can make semisupervised learning of time series classifiers         tractable.      </content></document><document><year>2006</year><authors>Gong Chen1 | Xindong Wu1 | Xingquan Zhu1 | Abdullah N. Arslan1  | Yu He1 </authors><title>Efficient string matching with wildcards and length constraints</title><content>This paper defines a challenging problem of pattern matching between a pattern P and a text T, with wildcards and length constraints, and designs an efficient algorithm to return each pattern occurrence in an online manner. In this pattern matching problem, the user can specify the constraints on the number of wildcards between each two consecutive letters of P and the constraints on the length of each matching substring in T. We design a complete algorithm, SAIL that returns each matching substring of P in T as soon as it appears in T in an O(n+klmg) time with an O(lm) space overhead, where n is the length of T, k is the frequency of P's last letter occurring in T, l is the user-specified maximum length for each matching substring, m is the length of P, and g is the maximum difference between the user-specified maximum and minimum numbers of wildcards allowed between two consecutive letters in P.</content></document><document><year>2006</year><authors>Yannis Tzitzikas1| 2 </authors><title>Evolution of faceted taxonomies and CTCA expressions      </title><content>A faceted taxonomy is a set of taxonomies each describing the application domain from a different (preferably orthogonal)         point of view. CTCA is an algebra that allows specifying the set of meaningful compound terms (meaningful conjunctions of         terms) over a faceted taxonomy in a flexible and efficient manner. However, taxonomy updates may turn a CTCA expression e not well-formed and may turn the compound terms specified by e to no longer reflect the domain knowledge originally expressed in e. This paper shows how we can revise e after a taxonomy update and reach an expression e&amp;#8242; that is both well-formed and whose semantics (compound terms defined) is as close as possible to the semantics of the original         expression e before the update. Various cases are analyzed and the revising algorithms are given. The proposed technique can enhance the         robustness and usability of systems that are based on CTCA and allows optimizing several other tasks where CTCA can be used         (including mining and compressing).      </content></document><document><year>2006</year><authors>Mei Kobayashi1  | Masaki Aono2 </authors><title>Exploring overlapping clusters using dynamic re-scaling and sampling</title><content>Until recently, the aim of most text-mining work has been to understand major topics and clusters. Minor topics and clusters have been relatively neglected even though they may represent important information on rare events. We present a novel method for exploring overlapping clusters of heterogeneous sizes, which is based on vector space modeling, covariance matrix analysis, random sampling, and dynamic re-weighting of document vectors in massive databases. Our system addresses a combination of difficult issues in database analysis, such as synonymy and polysemy, identification of minor clusters, accommodation of cluster overlap, automatic labeling of clusters based on their document contents, and the user-controlled trade-off between speed of computation and quality of results. We conducted implementation studies with new articles from the Reuters and LA Times TREC data sets and artificially generated data with a known cluster structure to demonstrate the effectiveness of our system.</content></document><document><year>2006</year><authors>Karin Kailing1 | Hans-Peter Kriegel1 | Martin Pfeifle1  | Stefan SchГ¶nauer1 </authors><title>Extending metric index structures for efficient range query processing</title><content>Databases are getting more and more important for storing complex objects from scientific, engineering, or multimedia applications. Examples for such data are chemical compounds, CAD drawings, or XML data. The efficient search for similar objects in such databases is a key feature. However, the general problem of many similarity measures for complex objects is their computational complexity, which makes them unusable for large databases. In this paper, we combine and extend the two techniques of metric index structures and multi-step query processing to improve the performance of range query processing. The efficiency of our methods is demonstrated in extensive experiments on real-world data including graphs, trees, and vector sets.</content></document><document><year>2006</year><authors>Hisashi Koga1 | Tetsuo Ishibashi1 | Toshinori Watanabe1</authors><title>Fast agglomerative hierarchical clustering algorithm using Locality-Sensitive Hashing      </title><content>The single linkage method is a fundamental agglomerative hierarchical clustering algorithm. This algorithm regards each point         as a single cluster initially. In the agglomeration step, it connects a pair of clusters such that the distance between the         nearest members is the shortest. This step is repeated until only one cluster remains. The single linkage method can efficiently         detect clusters in arbitrary shapes. However, a drawback of this method is a large time complexity of O(n         2), where n represents the number of data points. This time complexity makes this method infeasible for large data. This paper proposes         a fast approximation algorithm for the single linkage method. Our algorithm reduces the time complexity to O(nB) by rapidly finding the near clusters to be connected by Locality-Sensitive Hashing, a fast algorithm for the approximate         nearest neighbor search. Here, B represents the maximum number of points going into a single hash entry and it practically diminishes to a small constant         as compared to n for sufficiently large hash tables. Experimentally, we show that (1) the proposed algorithm obtains clustering results similar         to those obtained by the single linkage method and (2) it runs faster for large data than the single linkage method.      </content></document><document><year>2006</year><authors>Tianming Hu1  | Sam Yuan Sung2</authors><title>Finding centroid clusterings with entropy-based criteria</title><content>We investigate the following problem: Given a set of candidate clusterings for a common set of objects, find a centroid clustering that is most compatible to the input set. First, we propose a series of entropy-based distance functions for comparing various clusterings. Such functions enable us to directly select the local centroid from the candidate set. Second, we present two combining methods for the global centroid. The selected/combined centroid clustering is likely to be a good choice, i.e., top or middle ranked in terms of closeness to the true clustering. Finally, we evaluate their effectiveness on both artificial and real data sets.</content></document><document><year>2006</year><authors>Eamonn Keogh1 | Jessica Lin2 | Sang-Hee Lee3  | Helga Van Herle4 </authors><title>Finding the most unusual time series subsequence: algorithms and applications</title><content>In this work we introduce the new problem of finding time seriesdiscords. Time series discords are subsequences of longer time series that are maximally different to all the rest of the time series subsequences. They thus capture the sense of the most unusual subsequence within a time series. While discords have many uses for data mining, they are particularly attractive as anomaly detectors because they only require one intuitive parameter (the length of the subsequence) unlike most anomaly detection algorithms that typically require many parameters. While the brute force algorithm to discover time series discords is quadratic in the length of the time series, we show a simple algorithm that is three to four orders of magnitude faster than brute force, while guaranteed to produce identical results. We evaluate our work with a comprehensive set of experiments on diverse data sources including electrocardiograms, space telemetry, respiration physiology, anthropological and video datasets.</content></document><document><year>2006</year><authors>Hansheng Lei1  | Venu Govindaraju2</authors><title>Generalized regression model for sequence matching and clustering      </title><content>Linear relation has been found to be valuable in rule discovery of stocks, such as if stock X goes up a, stock Y will go down b. The traditional linear regression models the linear relation of two sequences faithfully. However, if a user requires clustering         of stocks into groups where sequences have high linearity or similarity with each other, it is prohibitively expensive to         compare sequences one by one. In this paper, we present generalized regression model (GRM) to match the linearity of multiple         sequences at a time. GRM also gives strong heuristic support for graceful and efficient clustering. The experiments on the         stocks in the NASDAQ market mined interesting clusters of stock trends efficiently.      </content></document><document><year>2006</year><authors>Michael Steinbach1  | Vipin Kumar1</authors><title>Generalizing the notion of confidence      </title><content>In this paper, we explore extending association analysis to non-traditional types of patterns and non-binary data by generalizing         the notion of confidence. We begin by describing a general framework that measures the strength of the connection between         two association patterns by the extent to which the strength of one association pattern provides information about the strength         of another. Although this framework can serve as the basis for designing or analyzing measures of association, the focus in         this paper is to use the framework as the basis for extending the traditional concept of confidence to error-tolerant itemsets         (ETIs) and continuous data. To that end, we provide two examples. First, we (1) describe an approach to defining confidence         for ETIs that preserves the interpretation of confidence as an estimate of a conditional probability, and (2) show how association         rules based on ETIs can have better coverage (at an equivalent confidence level) than rules based on traditional itemsets.         Next, we derive a confidence measure for continuous data that agrees with the standard confidence measure when applied to         binary transaction data. Further analysis of this result exposes some of the important issues involved in constructing a confidence         measure for continuous data.      </content></document><document><year>2006</year><authors>Ke Wang1 | Benjamin C. M. Fung1 | Philip S. Yu2</authors><title>Handicapping attacker's confidence: an alternative to k-anonymization      </title><content>We present an approach of limiting the confidence of inferring sensitive properties to protect against the threats caused         by data mining abilities. The problem has dual goals: preserve the information for a wanted data analysis request and limit         the usefulness of unwanted sensitive inferences that may be derived from the release of data. Sensitive inferences are specified         by a set of &amp;#8220;privacy templates". Each template specifies the sensitive property to be protected, the attributes identifying         a group of individuals, and a maximum threshold for the confidence of inferring the sensitive property given the identifying         attributes. We show that suppressing the domain values monotonically decreases the maximum confidence of such sensitive inferences.         Hence, we propose a data transformation that minimally suppresses the domain values in the data to satisfy the set of privacy         templates. The transformed data is free of sensitive inferences even in the presence of data mining algorithms. The prior         k-anonymization k has been italicized consistently throughout this article. focuses on personal identities. This work focuses on the association         between personal identities and sensitive properties.      </content></document><document><year>2006</year><authors>Kedian Mu1 | Zhi Jin2| Ruqian Lu2 | Yan Peng1</authors><title>Handling non-canonical software requirements based on Annotated Predicate Calculus</title><content>Eliciting requirements for a proposed system inevitably involves the problem of handling undesirable information about customer's needs, including inconsistency, vagueness, redundancy, or incompleteness. We term the requirements statements involved in the undesirable information non-canonical software requirements. In this paper, we propose an approach to handling non-canonical software requirements based on Annotated Predicate Calculus (APC). Informally, by defining a special belief lattice appropriate for representing the stakeholder's belief in requirements statements, we construct a new form of APC to formalize requirements specifications. We then show how the APC can be employed to characterize non-canonical requirements. Finally, we show how the approach can be used to handle non-canonical requirements through a case study.</content></document><document><year>2006</year><authors>F. Esposito1 | S. Ferilli1 | T. M. A. Basile1  | N. Di Mauro1 </authors><title>Inference of abduction theories for handling incompleteness in first-order learning      </title><content>In real-life domains, learning systems often have to deal with various kinds of imperfections in data such as noise, incompleteness         and inexactness. This problem seriously affects the knowledge discovery process, specifically in the case of traditional Machine         Learning approaches that exploit simple or constrained knowledge representations and are based on single inference mechanisms.         Indeed, this limits their capability of discovering fundamental knowledge in those situations. In order to broaden the investigation         and the applicability of machine learning schemes in such particular situations, it is necessary to move on to more expressive         representations which require more complex inference mechanisms. However, the applicability of such new and complex inference         mechanisms, such as abductive reasoning, strongly relies on a deep background knowledge about the specific application domain.         This work aims at automatically discovering the meta-knowledge needed to abduction inference strategy to complete the incoming         information in order to handle cases of missing knowledge.      </content></document><document><year>2006</year><authors>Julien Blanchard1 | Fabrice Guillet1 | Henri Bri|1</authors><title>Interactive visual exploration of association rules with rule-focusing methodology      </title><content>On account of the enormous amounts of rules that can be produced by data mining algorithms, knowledge post-processing is a         difficult stage in an association rule discovery process. In order to find relevant knowledge for decision making, the user         (a decision maker specialized in the data studied) needs to rummage through the rules. To assist him/her in this task, we         here propose the rule-focusing methodology, an interactive methodology for the visual post-processing of association rules. It allows the user to explore         large sets of rules freely by focusing his/her attention on limited subsets. This new approach relies on rule interestingness         measures, on a visual representation, and on interactive navigation among the rules. We have implemented the rule-focusing         methodology in a prototype system called ARVis. It exploits the user's focus to guide the generation of the rules by means of a specific constraint-based rule-mining algorithm.      </content></document><document><year>2006</year><authors>Introduction</authors><title>Without Abstract</title><content/></document><document><year>2007</year><authors>Srividya Kadiyala1  | Nematollaah Shiri1 </authors><title>A compact multi-resolution index for variable length queries in time series databases      </title><content>We study the problem of searching similar patterns in time series data for variable length queries. Recently, a multi-resolution         indexing technique (MRI) was proposed in (Kahveci and Singh, in proceedings of the international conference on data engineering,         pp. 273&amp;#8211;282, 2001; Kahveci and Singh, IEEE Trans Knowl Data Eng 16(4):418&amp;#8211;433, 2004) to address this problem, which uses compression         as an additional step to reduce the index size. In this paper, we propose an alternative technique, called compact MRI (CMRI),         which uses adaptive piecewise constant approximation (APCA) representation as dimensionality reduction technique, and which         occupies much less space without requiring compression. We implemented both MRI and CMRI, and conducted extensive experiments         to evaluate and compare their performance on real stock data as well as synthetic. Our results indicate that CMRI provides         a much better precision ranging from 0.75 to 0.89 on real data, and from 0.80 to 0.95 on synthetic data, while for MRI, these         ranges are from 0.16 to 0.34, and from 0.03 to 0.65, respectively. Compared to sequential scan, we found that CMRI is 4&amp;#8211;30         times faster and the number of disk I/Os it required is close to minimal. In terms of storage utilization, CMRI occupies 1%         of the memory occupied by MRI. These results and analysis show CMRI to be an efficient and scalable indexing technique for         large time series databases.      </content></document><document><year>2007</year><authors>I. LГіpez-ArГ©valo1| R. BaГ±ares-AlcГЎntara2 | A. Aldea3 | A. RodrГ­guez-MartГ­nez4</authors><title>A hierarchical approach for the redesign of chemical processes      </title><content>An approach to improve the management of complexity during the redesign of technical processes is proposed. The approach consists         of two abstract steps. In the first step, model-based reasoning is used to generate automatically alternative representations         of an existing process at several levels of abstraction. In the second step, process alternatives are generated through the         application of case-based reasoning. The key point of our framework is the modeling approach, which is an extension of the         Multimodeling and Multilevel Flow Modeling methodologies. These, together with a systematic design methodology, are used to represent a process hierarchically, thus         improving the identification of analogous equipment/sections from different processes. The hierarchical representation results         in sets of equipment/sections organized according to their functions and intentions. A case-based reasoning system then retrieves         from a library of cases similar equipment/sections to the one selected by the user. The final output is a set of equipment/sections         ordered according to their similarity. Human intervention is necessary to adapt the most promising case within the original         process.      </content></document><document><year>2007</year><authors>Arjan J. F. Kok1  | Robert van Liere2 </authors><title>A multimodal virtual reality interface for 3D interaction with VTK      </title><content>The object-oriented visualization Toolkit (VTK) is widely used for scientific visualization. VTK is a visualization library         that provides a large number of functions for presenting three-dimensional data. Interaction with the visualized data is controlled         with two-dimensional input devices, such as mouse and keyboard. Support for real three-dimensional and multimodal input is         non-existent. This paper describes VR-VTK: a multimodal interface to VTK on a virtual environment. Six degree of freedom input         devices are used for spatial 3D interaction. They control the 3D widgets that are used to interact with the visualized data.         Head tracking is used for camera control. Pedals are used for clutching. Speech input is used for application commands and         system control. To address several problems specific for spatial 3D interaction, a number of additional features, such as         more complex interaction methods and enhanced depth perception, are discussed. Furthermore, the need for multimodal input         to support interaction with the visualization is shown. Two existing VTK applications are ported using VR-VTK to run in a         desktop virtual reality system. Informal user experiences are presented.      </content></document><document><year>2007</year><authors>M. Senthil Arumugam1 | M. V. C. Rao1 | Aarthi Ch|ramohan2</authors><title>A new and improved version of particle swarm optimization algorithm with global&amp;#8211;local best parameters      </title><content>This paper presents a new and improved version of particle swarm optimization algorithm (PSO) combining the global best and         local best model, termed GLBest-PSO. The GLBest-PSO incorporates global&amp;#8211;local best inertia weight (GLBest IW) with global&amp;#8211;local         best acceleration coefficient (GLBest Ac). The velocity equation of the GLBest-PSO is also simplified. The ability of the         GLBest-PSO is tested with a set of bench mark problems and the results are compared with those obtained through conventional         PSO (cPSO), which uses time varying inertia weight (TVIW) and acceleration coefficient (TVAC). Fine tuning variants such as         mutation, cross-over and RMS variants are also included with both cPSO and GLBest-PSO to improve the performance. The simulation         results clearly elucidate the advantage of the fine tuning variants, which sharpen the convergence and tune to the best solution         for both cPSO and GLBest-PSO. To compare and verify the validity and effectiveness of the GLBest-PSO, a number of statistical         analyses are carried out. It is also observed that the convergence speed of GLBest-PSO is considerably higher than cPSO. All         the results clearly demonstrate the superiority of the GLBest-PSO.      </content></document><document><year>2007</year><authors>Guimei Liu1 | Jinyan Li2 | Limsoon Wong1</authors><title>A new concise representation of frequent itemsets using generators and a positive border      </title><content>A complete set of frequent itemsets can get undesirably large due to redundancy when the minimum support threshold is low         or when the database is dense. Several concise representations have been previously proposed to eliminate the redundancy.         Generator based representations rely on a negative border to make the representation lossless. However, the number of itemsets         on a negative border sometimes even exceeds the total number of frequent itemsets. In this paper, we propose to use a positive         border together with frequent generators to form a lossless representation. A positive border is usually orders of magnitude         smaller than its corresponding negative border. A set of frequent generators plus its positive border is always no larger         than the corresponding complete set of frequent itemsets, thus it is a true concise representation. The generalized form of         this representation is also proposed. We develop an efficient algorithm, called GrGrowth, to mine generators and positive         borders as well as their generalizations. The GrGrowth algorithm uses the depth-first-search strategy to explore the search         space, which is much more efficient than the breadth-first-search strategy adopted by most of the existing generator mining         algorithms. Our experiment results show that the GrGrowth algorithm is significantly faster than level-wise algorithms for         mining generator based representations, and is comparable to the state-of-the-art algorithms for mining frequent closed itemsets.      </content></document><document><year>2007</year><authors>Cheng Luo1 | Zhewei Jiang1| Wen-Chi Hou1| Feng Yan1 | Qiang Zhu2</authors><title>A relational model for XML structural joins and their size estimations      </title><content>XML structural joins, which evaluate the containment (ancestor-descendant) relationships between XML elements, are important         operations of XML query processing. Estimating structural join size accurately and quickly is crucial to the success of XML         query plan selection and the query optimization. XML structural joins are essentially complex &amp;#952;-joins, which render well-known         estimation techniques for relational equijoins, such as discrete cosine transform, wavelet transform, and sketch, not applicable.         In this paper, we model structural joins from a relational point of view and convert the complex &amp;#952;-joins to equijoins so that         those well-known estimation techniques become applicable to structural join size estimation. Theoretical analyses and extensive         experiments have been performed on these estimation methods. It is shown that discrete cosine transform requires the least         memory and yields the best estimates among the three techniques. Compared with state-of-the-art method IM-DA-Est, discrete         cosine transform is much faster, requires less memory, and yields comparable estimates.       </content></document><document><year>2007</year><authors>Congnan Luo1  | Soon M. Chung1 </authors><title>A scalable algorithm for mining maximal frequent sequences using a sample      </title><content>In this paper, we propose an efficient scalable algorithm for mining Maximal Sequential Patterns using Sampling (MSPS). The MSPS algorithm reduces much more search space than other algorithms because both the subsequence infrequency-based         pruning and the supersequence frequency-based pruning are applied. In MSPS, a sampling technique is used to identify long         frequent sequences earlier, instead of enumerating all their subsequences. We propose how to adjust the user-specified minimum         support level for mining a sample of the database to achieve better overall performance. This method makes sampling more efficient         when the minimum support is small. A signature-based method and a hash-based method are developed for the subsequence infrequency-based         pruning when the seed set of frequent sequences for the candidate generation is too big to be loaded into memory. A prefix         tree structure is developed to count the candidate sequences of different sizes during the database scanning, and it also         facilitates the customer sequence trimming. Our experiments showed MSPS has very good performance and better scalability than         other algorithms.      </content></document><document><year>2007</year><authors>James Cheng1 | Yiping Ke1 | Wilfred Ng1</authors><title>A survey on algorithms for mining frequent itemsets over data streams      </title><content>The increasing prominence of data streams arising in a wide range of advanced applications such as fraud detection and trend         learning has led to the study of online mining of frequent itemsets (FIs). Unlike mining static databases, mining data streams         poses many new challenges. In addition to the one-scan nature, the unbounded memory requirement and the high data arrival         rate of data streams, the combinatorial explosion of itemsets exacerbates the mining task. The high complexity of the FI mining         problem hinders the application of the stream mining techniques. We recognize that a critical review of existing techniques         is needed in order to design and develop efficient mining algorithms and data structures that are able to match the processing         rate of the mining with the high arrival rate of data streams. Within a unifying set of notations and terminologies, we describe         in this paper the efforts and main techniques for mining data streams and present a comprehensive survey of a number of the         state-of-the-art algorithms on mining frequent itemsets over data streams. We classify the stream-mining techniques into two         categories based on the window model that they adopt in order to provide insights into how and why the techniques are useful.         Then, we further analyze the algorithms according to whether they are exact or approximate and, for approximate approaches, whether they are false-positive or false-negative. We also discuss various interesting issues, including the merits and limitations in existing research and substantive areas         for future research.      </content></document><document><year>2007</year><authors>Shaozhi Ye1 | Ji-Rong Wen2 | Wei-Ying Ma2</authors><title>A systematic study on parameter correlations in large-scale duplicate document detection      </title><content>Although much work has been done on duplicate document detection (DDD) and its applications, we observe the absence of a systematic         study on the performance and scalability of large-scale DDD algorithms. It is still unclear how various parameters in DDD         correlate mutually, such as similarity threshold, precision/recall requirement, sampling ratio, and document size. This paper         explores the correlations among several most important parameters in DDD and the impact of sampling ratio is of most interest         since it heavily affects the accuracy and scalability of DDD algorithms. An empirical analysis is conducted on a million HTML         documents from the TREC .GOV collection. Experimental results show that even when using the same sampling ratio, the precision         of DDD varies greatly on documents with different sizes. Based on this observation, we propose an adaptive sampling strategy         for DDD, which minimizes the sampling ratio with the constraint of a given precision requirement. We believe that the insights         from our analysis are helpful for guiding the future large-scale DDD work.      </content></document><document><year>2007</year><authors>Corina Sas1  | Nikita Schmidt2</authors><title>A typology of course of motion in simulated environments based on BГ©zier curve analysis      </title><content>This paper proposes a novel method of analysing trajectories followed by people while they perform navigational tasks. The         results indicate that modelling trajectories with BГ©zier curves provides a basis for the diagnosis of navigational patterns.         The method offers five indicators: goodness of fit, average curvature, number of inflexion points, lengths of straight line         segments, and area covered. Study results, obtained in a virtual environment show that these indicators carry important information         about user performance, specifically spatial knowledge acquisition.      </content></document><document><year>2007</year><authors>Yiping Ke1 | James Cheng1 | Wilfred Ng1</authors><title>An information-theoretic approach to quantitative association rule mining      </title><content>Quantitative association rule (QAR) mining has been recognized an influential research problem over the last decade due to         the popularity of quantitative databases and the usefulness of association rules in real life. Unlike boolean association         rules (BARs), which only consider boolean attributes, QARs consist of quantitative attributes which contain much richer information         than the boolean attributes. However, the combination of these quantitative attributes and their value intervals always gives         rise to the generation of an explosively large number of itemsets, thereby severely degrading the mining efficiency. In this         paper, we propose an information-theoretic approach to avoid unrewarding combinations of both the attributes and their value         intervals being generated in the mining process. We study the mutual information between the attributes in a quantitative         database and devise a normalization on the mutual information to make it applicable in the context of QAR mining. To indicate         the strong informative relationships among the attributes, we construct a mutual information graph (MI graph), whose edges         are attribute pairs that have normalized mutual information no less than a predefined information threshold. We find that         the cliques in the MI graph represent a majority of the frequent itemsets. We also show that frequent itemsets that do not         form a clique in the MI graph are those whose attributes are not informatively correlated to each other. By utilizing the         cliques in the MI graph, we devise an efficient algorithm that significantly reduces the number of value intervals of the         attribute sets to be joined during the mining process. Extensive experiments show that our algorithm speeds up the mining         process by up to two orders of magnitude. Most importantly, we are able to obtain most of the high-confidence QARs, whereas         the QARs that are not returned by MIC are shown to be less interesting.      </content></document><document><year>2007</year><authors>Han-joon Kim1  | Sang-goo Lee2</authors><title>An intelligent information system for organizing online text documents      </title><content>This paper describes an intelligent information system for effectively managing huge amounts of online text documents (such         as Web documents) in a hierarchical manner. The organizational capabilities of this system are able to evolve semi-automatically         with minimal human input. The system starts with an initial taxonomy in which documents are automatically categorized, and         then evolves so as to provide a good indexing service as the document collection grows or its usage changes. To this end,         we propose a series of algorithms that utilize text-mining technologies such as document clustering, document categorization,         and hierarchy reorganization. In particular, clustering and categorization algorithms have been intensively studied in order         to provide evolving facilities for hierarchical structures and categorization criteria. Through experiments using the Reuters-21578         document collection, we evaluate the performance of the proposed clustering and categorization methods by comparing them to         those of well-known conventional methods.      </content></document><document><year>2007</year><authors>Rajagopalan Srinivasan1| 2 </authors><title>Artificial intelligence methodologies for agile refining: an overview      </title><content>Agile manufacturing is the capability to prosper in a competitive environment of continuous and unpredictable changes by reacting         quickly and effectively to the changing markets and other exogenous factors. Agility of petroleum refineries is determined         by two factors &amp;#8211; ability to control the process and ability to efficiently manage the supply chain. In this paper, we outline         some challenges faced by refineries that seek to be lean, nimble, and proactive. These problems, which arise in supply chain         management and operations management are seldom amenable to traditional, monolithic solutions. As discussed here using several         examples, methodologies drawn from artificial intelligence &amp;#8211; software agents, pattern recognition, expert systems &amp;#8211; have a         role to play in this path toward agility.      </content></document><document><year>2007</year><authors>Eugenio Cesario1| Francesco Folino1| Antonio Locane1| Giuseppe Manco1  | Riccardo Ortale1</authors><title>Boosting text segmentation via progressive classification      </title><content>A novel approach for reconciling tuples stored as free text into an existing attribute schema is proposed. The basic idea         is to subject the available text to progressive classification, i.e., a multi-stage classification scheme where, at each intermediate stage, a classifier is learnt that analyzes the textual         fragments not reconciled at the end of the previous steps. Classification is accomplished by an ad hoc exploitation of traditional         association mining algorithms, and is supported by a data transformation scheme which takes advantage of domain-specific dictionaries/ontologies.         A key feature is the capability of progressively enriching the available ontology with the results of the previous stages         of classification, thus significantly improving the overall classification accuracy. An extensive experimental evaluation         shows the effectiveness of our approach.      </content></document><document><year>2007</year><authors>Ke Xu1  | HГЁctor MuГ±oz-Avila1 </authors><title>CaBMA: a case-based reasoning system for capturing, refining, and reusing project plans      </title><content>In this paper, we present CaBMA, a prototype of a knowledge-based system designed to assist with project planning tasks using         case-based reasoning. CaBMA introduces a novel approach to project planning in that, for the first time, a knowledge layer         is added on top of traditional project management software. Project management software provides editing and bookkeeping capabilities.         CaBMA enhances these capabilities by automatically capturing project plans in the form of cases, refining these cases over         time to avoid potential inconsistency between them, reusing these cases to generate plans for new projects, and indicating         possible repairs for project plans when they derive away from existing knowledge. We will give an overview of the system,         provide a detailed explanation on each component, and present an empirical study based on synthetic data.      </content></document><document><year>2007</year><authors>Ziv Bar-Yossef1| 4| Ido Guy2| 3 | Ronny Lempel3| YoГ«lle S. Maarek4 | Vladimir Soroka3</authors><title>Cluster ranking with an application to mining mailbox networks      </title><content>We initiate the study of a new clustering framework, called cluster ranking. Rather than simply partitioning a network into clusters, a cluster ranking algorithm also orders the clusters by their strength. To this end, we introduce a novel strength measure for clusters&amp;#8212;the integrated cohesion&amp;#8212;which is applicable to arbitrary weighted networks. We then present a new cluster ranking algorithm, called C-Rank. We provide         extensive theoretical and empirical analysis of C-Rank and show that it is likely to have high precision and recall. A main         component of C-Rank is a heuristic algorithm for finding sparse vertex separators. At the core of this algorithm is a new         connection between vertex betweenness and multicommodity flow. Our experiments focus on mining mailbox networks. A mailbox network is an egocentric social network, consisting of contacts with whom an individual exchanges email. Edges         between contacts represent the frequency of their co&amp;#8211;occurrence on message headers. C-Rank is well suited to mine such networks,         since they are abundant with overlapping communities of highly variable strengths. We demonstrate the effectiveness of C-Rank         on the Enron data set, consisting of 130 mailbox networks.      </content></document><document><year>2007</year><authors>R. Chen1| K. Sivakumar1  | H. Kargupta2</authors><title>Collective mining of Bayesian networks from distributed heterogeneous data      </title><content>We present a collective approach to learning a Bayesian network from distributed heterogeneous data. In this approach, we         first learn a local Bayesian network at each site using the local data. Then each site identifies the observations that are         most likely to be evidence of coupling between local and non-local variables and transmits a subset of these observations         to a central site. Another Bayesian network is learnt at the central site using the data transmitted from the local site.         The local and central Bayesian networks are combined to obtain a collective Bayesian network, which models the entire data.         Experimental results and theoretical justification that demonstrate the feasibility of our approach are presented.      </content></document><document><year>2007</year><authors>Nikil Wale1 | Ian A. Watson2  | George Karypis3 </authors><title>Comparison of descriptor spaces for chemical compound retrieval and classification      </title><content>In recent years the development of computational techniques that build models to correctly assign chemical compounds to various         classes or to retrieve potential drug-like compounds has been an active area of research. Many of the best-performing techniques         for these tasks utilize a descriptor-based representation of the compound that captures various aspects of the underlying         molecular graph&amp;#8217;s topology. In this paper we compare five different set of descriptors that are currently used for chemical         compound classification. We also introduce four different descriptors derived from all connected fragments present in the         molecular graphs primarily for the purpose of comparing them to the currently used descriptor spaces and analyzing what properties         of descriptor spaces are helpful in providing effective representation for molecular graphs. In addition, we introduce an         extension to existing vector-based kernel functions to take into account the length of the fragments present in the descriptors.         We experimentally evaluate the performance of the previously introduced and the new descriptors in the context of SVM-based         classification and ranked-retrieval on 28 classification and retrieval problems derived from 18 datasets. Our experiments         show that for both of these tasks, two of the four descriptors introduced in this paper along with the extended connectivity         fingerprint based descriptors consistently and statistically outperform previously developed schemes based on the widely used         fingerprint- and Maccs keys-based descriptors, as well as recently introduced descriptors obtained by mining and analyzing         the structure of the molecular graphs.      </content></document><document><year>2007</year><authors>Filippo Furfaro1 | Giuseppe M. Mazzeo1| Domenico SaccГ 1| 2 | Cristina Sirangelo1</authors><title>Compressed hierarchical binary histograms for summarizing multi-dimensional data      </title><content>Hierarchical binary partitions of multi-dimensional data are investigated as a basis for the construction of effective histograms.         Specifically, the impact of adopting lossless compression techniques for representing the histogram on both the accuracy and         the efficiency of query answering is investigated. Compression is obtained by exploiting the hierarchical partition scheme         underlying the histogram, and then introducing further restrictions on the partitioning which enable a more compact representation         of bucket boundaries. Basically, these restrictions consist of constraining the splits of the partition to be laid onto regular         grids defined on the buckets. Several heuristics guiding the histogram construction are also proposed, and a thorough experimental         analysis comparing the accuracy of histograms resulting from combining different heuristics with different representation         models (both the new compression-based and the traditional ones) is provided. The best accuracy turns out from combining our         grid-constrained partitioning scheme with one of the new heuristics. Histograms resulting from this combination are compared         with state-of-the-art summarization techniques, showing that the proposed approach yields lower error rates and is much less         sensitive to dimensionality, and that adopting our compression scheme results in improving the efficiency of query estimation.      </content></document><document><year>2007</year><authors>Shichao Zhang1 | Xindong Wu2 | Chengqi Zhang3  | Jingli Lu4 </authors><title>Computing the minimum-support for mining frequent patterns      </title><content>Frequent pattern mining is based on the assumption that users can specify the minimum-support for mining their databases.         It has been recognized that setting the minimum-support is a difficult task to users. This can hinder the widespread applications         of these algorithms. In this paper we propose a computational strategy for identifying frequent itemsets, consisting of polynomial         approximation and fuzzy estimation. More specifically, our algorithms (polynomial approximation and fuzzy estimation) automatically         generate actual minimum-supports (appropriate to a database to be mined) according to users&amp;#8217; mining requirements. We experimentally         examine the algorithms using different datasets, and demonstrate that our fuzzy estimation algorithm fittingly approximates         actual minimum-supports from the commonly-used requirements.      </content></document><document><year>2007</year><authors>MatГ­as Alvarado1 | Leonid Sheremetov2| RenГ© BaГ±ares-AlcГЎntara3 | Francisco CantГє-Ortiz4</authors><title>Current challenges and trends in intelligent computing and knowledge management in industry      </title><content>Without Abstract</content></document><document><year>2007</year><authors>MatГ­as Alvarado1 | Miguel A. RodrГ­guez-Toral2| Arm|o Rosas2 | Sergio Ayala2</authors><title>Decision-making on pipe stress analysis enabled by knowledge-based systems      </title><content>This paper presents engineering decision-making on pipe stress analysis through the application of knowledge-based systems         (KBS). Stress analysis, as part of the design and analysis of process pipe networks, serves to identify whether a given pipe         arrangement can cope with weight, thermal, and pressure stress at safe operation levels. An iterative process of design and         analysis cycle is done routinely by engineers while analyzing the existing networks or while designing the process pipe networks.         In our proposal, the KBS establishes a bidirectional communication with the current engineering software for pipe stress analysis,         so that the user benefits from this integration. The stress analysis knowledge base is constructed by registering the senior         engineers&amp;#8217; know-how. The engineers&amp;#8217; overall strategy to follow up during the pipe stress analysis, to some extent contained         by the KBS, is presented. Advantages in saving engineering man-hours and usefulness in guiding experts in pipe stress analysis         are the major services for the process industry.      </content></document><document><year>2007</year><authors>Sougata Mukherjea1 </authors><title>Discovering and analyzing World Wide Web collections      </title><content>With the explosive growth of the World Wide Web, it is becoming increasingly difficult for users to discover Web pages that         are relevant to a topic. To address this problem we are developing a system that allows the collection and analysis of Web         pages related to a particular topic. In this paper we present the system's overall architecture and introduce the focused         crawler used by the system. We also discuss the various techniques we use to allow the user to analyze and gain useful insinghts         about a collection. Finally, we present some statistics on the collections.      </content></document><document><year>2007</year><authors>Wee Keong Ng1 | Masaru Kitsuregawa2 | Jianzhong Li3</authors><title>Editorial      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Xingquan Zhu1| 2 | Taghi M. Khoshgoftaar1| Ian Davidson3 | Shichao Zhang4</authors><title>Editorial: Special issue on mining low-quality data      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Aris Anagnostopoulos1| Andrei Broder1 | Kunal Punera2 </authors><title>Effective and efficient classification on a search-engine model      </title><content>Traditional document classification frameworks, which apply the learned classifier to each document in a corpus one by one,         are infeasible for extremely large document corpora, like the Web or large corporate intranets. We consider the classification         problem on a corpus that has been processed primarily for the purpose of searching, and thus our access to documents is solely         through the inverted index of a large scale search engine. Our main goal is to build the &amp;#8220;best&amp;#8221; short query that characterizes         a document class using operators normally available within search engines. We show that surprisingly good classification accuracy         can be achieved on average over multiple classes by queries with as few as 10 terms. As part of our study, we enhance some         of the feature-selection techniques that are found in the literature by forcing the inclusion of terms that are negatively         correlated with the target class and by making use of term correlations; we show that both of those techniques can offer significant         advantages. Moreover, we show that optimizing the efficiency of query execution by careful selection of terms can further         reduce the query costs. More precisely, we show that on our set-up the best 10-term query can achieve 93% of the accuracy         of the best SVM classifier (14,000 terms), and if we are willing to tolerate a reduction to 89% of the best SVM, we can build         a 10-term query that can be executed more than twice as fast as the best 10-term query.      </content></document><document><year>2007</year><authors>Soon M. Chung1  | Congnan Luo1 </authors><title>Efficient mining of maximal frequent itemsets from databases on a cluster of workstations      </title><content>In this paper, we propose two parallel algorithms for mining maximal frequent itemsets from databases. A frequent itemset         is maximal if none of its supersets is frequent. One parallel algorithm is named distributed max-miner (DMM), and it requires very low communication and synchronization overhead in distributed computing systems. DMM has the         local mining phase and the global mining phase. During the local mining phase, each node mines the local database to discover         the local maximal frequent itemsets, then they form a set of maximal candidate itemsets for the top-down search in the subsequent         global mining phase. A new prefix tree data structure is developed to facilitate the storage and counting of the global candidate         itemsets of different sizes. This global mining phase using the prefix tree can work with any local mining algorithm. Another         parallel algorithm, named parallel max-miner (PMM), is a parallel version of the sequential max-miner algorithm (Proc of ACM SIGMOD Int Conf on Management of Data, 1998,         pp 85&amp;#8211;93). Most of existing mining algorithms discover the frequent k-itemsets on the kth pass over the databases, and then generate the candidate (k;+;1)-itemsets for the next pass. Compared to those level-wise algorithms, PMM looks ahead at each pass and prunes more candidate         itemsets by checking the frequencies of their supersets. Both DMM and PMM were implemented on a cluster of workstations, and         their performance was evaluated for various cases. They demonstrate very good performance and scalability even when there         are large maximal frequent itemsets (i.e., long patterns) in databases.      </content></document><document><year>2007</year><authors>Christine Preisach1  | Lars Schmidt-Thieme1 </authors><title>Ensembles of relational classifiers      </title><content>Relational classification aims at including relations among entities into the classification process, for example taking relations         among documents such as common authors or citations into account. However, considering more than one relation can further         improve classification accuracy. Here we introduce a new approach to make use of several relations as well as both, relations         and local attributes for classification using ensemble methods. To accomplish this, we present a generic relational ensemble         model that can use different relational and local classifiers as components. Furthermore, we discuss solutions for several problems concerning relational data such as heterogeneity, sparsity,         and multiple relations. Especially the sparsity problem will be discussed in more detail. We introduce a new method called         PRNMultiHop that tries to handle this problem. Furthermore we categorize relational methods in a systematic way. Finally, we provide         empirical evidence, that our relational ensemble methods outperform existing relational classification methods, even rather         complex models such as relational probability trees (RPTs), relational dependency networks (RDNs) and relational Bayesian         classifiers (RBCs).      </content></document><document><year>2007</year><authors>Emine Yilmaz1  | Javed A. Aslam1 </authors><title>Estimating average precision when judgments are incomplete      </title><content>We consider the problem of evaluating retrieval systems with incomplete relevance judgments. Recently, Buckley and Voorhees         showed that standard measures of retrieval performance are not robust to incomplete judgments, and they proposed a new measure,         bpref, that is much more robust to incomplete judgments. Although bpref is highly correlated with average precision when the judgments         are effectively complete, the value of bpref deviates from average precision and from its own value as the judgment set degrades,         especially at very low levels of assessment. In this work, we propose three new evaluation measures induced AP, subcollection AP, and inferred AP that are equivalent to average precision when the relevance judgments are complete and that are statistical estimates of average precision when relevance judgments are a random subset of complete judgments. We consider natural scenarios which         yield highly incomplete judgments such as random judgment sets or very shallow depth pools. We compare and contrast the robustness         of the three measures proposed in this work with bpref for both of these scenarios. Through the use of TREC data, we demonstrate         that these measures are more robust to incomplete relevance judgments than bpref, both in terms of how well the measures estimate         average precision (as measured with complete relevance judgments) and how well they estimate themselves (as measured with         complete relevance judgments). Finally, since inferred AP is the most accurate approximation to average precision and the         most robust measure in the presence of incomplete judgments, we provide a detailed analysis of this measure, both in terms         of its behavior in theory and its implementation in practice.      </content></document><document><year>2007</year><authors>Peter Haase1| Ronny Siebes2  | Frank van Harmelen2</authors><title>Expertise-based peer selection in Peer-to-Peer networks      </title><content>Peer-to-Peer systems have proven to be an effective way of sharing data. Modern protocols are able to efficiently route a         message to a given peer. However, determining the destination peer in the first place is not always trivial. We propose a         model in which peers advertise their expertise in the Peer-to-Peer network. The knowledge about the expertise of other peers         forms a semantic topology. Based on the semantic similarity between the subject of a query and the expertise of other peers,         a peer can select appropriate peers to forward queries to, instead of broadcasting the query or sending it to a random set         of peers. To calculate our semantic similarity measure, we make the simplifying assumption that the peers share the same ontology.         We evaluate the model in a bibliographic scenario, where peers share bibliographic descriptions of publications among each         other. In simulation experiments complemented with a real-world field experiment, we show how expertise-based peer selection         improves the performance of a Peer-to-Peer system with respect to precision, recall and the number of messages.      </content></document><document><year>2007</year><authors>Tatiana G. Evreinova1 | Leena K. Vesterinen1 | Grigori Evreinov1  | Roope Raisamo1 </authors><title>Exploration of directional-predictive sounds for nonvisual interaction with graphs      </title><content>Sonification of stylus movements accompanied with kinesthetic feedback is one of possible techniques to develop cross-modal         coordination in the absence of visual information. The investigated problems are the following: how to minimize the number         of sounds while increasing the information they contain and how to choose a natural sonification grammar which would not require         extra cognitive efforts. We demonstrate two case studies of employing directional-predictive sounds (DPS). Stylus movements         were sonified through three sound signals taking into account the exploration behavior and the concept of the capture radius.         The performance of eight subjects was evaluated in terms of the stylus deviation in relation to the points of the virtual         graph, a length of the scanpaths, and the task completion time. When stylus movements were accompanied with the DPS signals         within four capture radiuses, the deviation of the stylus from the graph inspected was always less than one capture radius.         The scanpaths were 24&amp;#8211;40% shorter in length and the task completion times decreased by 20&amp;#8211;25%. We also demonstrate the game         application which was designed to optimize an exploration behavior enhanced by the DPS. The results of the proposed sonification         technique based on the model of the exploration behavior are discussed.      </content></document><document><year>2007</year><authors>Richi Nayak1 </authors><title>Fast and effective clustering of XML data using structural information      </title><content>This paper presents the incremental clustering algorithm, XML documents Clustering with Level Similarity (XCLS), that groups         the XML documents according to structural similarity. A level structure format is introduced to represent the structure of         XML documents for efficient processing. A global criterion function that measures the similarity between the new document         and existing clusters is developed. It avoids the need to compute the pair-wise similarity between two individual documents         and hence saves a huge amount of computing effort. XCLS is further modified to incorporate the semantic meanings of XML tags         for investigating the trade-offs between accuracy and efficiency. The empirical analysis shows that the structural similarity         overplays the semantic similarity in the clustering process of the structured data such as XML. The experimental analysis         shows that the XCLS method is fast and accurate in clustering the heterogeneous documents by structures.      </content></document><document><year>2007</year><authors>Kun Zhang1  | Wei Fan2 </authors><title>Forecasting skewed biased stochastic ozone days: analyses, solutions and beyond      </title><content>Much work on skewed, stochastic, high dimensional, and biased datasets usually implicitly solve each problem separately. Recently,         we have been approached by Texas Commission on Environmental Quality (TCEQ) to help them build highly accurate ozone level         alarm forecasting models for the Houston area, where these technical difficulties come together in one single problem. Key         characteristics of this problem that is challenging and interesting include: (1) the dataset is sparse (72 features, and 2         or 5% positives depending on the criteria of &amp;#8220;ozone days&amp;#8221;), (2) evolving over time from year to year, (3) limited in collected         data size (7; years or around 2,500 data entries), (4) contains a large number of irrelevant features, (5) is biased in terms         of &amp;#8220;sample selection bias&amp;#8221;, and (6) the true model is stochastic as a function of measurable factors. Besides solving a difficult         application problem, this dataset offers a unique opportunity to explore new and existing data mining techniques, and to provide         experience, guidance and solution for similar problems. Our main technical focus addresses on how to estimate reliable probability         given both sample selection bias and a large number of irrelevant features, and how to choose the most reliable decision threshold         to predict the unknown future with different distribution. On the application side, the prediction accuracy of our chosen         approach (bagging probabilistic decision trees and random decision trees) is 20% higher in recall (correctly detects 1&amp;#8211;3 more         ozone days, depending on the year) and 10% higher in precision (15&amp;#8211;30 fewer false alarm days per year) than state-of-the-art         methods used by air quality control scientists, and these results are significant for TCEQ. On the technical side of data         mining, extensive empirical results demonstrate that, at least for this problem, and probably other problems with similar         characteristics, these two straight-forward non-parametric methods can provide significantly more accurate and reliable solutions         than a number of sophisticated and well-known algorithms, such as SVM and AdaBoost among many others.      </content></document><document><year>2007</year><authors>Huimin Zhao1 </authors><title>Instance weighting versus threshold adjusting for cost-sensitive classification      </title><content>In real-world classification problems, different types of misclassification errors often have asymmetric costs, thus demanding         cost-sensitive learning methods that attempt to minimize average misclassification cost rather than plain error rate. Instance         weighting and post hoc threshold adjusting are two major approaches to cost-sensitive classifier learning. This paper compares         the effects of these two approaches on several standard, off-the-shelf classification methods. The comparison indicates that         the two approaches lead to similar results for some classification methods, such as NaГЇve Bayes, logistic regression, and         backpropagation neural network, but very different results for other methods, such as decision tree, decision table, and decision         rule learners. The findings from this research have important implications on the selection of the cost-sensitive classifier         learning approach as well as on the interpretation of a recently published finding about the relative performance of NaГЇve         Bayes and decision trees.      </content></document><document><year>2007</year><authors>Oleg Gusikhin1 | Nestor Rychtyckyj1  | Dimitar Filev1 </authors><title>Intelligent systems in the automotive industry: applications and trends      </title><content>There is a common misconception that the automobile industry is slow to adapt new technologies, such as artificial intelligence         (AI) and soft computing. The reality is that many new technologies are deployed and brought to the public through the vehicles         that they drive. This paper provides an overview and a sampling of many of the ways that the automotive industry has utilized         AI, soft computing and other intelligent system technologies in such diverse domains like manufacturing, diagnostics, on-board         systems, warranty analysis and design.      </content></document><document><year>2007</year><authors>RamГіon F. Brena1 | JosГ© L. Aguirre1 | Carlos ChesГ±evar2 | Eduardo H. RamГ­rez1  | Leonardo Garrido1 </authors><title>Knowledge and information distribution leveraged by intelligent agents      </title><content>Knowledge and Information distribution is indeed one of the main processes in Knowledge Management. Today, most Information         Technology tools for supporting this distribution are based on repositories accessed through Web-based systems. This approach         has, however, many practical limitations, mainly due to the strain they put on the user, who is responsible of accessing the         right Knowledge and Information at the right moments. As a solution for this problem, we have proposed an alternative approach         which is based on the notion of delegation of distribution tasks to synthetic agents, which become responsible of taking care of the organization's as well as the individuals'         interests. In this way, many Knowledge and Information distribution tasks can be performed on the background, and the agents         can recognize relevant events as triggers for distributing the right information to the right users at the right time.                     In this paper, we present the JITIK approach to model knowledge and information distribution, giving a high-level account               of the research made around this project, emphasizing two particular aspects: a sophisticated argument-based mechanism for               deciding among conflicting distribution policies, and the embedding of JITIK agents in enterprises using the service-oriented               architecture paradigm. It must be remarked that a JITIK-based application is currently being implemented for one of the leading               industries in Mexico.            </content></document><document><year>2007</year><authors>L. Karl Branting1 </authors><title>Learning feature weights from customer return-set selections      </title><content>This paper describes LCW, a procedure for learning customer preferences represented as feature weights by observing customers'         selections from return sets. An empirical evaluation on simulated customer behavior indicated that uninformed hypotheses about         customer weights lead to low ranking accuracy unless customers place some importance on almost all features or the total number         of features is quite small. In contrast, LCW's estimate of the mean preferences of a customer population improved as the number         of customers increased, even for larger numbers of features of widely differing importance. This improvement in the estimate         of mean customer preferences led to improved prediction of individual customers' rankings, irrespective of the extent of variation         among customers and whether a single or multiple retrievals were permitted. The experimental results suggest that the return         set that optimizes benefit may be smaller for customer populations with little variation than for customer populations with         wide variation.      </content></document><document><year>2007</year><authors>Tak-Lam Wong1  | Wai Lam2</authors><title>Learning to extract and summarize hot item features from multiple auction web sites      </title><content>It is difficult to digest the poorly organized and vast amount of information contained in auction Web sites which are fast         changing and highly dynamic. We develop a unified framework which can automatically extract product features and summarize hot item features from multiple auction sites. To         deal with the irregularity in the layout format of Web pages and harness the uncertainty involved, we formulate the tasks         of product feature extraction and hot item feature summarization as a single graph labeling problem using conditional random         fields. One characteristic of this graphical model is that it can model the inter-dependence between neighbouring tokens in         a Web page, tokens in different Web pages, as well as various information such as hot item features across different auction         sites. We have conducted extensive experiments on several real-world auction Web sites to demonstrate the effectiveness of         our framework.      </content></document><document><year>2007</year><authors>Yun Zhou1  | W. Bruce Croft1 </authors><title>Measuring ranked list robustness for query performance prediction      </title><content>We introduce the notion of ranking robustness, which refers to a property of a ranked list of documents that indicates how         stable the ranking is in the presence of uncertainty in the ranked documents. We propose a statistical measure called the         robustness score to quantify this notion. Our initial motivation for measuring ranking robustness is to predict topic difficulty         for content-based queries in the ad-hoc retrieval task. Our results demonstrate that the robustness score is positively and         consistently correlation with average precision of content-based queries across a variety of TREC test collections. Though         our focus is on prediction under the ad-hoc retrieval task, we observe an interesting negative correlation with query performance         when our technique is applied to named-page finding queries, which are a fundamentally different kind of queries. A side effect         of this different behavior of the robustness score between the two types of queries is that the robustness score is also found         to be a good feature for query classification.      </content></document><document><year>2007</year><authors>Shichao Zhang1 | Zifang Huang2 | Jilian Zhang1  | Xiaofeng Zhu1 </authors><title>Mining follow-up correlation patterns from time-related databases      </title><content>Research on traditional association rules has gained a great attention during the past decade. Generally, an association rule         A &amp;#8594; B is used to predict that B likely occurs when A occurs. This is a kind of strong correlation, and indicates that the two events will probably happen simultaneously. However,         in real world applications such as bioinformatics and medical research, there are many follow-up correlations between itemsets         A and B, such as, B is likely to occur n times after A has occurred m times. That is, the correlative itemsets do not belong to the same transaction. We refer to this relation as a follow-up         correlation pattern (FCP). The task of mining FCP patterns brings more challenges on efficient processing than normal pattern         discovery because the number of potentially interesting patterns becomes extremely large as the length limit of transactions         no longer exists. In this paper, we develop an efficient algorithm to identify FCP patterns in time-related databases. We         also experimentally evaluate our approach, and provide extensive results on mining this new kind of patterns.      </content></document><document><year>2007</year><authors>Yun Sing Koh1 | Nathan Rountree1 | Richard A. O&amp;#8217 Keefe1</authors><title>Mining interesting imperfectly sporadic rules      </title><content>Detecting association rules with low support but high confidence is a difficult data mining problem. To find such rules using         approaches like the Apriori algorithm, minimum support must be set very low, which results in a large number of redundant rules. We are interested in sporadic rules; i.e. those that fall below a maximum support level but above the level of support expected from random coincidence. There are two types of sporadic rules: perfectly         sporadic and imperfectly sporadic. Here we are more concerned about finding imperfectly sporadic rules, where the support         of the antecedent as a whole falls below maximum support, but where items may have quite high support individually. In this         paper, we introduce an algorithm called Mining Interesting Imperfectly Sporadic Rules (MIISR) to find imperfectly sporadic         rules efficiently, e.g. fever, headache, stiff neck &amp;#8594; meningitis. Our proposed method uses item constraints and coincidence pruning to discover these rules in reasonable time. This paper         is an expanded version of Koh et al. [Advances in knowledge discovery and data mining: 10th Pacific-Asia Conference (PAKDD         2006), Singapore. Lecture Notes in Computer Science 3918, Springer, Berlin, pp 473&amp;#8211;482].      </content></document><document><year>2007</year><authors>Birgit Hay1| Geert Wets1 | Koen Vanhoof1</authors><title>Mining navigation patterns using a sequence alignment method      </title><content>In this article, a new method is illustrated for mining navigation patterns on a web site. Instead of clustering patterns         by means of a Euclidean distance measure, in this approach users are partitioned into clusters using a non-Euclidean distance         measure called the Sequence Alignment Method (SAM). This method partitions navigation patterns according to the order in which         web pages are requested and handles the problem of clustering sequences of different lengths. The performance of the algorithm         is compared with the results of a method based on Euclidean distance measures. SAM is validated by means of user-traffic data         of two different web sites. Empirical results show that SAM identifies sequences with similar behavioral patterns not only         with regard to content, but also considering the order of pages visited in a sequence.      </content></document><document><year>2007</year><authors>Elena Zudilova-Seinstra1 </authors><title>On the role of individual human abilities in the design of adaptive user interfaces for scientific problem solving environments      </title><content>A scientific problem solving environment should be built in such a way that users (scientists) might exploit underlying technologies         without a specialised knowledge about available tools and resources. An adaptive user interface can be considered as an opportunity         in addressing this challenge. This paper explores the importance of individual human abilities in the design of adaptive user         interfaces for scientific problem solving environments. In total, seven human factors (gender, learning abilities, locus of         control, attention focus, cognitive strategy and verbal and nonverbal IQs) have been evaluated regarding their impact on interface         adjustments done manually by users. People&amp;#8217;s preferences for different interface configurations have been investigated. The         experimental study suggests criteria for the inclusion of human factors into the user model guiding and controlling the adaptation         process. To provide automatic means of adaptation, the Intelligent System for User Modelling has been developed.      </content></document><document><year>2007</year><authors>Keith Marsolo1 | Srinivasan Parthasarathy1 </authors><title>On the use of structure and sequence-based features for protein classification and retrieval      </title><content>The need to retrieve or classify proteins using structure or sequence-based similarity underlies many biomedical applications.         In drug discovery, researchers search for proteins that share specific chemical properties as sources for new treatment. With         folding simulations, similar intermediate structures might be indicative of a common folding pathway. Here we present two         normalized, stand-alone representations of proteins that enable fast and efficient object retrieval based on sequence or structure.         To create our sequence-based representation, we take the profiles returned by the PSI-BLAST alignment algorithm and create         a normalized summary using a discrete wavelet transform. For our structural representation, we transform each 3D structure         into a normalized 2D distance matrix and apply a 2D wavelet decomposition to generate our descriptor. We also create a hybrid         representation by concatenating together the above descriptors. We evaluate the generality of our models by using them as         indices for database retrieval experiments as well as feature vectors for classification. We find that our methods provide         excellent performance when compared with the state-of-the-art for each task. Our results show that the sequence-based representation         is generally superior to the structure-based representation and that in the classification context, the hybrid strategy affords         a significant improvement over sequence or structure.      </content></document><document><year>2007</year><authors>Xuan Hong Dang1 | Wee-Keong Ng1 | Kok-Leong Ong2</authors><title>Online mining of frequent sets in data streams with error guarantee      </title><content>For most data stream applications, the volume of data is too huge to be stored in permanent devices or to be thoroughly scanned         more than once. It is hence recognized that approximate answers are usually sufficient, where a good approximation obtained         in a timely manner is often better than the exact answer that is delayed beyond the window of opportunity. Unfortunately,         this is not the case for mining frequent patterns over data streams where algorithms capable of online processing data streams         do not conform strictly to a precise error guarantee. Since the quality of approximate answers is as important as their timely         delivery, it is necessary to design algorithms to meet both criteria at the same time. In this paper, we propose an algorithm         that allows online processing of streaming data and yet guaranteeing the support error of frequent patterns strictly within         a user-specified threshold. Our theoretical and experimental studies show that our algorithm is an effective and reliable         method for finding frequent sets in data stream environments when both constraints need to be satisfied.      </content></document><document><year>2007</year><authors>Alex|er Smirnov1 | Nikolay Shilov1 | Tatiana Levashova1 | Leonid Sheremetov2  | Miguel Contreras2 </authors><title>Ontology-driven intelligent service for configuration support in networked organizations      </title><content>Nowadays, organizations must continually adapt to market and organizational changes to achieve their most important goals.         Migration to business services and service-oriented architectures provides a valuable opportunity to attain the organization         objectives. This migration causes evolution both in organizational structure and in technology-enabling businesses to dynamically         change vendors and services. One of the forms of organizational structures is the form of networked organization. Technologies         of business intelligence and Web intelligence effectively support business processes within the networked organizations. While business intelligence focuses on development of services for consumer needs recognition, information search, and evaluation of alternatives; Web intelligence addresses advancement of Web-empowered systems, services, and environments. The paper proposes a technological ontology-driven         framework for configuration support as applied to networked organization. The framework integrates concepts of business intelligence and Web intelligence into a collaboration environment of a networked organization on the base of attainment of knowledge logistics purposes. This framework referred to as KSNet is based on the integration of software agent technology and Web services. Knowledge logistics functions of KSNet are complemented by technological functions of knowledge-gathering agents. The services         of these agents are implemented with CAPNET, a FIPA compliant agent platform. CAPNET allows consuming services of agents in         a service-oriented way. Applicability of the approach is illustrated through a &amp;#8220;Binni scenario&amp;#8221;-based case study of a portable         field hospital configuration.      </content></document><document><year>2007</year><authors>Robert Gwadera1 | Aristides Gionis1 | Heikki Mannila1</authors><title>Optimal segmentation using tree models      </title><content>Sequence data are abundant in application areas such as computational biology, environmental sciences, and telecommunications.         Many real-life sequences have a strong segmental structure, with segments of different complexities. In this paper we study         the description of sequence segments using variable length Markov chains (VLMCs), also known as tree models. We discover the         segment boundaries of a sequence and at the same time we compute a VLMC for each segment. We use the Bayesian information         criterion (BIC) and a variant of the minimum description length (MDL) principle that uses the Krichevsky-Trofimov (KT) code         length to select the number of segments of a sequence. On DNA data the method selects segments that closely correspond to         the annotated regions of the genes.      </content></document><document><year>2007</year><authors>Yumao Lu1  | Vwani Roychowdhury2</authors><title>Parallel randomized sampling for support vector machine (SVM) and support vector regression (SVR)      </title><content>A parallel randomized support vector machine (PRSVM) and a parallel randomized support vector regression (PRSVR) algorithm         based on a randomized sampling technique are proposed in this paper. The proposed PRSVM and PRSVR have four major advantages         over previous methods. (1) We prove that the proposed algorithms achieve an average convergence rate that is so far the fastest         bounded convergence rate, among all SVM decomposition training algorithms to the best of our knowledge. The fast average convergence         bound is achieved by a unique priority based sampling mechanism. (2) Unlike previous work (Provably fast training algorithm         for support vector machines, 2001) the proposed algorithms work for general linear-nonseparable SVM and general non-linear         SVR problems. This improvement is achieved by modeling new LP-type problems based on Karush&amp;#8211;Kuhn&amp;#8211;Tucker optimality conditions.         (3) The proposed algorithms are the first parallel version of randomized sampling algorithms for SVM and SVR. Both the analytical         convergence bound and the numerical results in a real application show that the proposed algorithm has good scalability. (4)         We present demonstrations of the algorithms based on both synthetic data and data obtained from a real word application. Performance         comparisons with SVMlight show that the proposed algorithms may be efficiently implemented.      </content></document><document><year>2007</year><authors>Jaideep Vaidya1 | Hwanjo Yu2 | Xiaoqian Jiang2</authors><title>Privacy-preserving SVM classification      </title><content>Traditional Data Mining and Knowledge Discovery algorithms assume free access to data, either at a centralized location or         in federated form. Increasingly, privacy and security concerns restrict this access, thus derailing data mining projects.         What is required is distributed knowledge discovery that is sensitive to this problem. The key is to obtain valid results,         while providing guarantees on the nondisclosure of data. Support vector machine classification is one of the most widely used         classification methodologies in data mining and machine learning. It is based on solid theoretical foundations and has wide         practical application. This paper proposes a privacy-preserving solution for support vector machine (SVM) classification,         PP-SVM for short. Our solution constructs the global SVM classification model from data distributed at multiple parties, without         disclosing the data of each party to others. Solutions are sketched out for data that is vertically, horizontally, or even         arbitrarily partitioned. We quantify the security and efficiency of the proposed method, and highlight future challenges.      </content></document><document><year>2007</year><authors>Ling Qiu1 | Yingjiu Li2  | Xintao Wu3 </authors><title>Protecting business intelligence and customer privacy while outsourcing data mining tasks      </title><content>Nowadays data mining plays an important role in decision making. Since many organizations do not possess the in-house expertise         of data mining, it is beneficial to outsource data mining tasks to external service providers. However, most organizations         hesitate to do so due to the concern of loss of business intelligence and customer privacy. In this paper, we present a Bloom         filter based solution to enable organizations to outsource their tasks of mining association rules, at the same time, protect         their business intelligence and customer privacy. Our approach can achieve high precision in data mining by trading-off the         storage requirement.      </content></document><document><year>2007</year><authors>Te-Wei Chiang1  | Tienwei Tsai2 </authors><title>Querying color images using user-specified wavelet features      </title><content>In this paper, an image retrieval method based on wavelet features is proposed. Due to the superiority in multiresolution         analysis and spatial-frequency localization, the discrete wavelet transform (DWT) is used to extract wavelet features (i.e.,         approximations, horizontal details, vertical details, and diagonal details) at each resolution level. During the feature-extraction         process, each image is first transformed from the standard RGB color space to the YUV space for the purpose of efficiency         and ease of extracting the features based on color tones; then each component (i.e., Y, U, and V) of the image is further         transformed to the wavelet domain. In the image database establishing phase, the wavelet coefficients of each image are stored;         in the image retrieving phase, the system compares the wavelet coefficients of the Y, U, and V components of the query image         with those of the images in the database, based on the weight factors adjusted by users, and find out good matches. To benefit         from the user&amp;#8211;machine interaction, a friendly graphic user interface (GUI) for fuzzy cognition is developed, allowing users         to easily adjust weights for each feature according to their preferences. In our experiment, 1000 test images are used to         demonstrate the effectiveness of our system.      </content></document><document><year>2007</year><authors>Hanghang Tong1 | Christos Faloutsos1  | Jia-Yu Pan1 </authors><title>Random walk with restart: fast solutions and applications      </title><content>How closely related are two nodes in a graph? How to compute this score quickly, on huge, disk-resident, real graphs? Random         walk with restart (RWR) provides a good relevance score between two nodes in a weighted graph, and it has been successfully         used in numerous settings, like automatic captioning of images, generalizations to the &amp;#8220;connection subgraphs&amp;#8221;, personalized         PageRank, and many more. However, the straightforward implementations of RWR do not scale for large graphs, requiring either         quadratic space and cubic pre-computation time, or slow response time on queries. We propose fast solutions to this problem.         The heart of our approach is to exploit two important properties shared by many real graphs: (a) linear correlations and (b)         block-wise, community-like structure. We exploit the linearity by using low-rank matrix approximation, and the community structure         by graph partitioning, followed by the Sherman&amp;#8211;Morrison lemma for matrix inversion. Experimental results on the Corel image         and the DBLP dabasets demonstrate that our proposed methods achieve significant savings over the straightforward implementations:         they can save several orders of magnitude in pre-computation and storage cost, and they achieve up to 150;Г— speed up with 90%+ quality preservation.      </content></document><document><year>2007</year><authors>Gabriela Moise1 | JГ¶rg S|er1 | Martin Ester2</authors><title>Robust projected clustering      </title><content>Projected clustering partitions a data set into several disjoint clusters, plus outliers, so that each cluster exists in a         subspace. Subspace clustering enumerates clusters of objects in all subspaces of a data set, and it tends to produce many         overlapping clusters. Such algorithms have been extensively studied for numerical data, but only a few have been proposed         for categorical data. Typical drawbacks of existing projected and subspace clustering algorithms for numerical or categorical         data are that they rely on parameters whose appropriate values are difficult to set appropriately or that they are unable         to identify projected clusters with few relevant attributes. We present P3C, a robust algorithm for projected clustering that         can effectively discover projected clusters in the data while minimizing the number of required parameters. P3C does not need         the number of projected clusters as input, and can discover, under very general conditions, the true number of projected clusters.         P3C is effective in detecting very low-dimensional projected clusters embedded in high dimensional spaces. P3C positions itself         between projected and subspace clustering in that it can compute both disjoint or overlapping clusters. P3C is the first projected         clustering algorithm for both numerical and categorical data.      </content></document><document><year>2007</year><authors>Chunming Hu1 | Yanmin Zhu2| Jinpeng Huai1| Yunhao Liu2 | Lionel M. Ni2</authors><title>S-Club: an overlay-based efficient service discovery mechanism in CROWN Grid      </title><content>Information service plays a key role in grid system, handles resource discovery and management process. Employing existing         information service architectures suffers from poor scalability, long search response time, and large traffic overhead. In         this paper, we propose a service club mechanism, called S-Club, for efficient service discovery. In S-Club, an overlay based         on existing Grid Information Service (GIS) mesh network of CROWN is built, so that GISs are organized as service clubs. Each         club serves for a certain type of service while each GIS may join one or more clubs. S-Club is adopted in our CROWN Grid and         the performance of S-Club is evaluated by comprehensive simulations. The results show that S-Club scheme significantly improves         search performance and outperforms existing approaches.      </content></document><document><year>2007</year><authors>Ramazan S. AygГјn1 </authors><title>S2S: structural-to-syntactic matching similar documents      </title><content>Management of large collection of replicated data in centralized or distributed environments is important for many systems         that provide data mining, mirroring, storage, and content distribution. In its simplest form, the documents are generated,         duplicated and updated by emails and web pages. Although redundancy may increase the reliability at a level, uncontrolled         redundancy aggravates the retrieval performance and might be useless if the returned documents are obsolete. Document similarity         matching algorithms do not provide the information on the differences of documents, and file synchronization algorithms are         usually inefficient and ignore the structural and syntactic organization of documents. In this paper, we propose the S2S matching approach. The S2S matching is composed of structural and syntactic phases to compare documents. Firstly, in the structural phase, documents         are decomposed into components by its syntax and compared at the coarse level. The structural mapping processes the decomposed         documents based on its syntax without actually mapping at the word level. The structural mapping can be applied in a hierarchical         way based on the structural organization of a document. Secondly, the syntactic matching algorithm uses a heuristic look-ahead         algorithm for matching consecutive tokens with a verification patch. Our two-phase S2S matching approach provides faster results than currently available string matching algorithms.      </content></document><document><year>2007</year><authors>Benjamin Rozenfeld1  | Ronen Feldman1 </authors><title>Self-supervised relation extraction from the Web      </title><content>Web extraction systems attempt to use the immense amount of unlabeled text in the Web in order to create large lists of entities         and relations. Unlike traditional Information Extraction methods, the Web extraction systems do not label every mention of         the target entity or relation, instead focusing on extracting as many different instances as possible while keeping the precision         of the resulting list reasonably high. SRES is a self-supervised Web relation extraction system that learns powerful extraction         patterns from unlabeled text, using short descriptions of the target relations and their attributes. SRES automatically generates         the training data needed for its pattern-learning component. The performance of SRES is further enhanced by classifying its         output instances using the properties of the instances and the patterns. The features we use for classification and the trained         classification model are independent from the target relation, which we demonstrate in a series of experiments. We also compare         the performance of SRES to the performance of the state-of-the-art KnowItAll system, and to the performance of its pattern         learning component, which learns simpler pattern language than SRES.      </content></document><document><year>2007</year><authors>Dacheng Tao1 | Xuelong Li1| Xindong Wu2| Weiming Hu3 | Stephen J. Maybank1</authors><title>Supervised tensor learning      </title><content>Tensor representation is helpful to reduce the small sample size problem in discriminative subspace selection. As pointed         by this paper, this is mainly because the structure information of objects in computer vision research is a reasonable constraint         to reduce the number of unknown parameters used to represent a learning model. Therefore, we apply this information to the         vector-based learning and generalize the vector-based learning to the tensor-based learning as the supervised tensor learning         (STL) framework, which accepts tensors as input. To obtain the solution of STL, the alternating projection optimization procedure         is developed. The STL framework is a combination of the convex optimization and the operations in multilinear algebra. The         tensor representation helps reduce the overfitting problem in vector-based learning. Based on STL and its alternating projection         optimization procedure, we generalize support vector machines, minimax probability machine, Fisher discriminant analysis,         and distance metric learning, to support tensor machines, tensor minimax probability machine, tensor Fisher discriminant analysis,         and the multiple distance metrics learning, respectively. We also study the iterative procedure for feature extraction within         STL. To examine the effectiveness of STL, we implement the tensor minimax probability machine for image classification. By         comparing with minimax probability machine, the tensor version reduces the overfitting problem.      </content></document><document><year>2007</year><authors>Tao Peng1 | Wanli Zuo1| 2 | Fengling He1| 2</authors><title>SVM based adaptive learning method for text classification from positive and unlabeled documents      </title><content>Automatic text classification is one of the most important tools in Information Retrieval. This paper presents a novel text         classifier using positive and unlabeled examples. The primary challenge of this problem as compared with the classical text         classification problem is that no labeled negative documents are available in the training example set. Firstly, we identify         many more reliable negative documents by an improved 1-DNF algorithm with a very low error rate. Secondly, we build a set         of classifiers by iteratively applying the SVM algorithm on a training data set, which is augmented during iteration. Thirdly,         different from previous PU-oriented text classification works, we adopt the weighted vote of all classifiers generated in         the iteration steps to construct the final classifier instead of choosing one of the classifiers as the final classifier.         Finally, we discuss an approach to evaluate the weighted vote of all classifiers generated in the iteration steps to construct         the final classifier based on PSO (Particle Swarm Optimization), which can discover the best combination of the weights. In         addition, we built a focused crawler based on link-contexts guided by different classifiers to evaluate our method. Several         comprehensive experiments have been conducted using the Reuters data set and thousands of web pages. Experimental results         show that our method increases the performance (F1-measure) compared with PEBL, and a focused web crawler guided by our PSO-based classifier outperforms other several classifiers         both in harvest rate and target recall.      </content></document><document><year>2007</year><authors>Gilbert L. Peterson1  | Brent T. McBride1</authors><title>The importance of generalizability for anomaly detection      </title><content>In security-related areas there is concern over novel &amp;#8220;zero-day&amp;#8221; attacks that penetrate system defenses and wreak havoc. The         best methods for countering these threats are recognizing &amp;#8220;nonself&amp;#8221; as in an Artificial Immune System or recognizing &amp;#8220;self&amp;#8221;         through clustering. For either case, the concern remains that something that appears similar to self could be missed. Given         this situation, one could incorrectly assume that a preference for a tighter fit to self over generalizability is important         for false positive reduction in this type of learning problem. This article confirms that in anomaly detection as in other         forms of classification a tight fit, although important, does not supersede model generality. This is shown using three systems         each with a different geometric bias in the decision space. The first two use spherical and ellipsoid clusters with a k-means algorithm modified to work on the one-class/blind classification problem. The third is based on wrapping the self points         with a multidimensional convex hull (polytope) algorithm capable of learning disjunctive concepts via a thresholding constant.         All three of these algorithms are tested using the Voting dataset from the UCI Machine Learning Repository, the MIT Lincoln         Labs intrusion detection dataset, and the lossy-compressed steganalysis domain.      </content></document><document><year>2007</year><authors>Xindong Wu1 | Vipin Kumar2 | J. Ross Quinlan3 | Joydeep Ghosh4 | Qiang Yang5 | Hiroshi Motoda6 | Geoffrey J. McLachlan7 | Angus Ng8| Bing Liu9| Philip S. Yu10 | Zhi-Hua Zhou11 | Michael Steinbach12 | David J. H|13  | Dan Steinberg14 </authors><title>Top 10 algorithms in data mining      </title><content>This paper presents the top 10 data mining algorithms identified by the IEEE International Conference on Data Mining (ICDM)         in December 2006: C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART. These top 10 algorithms are among the most influential data mining algorithms in the research community.         With each algorithm, we provide a description of the algorithm, discuss the impact of the algorithm, and review current and         further research on the algorithm. These 10 algorithms cover classification, clustering, statistical learning, association         analysis, and link mining, which are all among the most important topics in data mining research and development.      </content></document><document><year>2007</year><authors>Brian Corrie1  | Margaret-Anne Storey1 </authors><title>Toward understanding the importance of gesture in distributed scientific collaboration      </title><content>In this paper, we explore the importance of gesture in distributed, scientific collaboration. In particular, we are interested         in the impact that distance has when remote collaborators are working together with digital artifacts that are complex (and         often visual) in form, such as data that results from complex scientific simulations. We call this artifact-centric collaboration.         In order to understand such collaborations, we performed a longitudinal ethnographic study of a group of collaborating scientific         researchers. We observed a single research group during its regular research meetings, performing over 18;h of observations         spanning a 5-month period. In this paper, we present a detailed analysis of two meetings where artifact interaction is prominent,         one where all participants are collocated and the other where participants are distributed. Our analysis consists of a detailed         coding of the artifact-centric interactions in both meetings as well as an analysis of these interactions. We conclude the         paper with a summary of our findings, including a set of guidelines that can be used to inform the design of collaboration         software that supports distributed, artifact-centric collaboration.      </content></document><document><year>2007</year><authors>Aoying Zhou1 | Feng Cao2| Weining Qian1 | Cheqing Jin1| 3</authors><title>Tracking clusters in evolving data streams over sliding windows      </title><content>Mining data streams poses great challenges due to the limited memory availability and real-time query response requirement.         Clustering an evolving data stream is especially interesting because it captures not only the changing distribution of clusters         but also the evolving behaviors of individual clusters. In this paper, we present a novel method for tracking the evolution         of clusters over sliding windows. In our SWClustering algorithm, we combine the exponential histogram with the temporal cluster         features, propose a novel data structure, the Exponential Histogram of Cluster Features (EHCF). The exponential histogram         is used to handle the in-cluster evolution, and the temporal cluster features represent the change of the cluster distribution.         Our approach has several advantages over existing methods: (1) the quality of the clusters is improved because the EHCF captures         the distribution of recent records precisely; (2) compared with previous methods, the mechanism employed to adaptively maintain the in-cluster synopsis         can track the cluster evolution better, while consuming much less memory; (3) the EHCF provides a flexible framework for analyzing         the cluster evolution and tracking a specific cluster efficiently without interfering with other clusters, thus reducing the         consumption of computing resources for data stream clustering. Both the theoretical analysis and extensive experiments show         the effectiveness and efficiency of the proposed method.      </content></document><document><year>2007</year><authors>Klaus-Dieter Schewe1 | Jane Zhao1 </authors><title>Typed Abstract State Machines for data-intensive applications      </title><content>This paper focuses on extensions of Abstract State Machines (ASMs) for the benefit of a refinement calculus in the area of         data-intensive applications, in particular data warehouses and on-line analytical processing. The extensions lead to a typed         version of ASMs and a more specific notion of strong data refinement, which incorporates the preservation of information content         by means of schema dominance. Each typed ASM can be effectively translated into an equivalent ordinary one. Providing typed         ASMs helps to exploit the existing logical formalisms used in data-intensive applications to define a ground model. Furthermore,         strong data refinement helps to set up a refinement-based development method that uses provably correct standard refinement         rules.      </content></document><document><year>2007</year><authors>Yang Cai1 | Richard Stumpf2 | Timothy Wynne2| Michelle Tomlinson2| Daniel Sai Ho Chung1| Xavier Boutonnier1| Matthias Ihmig1| Rafael Franco1 | Nathaniel Bauernfeind1</authors><title>Visual transformation for interactive spatiotemporal data mining      </title><content>Analytical models intend to reveal inner structure, dynamics, or relationship of things. However, they are not necessarily         intuitive to humans. Conventional scientific visualization methods are intuitive, but limited by depth, dimension, and resolution.         The purpose of this study is to bridge the gap with transformation algorithms for mapping the data from an abstract space         to an intuitive one, which include shape correlation, periodicity, multiphysics, and spatial Bayesian. We tested this approach         with the oceanographic case study. We found that the interactive visualization increases robustness in object tracking and         positive detection accuracy in object prediction. We also found that the interactive method enables the user to process the         image data at less than 1 min per image versus 30 min per image manually. As a result, our test system can handle at least         10 times more data sets than traditional manual analyses. The results also suggest that minimal human interactions with appropriate         computational transformations or cues may significantly increase the overall productivity.      </content></document><document><year>2007</year><authors>Elena Zudilova-Seinstra1  | Tony Adriaansen2 </authors><title>Visualisation and interaction for scientific exploration and knowledge discovery      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Craig Macdonald1  | Iadh Ounis1 </authors><title>Voting techniques for expert search      </title><content>In an expert search task, the users&amp;#8217; need is to identify people who have relevant expertise to a topic of interest. An expert         search system predicts and ranks the expertise of a set of candidate persons with respect to the users&amp;#8217; query. In this paper,         we propose a novel approach for predicting and ranking candidate expertise with respect to a query, called the Voting Model         for Expert Search. In the Voting Model, we see the problem of ranking experts as a voting problem. We model the voting problem         using 12 various voting techniques, which are inspired from the data fusion field. We investigate the effectiveness of the         Voting Model and the associated voting techniques across a range of document weighting models, in the context of the TREC         2005 and TREC 2006 Enterprise tracks. The evaluation results show that the voting paradigm is very effective, without using         any query or collection-specific heuristics. Moreover, we show that improving the quality of the underlying document representation         can significantly improve the retrieval performance of the voting techniques on an expert search task. In particular, we demonstrate         that applying field-based weighting models improves the ranking of candidates. Finally, we demonstrate that the relative performance         of the voting techniques for the proposed approach is stable on a given task regardless of the used weighting models, suggesting         that some of the proposed voting techniques will always perform better than other voting techniques.      </content></document><document><year>2007</year><authors>Wei Wang1 | Jiong Yang2 | Philip Yu3</authors><title>WAR: Weighted association rules for item intensities      </title><content>In this paper, we extend the traditional association rule problem by allowing a weight to be associated with each item in         a transaction to reflect the interest/intensity of each item within the transaction. In turn, this provides us with an opportunity         to associate a weight parameter with each item in a resulting association rule; we call them weighted association rules (WAR).         One example of such a rule might be 80% of people buying more than three bottles of soda will also be likely to buy more than         four packages of snack food, while a conventional association rule might just be 60% of people buying soda will be also be         likely to buy snack food. Thus WARs cannot only improve the confidence of the rules, but also provide a mechanism to do more         effective target marketing by identifying or segmenting customers based on their potential degree of loyalty or volume of         purchases. Our approach mines WARs by first ignoring the weight and finding the frequent itemsets (via a traditional frequent         itemset discovery algorithm), followed by introducing the weight during the rule generation. Specifically, the rule generation         is achieved by partitioning the weight domain space of each frequent itemset into fine grids, and the identifying the popular         regions within the domain space to derive WARs. This approach does not only support the batch mode mining, i.e., finding WARs         for the dataset, but also supports the interactive mode, i.e., finding and refining WARs for a given (set) of frequent itemset(s).      </content></document><document><year>2003</year><authors>Qiang Zhu1| Satyanarayana Motheramgari1 | Yu Sun1</authors><title>Cost Estimation for Queries Experiencing Multiple Contention States in Dynamic          Multidatabase Environments               </title><content>Abstract.;; Accurate query cost estimation is crucial to query optimization in a multidatabase          system. Several estimation techniques for a static environment have been suggested          in the literature. To develop a cost model for a dynamic environment, we recently          introduced a multistate query-sampling method. It has been shown that this technique is          promising in estimating the cost ofa query run in any given contention state for a dynamic          environment. In this paper, we study a new problem on how to estimate the cost of a          large query that may experience multiple contention states. Following the discussion of          limitations for two simple approaches, i.e., single state analysis and average cost analysis,          we propose two novel techniques to tackle this challenge. The first one, called fractional          analysis, is suitable for a gradually and smoothly changing environment, while the second          one, called the probabilistic approach, is developed for a rapidly and randomly changing          environment. The former estimates a query cost by analyzing its fractions, and the latter          estimates a query cost based on Markov chain theory. The related issues including cost          formula development, error analysis, and comparison among different approaches are          discussed. Experiments demonstrate that the proposed techniques are quite promising in          solving the new problem.               </content></document><document><year>2003</year><authors>Debbie Richards1</authors><title>Knowledge-Based System Explanation: The Ripple-Down Rules Alternative               </title><content>Abstract.;; The ability to provide explanations has been seen as a key feature of expert          systems (ES) typically not offered by other types ofcomputer systems. ES need to offer          explanations because ofimprecise domains and the use ofheuristics. Verification is not          enough. ES need to justify and be accountable. Explanation is seen as an important          activity for knowledge-based systems as it satisfies the user's need to decide whether to          accept or reject a recommendation. In this paper we review explanation in first-generation          and second-generation ES. An alternative is offered to the main approaches which uses          multiple classification ripple-down rules and challenges even the goals of explanation.          Instead of trying to give explanations which provide a meaningful line of reasoning and          which are tailored to suit the individual it may be just as useful to provide the user with          sufficient information and browsing tools to develop their own line of reasoning. The type          of information that can assist understanding is the context in which the recommendation          applies (which is provided through the display ofrelevant cases and exception rule history)          and the ability to explore an abstraction hierarchy of the rules using formal concept          analysis. An explanation tool kit aimed at putting the user in control is described and          evaluated in this paper.               </content></document><document><year>2003</year><authors>Jixue Liu1| Millist W. Vincent1 | Mukesh Mohania2</authors><title>Maintaining Views in Object-Relational Databases               </title><content>Abstract.;; View materialization is an important way of improving the performance of query          processing. When an update occurs to the source data from which a materialized view is          derived, the materialized view has to be updated so that it is consistent with the source          data. This update process is called view maintenance. The incremental method of view          maintenance, which computes the new view using the old view and the update to the          source data, is widely preferred to full view recomputation when the update is small in          size. In this paper we investigate how to incrementally maintain views in object-relational          (OR) databases. The investigation focuses on maintaining views defined in OR-SQL, a          language containing the features of object referencing, inheritance, collection, and aggregate          functions including user-defined set aggregate functions. We propose an architecture and          algorithms for incremental OR viewmaintenance. We implement all algorithms and analyze          the performance of them in comparison with full view recomputation. The analysis shows          that the algorithms significantly reduce the cost of updating a vieww hen the size of an          update to the source data is relatively small.               </content></document><document><year>2003</year><authors>Xindong Wu1 | Christiane Notarmarco2</authors><title>Online First publication      </title><content>Without Abstract</content></document><document><year>2003</year><authors>Dieter Fensel1| Enrico Motta2| Frank van Harmelen1| V. Richard Benjamins3| Monica Crubezy4| Stefan Decker5| Mauro Gaspari6|  1| Rix Groenboom7| William Grosso4| Mark Musen4| Enric Plaza8| Guus Schreiber3| Rudi Studer5 | Bob Wielinga3</authors><title>The Unified Problem-Solving Method          Development Language UPML               </title><content>Abstract.;; Problem-solving methods provide reusable architectures and components for implementing          the reasoning part of knowledge-based systems. The Unified Problem-Solving                Method Description Language (UPML) has been developed to describe and implement          such architectures and components to facilitate their semi-automatic reuse and adaptation.          In a nutshell, UPML is a framework for developing knowledge-intensive reasoning          systems based on libraries ofg eneric problem-solving components. The paper describes          the components and adapters, architectural constraints, development guidelines, and tools          provided by UPML. UPML is developed as part of the IBROW project, which provides          an Internet-based brokering service for reusing problem-solving methods.               </content></document><document><year>2002</year><authors> </authors><title>2002 KAIS Reviewers               </title><content>;;Knowledge and Information Systems wishes to acknowledge and thank the individuals listed below who have provided their time         and valuable experience to review manuscripts during the past year. It is only through the dedicated efforts of these individuals         and those on the Editorial Board listed on the inner front cover, who have also provided valuable input and support, that         the quality of publication in the Journal can be maintained.               </content></document><document><year>2002</year><authors>Veyis Gunes1| Pierre Loonis1 | Michel MГ©nard1</authors><title>A Fuzzy Petri Net for Pattern Recognition: Application to Dynamic Classes      </title><content>Abstract.;; When involving evolutionary natural objects, the odeling of dynamic lasses is the main issue for a pattern recognition system.         This problem an be avoided by making dynamic the syste of pattern recognition which an then enter into various states according         to the evolution of the lasses. We propose a dynamic recognition system founded on two types of learning. The static aspect         of the learning is ensured by lassifiers or systems of lassifiers, while the dynamic aspect is translated by the learning         of the planning of the various states by a fuzzy Petri net. The method is sucessfully applied to a synthetic data set.      </content></document><document><year>2002</year><authors>Dongsong Zhang1| Lina Zhou1 | Jay F. Nunamaker Jr1</authors><title>A Knowledge Management Framework for the Support of Decision Making in Humanitarian Assistance/Disaster Relief      </title><content>Abstract.;; The major challenge in current humanitarian assistance/disaster relief (HA/DR) efforts is that diverse information and knowledge         are widely distributed and owned by different organizations. These resources are not efficiently organized and utilized during         HA/DR operations. We present a knowledge management framework that integrates multiple information technologies to collect,         analyze, and manage information and knowledge for supporting decision making in HA/DR. The framework will help identify the         information needs, be aware of a disaster situation, and provide decision-makers with useful relief recommendations based         on past experience. A comprehensive, consistent and authoritative knowledge base within the framework will facilitate knowledge         sharing and reuse. This framework can also be applied to other similar real-time decision-making environments, such as crisis         management and emergency medical assistance.      </content></document><document><year>2002</year><authors>Michael Hadjimichael1| Arunas P. Kuciauskas1| Paul M. Tag1| Richard L. Bankert1 | James E. Peak2</authors><title>A Meteorological Fuzzy Expert System Incorporating Subjective User Input      </title><content>Abstract.;; We present a fuzzy expert system, MEDEX, for forecasting gale-force winds in the Mediterranean basin. The most successful         local wind forecasting in this region is achieved by an expert human forecaster with access to numerical weather prediction         products. That forecaster's knowledge is expressed as a set of &amp;#8216;rules-of-thumb&amp;#8217;. Fuzzy set methodologies have proved well         suited for encoding the forecaster's knowledge, and for accommodating the uncertainty inherent in the specification of rules,         as well as in subjective and objective input. MEDEX uses fuzzy set theory in two ways: as a fuzzy rule base in the expert         system, and for fuzzy pattern matching to select dominant wind circulation patterns as one input to the expert system. The         system was developed, tuned, and verified over a two-year period, during which the weather conditions from 539 days were individually         analyzed. Evaluations of MEDEX performance for both the onset and cessation of winter and summer winds are presented, and         demonstrate that MEDEX has forecasting skill competitive with the US Navy's regional forecasting center in Rota, Spain.      </content></document><document><year>2002</year><authors>Qinghua Zou1| Wesley Chu1| David Johnson2 | Henry Chiu3</authors><title>A Pattern Decomposition Algorithm for Data Mining of Frequent Patterns               </title><content>Abstract.;; Efficient algorithms to mine frequent patterns are crucial to many tasks in data mining. Since the Apriori algorithm was         proposed in 1994, there have been several methods proposed to improve its performance. However, most still adopt its candidate         set generation-and-test approach. In addition, many methods do not generate all frequent patterns, making them inadequate         to derive association rules. We propose a pattern decomposition (PD) algorithm that can significantly reduce the size of the         dataset on each pass, making it more efficient to mine all frequent patterns in a large dataset. The proposed algorithm avoids         the costly process of candidate set generation and saves time by reducing the size of the dataset. Our empirical evaluation         shows that the algorithm outperforms Apriori by one order of magnitude and is faster than FP-tree algorithm.               </content></document><document><year>2002</year><authors>Shyh-Horng Jou1 | Shang-Juh Kao1</authors><title>Agent-Based Infrastructure and an Application to Internet Information Gathering      </title><content>Abstract.;; Pragmatic applications and studies of agent-based software engineering have evolved over the last decade. In order to explore         how an agent is organized and applied, in this paper an agent framework is presented and applied to Internet information gathering.         Agent systems are classified as micro or macro perspectives and agent applications are characterized by the four feature dimensions:         behavior (user), knowledge (task), safety (time), and cooperation (social). An agent itself can be modeled according to the         information, behavior, and organization aspects of the agent&amp;#8217;s functional modules as proposed in this paper. A three-tier         multi-agent and JAVA-implemented system, which coordinates information-gathering activities using KQML for inter-agent communication,         is developed on the basis of the proposed architectural modules. Finally, we explore possible areas for future study.      </content></document><document><year>2002</year><authors>Minghua He1 | Ho-fung Leung1</authors><title>Agents in E-Commerce: State of the Art      </title><content>Abstract.;; This paper surveys the state of the art of agent-mediated electronic commerce (e-commerce), especially in business-to-consumer         (B2C) e-commerce and business-to-business (B2B) e-commerce. From the consumer buying behaviour perspective, the roles of agents         in B2C e-commerce are: product brokering, merchant brokering, and negotiation. The applications of agents in B2B e-commerce         are mainly in supply chain management. Mobile agents, evolutionary agents, and data-mining agents are some special techniques         which can be applied in agent-mediated e-commerce. In addition, some technologies for implementation are briefly reviewed.         Finally, we conclude this paper by discussions on the future directions of agent-mediated e-commerce.      </content></document><document><year>2002</year><authors>Masahiro Terabe1| Takashi Washio2| Hiroshi Motoda2| Osamu Katai3 | Tetsuo Sawaragi4</authors><title>Attribute Generation Based on Association Rules      </title><content>Abstract.;; A decision tree is considered to be appropriate (1) if the tree can classify the unseen data accurately, and (2) if the size         of the tree is small. One of the approaches to induce such a good decision tree is to add new attributes and their values         to enhance the expressiveness of the training data at the data pre-processing stage. There are many existing methods for attribute         extraction and construction, but constructing new attributes is still an art. These methods are very time consuming, and some         of them need a priori knowledge of the data domain. They are not suitable for data mining dealing with large volumes of data.         We propose a novel approach that the knowledge on attributes relevant to the class is extracted as association rules from         the training data. The new attributes and the values are generated from the association rules among the originally given attributes.         We elaborate on the method and investigate its feature. The effectiveness of our approach is demonstrated through some experiments.      </content></document><document><year>2002</year><authors>Weiyi Meng1| Wenxian Wang1| Hongyu Sun1 | Clement Yu2</authors><title>Concept Hierarchy-Based Text Database Categorization      </title><content>Abstract.;; Document categorization as a technique to improve the retrieval of useful documents has been extensively investigated. One         important issue in a large-scale metasearch engine is to select text databases that are likely to contain useful documents         for a given query. We believe that database categorization can be a potentially effective technique for good database selection,         especially in the Internet environment where short queries are usually submitted. In this paper, we propose and evaluate several         database categorization algorithms. This study indicates that while some document categorization algorithms could be adopted         for database categorization, algorithms that take into consideration the special characteristics of databases may be more         effective. Preliminary experimental results are provided to compare the proposed database categorization algorithms. A prototype         database categorization system based on one of the proposed algorithms has been developed.      </content></document><document><year>2002</year><authors>Sung I. Yong1 | Won S. Lee1</authors><title>Content-Based Retrieval of Video Data with Flexibly Managed Attributes               </title><content>Abstract.;; Various types of information can be mixed in a continuous video stream without any clear boundary. For this reason, there         seems to be no simple solution to support content-based queries in a video database. The meaning of a video scene can be interpreted         by multiple levels of abstraction and its description can be varied among different users. Therefore, it is important for         a user to be able to describe a scene flexibly while the description given by different users should be maintained consistently.         This paper proposes an effective way to represent the different types of video information in the conventional relational         database model. Flexibly defined attributes and their values are organized as tree-structured dictionaries while the description         of video data is stored in a fixed database schema. In order to assist a user, two browsing methods are introduced. The dictionary         browser simplifies the annotation process as well as the querying process of a user while the result browser can help a user         analyze the results of a query in terms of various combinations of query conditions.               </content></document><document><year>2002</year><authors>Zhong Su1| Qiang Yang2| Hongjiang Zhang3| Xiaowei Xu4| Yu-Hen Hu5 | Shaoping Ma1</authors><title>Correlation-Based Web Document Clustering for Adaptive Web Interface Design      </title><content>Abstract.;; A great challenge for web site designers is how to ensure users' easy access to important web pages efficiently. In this         paper we present a clustering-based approach to address this problem. Our approach to this challenge is to perform efficient         and effective correlation analysis based on web logs and construct clusters of web pages to reflect the co-visit behavior         of web site users. We present a novel approach for adapting previous clustering algorithms that are designed for databases         in the problem domain of web page clustering, and show that our new methods can generate high-quality clusters for very large         web logs when previous methods fail. Based on the high-quality clustering results, we then apply the data-mined clustering         knowledge to the problem of adapting web interfaces to improve users' performance. We develop an automatic method for web         interface adaptation: by introducing index pages that minimize overall user browsing costs. The index pages are aimed at providing         short cuts for users to ensure that users get to their objective web pages fast, and we solve a previously open problem of         how to determine an optimal number of index pages. We empirically show that our approach performs better than many of the         previous algorithms based on experiments on several realistic web log files.      </content></document><document><year>2002</year><authors>Vincent Cho1 | Beat WГјthrich2</authors><title>Distributed Mining of Classification Rules      </title><content>Abstract.;; Many successful data-mining techniques and systems have been developed. These techniques usually apply to centralized databases         with less restricted requirements on learning and response time. Not so much effort has yet been put into mining distributed         databases and real-time issues. In this paper, we investigate issues of fast-distributed data mining. We assume that merging         the distributed databases into a single one would either be too costly (distributed case) or the individual fragments would         be non-uniform so that mining only one fragment would bias the result (fragmented case). The goal is to classify the objects         O of the database into one of several mutually exclusive classes C                     i            . Our approach to make mining fast and feasible is as follows. From each data site or fragment db                     k            , only a single rule r                     ik             is generated for each class C                     i            . A small subset {r                     i1            , &amp;#8230;, r                     ih            } of these individual rules is selected to form a rule set R                     i             for each class C                     i             . These rule subsets represent adequately the hidden knowledge of the entire database. Various selection criteria to form         R                     i             are discussed, both theoretically and experimentally.      </content></document><document><year>2002</year><authors>Yugyung Lee1 | James Geller2</authors><title>Efficient Transitive Closure Reasoning in a Combined Class/Part/Containment Hierarchy      </title><content>Abstract.;; Class hierarchies form the backbone of many implemented knowledge representation and reasoning systems. They are used for         inheritance, classification and transitive closure reasoning. Part hierarchies are also important in artificial intelligence.         Other hierarchies, e.g. containment hierarchies, have received less attention in artificial intelligence. This paper presents         an architecture and an implementation of a hierarchy reasoner that integrates a class hierarchy, a part hierarchy, and a containment         hierarchy into one structure. In order to make an implemented reasoner useful, it needs to operate at least at speeds comparable         to human reasoning. As real-world hierarchies are always large, special techniques need to be used to achieve this. We have         developed a set of parallel algorithms and a data representation called maximally reduced tree cover for that purpose. The         maximally reduced tree cover is an improvement of a materialized transitive closure representation which has appeared in the         literature. Our experiments with a medical vocabulary show that transitive closure reasoning for combined class/part/containment         hierarchies in near constant time is possible for a fixed hardware configuration.      </content></document><document><year>2002</year><authors>Kai Cheng1 | Yahiko Kambayashi1</authors><title>Enhanced Proxy Caching with Content Management      </title><content>Abstract.;; In this paper, we propose a novel approach to enhancing web proxy caching, an approach that integrates content management         with performance tuning techniques. We first develop a hierarchical model for management of web data, which consists of physical         pages, logical pages and topics corresponding to different abstraction levels. Content management based on this model enables         active utilization of the cached web contents. By defining priority on each abstraction level, the cache manager can make         replacement decisions on topics, logical pages, and physical pages hierarchically. As a result, a cache can always keep the         most relevant, popular, and high-quality content. To verify the proposed approach, we have designed a content-aware replacement         algorithm, LRU-SP+. We evaluate the algorithm through preliminary experiments. The results show that content management can         achieve 30% improvement of caching performance in terms of hit ratios and profit ratios (considering significance of topics)         compared to content-blind schemes.      </content></document><document><year>2002</year><authors>David Chek Ling Ngo1| Lian Seng Teo1 | John G. Byrne2</authors><title>Evaluating Interface Esthetics      </title><content>Abstract.;; Gestalt psychologists promulgated the principles of visual organization in the early twentieth century. These principles         have been discussed and re-emphasized, and their importance and relevance to user interface design are understood. However,         a limited number of systems represent and make adequate use of this knowledge in the form of a design tool that supports certain         aspects of the user interface design process. The graphic design rules that these systems use are extremely rudimentary and         often vastly oversimplified. Most of them have no concept of design basics such as visual balance or rhythm. In this paper,         we attempt to synthesize the guidelines and empirical data related to the formatting of screen layouts into a well-defined         model. Fourteen esthetic characteristics have been selected for the purpose. The results of our exercise suggest that these         characteristics are important to prospective viewers.      </content></document><document><year>2002</year><authors>Dantong Yu1| Gholamhosein Sheikholeslami1 | Aidong Zhang1</authors><title>         FindOut: Finding Outliers in Very Large Datasets               </title><content>Abstract.;; Finding the rare instances or the outliers is important in many KDD (knowledge discovery and data-mining) applications, such         as detecting credit card fraud or finding irregularities in gene expressions. Signal-processing techniques have been introduced         to transform images for enhancement, filtering, restoration, analysis, and reconstruction. In this paper, we present a new         method in which we apply signal-processing techniques to solve important problems in data mining. In particular, we introduce         a novel deviation (or outlier) detection approach, termed FindOut, based on wavelet transform. The main idea in FindOut is to remove the clusters from the original data and then identify         the outliers. Although previous research showed that such techniques may not be effective because of the nature of the clustering,         FindOut can successfully identify outliers from large datasets. Experimental results on very large datasets are presented         which show the efficiency and effectiveness of the proposed approach.               </content></document><document><year>2002</year><authors>Changzhou Wang1 | Xiaoyang Sean Wang2</authors><title>High-Dimensional Nearest Neighbor Search with Remote Data Centers               </title><content>Abstract.;; Many data centers have archived a tremendous amount of data and begun to publish them on the Web. Due to limited resources         and large amount of service requests, data centers usually do not directly support high-cost queries. On the other hand, users         are often overwhelmed by the huge data volume and cannot afford to download the whole data sets and search them locally. To         support high-dimensional nearest neighbor searches in this environment, the paper develops a multi-level approximation scheme.         The coarsest-level approximations are stored locally and searched first. The result is then refined gradually via accesses         to remote data centers. Data centers need only to deliver data items or their precomputed finer level approximations by their         identifiers.                     The searching process is usually long in this environment, since it involves remote sites. This paper describes an online               search process: the system periodically reports a data item and a positive integer M. The reported item is guaranteed to be one of the M nearest neighbors of the query one. The paper proposes two algorithms to minimize M in each period. Experiments show that one of them performs similarly as a theoretical a posteriori algorithm and significantly               outperforms the online extensions of two state-of-the-art nearest neighbor search methods.                           </content></document><document><year>2002</year><authors>Shouhong Wang1 | Hai Wang2</authors><title>Knowledge Discovery Through Self-Organizing Maps: Data Visualization and Query Processing      </title><content>Abstract.;; In data mining, the usefulness of a data pattern depends on the user of the database and does not solely depend on the statistical         strength of the pattern. Based on the premise that heuristic search in combinatorial spaces built on computer and human cognitive         theories is useful for effective knowledge discovery, this study investigates how the use of self-organizing maps as a tool         of data visualization in data mining plays a significant role in human&amp;#8211;computer interactive knowledge discovery. This article         presents the conceptual foundations of the integration of data visualization and query processing for knowledge discovery,         and proposes a set of query functions for the validation of self-organizing maps in data mining.      </content></document><document><year>2002</year><authors>Rattikorn Hewett1 | John Leuchner1</authors><title>Knowledge Discovery with Second-Order Relations               </title><content>Abstract.;; This paper presents an induction technique that discovers a set of classification rules, from a set of examples, using second-order         relations as a representational model. Second-order relations are database relations in which tuples have sets of atomic values         as components. Using sets of values, which are interpreted as disjunctions, provides compact representations that facilitate         efficient management and enhance comprehensibility. The second-order relational framework is based on theoretical foundations         that link relational database theory, machine learning, and logic synthesis. The rule induction technique can be viewed as         a second-order relation compression problem in which the original relation, representing training data, is transformed into         a second-order relation with fewer tuples by merging tuples in ways that preserve consistency with the training data. This         problem is closely related to two-level Boolean function minimization in logic synthesis. We describe a rule-mining system,         SORCER, and compare its performance to two state-of-the-art classification systems: C4.5 and CBA. Experimental results based         on the average of error rates over 26 data sets show that SORCER, using a simple compression scheme, outperforms C4.5 and         is competitive to CBA. Using a slightly more sophisticated compression scheme, SORCER outperforms both C4.5 and CBA.               </content></document><document><year>2002</year><authors>N. Xiong1| L. Litz1 | H. Ressom2</authors><title>Learning Premises of Fuzzy Rules for Knowledge Acquisition in Classification Problems      </title><content>Abstract.;; A key issue in building fuzzy classification systems is the specification of rule conditions, which determine the structure         of a knowledge base. This paper presents a new approach to automatically extract classification knowledge from numerical data         by means of premise learning. A genetic algorithm is employed to search for premise structure in combination with parameters         of membership functions of input fuzzy sets to yield optimal conditions of classification rules. The major advantage of our         work is that a parsimonious knowledge base with a low number of rules can be achieved. The practical applicability of the         proposed method is examined by computer simulations on two well-known benchmark problems of Iris Data and Cancer Data classification.      </content></document><document><year>2002</year><authors>E. F. Khor1| K. C. Tan1 | T. H. Lee1</authors><title>Learning the Search Range for Evolutionary Optimization in Dynamic Environments      </title><content>Abstract.;; Conventional evolutionary algorithms operate in a fixed search space with limiting parameter range, which is often predefined         via a priori knowledge or trial and error in order to &amp;#8216;guess&amp;#8217; a suitable region comprising the global optimal solution. This         requirement is hard, if not impossible, to fulfil in many real-world optimization problems since there is often no clue of         where the desired solutions are located in these problems. Thus, this paper proposes an inductive&amp;#8211;deductive learning approach         for single- and multi-objective evolutionary optimization. The method is capable of directing evolution towards more promising         search regions even if these regions are outside the initial predefined space. For problems where the global optimum is included         in the initial search space, it is capable of shrinking the search space dynamically for better resolution in genetic representation         to facilitate the evolutionary search towards more accurate optimal solutions. Validation results based on benchmark optimization         problems show that the proposed inductive&amp;#8211;deductive learning is capable of handling different fitness landscapes as well as         distributing nondominated solutions uniformly along the final trade-offs in multi-objective optimization, even if there exist         many local optima in a high-dimensional search space or the global optimum is outside the predefined search region.      </content></document><document><year>2002</year><authors>M. Chachoua1 | D. Pacholczyk1</authors><title>Qualitative Reasoning Under Ignorance and Information-Relevant Extraction               </title><content>Abstract.;; This paper is devoted to qualitative reasoning under ignorance. We show how to represent conditional ignorance and informational         relevance in the symbolic entropy theory that we have developed in our previous work. This theory allows us to represent uncertainty,         in the ignorance form, as in common-sense reasoning, by using the linguistic expressions of the interval [Certain, Completely uncertain]. We recall this theory, then we introduce the notions of conditional ignorance and of informational relevance. Finally we         present some theorems of qualitative reasoning with uncertain knowledge. Particularly, we show how to extract the best relevant         information in order to treat some problems under ignorance.               </content></document><document><year>2002</year><authors>Zohra Bellahsene1</authors><title>Schema Evolution in Data Warehouses      </title><content>Abstract.;; In this paper, we address the issues related to the evolution and maintenance of data warehousing systems, when underlying         data sources change their schema capabilities. These changes can invalidate views at the data warehousing system. We present         an approach for dynamically adapting views according to schema changes arising on source relations. This type of maintenance         concerns both the schema and the data of the data warehouse. The main issue is to avoid the view recomputation from scratch         especially when views are defined from multiple sources. The data of the data warehouse is used primarily in organizational         decision-making and may be strategic. Therefore, the schema of the data warehouse can evolve for modeling new requirements         resulting from analysis or data-mining processing. Our approach provides means to support schema evolution of the data warehouse         independently of the data sources.      </content></document><document><year>2002</year><authors>Zhixiang Chen1| Xiannong Meng2| Binhai Zhu3 | Richard H. Fowler1</authors><title>WebSail: From On-line Learning to Web Search      </title><content>Abstract.;; In this paper we report our research on building WebSail, an intelligent web search engine that is able to perform real-time adaptive learning. WebSail learns from the user's relevance feedback, so that it is able to speed up its search process and to enhance its search performance.         We design an efficient adaptive learning algorithm TW2 to search for web documents. WebSail employs TW2 together with an internal index database and a real-time meta-searcher to perform real-time adaptive learning to find desired         documents with as little relevance feedback from the user as possible. The architecture and performance of WebSail are also discussed.      </content></document><document><year>2002</year><authors>Evangelos Kotsakis1</authors><title>XSD: A Hierarchical Access Method for Indexing XML Schemata      </title><content>Abstract.;; Search operations and browsing facilities over an XML document database require special support at the physical level. Typical         search operations involve path queries. This paper proposes a hierarchical access method to support such operations and to         facilitate browsing. It advocates the idea of searching large XML collections by administering efficiently XML schemata. The         proposed approach may be used for indexing XML documents according to their structural proximity. This is obtained by organizing         the schemata of a large XML document collection in a hierarchical way by merging structurally close schemata. The proposed         structure, which is called XML Schema Directory (XSD), is a balanced tree and it may serve two purposes: (1) to accelerate XML query processing and (2) to facilitate browsing.      </content></document><document><year>2000</year><authors>Janusz A. Starzyk1| Dale E. Nelson2 | Kirk Sturtz3</authors><title>A Mathematical Foundation for Improved Reduct Generation in Information Systems      </title><content>Abstract.;; When data sets are analyzed, statistical pattern recognition is often used to find the information hidden in the data. Another         approach to information discovery is data mining. Data mining is concerned with finding previously undiscovered relationships         in data sets. Rough set theory provides a theoretical basis from which to find these undiscovered relationships. We define         a new theoretical concept, strong compressibility, and present the mathematical foundation for an efficient algorithm, the         Expansion Algorithm, for generation of all reducts of an information system. The process of finding reducts has been proven         to be NP-hard. Using the elimination method, problems of size 13 could be solved in reasonable times. Using our Expansion         Algorithm, the size of problems that can be solved has grown to 40. Further, by using the strong compressibility property         in the Expansion Algorithm, additional savings of up to 50% can be achieved. This paper presents this algorithm and the simulation         results obtained from randomly generated information systems.      </content></document><document><year>2000</year><authors>G. Antoniou1| Cara MacNish2 | N. Y. Foo3</authors><title>A Note on the Refinement of Nonmonotonic Knowledge Bases      </title><content>Abstract.;; Refinement of logical theories plays an important role in various application domains, notably in software engineering. This         note introduces and studies refinement notions for nonmonotonic knowledge bases in default logic. The paper motivates and         proposes refinement concepts, discusses their relationship, and establishes sufficient conditions for refinement.      </content></document><document><year>2000</year><authors>Cyrus Shahabi1| Latifur Khan1 | Dennis McLeod1</authors><title>A Probe-Based Technique to Optimize Join Queries in Distributed Internet Databases      </title><content>Abstract.;; An adaptive probe-based optimization technique is developed and demonstrated in the context of an Internet-based distributed         database environment. More and more common are database systems which are distributed across servers communicating via the         Internet where a query at a given site might require data from remote sites. Optimizing the response time of such queries         is a challenging task due to the unpredictability of server performance and network traffic at the time of data shipment;         this may result in the selection of an expensive query plan using a static query optimizer. We constructed an experimental         setup consisting of two servers running the same database management system connected via the Internet. Concentrating on join         queries, we demonstrate how a static query optimizer might choose an expensive plan by mistake. This is due to the lack of         a priori knowledge of the run-time environment, inaccurate statistical assumptions in size estimation, and neglecting the         cost of remote method invocation. These shortcomings are addressed collectively by proposing a probing mechanism. An implementation         of our run-time optimization technique for join queries was constructed in the Java language and incorporated into an experimental         setup. The results demonstrate the superiority of our probe-based optimization over a static optimization.      </content></document><document><year>2000</year><authors>Shichao Zhang1</authors><title>A Temporal Logic for Supporting Historical Databases      </title><content>Abstract.;; Tackling data with gap-interval time is an important issue faced by the temporal database community. While a number of interval         logics have been developed, less work has been reported on gap-interval time. To represent and handle data with time, a clause         &amp;#8216;when&amp;#8217; is generally added into each conventional operator so as to incorporate time dimension in temporal databases, which         clause &amp;#8216;when&amp;#8217; is really a temporal logical sentence. Unfortunately, though several temporal database models have dealt with         data with gap-interval time, they still put interval calculus methods on gap-intervals. Certainly, it is inadequate to tackle         data with gap-interval time using interval calculus methods in historical databases. Consequently, what temporal expressions         are valid in the clause &amp;#8216;when&amp;#8217; for tackling data with gap-interval time? Further, what temporal operations and relations can         be used in the clause &amp;#8216;when&amp;#8217;? To solve these problems, a formal tool for supporting data with gap-interval time must be explored.         For this reason, a gap-interval-based logic for historical databases is established in this paper. In particular, we discuss         how to determine the temporal relationships after an event explodes. This can be used to describe the temporal forms of tuples         splitting in historical databases.      </content></document><document><year>2000</year><authors>Xiong Wang1| Jason T. L. Wang1| King-Ip Lin2| Dennis Shasha3| Bruce A. Shapiro4 | Kaizhong Zhang5</authors><title>An Index Structure for Data Mining and Clustering      </title><content>Abstract.;; In this paper we present an index structure, called MetricMap, that takes a set of objects and a distance metric and then maps those objects to a k-dimensional space in such a way that the distances among objects are approximately preserved. The index structure is a useful         tool for clustering and visualization in data-intensive applications, because it replaces expensive distance calculations         by sum-of-square calculations. This can make clustering in large databases with expensive distance metrics practical. We compare         the index structure with another data mining index structure, FastMap, recently proposed by Faloutsos and Lin, according to two criteria: relative error and clustering accuracy. For relative         error, we show that (i) FastMap gives a lower relative error than MetricMap for Euclidean distances, (ii) MetricMap gives a lower relative error than FastMap for non-Euclidean distances (i.e., general distance metrics), and (iii) combining the two reduces the error yet further.         A similar result is obtained when comparing the accuracy of clustering. These results hold for different data sizes. The main         qualitative conclusion is that these two index structures capture complementary information about distance metrics and therefore         can be used together to great benefit. The net effect is that multi-day computations can be done in minutes.      </content></document><document><year>2000</year><authors>K. L. Poh1</authors><title>An Intelligent Decision Support System for Investment Analysis      </title><content>Abstract.;; We present an intelligent decision support system that combines decision analysis and traditional investment evaluation and         analysis. The system brings to the ordinary user expertise in both decision analysis and investment evaluation techniques,         as well as domain knowledge about the market. The system supports the entire decision analysis cycle and provides facilities         and tools for decision modeling, probability assessment, model evaluation, and sensitivity analysis. An example based on the         Shanghai Stock Market is presented. We demonstrate how an investment decision model for the stock market is constructed by         the system and how the optimal decision is obtained. Sensitivity and value of information analysis were also carried out by         the system.      </content></document><document><year>2000</year><authors>Takahiro Sasaki1 | Mario Tokoro1</authors><title>Comparison between Lamarckian and Darwinian Evolution on a Model Using Neural Networks and Genetic Algorithms      </title><content>Abstract.;; In this paper, we study the relationship between learning and evolution in a simple abstract model, where neural networks         capable of learning are evolved using genetic algorithms (GAs). Each individual tries to acquire a proper behavior under a         given environment through its lifetime learning, and the best individuals are selected to reproduce offspring, which then         conduct lifetime learning in the succeeding generation. The connective weights of individuals' neural networks undergo modification,         i.e., certain characters will be acquired, through their lifetime learning. By setting various rates for the heritability         of acquired characters, which control the strength of &amp;#8216;Lamarckian&amp;#8217; strategy, we observe adaptational processes of populations         over successive generations. By taking the degree of environmental changes into consideration, we show the following results.         Under static environments, populations with higher rates of heritability adapt themselves more quickly toward the environments,         and thus perform well. On the other hand, under nonstationary environments, populations with lower rates of heritability not         only show more stable behavior against environmental changes, but also maintain greater adaptability with respect to such         changing environments. Consequently, the population with zero heritability, i.e., the Darwinian population, attains the highest         level of adaptation towards dynamic environments.      </content></document><document><year>2000</year><authors>Wen-Syan Li1| Chris Clifton2 | Shu-Yao Liu3</authors><title>Database Integration Using Neural Networks: Implementation and Experiences      </title><content>Abstract.;; Applications in a wide variety of industries require access to multiple heterogeneous distributed databases. One step in         heterogeneous database integration is semantic integration: identifying corresponding attributes in different databases that         represent the same real world concept. The rules of semantic integration can not be &amp;#8216;pre-programmed&amp;#8217; since the information         to be accessed is heterogeneous and attribute correspondences could be fuzzy. Manually comparing all possible pairs of attributes         is an unreasonably large task. We have applied artificial neural networks (ANNs) to this problem. Metadata describing attributes         is automatically extracted from a database to represent their &amp;#8216;signatures&amp;#8217;. The metadata is used to train neural networks         to find similar patterns of metadata describing corresponding attributes from other databases. In our system, the rules to         determine corresponding attributes are discovered through machine learning. This paper describes how we applied neural network techniques in a database integration problem         and how we represent an attribute with its metadata as discriminators. This paper focuses on our experiments on effectiveness of neural networks and each discriminator. We also discuss difficulties         of using neural networks for this problem and our wish list for the Machine Learning community.      </content></document><document><year>2000</year><authors>Zhihua Zhou1| Shifu Chen1 | Zhaoqian Chen1</authors><title>FANNC: A Fast Adaptive Neural Network Classifier      </title><content>Abstract.;; In this paper, a fast adaptive neural network classifier named FANNC is proposed. FANNC exploits the advantages of both adaptive         resonance theory and field theory. It needs only one-pass learning, and achieves not only high predictive accuracy but also         fast learning speed. Besides, FANNC has incremental learning ability. When new instances are fed, it does not need to retrain         the whole training set. Instead, it could learn the knowledge encoded in those instances through slightly adjusting the network         topology when necessary, that is, adaptively appending one or two hidden units and corresponding connections to the existing         network. This characteristic makes FANNC fit for real-time online learning tasks. Moreover, since the network architecture         is adaptively set up, the disadvantage of manually determining the number of hidden units of most feed-forward neural networks         is overcome. Benchmark tests show that FANNC is a preferable neural network classifier, which is superior to several other         neural algorithms on both predictive accuracy and learning speed.      </content></document><document><year>2000</year><authors>Li Shen1| Hong Shen2| Ling Cheng1 | Paul Pritchard2</authors><title>Fast Association Discovery in Derivative Transaction Collections      </title><content>Abstract.;; Association discovery from a transaction collection is an important data-mining task. We study a new problem in this area         whose solution can provide users with valuable association rules in some relevant collections: association discovery in derivative transaction collections. In this problem, we are given association rules in two transaction collections D         1 and D         2, and aim to find new association rules in derivative transaction collections D         1&amp;#8726;D         2, D         1&amp;#8745;D         2, D         2&amp;#8726;D         1 and D         1&amp;#8746;D         2. Direct application of existing algorithms can solve this problem, but in an expensive way. We propose an efficient solution         through making full use of already discovered information, taking advantage of the relationships existing among relevant collections,         and avoiding unnecessary but expensive support-counting operations by scanning databases. Experiments on well-known synthetic         data show that our solution consistently outperforms the naive solution by factors from 2 to 3 in most cases. We also propose         an efficient parallelization of our approach, as parallel algorithms are often interesting and necessary in the area of data         mining.      </content></document><document><year>2000</year><authors>Ken C. K. Lee1| Hong Va Leong1 | Antonio Si2</authors><title>Incremental View Maintenance for Mobile Databases      </title><content>Abstract.;; In a mobile environment, querying a database at a stationary server from a mobile client is expensive due to the limited         bandwidth of a wireless channel and the instability of the wireless network. We address this problem by maintaining a materialized         view in a mobile client's local storage. Such a materialized view can be considered as a data warehouse. The materialized         view contains results of common queries in which the mobile client is interested. In this paper, we address the view update         problem for maintaining the consistency between a materialized view at a mobile client and the database server. The content         of a materialized view could become incoherent with that at the database server when the content of the database server and/or         when the location of the client is changed. Existing view update mechanisms are &amp;#8216;push-based&amp;#8217;. The server is responsible for         notifying all clients whose views might be affected by the changes in database or the mobility of the client. This is not         appropriate in a mobile environment due to the frequent wireless channel disconnection. Furthermore, it is not easy for a         server to keep track of client movements to update individual client location-dependent views. We propose a &amp;#8216;pull-based&amp;#8217; approach         that allows a materialized view to be updated at a client in an incremental manner, requiring a client to request changes         to its view from the server. We demonstrate the feasibility of our approach with experimental results.      </content></document><document><year>2000</year><authors>Klaus-Dieter Schewe1 | Bettina Schewe2</authors><title>Integrating Database and Dialogue Design      </title><content>Abstract.;; The user interface of data-intensive information systems may be described as a collection of semi-independent dialogues with         underlying procedures accessing databases. It will be shown how to conceptualize dialogues via dialogue objects. These are         organized in an object oriented fashion by dialogue classes modelled as extended database views. The description is based         on an object oriented model which enables a smooth integration of dialogues with the underlying database schema.                     The resulting integrated model can serve as a unifying conceptual umbrella encompassing both databases and user-interfaces.               The only remaining task for the presentation layer consists of suitable ergonomic presentations of dialogue objects on the               screen by means of a general user interface management system.            </content></document><document><year>2000</year><authors>Massimo Fasciano1 | Guy Lapalme1</authors><title>Intentions in the Coordinated Generation of Graphics and Text from Tabular Data      </title><content>Abstract.;; To use graphics efficiently in an automatic report generation system, one has to model messages and how they go from the         writer (intention) to the reader (interpretation). This paper describes PostGraphe, a system which generates a report integrating graphics and text from a set of writer's intentions. The system is given the         data in tabular form as might be found in a spreadsheet; also input is a declaration of the types of values in the columns         of the table. The user then indicates the intentions to be conveyed in the graphics (e.g., compare two variables or show the         evolution of a set of variables) and the system generates a report in LATEX with the appropriate PostScript graphic files. PostGraphe uses the same information to generate the accompanying text that helps the reader to focus on the important points of the         graphics. We also describe how these ideas have been embedded to create a new Chart Wizard for Microsoft Excel.      </content></document><document><year>2000</year><authors>Nicolae &amp;#354 &amp;#259 nd&amp;#259 reanu1</authors><title>Knowledge Bases with Output      </title><content>Abstract.;; In this paper we shall propose a knowledge representation method, the deduction process in a knowledge representation and reasoning system using this method and therefore the answer function of such a system. This is an algebraic method because some concepts of universal algebra are used. More precisely, we introduce         the concept of knowledge base with output (KBO), the central structure being that of labeled stratified graph. For such a base we distinguish a structure and two kinds of computations. The structure is described by a labeled stratified         graph and an output space. The computations in a KBO are represented by two levels: syntactic and semantic. Both the syntactic and the semantic computations in a KBO are described in this paper. Finally, we exemplify all the concepts and we apply the method to solve a given problem in travel         scheduling. All the theoretical results are proved in a separate section.      </content></document><document><year>2000</year><authors>Bernd Wondergem1| Patrick van Bommel1 | Theo van der Weide1</authors><title>Nesting and Defoliation of Index Expressions for Information Retrieval      </title><content>Abstract.;; In this article, a formalisation of index expressions is presented. Index expressions are more expressive than keywords while         maintaining a comprehensible complexity. Index expressions are well-known in Information Retrieval (IR), where they are used         for characterising document contents, formulation of user interests, and matching mechanisms. In addition, index expressions         have found both practical and theoretical applicability in 2-level hypermedia systems for IR. In these applications, properties         of (the structure of) index expressions are heavily relied upon. However, the presupposed mathematical formalisation of index         expressions and their properties still lacks. Our formalism is based on the structural notation of index expressions. It is         complete in the sense that several notions of subexpressions and defoliation of index expressions are also formalised. Defoliation,         which plays an important role in defining properties of index expressions, is provided as a recursively defined operator.         Finally, two other representational formalisms for index expressions are compared to ours.      </content></document><document><year>2000</year><authors>Geok See Ng1| Khue Hiang Chan1| Sevki S. Erdogan1 | Harcharan Singh1</authors><title>Neural Network Learning Using Entropy Cycle      </title><content>Abstract.;; In this paper, an additional entropy penalty term is used to steer the direction of the hidden node's activation in the process         of learning. A state with minimum entropy means that most nodes are operating in the non-linear zones (i.e. saturation zones)         near the extreme ends of the Sigmoid curve. As the training proceeds, redundant hidden nodes' activations are pushed towards         their extreme value corresponding to a low entropy state with maximum information, while some relevant nodes remain active         in the linear zone. As training progresses, more nodes get into saturation zones. The early creation of such nodes may impair         generalization performance. To prevent the network from being driven into saturation before it can really learn, an entropy         cycle is proposed in this paper to dampen the creation of such inactive nodes in the early stage of training. At the end of         training, these inactive nodes can then be eliminated without affecting the performance of the original network. The concept         has been successfully applied for pruning in two classification problems. The experiments indicate that redundant nodes are         pruned resulting in optimal network topologies.      </content></document><document><year>2000</year><authors>Steven Walczak1 | Reijer Grimbergen2</authors><title>Pattern Analysis and Analogy in Shogi: Predicting Shogi Moves from Prior Experience      </title><content>Abstract.;; As a research paradigm, pattern analysis has been shown to be an effective tool for analyzing complex game situations in         both chess and go. We extend the prior pattern analysis research in chess to the domain of shogi. Shogi is computationally         more complex than chess and should realize greater benefits than the chess domain from pattern recognition and pattern exploitation         research. The IAM program, which has accurately predicted up to 28% of the moves for a specific chess player, is redesigned         to operate in the domain of shogi. Results similar to those achieved for the domain of chess are achieved in shogi.      </content></document><document><year>2000</year><authors>El-Sayed El-Azhary1| Hesham A. Hassan1 | A. Rafea1</authors><title>Pest Control Expert System for Tomato (PCEST)      </title><content>Abstract.;; This paper presents a real-life pest control expert system for tomato. The system involves two main subtasks, namely: &amp;#8216;diagnose&amp;#8217;         and &amp;#8216;treat&amp;#8217;. The &amp;#8216;diagnose&amp;#8217; subtask finds out the causes of the growers' complaints, while the &amp;#8216;treat&amp;#8217; subtask finds out a         treatment plan for these causes. CommonKADS methodology has been used to develop the system. Dependency network is used as         one of our knowledge representation schemes in both subtasks. An expert system evaluation methodology has been suggested and         applied to the developed system.      </content></document><document><year>2000</year><authors>Julio CГ©sar ArpГ­rez1| AsunciГіn GГіmez-PГ©rez1| Adolfo Lozano-Tello2 | Helena Sofia Andrade N. P. Pinto3</authors><title>Reference Ontology and (ONTO)2 Agent: The Ontology Yellow Pages      </title><content>Abstract.;; Knowledge reuse by means of ontologies faces three important problems at present: (1) there are no standardized identifying         features that characterize ontologies from the user point of view; (2) there are no web sites using the same logical organization,         presenting relevant information about ontologies; and (3) the search for appropriate ontologies is hard, time-consuming and         usually fruitless. To solve the above problems, we present: (1) a living set of features that allow us to characterize ontologies         from the user point of view and have the same logical organization; (2) a living domain ontology about ontologies (called         Reference Ontology) that gathers, describes and has links to existing ontologies; and (3) (ONTO)2 Agent, the ontology-based WWW broker about ontologies that uses Reference Ontology as a source of its knowledge and retrieves         descriptions of ontologies that satisfy a given set of constraints.      </content></document><document><year>2000</year><authors>Weiru Liu1 | Jun Hong1</authors><title>Reinvestigating Dempster's Idea on Evidence Combination      </title><content>Abstract.;; In this paper, we investigate the problem encountered by Dempster's combination rule in view of Dempster's original combination         framework. We first show that the root of Dempster's combination rule (defined and named by Shafer) is Dempster's original         idea on evidence combination. We then argue that Dempster's original idea on evidence combination is, in fact, richer than         what has been formulated in the rule. We conclude that, by strictly following what Dempster has suggested, there should be         no counterintuitive results when combining evidence.      </content></document><document><year>2000</year><authors>Shu-Heng Chen1 | Chih-Chi Ni1</authors><title>Simulating the Ecology of Oligopolistic Competition with Genetic Algorithms      </title><content>Abstract.;; In economics, the n-person oligopoly game and the n-person IPD game are often considered close in spirit. Our analytical framework shows that this is not the case owing to the         path dependence of the pay-off matrix of the oligopoly game. By simulating the evolution of a three-person oligopoly game         with genetic algorithms, we explore the significance of the path dependence property to the rich ecology of oligopoly. The         emergent behavior of oligopolists in the simulations indicates how the path dependence nature may shed light on the phenotypes         and genotypes coming into existence. The features shown in this research can be further exploited in more practical contexts         so that nontrivial policy issues in industrial economics can be seriously tackled.      </content></document><document><year>2000</year><authors>Susan E. George1</authors><title>Spatio-Temporal Analysis with the Self-Organizing Feature Map      </title><content>Abstract.;; Spatio-temporal pattern recognition problems are particularly challenging. They typically involve detecting change that occurs         over time in two-dimensional patterns. Analytic techniques devised for temporal data must take into account the spatial relationships         among data points. An artificial neural network known as the self-organizing feature map (SOM) has been used to analyze spatial         data. This paper further investigates the use of the SOM with spatio-temporal pattern recognition. The principles of the two-dimensional         SOM are developed into a novel three-dimensional network and experiments demonstrate that (i) the three-dimensional network         makes a better topological ordering and (ii) there is a difference in terms of the spatio-temporal analysis that can be made         with the three-dimensional network.      </content></document><document><year>2000</year><authors>Yeon-Gyu Seo1| Sung-Bae Cho1 | Xin Yao2</authors><title>The Impact of Payoff Function and Local Interaction on the N-Player Iterated Prisoner's Dilemma      </title><content>Abstract.;; The N-player iterated prisoner's dilemma (NIPD) game has been widely used to study the evolution of cooperation in social, economic         and biological systems. This paper studies the impact of different payoff functions and local interactions on the NIPD game.         The evolutionary approach is used to evolve game-playing strategies starting from a population of random strategies. The different         payoff functions used in our study describe different behaviors of cooperation and defection among a group of players. Local         interaction introduces neighborhoods into the NIPD game. A player does not play against every other player in a group any         more. He only interacts with his neighbors. We investigate the impact of neighborhood size on the evolution of cooperation         in the NIPD game and the generalization ability of evolved strategies.      </content></document><document><year>2000</year><authors>Mamadou Tadiou Kone1| Akira Shimazu1 | Tatsuo Nakajima2</authors><title>The State of the Art in Agent Communication Languages      </title><content>Abstract.;; Like societies of humans, there is a need for agents in a multi-agent system to rely on one another, enlist the support of         peers in order to solve complex tasks. Agents will be able to cooperate only through a meaningful communication language that         can bear correctly their mental states and convey precisely the content of their messages. In search for the ideal agent communication         language (ACL), several initiatives like the pioneering work of the Knowledge Sharing Effort and the Foundation for Intelligent         Physical Agents (FIPA) are paving the way for a platform where all agents would be able to interact regardless of their implementation         environment. ACL is a new field of study that could gain from a survey in expanding its application areas. For this purpose,         we examine in this paper the state of the art in ACL design and suggest some principles for building a generalized ACL framework.         We then evaluate some existing ACL models, and present the current issues in ACL research, and new perspectives.      </content></document><document><year>2000</year><authors>Nadim Obeid1</authors><title>Towards a Model of Learning through Communication      </title><content>Abstract.;; Communication is an interactive, complex, structured process involving agents that are capable of drawing conclusions from         the information they have available about some real-life situations. Such situations are generally characterized as being         imperfect. In this paper, we aim to address learning from the perspective of the communication between agents. To learn a collection of propositions concerning some situation         is to incorporate it within one's knowledge about that situation. That is, the key factor in this activity is for the goal         agent, where agents may switch role if appropriate, to integrate the information offered with what it already knows. This         may require a process of belief revision, which suggests that the process of incorporation of new information should be modeled         nonmonotonically. We shall employ for reasoning a three-valued based nonmonotonic logic that formalizes some aspects of revisable         reasoning and it is accessible to implementation. The logic is sound and complete. A theorem-prover of the logic has successfully         been implemented.      </content></document><document><year>2000</year><authors>Haifei Li1</authors><title>XML and Industrial Standards for Electronic Commerce      </title><content>Abstract.;; With the rapid adoption of XML as the meta data format for electronic commerce applications, many XML-based industrial standards         for electronic commerce have been proposed. Since XML only defines a standardized syntax that introduces new tags used to         represent data semantics, the task of defining a common set of tags and organizing tags into Data Type Definitions (DTDs)         is left to developers of XML applications. XML applications do not interact with each other if their tags and DTDs are different.         In this paper, we discuss and compare seven industrial standards for electronic commerce based on XML. They are BizTalk, CBL,         cXML, IOTP, OAGIS, OCF, and RETML. Each standard is categorized according to its coverage of business activities. The following         observations are made after examining industrial standards from different perspectives. Firstly, XML has a big impact on industrial         standards for electronic commerce. Secondly, most industrial standards are not quite mature at the current stage, and there         are no apparent leaders among the competitors. Thirdly, the complexity of their DTDs varies quantitatively and qualitatively.         Fourthly, the integration of industrial standards is imperative if applications based on different standards intend to exchange         XML documents smoothly.      </content></document><document><year>2005</year><authors>Springer London Ltd.1</authors><title>2005 KAIS Reviewers      </title><content>Without Abstract</content></document><document><year>2005</year><authors>Victoria J. Hodge1  | Jim Austin1</authors><title>A binary neural k-nearest neighbour technique</title><content>K-Nearest Neighbour (k-NN) is a widely used technique for classifying and clustering data. K-NN is effective but is often criticised for its polynomial run-time growth as k-NN calculates the distance to every other record in the data set for each record in turn. This paper evaluates a novel k-NN classifier with linear growth and faster run-time built from binary neural networks. The binary neural approach uses robust encoding to map standard ordinal, categorical and real-valued data sets onto a binary neural network. The binary neural network uses high speed pattern matching to recall the k-best matches. We compare various configurations of the binary approach to a conventional approach for memory overheads, training speed, retrieval speed and retrieval accuracy. We demonstrate the superior performance with respect to speed and memory requirements of the binary approach compared to the standard approach and we pinpoint the optimal configurations. </content></document><document><year>2005</year><authors>Mi Zhou1 | Man-Hon Wong1  | Kam-Wing Chu1 </authors><title>A geometrical solution to time series searching invariant to shifting and scaling      </title><content>The technique of searching for similar patterns among time series data is very useful in many applications. The problem becomes         difficult when shifting and scaling are considered. We find that we can treat the problem geometrically and the major contribution         of this paper is that a uniform geometrical model that can analyze the existing related methods is proposed. Based on the         analysis, we conclude that the angle between two vectors after the Shift-Eliminated Transformation is a more intrinsical similarity         measure invariant to shifting and scaling. We then enhance the original conical index to adapt to the geometrical properties         of the problem and compare its performance with that of sequential search and R*-tree. Experimental results show that the enhanced conical index achieves larger improvement on R*-tree and sequential search in high dimension. It can also keep a steady performance as the selectivity increases.      </content></document><document><year>2005</year><authors>Luigi Palopoli1| Domenico Rosaci2| Giorgio Terracina3  | Domenico Ursino2</authors><title>A graph-based approach for extracting terminological properties from information sources with heterogeneous formats      </title><content>The problem of handling both the integration and the cooperation of a large number of information sources characterised by         heterogeneous representation formats is a challenging issue. In this context, a central role can be played by the knowledge         about the semantic relationships holding between concepts belonging to different information sources (intersource properties).         In this paper, we propose a semiautomatic approach for extracting two kinds of intersource properties, namely synonymies and         homonymies, from heterogeneous information sources. In order to carry out the extraction task, we introduce both a conceptual         model, for representing involved sources, and a metrics, for measuring the strength of the semantic relationships holding         among concepts represented within the same source.       </content></document><document><year>2005</year><authors>Manish Gupta1| Manghui Tu1| Latifur Khan1 | Farokh Bastani1 | I-Ling Yen1</authors><title>A study of the model and algorithms for handling location-dependent continuous queries      </title><content>Advances in wireless and mobile computing environments allow a mobile user to access a wide range of applications. For example,         mobile users may want to retrieve data about unfamiliar places or local life styles related to their location. These queries         are called location-dependent queries. Furthermore, a mobile user may be interested in getting the query results repeatedly,         which is called location-dependent continuous querying. This continuous query emanating from a mobile user may retrieve information         from a single-zone (single-ZQ) or from multiple neighbouring zones (multiple-ZQ). We consider the problem of handling location-dependent         continuous queries with the main emphasis on reducing communication costs and making sure that the user gets correct current-query         result. The key contributions of this paper include: (1) Proposing a hierarchical database framework (tree architecture and         supporting continuous query algorithm) for handling location-dependent continuous queries. (2) Analysing the flexibility of         this framework for handling queries related to single-ZQ or multiple-ZQ and propose intelligent selective placement of location-dependent         databases. (3) Proposing an intelligent selective replication algorithm to facilitate time- and space-efficient processing         of location-dependent continuous queries retrieving single-ZQ information. (4) Demonstrating, using simulation, the significance         of our intelligent selective placement and selective replication model in terms of communication cost and storage constraints,         considering various types of queries.      </content></document><document><year>2005</year><authors>Xiangyang Li1  | Nong Ye2</authors><title>A supervised clustering algorithm for computer intrusion detection      </title><content>We previously developed a clustering and classification algorithm&amp;#8212;supervised (CCAS) to learn patterns of normal and intrusive         activities and to classify observed system activities. Here we further enhance the robustness of CCAS to the presentation         order of training data and the noises in training data. This robust CCAS adds data redistribution, a supervised hierarchical         grouping of clusters and removal of outliers as the postprocessing steps.       </content></document><document><year>2005</year><authors>Sheila Garfield1  | Stefan Wermter1</authors><title>Call classification using recurrent neural networks, support vector machines and finite state automata      </title><content>Our objective is spoken-language classification for helpdesk call routing using a scanning understanding and intelligent-system         techniques. In particular, we examine simple recurrent networks, support-vector machines and finite-state transducers for         their potential in this spoken-language-classification task and we describe an approach to classification of recorded operator-assistance         telephone utterances. The main contribution of the paper is a comparison of a variety of techniques in the domain of call         routing. Support-vector machines and transducers are shown to have some potential for spoken-language classification, but         the performance of the neural networks indicates that a simple recurrent network performs best for helpdesk call routing.      </content></document><document><year>2005</year><authors>Carlos Ordonez1 | Norberto Ezquerra2 | Cesar A. Santana3</authors><title>Constraining and summarizing association rules in medical data      </title><content>Association rules are a data mining technique used to discover frequent patterns in a data set. In this work, association         rules are used in the medical domain, where data sets are generally high dimensional and small. The chief disadvantage about         mining association rules in a high dimensional data set is the huge number of patterns that are discovered, most of which         are irrelevant or redundant. Several constraints are proposed for filtering purposes, since our aim is to discover only significant         association rules and accelerate the search process. A greedy algorithm is introduced to compute rule covers in order to summarize         rules having the same consequent. The significance of association rules is evaluated using three metrics: support, confidence         and lift. Experiments focus on discovering association rules on a real data set to predict absence or existence of heart disease.         Constraints are shown to significantly reduce the number of discovered rules and improve running time. Rule covers summarize         a large number of rules by producing a succinct set of rules with high-quality metrics.      </content></document><document><year>2005</year><authors>Xingquan Zhu1 | Xindong Wu1 | Ying Yang2</authors><title>Effective classification of noisy data streams with attribute-oriented dynamic classifier selection      </title><content>Recently, mining from data streams has become an important and challenging task for many real-world applications such as credit         card fraud protection and sensor networking. One popular solution is to separate stream data into chunks, learn a base classifier         from each chunk, and then integrate all base classifiers for effective classification. In this paper, we propose a new dynamic         classifier selection (DCS) mechanism to integrate base classifiers for effective mining from data streams. The proposed algorithm         dynamically selects a single &amp;#8220;best&amp;#8221; classifier to classify each test instance at run time. Our scheme uses statistical information         from attribute values, and uses each attribute to partition the evaluation set into disjoint subsets, followed by a procedure         that evaluates the classification accuracy of each base classifier on these subsets. Given a test instance, its attribute         values determine the subsets that the similar instances in the evaluation set have constructed, and the classifier with the         highest classification accuracy on those subsets is selected to classify the test instance. Experimental results and comparative         studies demonstrate the efficiency and efficacy of our method. Such a DCS scheme appears to be promising in mining data streams         with dramatic concept drifting or with a significant amount of noise, where the base classifiers are likely conflictive or         have low confidence.      </content></document><document><year>2005</year><authors>Imad Rahal1 | Dongmei Ren2| Weihua Wu2| Anne Denton2| Christopher Besemann2 | William Perrizo2</authors><title>Exploiting edge semantics in citation graphs using efficient, vertical ARM</title><content>Graphs are increasingly becoming a vital source of information within which a great deal of semantics is embedded. As the size of available graphs increases, our ability to arrive at the embedded semantics grows into a much more complicated task. One form of important hidden semantics is that which is embedded in the edges of directed graphs. Citation graphs serve as a good example in this context. This paper attempts to understand temporal aspects in publication trends through citation graphs, by identifying patterns in the subject matters of scientific publications using an efficient, vertical association rule mining model. Such patterns can (a) indicate subject-matter evolutionary history, (b) highlight subject-matter future extensions, and (c) give insights on the potential effects of current research on future research. We highlight our major differences with previous work in the areas of graph mining, citation mining, and Web-structure mining, propose an efficient vertical data representation model, introduce a new subjective interestingness measure for evaluating patterns with a special focus on those patterns that signify strong associations between properties of cited papers and citing papers, and present an efficient algorithm for the purpose of discovering rules of interest followed by a detailed experimental analysis.</content></document><document><year>2005</year><authors>Ruoming Jin1 | Anjan Goswami2  | Gagan Agrawal2 </authors><title>Fast and exact out-of-core and distributed k-means clustering</title><content>Clustering has been one of the most widely studied topics in data mining and k-means clustering has been one of the popular clustering algorithms. K-means requires several passes on the entire dataset, which can make it very expensive for large disk-resident datasets. In view of this, a lot of work has been done on various approximate versions of k-means, which require only one or a small number of passes on the entire dataset.In this paper, we present a new algorithm, called fast and exact k-means clustering (FEKM), which typically requires only one or a small number of passes on the entire dataset and provably produces the same cluster centres as reported by the original k-means algorithm. The algorithm uses sampling to create initial cluster centres and then takes one or more passes over the entire dataset to adjust these cluster centres. We provide theoretical analysis to show that the cluster centres thus reported are the same as the ones computed by the original k-means algorithm. Experimental results from a number of real and synthetic datasets show speedup between a factor of 2 and 4.5, as compared with k-means.</content></document><document><year>2005</year><authors>Jeffrey Xu Yu1 | Weining Qian2| Hongjun Lu3 | Aoying Zhou2</authors><title>Finding centric local outliers in categorical/numerical spaces      </title><content>Outlier detection techniques are widely used in many applications such as credit-card fraud detection, monitoring criminal         activities in electronic commerce, etc. These applications attempt to identify outliers as noises, exceptions, or objects         around the border. The existing density-based local outlier detection assigns the degree to which an object is an outlier         in a numerical space. In this paper, we propose a novel mutual-reinforcement-based local outlier detection approach. Instead         of detecting local outliers as noise, we attempt to identify local outliers in the center, where they are similar to some         clusters of objects on one hand, and are unique on the other. Our technique can be used for bank investment to identify a         unique body, similar to many good competitors, in which to invest. We attempt to detect local outliers in categorical, ordinal         as well as numerical data. In categorical data, the challenge is that there are many similar but different ways to specify         relationships among the data items. Our mutual-reinforcement-based approach is stable, with similar but different user-defined         relationships. Our technique can reduce the burden for users to determine the relationships among data items, and find the         explanations why the outliers are found. We conducted extensive experimental studies using real datasets.      </content></document><document><year>2005</year><authors>Shi Zhong1  | Joydeep Ghosh2</authors><title>Generative model-based document clustering: a comparative study</title><content>This paper presents a detailed empirical study of 12 generative approaches to text clustering, obtained by applying four types of document-to-cluster assignment strategies (hard, stochastic, soft and deterministic annealing (DA) based assignments) to each of three base models, namely mixtures of multivariate Bernoulli, multinomial, and von Mises-Fisher (vMF) distributions. A large variety of text collections, both with and without feature selection, are used for the study, which yields several insights, including (a) showing situations wherein the vMF-centric approaches, which are based on directional statistics, fare better than multinomial model-based methods, and (b) quantifying the trade-off between increased performance of the soft and DA assignments and their increased computational demands. We also compare all the model-based algorithms with two state-of-the-art discriminative approaches to document clustering based, respectively, on graph partitioning (CLUTO) and a spectral coclustering method. Overall, DA and CLUTO perform the best but are also the most computationally expensive. The vMF models provide good performance at low cost while the spectral coclustering algorithm fares worse than vMF-based methods for a majority of the datasets. </content></document><document><year>2005</year><authors>Jan Bakus1  | Mohamed S. Kamel2</authors><title>Higher order feature selection for text classification      </title><content>In this paper. we present the MIFS-C variant of the mutual information feature-selection algorithms. We present an algorithm         to find the optimal value of the redundancy parameter, which is a key parameter in the MIFS-type algorithms. Furthermore,         we present an algorithm that speeds up the execution time of all the MIFS variants. Overall, the presented MIFS-C has comparable         classification accuracy (in some cases even better) compared with other MIFS algorithms, while its running time is faster.         We compared this feature selector with other feature selectors, and found that it performs better in most cases. The MIFS-C         performed especially well for the breakeven and F-measure because the algorithm can be tuned to optimise these evaluation measures.      </content></document><document><year>2005</year><authors>Konstantinos Kotis1  | George A. Vouros1 </authors><title>Human-centered ontology engineering: The HCOME methodology</title><content>The fast emergent and continuously evolving areas of the Semantic Web and Knowledge Management make the incorporation of ontology engineering tasks in knowledge-empowered organizations and in the World Wide Web more than necessary. In such environments, the development and evolution of ontologies must be seen as a dynamic process that has to be supported through the entire ontology life cycle, resulting to living ontologies. The aim of this paper is to present the Human-Centered Ontology Engineering Methodology (HCOME) for the development and evaluation of living ontologies in the context of communities of knowledge workers. The methodology aims to empower knowledge workers to continuously manage their formal conceptualizations in their day-to-day activities and shape their information space by being actively involved in the ontology life cycle. The paper also demonstrates the Human Centered ONtology Engineering Environment, HCONE, which can effectively support this methodology.</content></document><document><year>2005</year><authors>Liang Chen1 | Ruoyu Chen2  | Sharmin Nilufar1 </authors><title>Improving the performance of 1D object classification by using the Electoral College</title><content>It has been proven that districted matching schemes (e.g., the US presidential election scheme, also called the Electoral College) are more stable than undistricted matching schemes (e.g., the popular voting scheme for selecting a governor in California), and that the theory can be used in pattern classification applications, such as image classification, where by its nature an object to be classified consists of elements distributed in a bounded 2D space. However, the objects of some pattern classification applications consist of features/values of elements lying on a limited 1D line segment. This paper will prove that districted matching scheme can still outperform undistricted matching scheme in these applications, and the improved performance of districted vote scheme is even more substantial for these 1D objects than for 2D objects. The theoretical result suggests the use of districted matching schemes for pattern recognition of 1D objects. We verified the theoretical analysis through artificial neural network-based approaches for the prediction of start codons of nucleotide sequences.</content></document><document><year>2005</year><authors>J. Zhang1 | D.-K. Kang1| A. Silvescu1 | V. Honavar2</authors><title>Learning accurate and concise naГЇve Bayes classifiers from attribute value taxonomies and data      </title><content>In many application domains, there is a need for learning algorithms that can effectively exploit attribute value taxonomies         (AVT)&amp;#8212;hierarchical groupings of attribute values&amp;#8212;to learn compact, comprehensible and accurate classifiers from data&amp;#8212;including         data that are partially specified. This paper describes AVT-NBL, a natural generalization of the naГЇve Bayes learner (NBL), for learning classifiers from AVT         and data. Our experimental results show that AVT-NBL is able to generate classifiers that are substantially more compact and         more accurate than those produced by NBL on a broad range of data sets with different percentages of partially specified values.         We also show that AVT-NBL is more efficient in its use of training data: AVT-NBL produces classifiers that outperform those         produced by NBL using substantially fewer training examples.      </content></document><document><year>2005</year><authors>Isis Truck1  | Herman Akdag1</authors><title>Manipulation of qualitative degrees to handle uncertainty:         formal models and applications      </title><content>In this article, qualitative, symbolic and linguistic models for knowledge representation are presented as well as their applications.         Such models are useful in decision making problems when information from the experts' knowledge is expressed through different         heterogeneous types such as numerical, interval-valued, symbolic, linguistic, &amp;#8230; The whole work proposed here takes place in         a given many-valued logic. First, as an alternative to classic probabilities, a method using qualitative degrees is described         and an application in supervised learning is proposed. Then we study the transformation of these degrees when they are subjected         to a modification: thus we present the Generalized Symbolic Modifiers. These tools are defined as manipulations computed on a pair (degree, scale). They are grouped together into several families         and thus offer many possibilities to handle uncertainty. An application in colorimetrics is described and shows the feasibility         of the approach. The last point addressed in this article is the data combination. An operator called the Symbolic Weighted Median gives a summary of several qualitative degrees with associated weights. One particularity is that this median is constructed         on the Generalized Symbolic Modifiers. Finally we explain how the Symbolic Weighted Median is exploited in the internal mechanism of the application in colorimetrics.      </content></document><document><year>2005</year><authors>Anthony Hunter1  | Weiru Liu2</authors><title>Merging uncertain information with semantic heterogeneity in XML      </title><content>Semistructured information can be merged in a logic-based framework [6, 7]. This framework has been extended to deal with         uncertainty, in the form of probability values, degrees of beliefs, or necessity measures, associated with leaves (i.e. textentries)         in the XML documents [3]. In this paper we further extend this approach to modelling and merging uncertain information that         is defined at different levels of granularity of XML textentries, and to modelling and reasoning with XML documents that contain         semantically heterogeneous uncertain information on more complex elements in XML subtrees. We present the formal definitions         for modelling, propagating and merging semantically heterogeneous uncertain information and explain how they can be handled         using logic-based fusion techniques.      </content></document><document><year>2005</year><authors>Yannis Tzitzikas1 | Anastasia Analyti2 </authors><title>Mining the meaningful term conjunctions from materialised faceted taxonomies: algorithms and complexity      </title><content>A materialised faceted taxonomy is an information source where the objects of interest are indexed according to a faceted taxonomy. This paper shows how from a materialised faceted taxonomy, we can mine an expression of the Compound Term Composition Algebra that specifies exactly those compound terms (conjunctions of terms) that have non-empty interpretation. The mined expressions         can be used for encoding in a very compact form (and subsequently reusing), the domain knowledge that is stored in existing materialised faceted taxonomies. A distinctive         characteristic of this mining task is that the focus is given on minimising the storage space requirements of the mined set         of compound terms. This paper formulates the problem of expression mining, gives several algorithms for expression mining,         analyses their computational complexity, provides techniques for optimisation, and discusses several novel applications that         now become possible.      </content></document><document><year>2005</year><authors>Stefan Brecheisen1 | Hans-Peter Kriegel1  | Martin Pfeifle1 </authors><title>Multi-step density-based clustering      </title><content>Data mining in large databases of complex objects from scientific, engineering or multimedia applications is getting more         and more important. In many areas, complex distance measures are first choice but also simpler distance functions are available         which can be computed much more efficiently. In this paper, we will demonstrate how the paradigm of multi-step query processing         which relies on exact as well as on lower-bounding approximated distance functions can be integrated into the two density-based         clustering algorithms DBSCAN and OPTICS resulting in a considerable efficiency boost. Our approach tries to confine itself         to &amp;#603;-range queries on the simple distance functions and carries out complex distance computations only at that stage of the         clustering algorithm where they are compulsory to compute the correct clustering result. Furthermore, we will show how our         approach can be used for approximated clustering allowing the user to find an individual trade-off between quality and efficiency.         In order to assess the quality of the resulting clusterings, we introduce suitable quality measures which can be used generally         for evaluating the quality of approximated partitioning and hierarchical clusterings. In a broad experimental evaluation based         on real-world test data sets, we demonstrate that our approach accelerates the generation of exact density-based clusterings         by more than one order of magnitude. Furthermore, we show that our approximated clustering approach results in high quality         clusterings where the desired quality is scalable with respect to (w.r.t.) the overall number of exact distance computations.      </content></document><document><year>2005</year><authors>Fadi Abdeljaber Thabtah1 | Peter Cowling1 | Yonghong Peng2</authors><title>Multiple labels associative classification      </title><content>Building fast and accurate classifiers for large-scale databases is an important task in data mining. There is growing evidence         that integrating classification and association rule mining can produce more efficient and accurate classifiers than traditional         techniques. In this paper, the problem of producing rules with multiple labels is investigated, and we propose a multi-class,         multi-label associative classification approach (MMAC). In addition, four measures are presented in this paper for evaluating         the accuracy of classification approaches to a wide range of traditional and multi-label classification problems. Results         for 19 different data sets from the UCI data collection and nine hyperheuristic scheduling runs show that the proposed approach         is an accurate and effective classification technique, highly competitive and scalable if compared with other traditional         and associative classification approaches.      </content></document><document><year>2005</year><authors>Francesco Bonchi1  | Claudio Lucchese2 </authors><title>On condensed representations of constrained frequent patterns      </title><content>Constrained frequent patterns and closed frequent patterns are two paradigms aimed at reducing the set of extracted patterns         to a smaller, more interesting, subset. Although a lot of work has been done with both these paradigms, there is still confusion         around the mining problem obtained by joining closed and constrained frequent patterns in a unique framework. In this paper,         we shed light on this problem by providing a formal definition and a thorough characterisation. We also study computational         issues and show how to combine the most recent results in both paradigms, providing a very efficient algorithm that exploits         the two requirements (satisfying constraints and being closed) together at mining time in order to reduce the computation         as much as possible.      </content></document><document><year>2005</year><authors>Jianyong Wang1 | George Karypis2 </authors><title>On efficiently summarizing categorical databases      </title><content>Frequent itemset mining was initially proposed and has been studied extensively in the context of association rule mining.         In recent years, several studies have also extended its application to transaction or document clustering. However, most of         the frequent itemset based clustering algorithms need to first mine a large intermediate set of frequent itemsets in order         to identify a subset of the most promising ones that can be used for clustering. In this paper, we study how to directly find         a subset of high quality frequent itemsets that can be used as a concise summary of the transaction database and to cluster         the categorical data. By exploring key properties of the subset of itemsets that we are interested in, we proposed several         search space pruning methods and designed an efficient algorithm called SUMMARY. Our empirical results show that SUMMARY runs         very fast even when the minimum support is extremely low and scales very well with respect to the database size, and surprisingly,         as a pure frequent itemset mining algorithm it is very effective in clustering the categorical data and summarizing the dense         transaction databases.      </content></document><document><year>2005</year><authors>Qiang Zhu1 | Yingying Tao1 | Calisto Zuzarte2</authors><title>Optimizing complex queries based on similarities of subqueries</title><content>As database technology is applied to more and more application domains, user queries are becoming increasingly complex (e.g. involving a large number of joins and a complex query structure). Query optimizers in existing database management systems (DBMS) were not developed for efficiently processing such queries and often suffer from problems such as intolerably long optimization time and poor optimization results. To tackle this challenge, we present a new similarity-based approach to optimizing complex queries in this paper. The key idea is to identify similar subqueries that often appear in a complex query and share the optimization result among similar subqueries in the query. Different levels of similarity for subqueries are introduced. Efficient algorithms to identify similar queries in a given query and optimize the query based on similarity are presented. Related issues, such as choosing good starting nodes in a query graph, evaluating identified similar subqueries and analyzing algorithm complexities, are discussed. Our experimental results demonstrate that the proposed similarity-based approach is quite promising in optimizing complex queries with similar subqueries in a DBMS. </content></document><document><year>2005</year><authors>Sam Y. Sung1 | Yao Liu1| Hui Xiong2 | Peter A. Ng3</authors><title>Privacy preservation for data cubes      </title><content>A range query finds the aggregated values over all selected cells of an online analytical processing (OLAP) data cube where         the selection is specified by the ranges of contiguous values for each dimension. An important issue in reality is how to         preserve the confidential information in individual data cells while still providing an accurate estimation of the original         aggregated values for range queries. In this paper, we propose an effective solution, called the zero-sum method, to this         problem. We derive theoretical formulas to analyse the performance of our method. Empirical experiments are also carried out         by using analytical processing benchmark (APB) dataset from the OLAP Council. Various parameters, such as the privacy factor         and the accuracy factor, have been considered and tested in the experiments. Finally, our experimental results show that there         is a trade-off between privacy preservation and range query accuracy, and the zero-sum method has fulfilled three design goals:         security, accuracy, and accessibility.      </content></document><document><year>2005</year><authors>Matja&amp;#382  Kukar1 </authors><title>Quality assessment of individual classifications in machine learning and data mining      </title><content>Although in the past machine learning algorithms have been successfully used in many problems, their serious practical use         is affected by the fact that often they cannot produce reliable and unbiased assessments of their predictions' quality. In         last few years, several approaches for estimating reliability or confidence of individual classifiers have emerged, many of         them building upon the algorithmic theory of randomness, such as (historically ordered) transduction-based confidence estimation,         typicalness-based confidence estimation, and transductive reliability estimation. Unfortunately, they all have weaknesses:         either they are tightly bound with particular learning algorithms, or the interpretation of reliability estimations is not         always consistent with statistical confidence levels. In the paper we describe typicalness and transductive reliability estimation         frameworks and propose a joint approach that compensates the above-mentioned weaknesses by integrating typicalness-based confidence         estimation and transductive reliability estimation into a joint confidence machine. The resulting confidence machine produces         confidence values in the statistical sense. We perform series of tests with several different machine learning algorithms         in several problem domains. We compare our results with that of a proprietary method as well as with kernel density estimation.         We show that the proposed method performs as well as proprietary methods and significantly outperforms density estimation         methods.      </content></document><document><year>2005</year><authors>Kun-Lung Wu1 | Shyh-Kwei Chen1 | Philip S. Yu1</authors><title>Query indexing with containment-encoded intervals for efficient stream processing      </title><content>Many continual range queries can be issued against data streams. To efficiently evaluate continual queries against a stream,         a main memory-based query index with a small storage cost and a fast search time is needed, especially if the stream is rapid.         In this paper, we study a CEI-based query index that meets both criteria for efficient processing of continual interval queries.         This new query index is an indirect indexing approach. It centres around a set of predefined virtual containment-encoded intervals, or CEIs. The CEIs are used to first decompose query intervals and then perform efficient search operations. The CEIs are         defined and labeled such that containment relationships among them are encoded in their IDs. The containment encoding makes         decomposition and search operations efficient; from the encoding of the smallest CEI containing a data point, the encodings         of other containing CEIs can be easily derived. Closed-form formulae for the bounds of the average index storage cost are         derived. Simulations are conducted to evaluate the effectiveness of the CEI-based query index and to compare it with alternative         approaches. The results show that the CEI-based query index significantly outperforms existing approaches in terms of both         storage cost and search time.      </content></document><document><year>2005</year><authors>Hiroshi Mamitsuka1 </authors><title>Query-learning-based iterative feature-subset selection for learning from high-dimensional data sets      </title><content>We propose a new data-mining method that is effective for learning from extremely high-dimensional data sets. Our proposed         method selects a subset of features from a high-dimensional data set by a process of iterative refinement. Our selection of         a feature-subset has two steps. The first step selects a subset of instances, to which predictions by hypotheses previously         obtained are most unreliable, from the data set. The second step selects a subset of features whose values in the selected         instances vary the most from those in all instances of the database. We empirically evaluate the effectiveness of the proposed         method by comparing its performance with those of four other methods, including one of the latest feature-subset selection         methods. The evaluation was performed on a real-world data set with approximately 140,000 features. Our results show that         the performance of the proposed method exceeds those of the other methods in terms of prediction accuracy, precision at a         certain recall value, and computation time to reach a certain prediction accuracy. We have also examined the effect of noise         in the data and found that the advantage of the proposed method becomes more pronounced for larger noise levels. Extended         abstracts of parts of the work presented in this paper have appeared in Mamitsuka [14] and Mamitsuka [15].      </content></document><document><year>2005</year><authors>Massih R. Amini1  | Patrick Gallinari1</authors><title>Semi-supervised learning with an imperfect supervisor      </title><content>Real-life applications may involve huge data sets with misclassified or partially classified training data. Semi-supervised         learning and learning in the presence of label noise have recently emerged as new paradigms in the machine learning community         to cope with this kind of problems. This paper describes a new discriminant algorithm for semi-supervised learning. This algorithm         optimizes the classification maximum likelihood (CML) of a set of labeled&amp;#8211;unlabeled data, using a discriminant extension of         the Classification Expectation Maximization algorithm. We further propose to extend this algorithm by modeling imperfections         in the estimated class labels for unlabeled data. The parameters of this label-error model are learned together with the semi-supervised         classifier parameters. We demonstrate the effectiveness of the approach using extensive experiments on different datasets.      </content></document><document><year>2005</year><authors>Sanjay Chawla1 | Pei Sun1</authors><title>SLOM: a new measure for local spatial outliers      </title><content>We propose a measure, spatial local outlier measure (SLOM), which captures the local behaviour of datum in their spatial neighbourhood.         With the help of SLOM, we are able to discern local spatial outliers that are usually missed by global techniques, like &amp;#8220;three         standard deviations away from the mean&amp;#8221;. Furthermore, the measure takes into account the local stability around a data point         and suppresses the reporting of outliers in highly unstable areas, where data are too heterogeneous and the notion of outliers         is not meaningful. We prove several properties of SLOM and report experiments on synthetic and real data sets that show that         our approach is novel and scalable to large datasets.      </content></document><document><year>2005</year><authors>Ronen Feldman1 | Benjamin Rosenfeld1 | Moshe Fresko1</authors><title>TEG&amp;#8212;a hybrid approach to information extraction      </title><content>This paper describes a hybrid statistical and knowledge-based information extraction model, able to extract entities and relations         at the sentence level. The model attempts to retain and improve the high accuracy levels of knowledge-based systems while         drastically reducing the amount of manual labour by relying on statistics drawn from a training corpus. The implementation         of the model, called TEG (trainable extraction grammar), can be adapted to any IE domain by writing a suitable set of rules         in a SCFG (stochastic context-free grammar)-based extraction language and training them using an annotated corpus. The system         does not contain any purely linguistic components, such as PoS tagger or shallow parser, but allows to using external linguistic         components if necessary. We demonstrate the performance of the system on several named entity extraction and relation extraction         tasks. The experiments show that our hybrid approach outperforms both purely statistical and purely knowledge-based systems,         while requiring orders of magnitude less manual rule writing and smaller amounts of training data. We also demonstrate the         robustness of our system under conditions of poor training-data quality.      </content></document><document><year>2005</year><authors>Miles Efron1 </authors><title>Using cocitation information to estimate political orientation in web documents      </title><content>This paper introduces a simple method for estimating cultural orientation, the affiliation of online entities in a polarized         field of discourse. In particular, cocitation information is used to estimate the political orientation of hypertext documents.         A type of cultural orientation, the political orientation of a document is the degree to which it participates in traditionally         left- or right-wing beliefs. Estimating documents' political orientation is of interest for personalized information retrieval         and recommender systems. In its application to politics, the method uses a simple probabilistic model to estimate the strength         of association between a document and left- and right-wing communities. The model estimates the likelihood of cocitation between         a document of interest and a small number of documents of known orientation. The model is tested on three sets of data, 695         partisan web documents, 162 political weblogs, and 198 nonpartisan documents. Accuracy above 90% is obtained from the cocitation         model, outperforming lexically based classifiers at statistically significant levels.      </content></document><document><year>2005</year><authors>Zhao Li1 | Wee Keong Ng1 | Aixin Sun1</authors><title>Web data extraction based on structural similarity      </title><content>Web data-extraction systems in use today mainly focus on the generation of extraction rules, i.e., wrapper induction. Thus,         they appear ad hoc and are difficult to integrate when a holistic view is taken. Each phase in the data-extraction process         is disconnected and does not share a common foundation to make the building of a complete system straightforward. In this         paper, we demonstrate a holistic approach to Web data extraction. The principal component of our proposal is the notion of         a document schema. Document schemata are patterns of structures embedded in documents. Once the document schemata are obtained, the various phases (e.g. training         set preparation, wrapper induction and document classification) can be easily integrated. The implication of this is improved         efficiency and better control over the extraction procedure. Our experimental results confirmed this. More importantly, because         a document can be represented as avector of schema, it can be easily incorporated into existing systems as the fabric for integration.       </content></document></documents>