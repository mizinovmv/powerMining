<?xml version="1.0" encoding="UTF-8" standalone="no"?><documents><document><year>2009</year><authors>Raja Gunasekaran1 | Vaidheyanathan Rhymend Uthariaraj2 | Uamapathy Yamini1 | Rajagopalan Sudharsan1  | Selvaraj Sujitha Priyadarshini1 </authors><title>A Distributed Mechanism for Handling of Adaptive/Intelligent Selfish Misbehaviour at MAC Layer in Mobile Ad Hoc Networks      </title><content>Medium access control (MAC) protocols such as IEEE 802.11 are used in wireless networks for sharing of the wireless medium.         The random nature of the protocol operation together with the inherent difficulty of monitoring in the open poses significant         challenges. All nodes are expected to comply with the protocol rules. But, some nodes in order to gain greater benefits misbehave         by not complying with the rules. One such selfish misbehavior is waiting for smaller back-off intervals when compared to the         other nodes in the same subnet. Such selfish misbehavior is being tackled in this paper. A diagnosis scheme and a penalty         scheme are being proposed for overcoming such selfish-misbehavior at MAC layer of mobile ad hoc networks which could be extended         to other types of networks also.      </content></document><document><year>2009</year><authors>Shin-ichi Nakano1 | Ryuhei Uehara2  | Takeaki Uno3 </authors><title>A New Approach to Graph Recognition and Applications to Distance-Hereditary Graphs      </title><content>Algorithms used in data mining and bioinformatics have to deal with huge amount of data efficiently. In many applications,         the data are supposed to have explicit or implicit structures. To develop efficient algorithms for such data, we have to propose         possible structure models and test if the models are feasible. Hence, it is important to make a compact model for structured         data, and enumerate all instances efficiently. There are few graph classes besides trees that can be used for a model. In         this paper, we investigate distance-hereditary graphs. This class of graphs consists of isometric graphs and hence contains         trees and cographs. First, a canonical and compact tree representation of the class is proposed. The tree representation can         be constructed in linear time by using prefix trees. Usually, prefix trees are used to maintain a set of strings. In our algorithm,         the prefix trees are used to maintain the neighborhood of vertices, which is a new approach unlike the lexicographically breadth-first         search used in other studies. Based on the canonical tree representation, efficient algorithms for the distance-hereditary         graphs are proposed, including linear time algorithms for graph recognition and graph isomorphism and an efficient enumeration         algorithm. An efficient coding for the tree representation is also presented; it requires &amp;#8968;3.59n&amp;#8969; bits for a distance-hereditary graph of n vertices and 3n bits for a cograph. The results of coding improve previously known upper bounds (both are 2            O(n log n)) of the number of distance-hereditary graphs and cographs to 2&amp;#8968;3.59n&amp;#8969; and 23n            , respectively.      </content></document><document><year>2009</year><authors>Sriparna Saha1  | Sanghamitra B|yopadhyay1 </authors><title>A New Line Symmetry Distance and Its Application to Data Clustering      </title><content>In this paper, at first a new line-symmetry-based distance is proposed. The properties of the proposed distance are then elaborately         described. Kd-tree-based nearest neighbor search is used to reduce the complexity of computing the proposed line-symmetry-based distance.         Thereafter an evolutionary clustering technique is developed that uses the new line-symmetry-based distance measure for assigning         points to different clusters. Adaptive mutation and crossover probabilities are used to accelerate the proposed clustering         technique. The proposed GA with line-symmetry-distance-based (GALSD) clustering technique is able to detect any type of clusters,         irrespective of their geometrical shape and overlapping nature, as long as they possess the characteristics of line symmetry.         GALSD is compared with the existing well-known K-means clustering algorithm and a newly developed genetic point-symmetry-distance-based clustering technique (GAPS) for three         artificial and two real-life data sets. The efficacy of the proposed line-symmetry-based distance is then shown in recognizing         human face from a given image.      </content></document><document><year>2009</year><authors>Jeff Kramer1  | Jeff Magee1 </authors><title>A Rigorous Architectural Approach to Adaptive Software Engineering      </title><content>The engineering of distributed adaptive software is a complex task which requires a rigorous approach. Software architectural         (structural) concepts and principles are highly beneficial in specifying, designing, analysing, constructing and evolving         distributed software. A rigorous architectural approach dictates formalisms and techniques that are compositional, components         that are context independent and systems that can be constructed and evolved incrementally. This paper overviews some of the         underlying reasons for adopting an architectural approach, including a brief &amp;#8220;rational history&amp;#8221; of our research work, and         indicates how an architectural model can potentially facilitate the provision of self-managed adaptive software system.      </content></document><document><year>2009</year><authors>Jiang Yu1 | Andrew Tappenden1 | James Miller1  | Michael Smith2 </authors><title>A Scalable Testing Framework for Location-Based Services      </title><content>A novel testing framework for location based services is introduced. In particular, the paper showcases a novel architecture         for such a framework. The implementation of the framework illustrates both the functionality and the feasibility of the framework         proposed and the utility of the architecture. The new framework is evaluated through comparison to several other methodologies         currently available for the testing of location-based applications. A case study is presented in which the testing framework         was applied to a typical mobile service tracking system. It is concluded that the proposed testing framework achieves the         best coverage of the entire location based service testing problem of the currently available methodologies; being equipped         to test the widest array of application attributes and allowing for the automation of testing activities.      </content></document><document><year>2009</year><authors>Santi MartГ­nez1 | Magda Valls2 | ConcepciГі Roig1 | Josep M. Miret2  | Francesc GinГ©1 </authors><title>A Secure Elliptic Curve-Based RFID Protocol      </title><content>Nowadays, the use of Radio Frequency Identification (RFID) systems in industry and stores has increased. Nevertheless, some         of these systems present privacy problems that may discourage potential users. Hence, high confidence and effient privacy         protocols are urgently needed. Previous studies in the literature proposed schemes that are proven to be secure, but they         have scalability problems. A feasible and scalable protocol to guarantee privacy is presented in this paper. The proposed         protocol uses elliptic curve cryptography combined with a zero knowledge-based authentication scheme. An analysis to prove         the system secure, and even forward secure is also provided.      </content></document><document><year>2009</year><authors>Feng Xu1| 2 | Jing Pan1| 2  | Wen Lu1| 2 </authors><title>A Trust-Based Approach to Estimating the Confidence of the Software System in Open Environments      </title><content>Emerging with open environments, the software paradigms, such as open resource coalition and Internetware, present several         novel characteristics including user-centric, non-central control, and continual evolution. The goal of obtaining high confidence         on such systems is more difficult to achieve. The general developer-oriented metrics and testing-based methods which are adopted         in the traditional measurement for high confidence software seem to be infeasible in the new situation. Firstly, the software         development is changed from the developer-centric to user-centric, while user's opinions are usually subjective, and cannot         be generalized in one objective metric. Secondly, there is non-central control to guarantee the testing on components which         formed the software system, and continual evolution makes it impossible to test on the whole software system. Therefore, this         paper proposes a trust-based approach that consists of three sequential sub-stages: 1) describing metrics for confidence estimation         from users; 2) estimating the confidence of the components based on the quantitative information from the trusted recommenders;         3) estimating the confidence of the whole software system based on the component confidences and their interactions, as well         as attempts to make a step toward a reasonable and effective method for confidence estimation of the software system in open         environments.      </content></document><document><year>2009</year><authors>Sa&amp;#8217 ed Abed1 | Otmane Ait Mohamed1  | Ghiath Al-Sammane1 </authors><title>An Abstract Reachability Approach by Combining HOL Induction and Multiway Decision Graphs      </title><content>In this paper, we provide a necessary infrastructure to define an abstract state exploration in the HOL theorem prover. Our         infrastructure is based on a deep embedding of the Multiway Decision Graphs (MDGs) theory in HOL. MDGs generalize Reduced         Ordered Binary Decision Diagrams (ROBDDs) to represent and manipulate a subset of first-order logic formulae. The MDGs embedding         is based on the logical formulation of an MDG as Directed Formulae (DF). Then, the MDGs operations are defined and the correctness         proof of each operation is provided. The MDG reachability algorithm is then defined as a conversion that uses our MDG theory         within HOL. Finally, a set of experimentations over benchmark circuits has been conducted to ensure the applicability and         to measure the performance of our approach.      </content></document><document><year>2009</year><authors>George W. Hart1 </authors><title>An Algorithm for Constructing 3D Struts      </title><content>A simple robust &amp;#8220;strut algorithm&amp;#8221; is presented which, when given a graph embedded in 3D space, thickens its edges into solid         struts. Various applications, crystallographic and sculptural, are shown in which smooth high-genus forms are the output.         A toolbox of algorithmic techniques allow for a variety of novel, visually engaging forms that express a mathematical aesthetic.         In sculptural examples, hyperbolic tessellations in the PoincarГ© plane are transformed in several ways to three-dimensional         networks of edges embodied within a plausibly organic organization. By the use of different transformations and adjustable         parameters in the algorithms, a variety of attractive forms result. The techniques produce watertight boundary representations         that can be built with solid freeform fabrication equipment. The final physical output satisfies the &amp;#8220;coolness criterion,&amp;#8221;         that passers by will pick them up and say &amp;#8220;Wow, that&amp;#8217;s cool!&amp;#8221;      </content></document><document><year>2009</year><authors>Hao Wen1 | Chuang Lin1 | Zhi-Jia Chen1 | Hao Yin1 | Tao He1  | Eryk Dutkiewicz2 </authors><title>An Improved Markov Model for IEEE 802.15.4 Slotted CSMA/CA Mechanism      </title><content>IEEE 802.15.4 protocol is proposed to meet the low latency and energy consumption needs in low-rate wireless applications,         however, few analytical models are tractable enough for comprehensive evaluation of the protocol. To evaluate the IEEE 802.15.4         slotted CSMA/CA channel access mechanism in this paper, we propose a practical and accurate discrete Markov chain model, which         can dynamically represent different network loads. By computing the steady-state distribution probability of the Markov chain,         we obtain an evaluation formula for throughput, energy consumption, and access latency. Then we further analyze the parameters         that influence performance including packet arrival rate, initial backoff exponent and maximum backoff number. Finally, NS2         simulator has been used to evaluate the performance of the 802.15.4 CSMA/CA mechanism under different scenarios and to validate         the accuracy of the proposed model.      </content></document><document><year>2009</year><authors>Patrick H. S. Brito1 | RogГ©rio de Lemos2 | CecГ­lia M. F. Rubira1  | Eliane Martins1 </authors><title>Architecting Fault Tolerance with Exception Handling: Verification and Validation      </title><content>When building dependable systems by integrating untrusted software components that were not originally designed to interact         with each other, it is likely the occurrence of architectural mismatches related to assumptions in their failure behaviour.         These mismatches, if not prevented during system design, have to be tolerated during runtime. This paper presents an architectural         abstraction based on exception handling for structuring fault-tolerant software systems. This abstraction comprises several         components and connectors that promote an existing untrusted software element into an idealised fault-tolerant architectural         element. Moreover, it is considered in the context of a rigorous software development approach based on formal methods for         representing the structure and behaviour of the software architecture. The proposed approach relies on a formal specification         and verification for analysing exception propagation, and verifying important dependability properties, such as deadlock freedom,         and scenarios of architectural reconfiguration. The formal models are automatically generated using model transformation from         UML diagrams: component diagram representing the system structure, and sequence diagrams representing the system behaviour.         Finally, the formal models are also used for generating unit and integration test cases that are used for assessing the correctness         of the source code. The feasibility of the proposed architectural approach was evaluated on an embedded critical case study.      </content></document><document><year>2009</year><authors>Th|ar Thein1  | Jong Sou Park1 </authors><title>Availability Analysis of Application Servers Using Software Rejuvenation and Virtualization      </title><content>Demands on software reliability and availability have increased tremendously due to the nature of present day applications.         We focus on the aspect of software for the high availability of application servers since the unavailability of servers more         often originates from software faults rather than hardware faults. The software rejuvenation technique has been widely used         to avoid the occurrence of unplanned failures, mainly due to the phenomena of software aging or caused by transient failures.         In this paper, first we present a new way of using the virtual machine based software rejuvenation named VMSR to offer high         availability for application server systems. Second we model a single physical server which is used to host multiple virtual         machines (VMs) with the VMSR framework using stochastic modeling and evaluate it through both numerical analysis and SHARPE         (Symbolic Hierarchical Automated Reliability and Performance Evaluator) tool simulation. This VMSR model is very general and         can capture application server characteristics, failure behavior, and performability measures. Our results demonstrate that         VMSR approach is a practical way to ensure uninterrupted availability and to optimize performance for aging applications.      </content></document><document><year>2009</year><authors>Yong-Xi Gong1 | Yu Liu1 | Lun Wu1  | Yu-Bo Xie2 </authors><title>Boolean Operations on Conic Polygons      </title><content>An algorithm for Boolean operations on conic polygons is proposed. Conic polygons are polygons consisting of conic segments         or bounded conics with directions. Preliminaries of Boolean operations on general polygons are presented. In our algorithm,         the intersection points and the topological relationships between two conic polygons are computed. Boundaries are obtained         by tracking path and selecting uncrossed boundaries following rule tables to build resulting conic polygons. We define a set         of rules for the intersection, union, and subtraction operations on conic polygons. The algorithm considers degeneration cases         such as homology, complement, interior, and exterior. The algorithm is also evaluated and implemented.      </content></document><document><year>2009</year><authors>Jing Zhou1 | Wendy Hall2  | David De Roure2 </authors><title>Building a Distributed Infrastructure for Scalable Triple Stores      </title><content>Built specifically for the Semantic Web, triple stores are required to accommodate a large number of RDF triples and remain         primarily centralized. As triple stores grow and evolve with time, there is a demanding need for scalable techniques to remove         resource and performance bottlenecks in such systems. To this end, we propose a fully decentralized peer-to-peer architecture         for large scale triple stores in which triples are maintained by individual stakeholders, and a semantics-directed search         protocol, mediated by topology reorganization, for locating triples of interest. We test our design through simulations and         the results show anticipated improvements over existing techniques for distributed triple stores. In addition to engineering         future large scale triple stores, our work will in particular benefit the federation of stand-alone triple stores of today         to achieve desired scalability.      </content></document><document><year>2009</year><authors>Long Li1| 2 | Yu Zhang1| 2 | Yi-Yun Chen1| 2  | Yong Li1| 2 </authors><title>Certifying Concurrent Programs Using Transactional Memory      </title><content>Transactional memory (TM) is a new promising concurrency-control mechanism that can avoid many of the pitfalls of the traditional         lock-based techniques. TM systems handle data races between threads automatically so that programmers do not have to reason         about the interaction of threads manually. TM provides a programming model that may make the development of multi-threaded         programs easier. Much work has been done to explore the various implementation strategies of TM systems and to achieve better         performance, but little has been done on how to formally reason about programs using TM and how to make sure that such reasoning         is sound. In this paper, we focus on the semantics of transactional memory and present a proof-carrying code (PCC) system         for reasoning about programs using TM. We formalize our reasoning with respect to the TM semantics, prove its soundness, and         use examples to demonstrate its effectiveness.      </content></document><document><year>2009</year><authors>FranГ§ois Bonnet1  | Michel Raynal1 </authors><title>Conditions for Set Agreement with an Application to Synchronous Systems      </title><content>The k-set agreement problem is a generalization of the consensus problem: considering a system made up of n processes where each process proposes a value, each non-faulty process has to decide a value such that a decided value is         a proposed value, and no more than k different values are decided. While this problem cannot be solved in an asynchronous system prone to t process crashes when t &amp;#8805; k, it can always be solved in a synchronous system;  is then a lower bound on the number of rounds (consecutive communication steps) for the non-faulty processes to decide. The         condition-based approach has been introduced in the consensus context. Its aim was to both circumvent the consensus impossibility in asynchronous         systems, and allow for more efficient consensus algorithms in synchronous systems. This paper addresses the condition-based         approach in the context of the k-set agreement problem. It has two main contributions. The first is the definition of a framework that allows defining conditions         suited to the k-set agreement problem and the second is a generic synchronous k-set agreement algorithm based on conditions.      </content></document><document><year>2009</year><authors>Ji Wang1 | Xiao-Dong Ma1 | Wei Dong1 | Hou-Feng Xu1  | Wan-Wei Liu1 </authors><title>Demand-Driven Memory Leak Detection Based on Flow- and Context-Sensitive Pointer Analysis      </title><content>We present a demand-driven approach to memory leak detection algorithm based on flow- and context-sensitive pointer analysis.         The detection algorithm firstly assumes the presence of a memory leak at some program point and then runs a backward analysis         to see if this assumption can be disproved. Our algorithm computes the memory abstraction of programs based on points-to graph         resulting from flow- and context-sensitive pointer analysis. We have implemented the algorithm in the SUIF2 compiler infrastructure         and used the implementation to analyze a set of C benchmark programs. The experimental results show that the approach has         better precision with satisfied scalability as expected.      </content></document><document><year>2009</year><authors>Javier GarzГЎs1 | FГ©lix GarcГ­a2  | Mario Piattini2 </authors><title>Do Rules and Patterns Affect Design Maintainability?      </title><content>At the present time, best rules and patterns have reached a zenith in popularity and diffusion, thanks to the software community&amp;#8217;s         efforts to discover, classify and spread knowledge concerning all types of rules and patterns. Rules and patterns are useful         elements, but many features remain to be studied if we wish to apply them in a rational manner. The improvement in quality         that rules and patterns can inject into design is a key issue to be analyzed, so a complete body of empirical knowledge dealing         with this is therefore necessary. This paper tackles the question of whether design rules and patterns can help to improve         the extent to which designs are easy to understand and modify. An empirical study, composed of one experiment and a replica,         was conducted with the aim of validating our conjecture. The results suggest that the use of rules and patterns affect the         understandability and modifiability of the design, as the diagrams with rules and patterns are more difficult to understand         than non-rule/pattern versions and more effort is required to carry out modifications to designs with rules and patterns.      </content></document><document><year>2009</year><authors>Muhammad Hussain1 </authors><title>Efficient Simplification Methods for Generating High Quality LODs of 3D Meshes      </title><content>Two simplification algorithms are proposed for automatic decimation of polygonal models, and for generating their LODs. Each         algorithm orders vertices according to their priority values and then removes them iteratively. For setting the priority value         of each vertex, exploiting normal field of its one-ring neighborhood, we introduce a new measure of geometric fidelity that         reflects well the local geometric features of the vertex. After a vertex is selected, using other measures of geometric distortion         that are based on normal field deviation and distance measure, it is decided which of the edges incident on the vertex is         to be collapsed for removing it. The collapsed edge is substituted with a new vertex whose position is found by minimizing         the local quadric error measure. A comparison with the state-of-the-art algorithms reveals that the proposed algorithms are         simple to implement, are computationally more efficient, generate LODs with better quality, and preserve salient features         even after drastic simplification. The methods are useful for applications such as 3D computer games, virtual reality, where         focus is on fast running time, reduced memory overhead, and high quality LODs.      </content></document><document><year>2009</year><authors>Zhe Bian1 | Shi-Min Hu1  | Ralph R. Martin2 </authors><title>Evaluation for Small Visual Difference Between Conforming Meshes on Strain Field      </title><content>This paper gives a method of quantifying small visual differences between 3D mesh models with conforming topology, based on         the theory of strain fields. Strain field is a geometric quantity in elasticity which is used to describe the deformation         of elastomer. In this paper we consider the 3D models as objects with elasticity. The further demonstrations are provided:         the first is intended to give the reader a visual impression of how our measure works in practice; and the second is to give         readers a visual impression of how our measure works in evaluating filter algorithms. Our experiments show that our difference         estimates are well correlated with human perception of differences. This work has applications in the evaluation of 3D mesh         watermarking, 3D mesh compression reconstruction, and 3D mesh filtering.      </content></document><document><year>2009</year><authors>Xian Xu1| 2 </authors><title>Expressing First-Order &amp;#960;-Calculus in Higher-Order Calculus of Communicating Systems      </title><content>In the study of process calculi, encoding between different calculi is an effective way to compare the expressive power of         calculi and can shed light on the essence of where the difference lies. Thomsen and Sangiorgi have worked on the higher-order         calculi (higher-order Calculus of Communicating Systems (CCS) and higher-order &amp;#960;-calculus, respectively) and the encoding         from and to first-order &amp;#960;-calculus. However a fully abstract encoding of first-order &amp;#960;-calculus with higher-order CCS is not         available up-today. This is what we intend to settle in this paper. We follow the encoding strategy, first proposed by Thomsen,         of translating first-order &amp;#960;-calculus into Plain CHOCS. We show that the encoding strategy is fully abstract with respect         to early bisimilarity (first-order &amp;#960;-calculus) and wired bisimilarity (Plain CHOCS) (which is a bisimulation defined on wired         processes only sending and receiving wires), that is the core of the encoding strategy. Moreover from the fact that the wired         bisimilarity is contained by the well-established context bisimilarity, we secure the soundness of the encoding, with respect         to early bisimilarity and context bisimilarity. We use index technique to get around all the technical details to reach these         main results of this paper. Finally, we make some discussion on our work and suggest some future work.      </content></document><document><year>2009</year><authors>Xin Peng1 | Seok-Won Lee2  | Wen-Yun Zhao1 </authors><title>Feature-Oriented Nonfunctional Requirement Analysis for Software Product Line      </title><content>Domain analysis in software product line (SPL) development provides a basis for core assets design and implementation by a         systematic and comprehensive commonality/variability analysis. In feature-oriented SPL methods, products of the domain analysis         are domain feature models and corresponding feature decision models to facilitate application-oriented customization. As in         requirement analysis for a single system, the domain analysis in the SPL development should consider both functional and nonfunctional         domain requirements. However, the nonfunctional requirements (NFRs) are often neglected in the existing domain analysis methods.         In this paper, we propose a context-based method of the NFR analysis for the SPL development. In the method, NFRs are materialized         by connecting nonfunctional goals with real-world context, thus NFR elicitation and variability analysis can be performed         by context analysis for the whole domain with the assistance of NFR templates and NFR graphs. After the variability analysis,         our method integrates both functional and nonfunctional perspectives by incorporating the nonfunctional goals and operationalizations         into an initial functional feature model. NFR-related constraints are also elicited and integrated. Finally, a decision model         with both functional and nonfunctional perspectives is constructed to facilitate application-oriented feature model customization.         A computer-aided grading system (CAGS) product line is employed to demonstrate the method throughout the paper.      </content></document><document><year>2009</year><authors>Tao Ju1 </authors><title>Fixing Geometric Errors on Polygonal Models: A Survey      </title><content>Polygonal models are popular representations of 3D objects. The use of polygonal models in computational applications often         requires a model to properly bound a 3D solid. That is, the polygonal model needs to be closed, manifold, and free of self-intersections.         This paper surveys a sizeable literature for repairing models that do not satisfy this criteria, focusing on categorizing         them by their methodology and capability. We hope to offer pointers to further readings for researchers and practitioners,         and suggestions of promising directions for future research endeavors.      </content></document><document><year>2009</year><authors>Leon J. Osterweil1 </authors><title>Formalisms to Support the Definition of Processes      </title><content>This paper emphasizes the importance of defining processes rigorously, completely, clearly, and in detail in order to support         the complex projects that are essential to the modern world. The paper argues that such process definitions provide needed         structure and context for the development of effective software systems. The centrality of process is argued by enumerating         seven key ways in which processes and their definitions are expected to provide important benefits to society. The paper provides         an example of a process formalism that makes good progress towards the difficult goal of being simultaneously rigorous, detailed,         broad, and clear. Early experience suggests that these four key characteristics of this formalism do indeed seem to help it         to support meeting the seven key benefits sought from process definitions. Additional research is suggested in order to gain         more insights into needs in the area of process definition formalisms.      </content></document><document><year>2009</year><authors>Chao Cai1 | Zong-Yan Qiu1 | Hong-Li Yang2  | Xiang-Peng Zhao1 </authors><title>Global-to-Local Approach to Rigorously Developing Distributed System with Exception Handling      </title><content>Cooperative distributed system covers a wide range of applications such as the systems for industrial controlling and business-to-business         trading, which are usually safety-critical. Coordinated exception handling (CEH) refers to exception handling in the cooperative         distributed systems, where exceptions raised on a peer should be dealt with by all relevant peers in a consistent manner.         Some CEH algorithms have been proposed. A crucial problem in using these algorithms is how to develop the peers which are         guaranteed coherent in both normal execution and exceptional execution. Straightforward testing or model checking is very         expensive. In this paper, we propose an effective way to rigorously develop the systems with correct CEH behavior. Firstly,         we formalize the CEH algorithm by proposing a Peer Process Language to precisely describe the distributed systems and their         operational semantics. Then we dig out a set of syntactic conditions, and prove its sufficiency for system coherence. Finally,         we propose a global-to-local approach, including a language describing the distributed systems from a global perspective and         a projection algorithm, for developing the systems. Given a well-formed global description, a set of peers can be generated         automatically. We prove the system composed of these peers satisfies the conditions, that is, it is always coherent and correct         for CEH.      </content></document><document><year>2009</year><authors>Wei-Wei Xu1  | Kun Zhou2 </authors><title>Gradient Domain Mesh Deformation &amp;#8212; A Survey      </title><content>This survey reviews the recent development of gradient domain mesh deformation method. Different to other deformation methods,         the gradient domain deformation method is a surface-based, variational optimization method. It directly encodes the geometric         details in differential coordinates, which are also called Laplacian coordinates in literature. By preserving the Laplacian         coordinates, the mesh details can be well preserved during deformation. Due to the locality of the Laplacian coordinates,         the variational optimization problem can be casted into a sparse linear system. Fast sparse linear solver can be adopted to         generate deformation result interactively, or even in real-time. The nonlinear nature of gradient domain mesh deformation         leads to the development of two categories of deformation methods: linearization methods and nonlinear optimization methods.         Basically, the linearization methods only need to solve the linear least-squares system once. They are fast, easy to understand         and control, while the deformation result might be suboptimal. Nonlinear optimization methods can reach optimal solution of         deformation energy function by iterative updating. Since the computation of nonlinear methods is expensive, reduced deformable         models should be adopted to achieve interactive performance. The nonlinear optimization methods avoid the user burden to input         transformation at deformation handles, and they can be extended to incorporate various nonlinear constraints, like volume         constraint, skeleton constraint, and so on. We review representative methods and related approaches of each category comparatively         and hope to help the user understand the motivation behind the algorithms. Finally, we discuss the relation between physical         simulation and gradient domain mesh deformation to reveal why it can achieve physically plausible deformation result.      </content></document><document><year>2009</year><authors>Liang Xu1| 2 | Wei Chen1| 2 | Yan-Yan Xu1| 2  | Wen-Hui Zhang1 </authors><title>Improved Bounded Model Checking for the Universal Fragment of CTL      </title><content>SAT-based bounded model checking (BMC) has been introduced as a complementary technique to BDD-based symbolic model checking         in recent years, and a lot of successful work has been done in this direction. The approach was first introduced by A. Biere         et al. in checking linear temporal logic (LTL) formulae and then also adapted to check formulae of the universal fragment of computation         tree logic (ACTL) by W. Penczek et al. As the efficiency of model checking is still an important issue, we present an improved BMC approach for ACTL based on Penczek&amp;#8217;s         method. We consider two aspects of the approach. One is reduction of the number of variables and transitions in the k-model by distinguishing the temporal operator EX from the others. The other is simplification of the transformation of formulae by using uniform path encoding instead of         a disjunction of all paths needed in the k-model. With these improvements, for an ACTL formula, the length of the final encoding of the formula in the worst case is         reduced. The improved approach is implemented in the tool BMV and is compared with the original one by applying both to two         well known examples, mutual exclusion and dining philosophers. The comparison shows the advantages of the improved approach         with respect to the efficiency of model checking.      </content></document><document><year>2009</year><authors>Fu-Hua (Frank) Cheng1 | Feng-Tao Fan1 | Shu-Hua Lai2 | Cong-Lin Huang1 | Jia-Xi Wang1  | Jun-Hai Yong3 </authors><title>Loop Subdivision Surface Based Progressive Interpolation      </title><content>A new method for constructing interpolating Loop subdivision surfaces is presented. The new method is an extension of the         progressive interpolation technique for B-splines. Given a triangular mesh M, the idea is to iteratively upgrade the vertices of M to generate a new control mesh  such that limit surface of  would interpolate M. It can be shown that the iterative process is convergent for Loop subdivision surfaces. Hence, the method is well-defined.         The new method has the advantages of both a local method and a global method, i.e., it can handle meshes of any size and any         topology while generating smooth interpolating subdivision surfaces that faithfully resemble the shape of the given meshes.         The meshes considered here can be open or closed.      </content></document><document><year>2009</year><authors>Bin Sheng1| 2 | Jian Zhu2 | En-Hua Wu2| 3  | Yan-Ci Zhang4 </authors><title>Lumiproxy: A Hybrid Representation of Image-Based Models      </title><content>In this paper, we present a hybrid representation of image-based models combining the textured planes and the hierarchical         points. Taking a set of depth images as input, our method starts from classifying input pixels into two categories, indicating         the planar and non-planar surfaces respectively. For the planar surfaces, the geometric coefficients are reconstructed to         form the uniformly sampled textures. For nearly planar surfaces, some textured planes, called lumiproxies, are constructed to represent the equivalent visual appearance. The Hough transform is used to find the positions of these         textured planes, and optic flow measures are used to determine their textures. For remaining pixels corresponding to the non-planar         geometries, the point primitive is applied, reorganized as the OBB-tree structure. Then, texture mapping and point splatting         are employed together to render the novel views, with the hardware acceleration.      </content></document><document><year>2009</year><authors>Xiao-Min Zhu1  | Pei-Zhong Lu1 </authors><title>Multi-Dimensional Scheduling for Real-Time Tasks on Heterogeneous Clusters      </title><content>Multiple performance requirements need to be guaranteed in some real-time applications such as multimedia data processing         and real-time signal processing in addition to timing constraints. Unfortunately, most conventional scheduling algorithms         only take one or two dimensions of them into account. Motivated by this fact, this paper investigates the problem of providing         multiple performance guarantees including timeliness, QoS, throughput, QoS fairness and load balancing for a set of independent         tasks by dynamic scheduling. We build a scheduler model that can be used for multi-dimensional scheduling. Based on the scheduler         model, we propose a heuristic multi-dimensional scheduling strategy, MDSS, consisting of three steps. The first step can be         of any existing real-time scheduling algorithm that determines to accept or reject a task. In step 2, we put forward a novel         algorithm MQFQ to enhance the QoS levels of accepted tasks, and to make these tasks have fair QoS levels at the same time.         Another new algorithm ITLB is proposed and used in step 3. The ITLB algorithm is capable of balancing load and improving throughput         of the system. To evaluate the performance of MDSS, we perform extensive simulation experiments to compare MDSS strategy with         MDSR strategy, DASAP and DALAP algorithms. Experimental results show that MDSS significantly outperforms MDSR, DASAP and DALAP.      </content></document><document><year>2009</year><authors>Kevin Chiew1  | Yingjiu Li1 </authors><title>Multistage Off-Line Permutation Packet Routing on a Mesh: An Approach with Elementary Mathematics      </title><content>Various methods have been proposed for off-line permutation packet routing on a mesh. One of the methods is known as multistage         routing, in which the first stage is crucial. For the first stage of routing, the previous study normally converts it to a         problem of graph theory and proves the existence of solutions. However, there is a lack of simple algorithms to the first         stage of routing. This article presents an explicit and simple approach for the first stage of routing based on elementary         mathematics.      </content></document><document><year>2009</year><authors>Zhiguo Wan1 | Robert H. Deng2 | Feng Bao3 | Bart Preneel4  | Ming Gu1 </authors><title>         nPAKE+: A Tree-Based Group Password-Authenticated Key Exchange Protocol Using Different Passwords      </title><content>Although two-party password-authenticated key exchange (PAKE) protocols have been intensively studied in recent years, group         PAKE protocols have received little attention. In this paper, we propose a tree-based group PAKE protocol &amp;#8212; nPAKE+ protocol under the setting where each party shares an independent password with a trusted server. The nPAKE+ protocol is a novel combination of the hierarchical key tree structure and the password-based Diffie-Hellman exchange, and         hence it achieves substantial gain in computation efficiency. In particular, the computation cost for each client in our protocol         is only O(log n). Additionally, the hierarchical feature of nPAKE+ enables every subgroup to obtain its own subgroup key in the end. We also prove the security of our protocol under the random         oracle model and the ideal cipher model.      </content></document><document><year>2009</year><authors>Xiao-Hua Wang1| 2 | Yi-Gang He2  | Tian-Zan Li1</authors><title>Neural Network Algorithm for Designing FIR Filters Utilizing Frequency-Response Masking Technique      </title><content>This paper presents a new joint optimization method for the design of sharp linear-phase finite-impulse response (FIR) digital         filters which are synthesized by using basic and multistage frequency-response-masking (FRM) techniques. The method is based         on a batch back-propagation neural network algorithm with a variable learning rate mode. We propose the following two-step         optimization technique in order to reduce the complexity. At the first step, an initial FRM filter is designed by alternately         optimizing the subfilters. At the second step, this solution is then used as a start-up solution to further optimization.         The further optimization problem is highly nonlinear with respect to the coefficients of all the subfilters. Therefore, it         is decomposed into several linear neural network optimization problems. Some examples from the literature are given, and the         results show that the proposed algorithm can design better FRM filters than several existing methods.      </content></document><document><year>2009</year><authors>Varun Gupta1  | Jitender Kumar Chhabra1 </authors><title>Package Coupling Measurement in Object-Oriented Software      </title><content>The grouping of correlated classes into a package helps in better organization of modern object-oriented software. The quality         of such packages needs to be measured so as to estimate their utilization. In this paper, new package coupling metrics are         proposed, which also take into consideration the hierarchical structure of packages and direction of connections among package         elements. The proposed measures have been validated theoretically as well as empirically using 18 packages taken from two         open source software systems. The results obtained from this study show strong correlation between package coupling and understandability         of the package which suggests that proposed metrics could be further used to represent other external software quality factors.      </content></document><document><year>2009</year><authors>Yan-Li Liu1| 2 | Xiao-Gang Xu2| Yan-Wen Guo3 | Jin Wang1 | Xin Duan1| Xi Chen1 | Qun-Sheng Peng1| 2</authors><title>Pores-Preserving Face Cleaning Based on Improved Empirical Mode Decomposition      </title><content>In this paper, we propose a novel method of cleaning up facial imperfections such as bumps and blemishes that may detract         from a pleasing digital portrait. Contrasting with traditional methods which tend to blur facial details, our method fully         retains fine scale skin textures (pores etc.) of the subject. Our key idea is to find a quantity, namely normalized local         energy, to capture different characteristics of fine scale details and distractions, based on empirical mode decomposition,         and then build a quantitative measurement of facial skin appearance which characterizes both imperfections and facial details         in a unified framework. Finally, we use the quantitative measurement as a guide to enhance facial skin. We also introduce         a few high-level, intuitive parameters for controlling the amount of enhancement. In addition, an adaptive local mean and         neighborhood limited empirical mode decomposition algorithm is also developed to improve in two respects the performance of         empirical mode decomposition. It can effectively avoid the gray spots effect commonly associated with traditional empirical         mode decomposition when dealing with high-nonstationary images.      </content></document><document><year>2008</year><authors>Zhao Ming 1 </authors><title>2-D EAG method for the recognition of hand-printed Chinese characters      </title><content>A method, called Two- Dimensional Extended Attribute Grammars (2-D EAGs), for the recognition of hand- printed Chinese characters         is presented. This method uses directly two dimensional information, and provides a scheme for dealing with various kinds         of specific cases in a uniform way. In this method, components are drawn in guided and redundant way and reductions are made         level by level just in accordance with the component combination relations of Chinese characters. The method provides also         polysemous grammars, coexisting grammars and structure inferrings which constrain redundant recognition by comparison among         similar characters or components and greatly increase the tolerance ability to distortion.      </content></document><document><year>2007</year><authors>Han-Bing Yan1 | Shi-Min Hu2  | Ralph R Martin3 </authors><title>3D Morphing Using Strain Field Interpolation      </title><content>In this paper, we present a new technique based on strain fields to carry out 3D shape morphing for applications in computer         graphics and related areas. Strain is an important geometric quantity used in mechanics to describe the deformation of objects.         We apply it in a novel way to analyze and control deformation in morphing. Using position vector fields, the strain field         relating source and target shapes can be obtained. By interpolating this strain field between zero and a final desired value         we can obtain the position field for intermediate shapes. This method ensures that the 3D morphing process is smooth. Locally,         volumes suffer minimal distortion, and no shape jittering or wobbling happens: other methods do not necessarily have these         desirable properties. We also show how to control the method so that changes of shape (in particular, size changes) vary linearly         with time.      </content></document><document><year>2007</year><authors>Dong-Yu Zheng1 | Yan Sun1| Shao-Qing Li1 | Liang Fang1</authors><title>A 485ps 64-Bit Parallel Adder in 0.18&amp;#956;m CMOS      </title><content>This paper presents an optimized 64-bit parallel adder. Sparse-tree architecture enables low carry-merge fan-outs and inter-stage         wiring complexity. Single-rail and semi-dynamic circuit improves operation speed. Simulation results show that the proposed         adder can operate at 485ps with power of 25.6mW in 0.18&amp;#956;m CMOS process. It achieves the goal of higher speed and lower power.      </content></document><document><year>2007</year><authors>Hui-Xuan Tang1| 2  | Hui Wei1| 2 </authors><title>A Coarse-to-Fine Method for Shape Recognition      </title><content>In this paper the deformation invariant curve matching problem is addressed. The proposed approach exploits an image pyramid         to constrain correspondence search at a finer level with those at a coarser level. In comparison to previous methods, this         approach conveys much richer information: curve topology, affine geometry and local intensity are combined together to seek         correspondences. In experiments, the method is tested in two applications, contour matching and shape recognition, and the         results show that the approach is effective under perspective and articulated deformations.      </content></document><document><year>2007</year><authors>Heitor Silverio Lopes1  | Reginaldo Bitello1 </authors><title>A Differential Evolution Approach for Protein Folding Using a Lattice Model      </title><content>Protein folding is a relevant computational problem in Bioinformatics, for which many heuristic algorithms have been proposed.         This work presents a methodology for the application of differential evolution (DE) to the problem of protein folding, using         the bi-dimensional hydrophobic-polar model. DE is a relatively recent evolutionary algorithm, and has been used successfully         in several engineering optimization problems, usually with continuous variables. We introduce the concept of genotype-phenotype         mapping in DE in order to provide a mapping between the real-valued vector and an actual folding. The methodology is detailed         and several experiments with benchmarks are done. We compared the results with other similar implementations. The proposed         DE has shown to be competitive, statistically consistent and very promising.      </content></document><document><year>2007</year><authors>Jun Teng1 | Marc Jaeger2  | Bao-Gang Hu1 </authors><title>A Fast Ambient Occlusion Method for Real-Time Plant Rendering      </title><content>Global illumination effects are crucial for virtual plant rendering. Whereas real-time global illumination rendering of plants         is impractical, ambient occlusion is an efficient alternative approximation. A tree model with millions of triangles is common,         and the triangles can be considered as randomly distributed. The existing ambient occlusion methods fail to apply on such         a type of object. In this paper, we present a new ambient occlusion method dedicated to real time plant rendering with limited         user interaction. This method is a three-step ambient occlusion calculation framework which is suitable for a huge number         of geometry objects distributed randomly in space. The complexity of the proposed algorithm is O(n), compared to the conventional methods with complexities of O(n         2). Furthermore, parameters in this method can be easily adjusted to achieve flexible ambient occlusion effects. With this         ambient occlusion calculation method, we can manipulate plant models with millions of organs, as well as geometry objects         with large number of randomly distributed components with affordable time, and with perceptual quality comparable to the previous         ambient occlusion methods.      </content></document><document><year>2007</year><authors>Bo Yang1  | Da-You Liu2 </authors><title>A Heuristic Clustering Algorithm for Mining Communities in Signed Networks      </title><content>Signed network is an important kind of complex network, which includes both positive relations and negative relations. Communities         of a signed network are defined as the groups of vertices, within which positive relations are dense and between which negative         relations are also dense. Being able to identify communities of signed networks is helpful for analysis of such networks.         Hitherto many algorithms for detecting network communities have been developed. However, most of them are designed exclusively         for the networks including only positive relations and are not suitable for signed networks. So the problem of mining communities         of signed networks quickly and correctly has not been solved satisfactorily. In this paper, we propose a heuristic algorithm         to address this issue. Compared with major existing methods, our approach has three distinct features. First, it is very fast         with a roughly linear time with respect to network size. Second, it exhibits a good clustering capability and especially can         work well with complex networks without well-defined community structures. Finally, it is insensitive to its built-in parameters         and requires no prior knowledge.      </content></document><document><year>2007</year><authors>Sheng-You Lin1  | Jiao-Ying Shi2 </authors><title>A Markov Random Field Model-Based Approach to Natural Image Matting      </title><content>This paper proposes a Markov Random Field (MRF) model-based approach to natural image matting with complex scenes. After the         trimap for matting is given manually, the unknown region is roughly segmented into several joint sub-regions. In each sub-region,         we partition the colors of neighboring background or foreground pixels into several clusters in RGB color space and assign         matting label to each unknown pixel. All the labels are modelled as an MRF and the matting problem is then formulated as a         maximum a posteriori (MAP) estimation problem. Simulated annealing is used to find the optimal MAP estimation. The better         results can be obtained under the same user-interactions when images are complex. Results of natural image matting experiments         performed on complex images using this approach are shown and compared in this paper.      </content></document><document><year>2007</year><authors>Youcef Derbal1 </authors><title>A Model of Grid Service Capacity      </title><content>Computational grids (CGs) are large scale networks of geographically distributed aggregates of resource clusters that may         be contributed by distinct organizations for the provision of computing services such as model simulation, compute cycle and         data mining. Traditionally, the decision-making strategies underlying the grid management mechanisms rely on the physical         view of the grid resource model. This entails the need for complex multi-dimensional search strategies and a considerable         level of resource state information exchange between the grid management domains. In this paper we argue that with the adoption         of service oriented grid architectures, a logical service-oriented view of the resource model provides a more appropriate         level of abstraction to express the grid capacity to handle incoming service requests. In this respect, we propose a quantification         model of the aggregated service capacity of the hosting environment that is updated based on the monitored state of the various         environmental resources required by the hosted services. A comparative experimental validation of the model shows its performance         towards enabling an adequate exploitation of provisioned services.      </content></document><document><year>2007</year><authors>Pyung-Soo Kim1  | Myung-Eui Lee2 </authors><title>A New FIR Filter for State Estimation and Its Application      </title><content>This paper proposes a new FIR (finite impulse response) filter under a least squares criterion using a forgetting factor.         The proposed FIR filter does not require information of the noise covariances as well as the initial state, and has some inherent         properties such as time-invariance, unbiasedness and deadbeat. The proposed FIR filter is represented in a batch form and         then a recursive form as an alternative form. From discussions about the choice of a forgetting factor and a window length,         it is shown that they can be considered as useful parameters to make the estimation performance of the proposed FIR filter         as good as possible. It is shown that the proposed FIR filter can outperform the existing FIR filter with incorrect noise         covariances via computer simulations. Finally, as a useful application, an image sequence stabilization problem is considered.         Through this application, the FIR filtering based approach is shown to be superior to the Kalman filtering based approach.      </content></document><document><year>2007</year><authors>Hai-Bo Tian1 | Xi Sun1 | Yu-Min Wang1</authors><title>A New Public-Key Encryption Scheme      </title><content>This paper proposes a new public-key encryption scheme which removes one element from the public-key tuple of the original         Cramer-Shoup scheme. As a result, a ciphertext is not a quadruple but a triple at the cost of a strong assumption, the third         version of knowledge of exponent assumption (KEA3). Under assumptions of KEA3, a decision Diffie-Hellman (DDH) and a variant         of target collision resistance (TCRv), the new scheme is proved secure against indistinguishable adaptive chosen ciphertext         attack (IND-CCA2). This scheme is as efficient as DamgГҐrd ElGamal (DEG) scheme when it makes use of a well-known algorithm         for product of exponentiations. The DEG scheme is recently proved IND-CCA1 secure by Bellare and Palacio in ASIACRYPT 2004         under another strong assumption. In addition to our IND-CCA2 secured scheme, we also believe that the security proof procedure         itself provides a well insight for ElGamal-based encryption schemes which are secure in real world.      </content></document><document><year>2007</year><authors>Tao-Yuan Cheng1  | Shan Wang1 </authors><title>A Novel Approach to Clustering Merchandise Records      </title><content>Object identification is one of the major challenges in integrating data from multiple information sources. Since being short         of global identifiers, it is hard to find all records referring to the same object in an integrated database. Traditional         object identification techniques tend to use character-based or vector space model-based similarity computing in judging,         but they cannot work well in merchandise databases. This paper brings forward a new approach to object identification. First,         we use merchandise images to judge whether two records belong to the same object; then, we use NaГЇve Bayesian Model to judge         whether two merchandise names have similar meaning. We do experiments on data downloaded from shopping websites, and the results         show good performance.      </content></document><document><year>2007</year><authors>Yu-Hai Zhao1| Guo-Ren Wang1| Ying Yin1  | Guang-Yu Xu1</authors><title>A Novel Approach to Revealing Positive and Negative Co-Regulated Genes      </title><content>As explored by biologists, there is a real and emerging need to identify co-regulated gene clusters, which include both positive         and negative regulated gene clusters. However, the existing pattern-based and tendency-based clustering approaches are only         designed for finding positive regulated gene clusters. In this paper, a new subspace clustering model called g-Cluster is         proposed for gene expression data. The proposed model has the following advantages: 1) find both positive and negative co-regulated         genes in a shot, 2) get away from the restriction of magnitude transformation relationship among co-regulated genes, and 3)         guarantee quality of clusters and significance of regulations using a novel similarity measurement gCode and a user-specified         regulation threshold &amp;#948;, respectively. No previous work measures up to the task which has been set. Moreover, MDL technique is introduced to avoid         insignificant g-Clusters generated. A tree structure, namely GS-tree, is also designed, and two algorithms combined with efficient         pruning and optimization strategies to identify all qualified g-Clusters. Extensive experiments are conducted on real and         synthetic datasets. The experimental results show that 1) the algorithm is able to find an amount of co-regulated gene clusters         missed by previous models, which are potentially of high biological significance, and 2) the algorithms are effective and         efficient, and outperform the existing approaches.      </content></document><document><year>2007</year><authors>Kai Liu1 | Ke-Yan Wang2| Yun-Song Li2 | Cheng-Ke Wu2</authors><title>A Novel VLSI Architecture for Real-Time Line-Based Wavelet Transform Using Lifting Scheme      </title><content>In this paper, we propose a VLSI architecture that performs the line-based discrete wavelet transform (DWT) using a lifting         scheme. The architecture consists of row processors, column processors, an intermediate buffer and a control module. Row processor         and Column processor work as the horizontal and vertical filters respectively. Intermediate buffer is composed of five FIFOs         to store temporary results of horizontal filter. Control module schedules the output order to external memory. Compared with         existing ones, the presented architecture parallelizes all levels of wavelet transform to compute multilevel DWT within one         image transmission time, and uses no external but one intermediate buffer to store several line results of horizontal filtering,         which decreases resource required significantly and reduces memory efficiently. This architecture is suitable for various         real-time image/video applications.      </content></document><document><year>2007</year><authors>Rong-Hua Li1| 2  | Chuan-Kun Wu1 </authors><title>A Protocol for a Private Set-Operation      </title><content>A new private set-operation problem is proposed. Suppose there are n parties with each owning a secret set. Let one of them, say P, be the leader, S be P&amp;#8217;s secret set, and t (less than n&amp;#8201;&amp;#8722;&amp;#8201;1) be a threshold value. For each element w of S, if w appears more than t times in the rest parties&amp;#8217; sets, then P learns which parties&amp;#8217; sets include w, otherwise P cannot know whether w appears in any party&amp;#8217;s set. For this problem, a secure protocol is proposed in the semi-honest model based on semantically         secure homomorphic encryption scheme, secure sharing scheme, and the polynomial representation of sets. The protocol only         needs constant rounds of communication.      </content></document><document><year>2007</year><authors>Xue-Gang Hu1 | Pei-Pei Li1 | Xin-Dong Wu1| 2  | Gong-Qing Wu1 </authors><title>A Semi-Random Multiple Decision-Tree Algorithm for Mining Data Streams      </title><content>Mining with streaming data is a hot topic in data mining. When performing classification on data streams, traditional classification         algorithms based on decision trees, such as ID3 and C4.5, have a relatively poor efficiency in both time and space due to         the characteristics of streaming data. There are some advantages in time and space when using random decision trees. An incremental         algorithm for mining data streams, SRMTDS (Semi-Random Multiple decision Trees for Data Streams), based on random decision         trees is proposed in this paper. SRMTDS uses the inequality of Hoeffding bounds to choose the minimum number of split-examples,         a heuristic method to compute the information gain for obtaining the split thresholds of numerical attributes, and a NaГЇve         Bayes classifier to estimate the class labels of tree leaves. Our extensive experimental study shows that SRMTDS has an improved         performance in time, space, accuracy and the anti-noise capability in comparison with VFDTc, a state-of-the-art decision-tree         algorithm for classifying data streams.      </content></document><document><year>2007</year><authors>Chong-Yi Yuan1 | Wen Zhao1 | Shi-Kun Zhang1  | Yu Huang1 </authors><title>A Three-Layer Model for Business Processes &amp;#8212; Process Logic, Case Semantics and Workflow Management      </title><content>Workflow management aims at the controlling, monitoring, optimizing and supporting of business processes. Well designed formal         models will facilitate such management since they provide explicit representations of business processes as the basis for         computerized analysis, verification and execution. Petri Nets have been recognized as the most suitable candidate for workflow         modeling, and as such, formal models based on Petri Nets have been proposed, among them WF-net by Aalst is the most popular         one. But WF-net has turned out to be conceptually chaotic as will be illustrated in this paper with an example from Aalst's         book. This paper proposes a series of models for the description and analysis of business processes at conceptually different         hierarchical layers. Analytic goals and methods at these layers are also discussed. The underlying structure, shared by all         these models, is SYNCHRONIZER, which is designed with the guidance of synchrony theory of GNT (General Net Theory) and serves         as the conceptual foundation of workflow formal models. Structurally, synchronizers connect tasks to form a whole while dynamically         synchronizers control tasks to achieve synchronization.      </content></document><document><year>2007</year><authors>Qiang Zhou1| Yi-Ci Cai1 | Duo Li1 | Xian-Long Hong1</authors><title>A Yield-Driven Gridless Router      </title><content>A new gridless router to improve the yield of IC layout is presented. The improvement of yield is achieved by reducing the         critical areas where the circuit failures are likely to happen. This gridless area router benefits from a novel cost function         to compute critical areas during routing process, and heuristically lays the patterns on the chip area where it is less possible         to induce critical area. The router also takes other objectives into consideration, such as routing completion rate and nets         length. It takes advantage of gridless routing to gain more flexibility and a higher completion rate. The experimental results         show that critical areas are effectively decreased by 21% on average while maintaining the routing completion rate over 99%.      </content></document><document><year>2007</year><authors>Chao-Kun Wang1 | Jian-Min Wang1 | Jia-Guang Sun1 | Sheng-Fei Shi2  | Hong Gao2 </authors><title>AbIx: An Approach to Content-Based Approximate Query Processing in Peer-to-Peer Data Systems      </title><content>In recent years there has been a significant interest in peer-to-peer (P2P) environments in the community of data management.         However, almost all work, so far, is focused on exact query processing in current P2P data systems. The autonomy of peers         also is not considered enough. In addition, the system cost is very high because the information publishing method of shared         data is based on each document instead of document set. In this paper, abstract indices (AbIx) are presented to implement         content-based approximate queries in centralized, distributed and structured P2P data systems. It can be used to search as         few peers as possible but get as many returns satisfying users&amp;#8217; queries as possible on the guarantee of high autonomy of peers.         Also, abstract indices have low system cost, can improve the query processing speed, and support very frequent updates and         the set information publishing method. In order to verify the effectiveness of abstract indices, a simulator of 10,000 peers,         over 3 million documents is made, and several metrics are proposed. The experimental results show that abstract indices work         well in various P2P data systems.      </content></document><document><year>2007</year><authors>Hao-Da Huang1| 3 | Xin Tong2  | Wen-Cheng Wang1 </authors><title>Accelerated Parallel Texture Optimization      </title><content>Texture optimization is a texture synthesis method that can efficiently reproduce various features of exemplar textures. However,         its slow synthesis speed limits its usage in many interactive or real time applications. In this paper, we propose a parallel         texture optimization algorithm to run on GPUs. In our algorithm, k-coherence search and principle component analysis (PCA) are used for hardware acceleration, and two acceleration techniques         are further developed to speed up our GPU-based texture optimization. With a reasonable precomputation cost, the online synthesis         speed of our algorithm is 4000+ times faster than that of the original texture optimization algorithm and thus our algorithm         is capable of interactive applications. The advantages of the new scheme are demonstrated by applying it to interactive editing         of flow-guided synthesis.      </content></document><document><year>2007</year><authors>Jian-Hui Huang1 | De-Pei Qian1| 2 | Sheng-Ling Wang1 </authors><title>Adaptive Call Admission Control Based on Reward-Penalty Model in Wireless/Mobile Network      </title><content>A dynamic threshold-based Call Admission Control (CAC) scheme used in wireless/mobile network for multi-class services is         proposed. In the scheme, each class&amp;#8217;s CAC thresholds are solved through establishing a reward-penalty model which strives         to maximize network&amp;#8217;s revenue. In order to lower Handoff Dropping Probability (HDP), the scheme joints packet and connection         levels Quality of Service constraints, designing a bandwidth degradation algorithm to accept handoff calls by degrading existing         calls&amp;#8217; bandwidth during network congestion. Analyses show that the CAC thresholds change adaptively with the average call         arrival rate. The performance comparison shows that the proposed scheme outperforms the Mobile IP Reservation scheme.      </content></document><document><year>2007</year><authors>Gang Xu1| 2  | Guo-Zhao Wang1| 2 </authors><title>AHT BГ©zier Curves and NUAHT B-Spline Curves      </title><content>In this paper, we present two new unified mathematics models of conics and polynomial curves, called algebraic hyperbolic trigonometric (AHT) BГ©zier curves and non-uniform algebraic hyperbolic trigonometric (NUAHT) B-spline curves of order n, which are generated over the space span {sin t, cos t, sinh t, cosh t, 1, t,...,t                     n&amp;#8201;&amp;#8722;&amp;#8201;5}, n&amp;#8201;&amp;#10878;&amp;#8201;5. The two kinds of curves share most of the properties as those of the BГ©zier curves and B-spline curves in polynomial         space. In particular, they can represent exactly some remarkable transcendental curves such as the helix, the cycloid and         the catenary. The subdivision formulae of these new kinds of curves are also given. The generations of the tensor product         surfaces are straightforward. Using the new mathematics models, we present the control mesh representations of two classes         of minimal surfaces.      </content></document><document><year>2007</year><authors>Hong-Ding Wang1| 2 | Yun-Hai Tong1| 2 | Shao-Hua Tan1| 2 | Shi-Wei Tang1| 2 | Dong-Qing Yang1  | Guo-Hui Sun3 </authors><title>An Adaptive Approach to Schema Classification for Data Warehouse Modeling      </title><content>Data warehouse (DW) modeling is a complicated task, involving both knowledge of business processes and familiarity with operational         information systems structure and behavior. Existing DW modeling techniques suffer from the following major drawbacks &amp;#8212; data-driven         approach requires high levels of expertise and neglects the requirements of end users, while demand-driven approach lacks         enterprise-wide vision and is regardless of existing models of underlying operational systems. In order to make up for those         shortcomings, a method of classification of schema elements for DW modeling is proposed in this paper. We first put forward         the vector space models for subjects and schema elements, then present an adaptive approach with self-tuning theory to construct         context vectors of subjects, and finally classify the source schema elements into different subjects of the DW automatically.         Benefited from the result of the schema elements classification, designers can model and construct a DW more easily.      </content></document><document><year>2007</year><authors>Xiaolan Zhang1  | Brian King2 </authors><title>An Anti-Counterfeiting RFID Privacy Protection Protocol      </title><content>The privacy problem of many RFID systems has been extensively studied. Yet integrity in RFID has not received much attention         as regular computer systems. When we evaluate an identification protocol for an RFID system for anti-counterfeiting, it is         important to consider integrity issues. Moreover, many RFID systems are accessed by multiple level trust parties, which makes         comprehensive integrity protection even harder. In this paper, we first propose an integrity model for RFID protocols. Then         we use the model to analyze the integrity problems in Squealing Euros protocol. Squealing Euros was proposed by Juels and         Pappu for RFID enabled banknotes that will support anti-forgery and lawful tracing yet preserve individual&amp;#8217;s privacy. We analyze         its integrity, we then discuss the problems that arise and propose some solutions to these problems. Then an improved protocol         with integrity protection for the law enforcement is constructed, which includes an unforgeable binding between the banknote         serial number and the RF ciphertext only readable to law enforcement. This same protocol can be applied in many other applications         which require a privacy protecting anti-counterfeiting mechanism.      </content></document><document><year>2007</year><authors>Kwangjin Park1 | Hyunseung Choo1  | Chong-Sun Hwang2 </authors><title>An Efficient Data Dissemination Scheme for Spatial Query Processing      </title><content>Due to the personal portable devices and advances in wireless communication technologies, Location Dependent Information Services         (LDISs) have received a lot of attention from both the industrial and academic communities. In LDISs, it is important to reduce         the query response time, since a late query response may contain out-of-date information. In this paper, we study the issue         of LDISs using a Voronoi Diagram. We introduce a new NN search method, called the Exponential Sequence Scheme (ESS), to support         NN query processing in periodic broadcast environment. This paper aims to provide research directions towards minimizing both         the access latency and energy consumption for the NN-query processing.      </content></document><document><year>2007</year><authors>Min Liu1| 3 | Zhong-Cheng Li1  | Xiao-Bing Guo2</authors><title>An Efficient Handoff Decision Algorithm for Vertical Handoff Between WWAN and WLAN      </title><content>Vertical handoff is one significant challenge for mobility management in heterogeneous wireless networks. Compared with horizontal         handoff, vertical handoff involves different wireless network technologies varying widely in terms of bandwidth, delay, coverage         area, power consumption, etc. In this paper, we analyze the signal strength model of mobile node and present a new vertical         handoff decision algorithm. This algorithm can adapt to the change of mobile node&amp;#8217;s velocity and improve the handoff efficiency         significantly. We analyze the algorithm&amp;#8217;s performance and the effect of different parameters on handoff triggering. In addition,         we propose three performance evaluation models and verify the algorithm&amp;#8217;s feasibility and effectiveness in simulations.      </content></document><document><year>2007</year><authors>Giuseppe Lami1  | Robert W. Ferguson2 </authors><title>An Empirical Study on the Impact of Automation on the Requirements Analysis Process      </title><content>Requirements analysis is an important phase in a software project. The analysis is often performed in an informal way by specialists         who review documents looking for ambiguities, technical inconsistencies and incomplete parts. Automation is still far from         being applied in requirements analyses, above all since natural languages are informal and thus difficult to treat automatically.         There are only a few tools that can analyse texts. One of them, called QuARS, was developed by the Istituto di Scienza e Tecnologie         dell'Informazione and can analyse texts in terms of ambiguity. This paper describes how QuARS was used in a formal empirical         experiment to assess the impact in terms of effectiveness and efficacy of the automation in the requirements review process         of a software company.      </content></document><document><year>2007</year><authors>Yu-Lai Zhao1 | Xian-Feng Li1 | Dong Tong1  | Xu Cheng1 </authors><title>An Energy-Efficient Instruction Scheduler Design with Two-Level Shelving and Adaptive Banking      </title><content>Mainstream processors implement the instruction scheduler using a monolithic CAM-based issue queue (IQ), which consumes increasingly         high energy as its size scales. In particular, its instruction wakeup logic accounts for a major portion of the consumed energy.         Our study shows that instructions with 2 non-ready operands (called 2OP instructions) are in small percentage, but tend to         spend long latencies in the IQ. They can be effectively shelved in a small RAM-based waiting instruction buffer (WIB) and         steered into the IQ at appropriate time. With this two-level shelving ability, half of the CAM tag comparators are eliminated         in the IQ, which significantly reduces the energy of wakeup operation. In addition, we propose an adaptive banking scheme         to downsize the IQ and reduce the bit-width of tag comparators. Experiments indicate that for an 8-wide issue superscalar         or SMT processor, the energy consumption of the instruction scheduler can be reduced by 67%. Furthermore, the new design has         potentially faster scheduler clock speed while maintaining close IPC to the monolithic scheduler design. Compared with the         previous work on eliminating tags through prediction, our design is superior in terms of both energy reduction and SMT support.      </content></document><document><year>2007</year><authors>Xiao-Dong Wang1  | Ying-Jie Wu1 </authors><title>An Improved HEAPSORT Algorithm with nlogn&amp;#8201;&amp;#8722;&amp;#8201;0.788928n Comparisons in the Worst Case      </title><content>A new variant of HEAPSORT is presented in this paper. The algorithm is not an internal sorting algorithm in the strong sense,         since extra storage for n integers is necessary. The basic idea of the new algorithm is similar to the classical sorting algorithm HEAPSORT, but the         algorithm rebuilds the heap in another way. The basic idea of the new algorithm is it uses only one comparison at each node.         The new algorithm shift walks down a path in the heap until a leaf is reached. The request of placing the element in the root         immediately to its destination is relaxed. The new algorithm requires about n log n&amp;#8201;&amp;#8722;&amp;#8201;0.788928n comparisons in the worst case and n log n&amp;#8201;&amp;#8722;&amp;#8201;n comparisons on the average which is only about 0.4n more than necessary. It beats on average even the clever variants of QUICKSORT, if n is not very small. The difference between the worst case and the best case indicates that there is still room for improvement         of the new algorithm by constructing heap more carefully.      </content></document><document><year>2007</year><authors>Yu-Yan Chao1| 2 | Li-Feng He3| 4| 7 | Tsuyoshi Nakamura5 | Zheng-Hao Shi5 | Kenji Suzuki6  | Hidenori Itoh5 </authors><title>An Improvement of Herbrand's Theorem and Its Application to Model Generation Theorem Proving      </title><content>This paper presents an improvement of Herbrand&amp;#8217;s theorem. We propose a method for specifying a sub-universe of the Herbrand         universe of a clause set                    for each argument of predicate symbols and function symbols in                   . We prove that a clause set                    is unsatisfiable if and only if there is a finite unsatisfiable set of ground instances of clauses of                    that are derived by only instantiating each variable, which appears as an argument of predicate symbols or function symbols,         in                    over its corresponding argument's sub-universe of the Herbrand universe of                   . Because such sub-universes are usually smaller (sometimes considerably) than the Herbrand universe of                   , the number of ground instances may decrease considerably in many cases. We present an algorithm for automatically deriving         the sub-universes for arguments in a given clause set, and show the correctness of our improvement. Moreover, we introduce         an application of our approach to model generation theorem proving for non-range-restricted problems, show the range-restriction         transformation algorithm based on our improvement and provide examples on benchmark problems to demonstrate the power of our         approach.      </content></document><document><year>2007</year><authors>Qiang Liu1| 2 | Tao Huang1| Shao-Hua Liu1 | Hua Zhong1</authors><title>An Ontology-Based Approach for Semantic Conflict Resolution in Database Integration      </title><content>An important task in database integration is to resolve data conflicts, on both schema-level and semantic-level. Especially         difficult the latter is. Some existing ontology-based approaches have been criticized for their lack of domain generality         and semantic richness. With the aim to overcome these limitations, this paper introduces a systematic approach for detecting         and resolving various semantic conflicts in heterogeneous databases, which includes two important parts: a semantic conflict         representation model based on our classification framework of semantic conflicts, and a methodology for detecting and resolving         semantic conflicts based on this model. The system has been developed, experimental evaluations on which indicate that this         approach can resolve much of the semantic conflicts effectively, and keep independent of domains and integration patterns.      </content></document><document><year>2007</year><authors>Unil Yun1 </authors><title>Analyzing Sequential Patterns in Retail Databases      </title><content>Finding correlated sequential patterns in large sequence databases is one of the essential tasks in data mining since a huge         number of sequential patterns are usually mined, but it is hard to find sequential patterns with the correlation. According         to the requirement of real applications, the needed data analysis should be different. In previous mining approaches, after         mining the sequential patterns, sequential patterns with the weak affinity are found even with a high minimum support. In         this paper, a new framework is suggested for mining weighted support affinity patterns in which an objective measure, sequential         ws-confidence is developed to detect correlated sequential patterns with weighted support affinity patterns. To efficiently         prune the weak affinity patterns, it is proved that ws-confidence measure satisfies the anti-monotone and cross weighted support         properties which can be applied to eliminate sequential patterns with dissimilar weighted support levels. Based on the framework,         a weighted support affinity pattern mining algorithm (WSMiner) is suggested. The performance study shows that WSMiner is efficient         and scalable for mining weighted support affinity patterns.      </content></document><document><year>2007</year><authors>Duo Liu1 | Ping Luo1  | Yi-Qi Dai1 </authors><title>Attack on Digital Multi-Signature Scheme Based on Elliptic Curve Cryptosystem      </title><content>The concept of multisignature, in which multiple signers can cooperate to sign the same message and any verifier can verify         the validity of the multi-signature, was first introduced by Itakura and Nakamura. Several multisignature schemes have been         proposed since. Chen et al. proposed a new digital multi-signature scheme based on the elliptic curve cryptosystem recently. In this paper, we show         that their scheme is insecure, for it is vulnerable to the so-called active attacks, such as the substitution of a &amp;#8220;false&amp;#8221;         public key to a &amp;#8220;true&amp;#8221; one in a key directory or during transmission. And then the attacker can sign a legal signature which         other users have signed and forge a signature himself which can be accepted by the verifier.      </content></document><document><year>2007</year><authors>Sheng-Qiang Li1| 2 | Zhi-Xiong Chen3| Xiao-Tong Fu1 | Guo-Zhen Xiao1</authors><title>Autocorrelation Values of New Generalized Cyclotomic Sequences of Order Two and Length pq               </title><content>Pseudo-random sequences are used extensively for their high speed and security level and less errors. As a branch, the cyclotomic         sequences and the generalized ones are studied widely because of their simple mathematical structures and excellent pseudo-random         properties. In 1998, Ding and Helleseth introduced a new generalized cyclotomy which includes the classical cyclotomy as a         special case. In this paper, based on the generalized cyclotomy, new generalized cyclotomic sequences with order two and length         pq are constructed. An equivalent definition of the sequences is deduced so that the autocorrelation values of these sequences         can be determined conveniently. The construction contributes to the understanding of the periodic autocorrelation structure         of cyclotomically-constructed binary sequences, and the autocorrelation function takes on only a few values.      </content></document><document><year>2007</year><authors>Xiao-Ju Dong1  | Yu-Xi Fu1 </authors><title>Barbed Congruence of Asymmetry and Mismatch      </title><content>The &amp;#967; calculus is a model of concurrent and mobile systems. It emphasizes that communications are information exchanges. In         the paper, two constructions are incorporated into the framework of the chi calculus, which are asymmetric communication and         mismatch condition widely used in applications. Since the barbed bisimilarity has proved its generality and gained its popularity         as an effective approach to generating a reasonable observational equivalence, we study both the operational and algebraic         properties of the barbed bisimilarity in this enriched calculus. The investigation supports an improved understanding of the         bisimulation behaviors of the model. It also gives a general picture of how the two constructions affect the observational         theory.      </content></document><document><year>2007</year><authors>Zhi-Hong Tao1 | Cong-Hua Zhou2| Zhong Chen1 | Li-Fu Wang1</authors><title>Bounded Model Checking of CTL      </title><content>Bounded Model Checking has been recently introduced as an efficient verification method for reactive systems. This technique         reduces model checking of linear temporal logic to propositional satisfiability. In this paper we first present how quantified         Boolean decision procedures can replace BDDs. We introduce a bounded model checking procedure for temporal logic CTL* which         reduces model checking to the satisfiability of quantified Boolean formulas. Our new technique avoids the space blow up of         BDDs, and extends the concept of bounded model checking.      </content></document><document><year>2007</year><authors>Xin-Yi Huang1 | Willy Susilo1 | Yi Mu1  | Fu-Tai Zhang2 </authors><title>Breaking and Repairing Trapdoor-Free Group Signature Schemes from Asiacrypt&amp;#8217;2004      </title><content>Group signature schemes allow a member of a group to sign messages anonymously on behalf of the group. In case of later dispute,         a designated group manager can revoke the anonymity and identify the originator of a signature. In Asiacrypt2004, Nguyen and         Safavi-Naini proposed a group signature scheme that has a constant-sized public key and signature length, and more importantly,         their group signature scheme does not require trapdoor. Their scheme is very efficient and the sizes of signatures are smaller than those of the other existing         schemes. In this paper, we point out that Nguyen and Safavi-Naini&amp;#8217;s scheme is insecure. In particular, it is shown in our         cryptanalysis of the scheme that it allows a non-member of the group to sign on behalf of the group. And the resulting signature         convinces any third party that a member of the group has indeed generated such a signature, although none of the members has         done so. Therefore is in case of dispute, even the group manager cannot identify who has signed the message. In the paper         a new scheme that does not suffer from this problem is provided.      </content></document><document><year>2007</year><authors>Wei Gao1 | Xue-Li Wang2  | Dong-Qing Xie3 </authors><title>Chameleon Hashes Without Key Exposure Based on Factoring      </title><content>Chameleon hash is the main primitive to construct a chameleon signature scheme which provides non-repudiation and non-transferability         simultaneously. However, the initial chameleon hash schemes suffer from the key exposure problem: non-transferability is based         on an unsound assumption that the designated receiver is willing to abuse his private key regardless of its exposure. Recently,         several key-exposure-free chameleon hashes have been constructed based on RSA assumption and SDH (strong Diffie-Hellman) assumption.         In this paper, we propose a factoring-based chameleon hash scheme which is proven to enjoy all advantages of the previous         schemes. In order to support it, we propose a variant Rabin signature scheme which is proven secure against a new type of         attack in the random oracle model.      </content></document><document><year>2007</year><authors>Jun Zhang1| 2| 3 | Zhao-Hui Peng1| 2 | Shan Wang1| 2  | Hui-Jing Nie1| 2 </authors><title>CLASCN: Candidate Network Selection for Efficient Top-k Keyword Queries over Databases      </title><content>Keyword Search Over Relational Databases (KSORD) enables casual or Web users easily access databases through free-form keyword         queries. Improving the performance of KSORD systems is a critical issue in this area. In this paper, a new approach CLASCN         (Classification, Learning And Selection of Candidate Network) is developed to efficiently perform top-k keyword queries in schema-graph-based online KSORD systems. In this approach, the Candidate Networks (CNs) from trained keyword         queries or executed user queries are classified and stored in the databases, and top-k results from the CNs are learned for constructing CN Language Models (CNLMs). The CNLMs are used to compute the similarity         scores between a new user query and the CNs from the query. The CNs with relatively large similarity score, which are the         most promising ones to produce top-k results, will be selected and performed. Currently, CLASCN is only applicable for past queries and New All-keyword-Used (NAU)         queries which are frequently submitted queries. Extensive experiments also show the efficiency and effectiveness of our CLASCN         approach.      </content></document><document><year>2007</year><authors>Xi-Shun Zhao1  | Yu-Ping Shen1 </authors><title>Comparison of Semantics of Disjunctive Logic Programs Based on Model-Equivalent Reduction      </title><content>In this paper, it is shown that stable model semantics, perfect model semantics, and partial stable model semantics of disjunctive         logic programs have the same expressive power with respect to the polynomial-time model-equivalent reduction. That is, taking         perfect model semantics and stable model semantic as an example, any logic program P can be transformed in polynomial time to another logic program P&amp;#8242; such that perfect models (resp. stable models) of P 1-1 correspond to stable models (resp. perfect models) of P&amp;#8242;, and the correspondence can be computed also in polynomial time. However, the minimal model semantics has weaker expressiveness         than other mentioned semantics, otherwise, the polynomial hierarchy would collapse to NP.      </content></document><document><year>2007</year><authors>Li-Guo Yu1  | Srini Ramaswamy2 </authors><title>Component Dependency in Object-Oriented Software      </title><content>Component dependency is an important software measure. It is directly related to software understandability, maintainability,         and reusability. Two important parameters in describing component dependency are the type of coupling between two components         and the type of the dependent component. Depending upon the different types of coupling and the type of the dependent components,         there can be different effects on component maintenance and component reuse. In this paper, we divide dependent components         into three types. We then classify various component dependencies and analyze their effects on maintenance and reuse. Based         on the classification, we present a dependency metric and validate it on 11 open-source Java components. Our study shows that         a strong correlation exists between the measurement of the dependency of the component and the effort to reuse the component.         This indicates that the classification of component dependency and the suggested metric could be further used to represent         other external software quality factors.      </content></document><document><year>2007</year><authors>Yi Zhuang1 | Yue-Ting Zhuang1  | Fei Wu1 </authors><title>Composite Distance Transformation for Indexing and k-Nearest-Neighbor Searching in High-Dimensional Spaces      </title><content>Due to the famous dimensionality curse problem, search in a high-dimensional space is considered as a &amp;#8220;hard&amp;#8221; problem. In this         paper, a novel composite distance transformation method, which is called CDT, is proposed to support a fast k-nearest-neighbor (k-NN) search in high-dimensional spaces. In CDT, all (n) data points are first grouped into some clusters by a k-Means clustering algorithm. Then a composite distance key of each data point is computed. Finally, these index keys of such         n data points are inserted by a partition-based B+-tree. Thus, given a query point, its k-NN search in high-dimensional spaces is transformed into the search in the single dimensional space with the aid of CDT index.         Extensive performance studies are conducted to evaluate the effectiveness and efficiency of the proposed scheme. Our results         show that this method outperforms the state-of-the-art high-dimensional search techniques, such as the X-Tree, VA-file, iDistance         and NB-Tree.      </content></document><document><year>2007</year><authors>Chang-Le Zhou1| 3 | Yun Yang1  | Xiao-Xi Huang2 </authors><title>Computational Mechanisms for Metaphor in Languages: A Survey      </title><content>Metaphor computation has attracted more and more attention because metaphor, to some extent, is the focus of mind and language         mechanism. However, it encounters problems not only due to the rich expressive power of natural language but also due to cognitive         nature of human being. Therefore machine-understanding of metaphor is now becoming a bottle-neck in natural language processing         and machine translation. This paper first suggests how a metaphor is understood and then presents a survey of current computational         approaches, in terms of their linguistic historical roots, underlying foundations, methods and techniques currently used,         advantages, limitations, and future trends. A comparison between metaphors in English and Chinese languages is also introduced         because compared with development in English language Chinese metaphor computation is just at its starting stage. So a separate         summarization of current progress made in Chinese metaphor computation is presented. As a conclusion, a few suggestions are         proposed for further research on metaphor computation especially on Chinese metaphor computation.      </content></document><document><year>2007</year><authors>Yi-Song Wang1| Ming-Yi Zhang2  | Yu-Ping Shen3</authors><title>Consistency Property of Finite FC-Normal Logic Programs      </title><content>Marek&amp;#8217;s forward-chaining construction is one of the important techniques for investigating the non-monotonic reasoning. By         introduction of consistency property over a logic program, they proposed a class of logic programs, FC-normal programs, each         of which has at least one stable model. However, it is not clear how to choose one appropriate consistency property for deciding         whether or not a logic program is FC-normal. In this paper, we firstly discover that, for any finite logic program &amp;#928;, there exists the least consistency property LCon (&amp;#928;) over &amp;#928;, which just depends on &amp;#928; itself, such that, &amp;#928; is FC-normal if and only if &amp;#928; is FC-normal with respect to (w.r.t.) LCon (&amp;#928;). Actually, in order to determine the FC-normality of a logic program, it is sufficient to check the monotonic closed sets         in LCon (&amp;#928;) for all non-monotonic rules, that is LFC (&amp;#928;). Secondly, we present an algorithm for computing LFC (&amp;#928;). Finally, we reveal that the brave reasoning task and cautious reasoning task for FC-normal logic programs are of the same         difficulty as that of normal logic programs.      </content></document><document><year>2007</year><authors>Francesc GinГ©1 | Francesc Solsona1 | Mauricio Hanzich2 | Porfidio HernГЎndez2  | Emilio Luque2 </authors><title>Cooperating CoScheduling: A Coscheduling Proposal Aimed at Non-Dedicated Heterogeneous NOWs      </title><content>Implicit coscheduling techniques applied to non-dedicated homogeneous Networks Of Workstations (NOWs) have shown they can         perform well when many local users compete with a single parallel job. Implicit coscheduling deals with minimizing the communication         waiting time of parallel processes by identifying the processes in need of coscheduling through gathering and analyzing implicit         runtime information, basically communication events. Unfortunately, implicit coscheduling techniques do not guarantee the         performance of local and parallel jobs, when the number of parallel jobs competing against each other is increased. Thus,         a low efficiency use of the idle computational resources is achieved.                     In order to solve these problems, a new technique, named Cooperating CoScheduling (CCS), is presented in this work. Unlike               traditional implicit coscheduling techniques, under CCS, each node takes its scheduling decisions from the occurrence of local               events, basically communication, memory, Input/Output and CPU, together with foreign events received from cooperating nodes.               This allows CCS to provide a social contract based on reserving a percentage of CPU and memory resources to ensure the progress               of parallel jobs without disturbing the local users, while coscheduling of communicating tasks is ensured. Besides, the CCS               algorithm uses status information from the cooperating nodes to balance the resources across the cluster when necessary. Experimental               results in a non-dedicated heterogeneous NOW reveal that CCS allows the idle resources to be exploited efficiently, thus obtaining               a satisfactory speedup and provoking an overhead that is imperceptible to the local user.            </content></document><document><year>2007</year><authors>Xiao-Li Huang1| 2  | Chuan-Kun Wu1 </authors><title>Cryptanalysis of Achterbahn-Version 1 and -Version 2      </title><content>Achterbahn is one of the candidate stream ciphers submitted to the eSTREAM, which is the ECRYPT Stream Cipher Project. The         cipher Achterbahn uses a new structure which is based on several nonlinear feedback shift registers (NLFSR) and a nonlinear         combining output Boolean function. This paper proposes distinguishing attacks on Achterbahn-Version 1 and -Version 2 on the         reduced mode and the full mode. These distinguishing attacks are based on linear approximations of the output functions. On         the basis of these linear approximations and the periods of the registers, parity checks with noticeable biases are found.         Then distinguishing attacks can be achieved through these biased parity checks. As to Achterbahn-Version 1, three cases that         the output function has three possibilities are analyzed. Achterbahn-Version 2, the modification version of Achterbahn-Version         1, is designed to avert attacks based on approximations of the output Boolean function. Our attack with even much lower complexities         on Achterbahn-Version 2 shows that Achterbahn-Version 2 cannot prevent attacks based on linear approximations.      </content></document><document><year>2007</year><authors>Dong-Xi Liu1 </authors><title>CSchema: A Downgrading Policy Language for XML Access Control      </title><content>The problem of regulating access to XML documents has attracted much attention from both academic and industry communities.         In existing approaches, the XML elements specified by access policies are either accessible or inaccessible according to their         sensitivity. However, in some cases, the original XML elements are sensitive and inaccessible, but after being processed in         some appropriate ways, the results become insensitive and thus accessible. This paper proposes a policy language to accommodate         such cases, which can express the downgrading operations on sensitive data in XML documents through explicit calculations         on them. The proposed policy language is called calculation-embedded schema (CSchema), which extends the ordinary schema languages with protection type for protecting sensitive data and specifying downgrading operations. CSchema language has a type system to guarantee the         type correctness of the embedded calculation expressions and moreover this type system also generates a security view after         type checking a CSchema policy. Access policies specified by CSchema are enforced by a validation procedure, which produces         the released documents containing only the accessible data by validating the protected documents against CSchema policies.         These released documents are then ready to be accessed by, for instance, XML query engines. By incorporating this validation         procedure, other XML processing technologies can use CSchema as the access control module.      </content></document><document><year>2007</year><authors>Pierre Bourque1 | Serge Oligny2 | Alain Abran1  | Bertr| Fournier3 </authors><title>Developing Project Duration Models in Software Engineering      </title><content>Based on the empirical analysis of data contained in the International Software Benchmarking Standards Group (ISBSG) repository,         this paper presents software engineering project duration models based on project effort. Duration models are built for the         entire dataset and for subsets of projects developed for personal computer, mid-range and mainframe platforms. Duration models         are also constructed for projects requiring fewer than 400 person-hours of effort and for projects requiring more than 400         person-hours of effort. The usefulness of adding the maximum number of assigned resources as a second independent variable         to explain duration is also analyzed. The opportunity to build duration models directly from project functional size in function         points is investigated as well.      </content></document><document><year>2007</year><authors>Yong-Wei Miao1| 2 | Jie-Qing Feng1 | Chun-Xia Xiao1 | Qun-Sheng Peng1  | A. R. Forrest3</authors><title>Differentials-Based Segmentation and Parameterization for Point-Sampled Surfaces      </title><content>Efficient parameterization of point-sampled surfaces is a fundamental problem in the field of digital geometry processing.         In order to parameterize a given point-sampled surface for minimal distance distortion, a differentials-based segmentation         and parameterization approach is proposed in this paper. Our approach partitions the point-sampled geometry based on two criteria:         variation of Euclidean distance between sample points, and angular difference between surface differential directions. According         to the analysis of normal curvatures for some specified directions, a new projection approach is adopted to estimate the local         surface differentials. Then a k-means clustering (k-MC) algorithm is used for partitioning the model into a set of charts based on the estimated local surface attributes. Finally,         each chart is parameterized with a statistical method &amp;#8212; multidimensional scaling (MDS) approach, and the parameterization         results of all charts form an atlas for compact storage.      </content></document><document><year>2007</year><authors>Jun Yao1| 2| Ji-Wu Shu1  | Wei-Min Zheng1</authors><title>Distributed Storage Cluster Design for Remote Mirroring Based on Storage Area Network      </title><content>With the explosion of information nowadays, applying data storage safety requirements has become a new challenge, especially         in high data available cluster environments. With the emergence of Storage Area Networks (SANs), storage can be network-based         and consolidated, and mass data movements via Fiber Channels (FCs) can be of very high speed. Based on these features, this         paper introduces a dual-node storage cluster designed for remote mirroring as a concurrent data replication method to protect         data during system failures. This design takes full advantage of a SAN system's benefits, and it adopts a synchronous protocol         to guarantee a fully up-to-date data copy on the remote site. By developing a Linux kernel module to control the I/O flow         and by using the technologies of software Logic Unit Number (LUN) masking, background online resynchronization and a self-management         daemon, we have achieved a reliable mirroring system with the characteristics of server-free data replication, fault tolerance,         online disaster recovery and high performance. In this study, we implemented the design in a remote mirror subsystem built         on a software Fiber Channel Storage Area Network (FC-SAN) system.      </content></document><document><year>2007</year><authors>Wei Fu1| 2  | Guang-Zhong Xing2 </authors><title>Edge-Oriented Spatial Interpolation for Error Concealment of Consecutive Blocks      </title><content>This paper proposes a low-complexity spatial-domain error concealment (EC) algorithm for recovering consecutive blocks error         in still images or intra-coded (I) frames of video sequences. The proposed algorithm works with the following steps. Firstly         the Sobel operator is performed on the top and bottom adjacent pixels to detect the most probable edge direction of current         block area. After that one-dimensional (1-D) matching is used on the available block boundaries. Displacement between edge         direction candidate and most probable edge direction is taken into consideration as an important factor to improve stability         of 1-D boundary matching. Then the corrupted pixels are recovered by linear weighting interpolation along the estimated edge         direction. Finally the interpolated values are merged to get last recovered picture. Simulation results demonstrate that the         proposed algorithms obtain good subjective quality and higher PSNR than the methods in literatures for most images.      </content></document><document><year>2007</year><authors>Yun-Jun Gao1 | Chun Li1 | Gen-Cai Chen1 | Ling Chen1| 2 | Xian-Ta Jiang1  | Chun Chen1 </authors><title>Efficient k-Nearest-Neighbor Search Algorithms for Historical Moving Object Trajectories      </title><content>         k Nearest Neighbor (kNN) search is one of the most important operations in spatial and spatio-temporal databases. Although it has received considerable         attention in the database literature, there is little prior work on kNN retrieval for moving object trajectories. Motivated by this observation, this paper studies the problem of efficiently         processing kNN (k&amp;#8201;&amp;#10878;&amp;#8201;1) search on R-tree-like structures storing historical information about moving object trajectories. Two algorithms are         developed based on best-first traversal paradigm, called BFPkNN and BFTkNN, which handle the kNN retrieval with respect to the static query point and the moving query trajectory, respectively. Both algorithms minimize         the number of node access, that is, they perform a single access only to those qualifying nodes that may contain the final         result. Aiming at saving main-memory consumption and reducing CPU cost further, several effective pruning heuristics are also         presented. Extensive experiments with synthetic and real datasets confirm that the proposed algorithms in this paper outperform         their competitors significantly in both efficiency and scalability.      </content></document><document><year>2007</year><authors>Yan Zhang1 | Zhi-Feng Chen2  | Yuan-Yuan Zhou3 </authors><title>Efficient Execution of Multiple Queries on Deep Memory Hierarchy      </title><content>This paper proposes a complementary novel idea, called MiniTasking to further reduce the number of cache misses by improving         the data temporal locality for multiple concurrent queries. Our idea is based on the observation that, in many workloads such as decision support systems (DSS),         there is usually significant amount of data sharing among different concurrent queries. MiniTasking exploits such data sharing         to improve data temporal locality by scheduling query execution at three levels: query level batching, operator level grouping         and mini-task level scheduling. The experimental results with various types of concurrent TPC-H query workloads show that,         with the traditional N-ary Storage Model (NSM) layout, MiniTasking significantly reduces the L2 cache misses by up to 83,         and thereby achieves 24% reduction in execution time. With the Partition Attributes Across (PAX) layout, MiniTasking further         reduces the cache misses by 65% and the execution time by 9%. For the TPC-H throughput test workload, MiniTasking improves         the end performance up to 20%.      </content></document><document><year>2007</year><authors>Zhen-Chuan Chai1 | Zhen-Fu Cao1  | Yuan Zhou2 </authors><title>Efficient ID-Based Multi-Decrypter Encryption with Short Ciphertexts      </title><content>Multi-decrypter encryption is a typical application in multi-user cryptographic branches. In multi-decrypter encryption, a         message is encrypted under multiple decrypters&amp;#8217; public keys in the way that only when all the decrypters cooperate, can the         message be read. However, trivial implementation of multi-decrypter encryption using standard approaches leads to heavy computation         costs and long ciphertext which grows as the receiver group expands. This consumes much precious bandwidth in wireless environment,         such as mobile ad hoc network. In this paper, we propose an efficient identity based multi-decrypter encryption scheme, which         needs only one or zero (if precomputed) pairing computation and the ciphertext contains only three group elements no matter         how many the receivers are. Moreover, we give a formal security definition for the scheme, and prove the scheme to be chosen         ciphertext secure in the random oracle model, and discuss how to modify the scheme to resist chosen ciphertext attack.      </content></document><document><year>2007</year><authors>Jian-Hua Feng1 | Qian Qian1 | Jian-Yong Wang1  | Li-Zhu Zhou1 </authors><title>Efficient Mining of Frequent Closed XML Query Pattern      </title><content>Previous research works have presented convincing arguments that a frequent pattern mining algorithm should not mine all frequent         but only the closed ones because the latter leads to not only more compact yet complete result set but also better efficiency.         Upon discovery of frequent closed XML query patterns, indexing and caching can be effectively adopted for query performance         enhancement. Most of the previous algorithms for finding frequent patterns basically introduced a straightforward generate-and-test         strategy. In this paper, we present SOLARIA*, an efficient algorithm for mining frequent closed XML query patterns without         candidate maintenance and costly tree-containment checking. Efficient algorithm of sequence mining is involved in discovering         frequent tree-structured patterns, which aims at replacing expensive containment testing with cheap parent-child checking         in sequences. SOLARIA* deeply prunes unrelated search space for frequent pattern enumeration by parent-child relationship         constraint. By a thorough experimental study on various real-life data, we demonstrate the efficiency and scalability of SOLARIA*         over the previous known alternative. SOLARIA* is also linearly scalable in terms of XML queries&amp;#8217; size.      </content></document><document><year>2007</year><authors>Yong Liao1 | Xu-Dong Chen1 | Guang-Ze Xiong1 | Qing-Xin Zhu1  | Nan Sang1 </authors><title>End-to-End Utilization Control for Aperiodic Tasks in Distributed Real-Time Systems      </title><content>An increasing number of DRTS (Distributed Real-Time Systems) are employing an end-to-end aperiodic task model. The key challenges         of such DRTS are guaranteeing utilization on multiple processors to achieve overload protection, and meeting the end-to-end         deadlines of aperiodic tasks. This paper proposes an end-to-end utilization control architecture and an IC-EAT (Integration         Control for End-to-End Aperiodic Tasks) algorithm, which features a distributed feedback loop that dynamically enforces the         desired utilization bound on multiple processors. IC-EAT integrates admission control with feedback control, which is able         to dynamically determine the QoS (Quality of Service) of incoming tasks and guarantee the end-to-end deadlines of admitted         tasks. Then an LQOCM (Linear Quadratic Optimal Control Model) is presented. Finally, experiments demonstrate that, for the         end-to-end DRTS whose control matrix G falls into the stable region, the IC-EAT is convergent and stable. Moreover, it is capable of providing better QoS guarantees         for end-to-end aperiodic tasks and improving the system throughput.      </content></document><document><year>2007</year><authors>Minghui Jiang1  | Joel Gillespie1 </authors><title>Engineering the Divide-and-Conquer Closest Pair Algorithm      </title><content>We improve the famous divide-and-conquer algorithm by Bentley and Shamos for the planar closest-pair problem. For n points on the plane, our algorithm keeps the optimal O(n log n) time complexity and, using a circle-packing property, computes at most 7n/2 Euclidean distances, which improves Ge et al.&amp;#8217;s bound of (3n log n)/2 Euclidean distances. We present experimental results of our comparative studies on four different versions of the divide-and-conquer         closest pair algorithm and propose two effective heuristics.      </content></document><document><year>2007</year><authors>Feng-Xi Song1| 2 | David Zhang3 | Cai-Kou Chen4  | Jing-Yu Yang4 </authors><title>Facial Feature Extraction Method Based on Coefficients of Variances      </title><content>Principal Component Analysis (PCA) and Linear Discriminant Analysis (LDA) are two popular feature extraction techniques in         statistical pattern recognition field. Due to small sample size problem LDA cannot be directly applied to appearance-based         face recognition tasks. As a consequence, a lot of LDA-based facial feature extraction techniques are proposed to deal with         the problem one after the other. Nullspace Method is one of the most effective methods among them. The Nullspace Method tries         to find a set of discriminant vectors which maximize the between-class scatter in the null space of the within-class scatter         matrix. The calculation of its discriminant vectors will involve performing singular value decomposition on a high-dimensional         matrix. It is generally memory- and time-consuming.                     Borrowing the key idea in Nullspace method and the concept of coefficient of variance in statistical analysis we present a               novel facial feature extraction method, i.e., Discriminant based on Coefficient of Variance (DCV) in this paper. Experimental               results performed on the FERET and AR face image databases demonstrate that DCV is a promising technique in comparison with               Eigenfaces, Nullspace Method, and other state-of-the-art facial feature extraction methods.            </content></document><document><year>2007</year><authors>Bo Li1| 2 | Run-Hai Jiao1  | Yuan-Cheng Li1 </authors><title>Fast Adaptive Wavelet for Remote Sensing Image Compression      </title><content>Remote sensing images are hard to achieve high compression ratio because of their rich texture. By analyzing the influence         of wavelet properties on image compression, this paper proposes wavelet construction rules and builds a new biorthogonal wavelet         construction model with parameters. The model parameters are optimized by using genetic algorithm and adopting energy compaction         as the optimization object function. In addition, in order to resolve the computation complexity problem of online construction,         according to the image classification rule proposed in this paper we construct wavelets for different classes of images and         implement the fast adaptive wavelet selection algorithm (FAWS). Experimental results show wavelet bases of FAWS gain better         compression performance than Daubechies9/7.      </content></document><document><year>2007</year><authors>Yu Li1 | Yong-Tian Wang1 | Yue Liu1</authors><title>Fiducial Marker Based on Projective Invariant for Augmented Reality      </title><content>Fiducial marker based Augmented Reality has many applications. So far the inner pattern of the fiducial marker is always used         to encode the markers. Thus a large portion of the fiducial marker image is used for encoding instead of providing corresponding         feature points for pose accuracy. This paper presents a novel method which utilizes directly the projective invariant contained         in the positional relation of the corresponding feature points to encode the marker. The proposed method does not require         the region of pattern image for encoding any more and can provide more corresponding feature points so that higher pose accuracy         can be achieved easily. Many related approaches such as cumulative distribution function, reprojection verification and robust         process are proposed to overcome the problem of sensibility of the projective invariant. Experimental results show that the         proposed fiducial marker system is reliable and robust, and can provide higher pose accuracy than that achieved by existing         fiducial marker systems.      </content></document><document><year>2007</year><authors>Jing Zhou1  | David De Roure2 </authors><title>FloodNet: Coupling Adaptive Sampling with Energy Aware Routing in a Flood Warning System      </title><content>We describe the design of FloodNet, a flood warning system, which uses a grid-based flood predictor model developed by environmental         experts to make flood predictions based on readings of water level collected by a set of sensor nodes. To optimize battery         consumption, the reporting frequency of sensor nodes is required to be adaptive to local conditions as well as the flood predictor         model. We therefore propose an energy aware routing protocol which allows sensor nodes to consume energy according to this         need. This system is notable both for the adaptive sampling regime and the methodology adopted in the design of the adaptive         behavior, which involved development of simulation tools and very close collaboration with environmental experts.      </content></document><document><year>2007</year><authors>Chun-Xiao Lin1 | Yi-Yun Chen1 | Long Li1  | Bei Hua1 </authors><title>Garbage Collector Verification for Proof-Carrying Code      </title><content>We present the verification of the machine-level implementation of a conservative variant of the standard mark-sweep garbage         collector in a Hoare-style program logic. The specification of the collector is given on a machine-level memory model using         separation logic, and is strong enough to preserve the safety property of any common mutator program. Our verification is         fully implemented in the Coq proof assistant and can be packed immediately as foundational proof-carrying code package. Our         work makes important attempt toward building fully certified production-quality garbage collectors.      </content></document><document><year>2007</year><authors>Yongxi Cheng1 </authors><title>Generating Combinations by Three Basic Operations      </title><content>We investigate the problem of listing combinations using a special class of operations, prefix shifts. Combinations are represented as bitstrings of 0's and 1's, and prefix shifts are the operations of rotating some prefix         of a bitstring by one position to left or right. We give a negative answer to an open problem asked by F. Ruskey and A. Williams         (Generating combinations by prefix shifts, In Proc. 11th Annual International Computing and Combinatorics Conference 2005,         LNCS 3595, Springer, 2005, pp.570&amp;#8211;576), that is whether we can generate combinations by only using three very basic prefix         shifts on bitstrings, which are transposition of the first two bits and the rotation of the entire bitstring by one position         in either direction (i.e., applying the permutations &amp;#963;2, &amp;#963;            n             and &amp;#963;            n                     &amp;#8722;1 to the indices of the bitstrings).      </content></document><document><year>2007</year><authors>Jian-Hua Feng1 | Yu-Guo Liao1  | Yong Zhang1 </authors><title>HCH for Checking Containment of XPath Fragment      </title><content>XPath is ubiquitous in XML applications for navigating XML trees and selecting a set of element nodes. In XPath query processing,         one of the most important issues is how to efficiently check containment relationship between two XPath expressions. To get         out of the intricacy and complexity caused by numerous XPath features, we investigate this issue on a frequently used fragment         of XPath expressions that consists of node tests, the child axis (/), the descendant axis (//), branches ([]) and label wildcards         (*). Prior work has shown that homomorphism technology can be used for containment checking. However, homomorphism is the         sufficient but not necessary condition for containment. For special classes of this fragment, the homomorphism algorithm returns         false negatives. To address this problem, this paper proposes two containment techniques, conditioned homomorphism and hidden         conditioned homomorphism, and then presents sound algorithms for checking containment. Experimental results confirm the practicability         and efficiency of the proposed algorithms.      </content></document><document><year>2007</year><authors>Xia-Fen Zhang1| 2 | Yue-Ting Zhuang1 | Jiang-Qin Wu1  | Fei Wu1 </authors><title>Hierarchical Approximate Matching for Retrieval of Chinese Historical Calligraphy Character      </title><content>As historical Chinese calligraphy works are being digitized, the problem of retrieval becomes a new challenge. But, currently         no OCR technique can convert calligraphy character images into text, nor can the existing Handwriting Character Recognition         approach does not work for it. This paper proposes a novel approach to efficiently retrieving Chinese calligraphy characters         on the basis of similarity: calligraphy character image is represented by a collection of discriminative features, and high         retrieval speed with reasonable effectiveness is achieved. First, calligraphy characters that have no possibility similar         to the query are filtered out step by step by comparing the character complexity, stroke density and stroke protrusion. Then,         similar calligraphy characters are retrieved and ranked according to their matching cost produced by approximate shape match.         In order to speed up the retrieval, we employed high dimensional data structure &amp;#8212; PK-tree. Finally, the efficiency of the         algorithm is demonstrated by a preliminary experiment with 3012 calligraphy character images.      </content></document><document><year>2007</year><authors>Issam W. Damaj1 </authors><title>Higher-Level Hardware Synthesis of the KASUMI Algorithm      </title><content>Programmable Logic Devices (PLDs) continue to grow in size and currently contain several millions of gates. At the same time,         research effort is going into higher-level hardware synthesis methodologies for reconfigurable computing that can exploit         PLD technology. In this paper, we explore the effectiveness and extend one such formal methodology in the design of massively         parallel algorithms. We take a step-wise refinement approach to the development of correct reconfigurable hardware circuits         from formal specifications. A functional programming notation is used for specifying algorithms and for reasoning about them.         The specifications are realised through the use of a combination of function decomposition strategies, data refinement techniques,         and off-the-shelf refinements based upon higher-order functions. The off-the-shelf refinements are inspired by the operators         of Communicating Sequential Processes (CSP) and map easily to programs in Handel-C (a hardware description language). The         Handel-C descriptions are directly compiled into reconfigurable hardware. The practical realisation of this methodology is         evidenced by a case studying the third generation mobile communication security algorithms. The investigated algorithm is         the KASUMI block cipher. In this paper, we obtain several hardware implementations with different performance characteristics         by applying different refinements to the algorithm. The developed designs are compiled and tested under Celoxica&amp;#8217;s RC-1000         reconfigurable computer with its 2 million gates Virtex-E FPGA. Performance analysis and evaluation of these implementations         are included.      </content></document><document><year>2007</year><authors>Murat Ekinci1  | Murat Aykut1</authors><title>Human Gait Recognition Based on Kernel PCA Using Projections      </title><content>This paper presents a novel approach for human identification at a distance using gait recognition. Recognition of a person         from their gait is a biometric of increasing interest. The proposed work introduces a nonlinear machine learning method, kernel         Principal Component Analysis (PCA), to extract gait features from silhouettes for individual recognition. Binarized silhouette         of a motion object is first represented by four 1-D signals which are the basic image features called the distance vectors.         Fourier transform is performed to achieve translation invariant for the gait patterns accumulated from silhouette sequences         which are extracted from different circumstances. Kernel PCA is then used to extract higher order relations among the gait         patterns for future recognition. A fusion strategy is finally executed to produce a final decision. The experiments are carried         out on the CMU and the USF gait databases and presented based on the different training gait cycles.      </content></document><document><year>2007</year><authors>Chang-Ji Wang1| 2| 3 | Yong Tang1 | Qing Li1</authors><title>ID-Based Fair Off-Line Electronic Cash System with Multiple Banks      </title><content>ID-based public key cryptography (ID-PKC) has many advantages over certificate-based public key cryptography (CA-PKC), and         has drawn researchers&amp;#8217; extensive attention in recent years. However, the existing electronic cash schemes are constructed         under CA-PKC, and there seems no electronic cash scheme under ID-PKC up to now to the best of our knowledge. It is important         to study how to construct electronic cash schemes based on ID-PKC from views on both practical perspective and pure research         issue. In this paper, we present a simpler and provably secure ID-based restrictive partially blind signature (RPBS), and         then propose an ID-based fair off-line electronic cash (ID-FOLC) scheme with multiple banks based on the proposed ID-based         RPBS. The proposed ID-FOLC scheme with multiple banks is more efficient than existing electronic cash schemes with multiple         banks based on group blind signature.      </content></document><document><year>2007</year><authors>Wei-Wu Hu1| 2 | Ji-Ye Zhao1| 2 | Shi-Qiang Zhong1| 2 | Xu Yang1| 2 | Elio Guidetti3  | Chris Wu3 </authors><title>Implementing a 1GHz Four-Issue Out-of-Order Execution Microprocessor in a Standard Cell ASIC Methodology      </title><content>This paper introduces the microarchitecture and physical implementation of the Godson-2E processor, which is a four-issue         superscalar RISC processor that supports the 64-bit MIPS instruction set. The adoption of the aggressive out-of-order execution         and memory hierarchy techniques help Godson-2E to achieve high performance. The Godson-2E processor has been physically designed         in a 7-metal 90nm CMOS process using the cell-based methodology with some bit-sliced manual placement and a number of crafted         cells and macros. The processor can be run at 1GHz and achieves a SPEC CPU2000 rate higher than 500.      </content></document><document><year>2007</year><authors>Wen-Ling Wu1 | Wen-Tao Zhang2  | Deng-Guo Feng1 </authors><title>Impossible Differential Cryptanalysis of Reduced-Round ARIA and Camellia      </title><content>This paper studies the security of the block ciphers ARIA and Camellia against impossible differential cryptanalysis. Our         work improves the best impossible differential cryptanalysis of ARIA and Camellia known so far. The designers of ARIA expected         no impossible differentials exist for 4-round ARIA. However, we found some nontrivial 4-round impossible differentials, which         may lead to a possible attack on 6-round ARIA. Moreover, we found some nontrivial 8-round impossible differentials for Camellia,         whereas only 7-round impossible differentials were previously known. By using the 8-round impossible differentials, we presented         an attack on 12-round Camellia without FL/FL         1 layers.      </content></document><document><year>2007</year><authors>Jie Liang1  | Xue-Jia Lai1 </authors><title>Improved Collision Attack on Hash Function MD5      </title><content>In this paper, we present a fast attack algorithm to find two-block collision of hash function MD5. The algorithm is based         on the two-block collision differential path of MD5 that was presented by Wang et al. in the Conference EUROCRYPT 2005. We found that the derived conditions for the desired collision differential path were         not sufficient to guarantee the path to hold and that some conditions could be modified to enlarge the collision set. By using         technique of small range searching and omitting the computing steps to check the characteristics in the attack algorithm,         we can speed up the attack of MD5 efficiently. Compared with the Advanced Message Modification technique presented by Wang         et al., the small range searching technique can correct 4 more conditions for the first iteration differential and 3 more conditions         for the second iteration differential, thus improving the probability and the complexity to find collisions. The whole attack         on the MD5 can be accomplished within 5 hours using a PC with Pentium4 1.70GHz CPU.      </content></document><document><year>2007</year><authors>Piotr Tomaszewski1 | Lars Lundberg1  | HГҐkan Grahn1 </authors><title>Improving Fault Detection in Modified Code &amp;#8212; A Study from the Telecommunication Industry      </title><content>Many software systems are developed in a number of consecutive releases. In each release not only new code is added but also         existing code is often modified. In this study we show that the modified code can be an important source of faults. Faults         are widely recognized as one of the major cost drivers in software projects. Therefore, we look for methods that improve the         fault detection in the modified code. We propose and evaluate a number of prediction models that increase the efficiency of         fault detection. To build and evaluate our models we use data collected from two large telecommunication systems produced         by Ericsson. We evaluate the performance of our models by applying them both to a different release of the system than the         one they are built on and to a different system. The performance of our models is compared to the performance of the theoretical         best model, a simple model based on size, as well as to analyzing the code in a random order (not using any model). We find         that the use of our models provides a significant improvement over not using any model at all and over using a simple model         based on the class size. The gain offered by our models corresponds to 38&amp;#8211;57% of the theoretical maximum gain.      </content></document><document><year>2007</year><authors>Taghi M. Khoshgoftaar1  | Pierre Rebours1 </authors><title>Improving Software Quality Prediction by Noise Filtering Techniques      </title><content>Accuracy of machine learners is affected by quality of the data the learners are induced on. In this paper, quality of the         training dataset is improved by removing instances detected as noisy by the Partitioning Filter. The fit dataset is first         split into subsets, and different base learners are induced on each of these splits. The predictions are combined in such         a way that an instance is identified as noisy if it is misclassified by a certain number of base learners. Two versions of         the Partitioning Filter are used: Multiple-Partitioning Filter and Iterative-Partitioning Filter. The number of instances         removed by the filters is tuned by the voting scheme of the filter and the number of iterations. The primary aim of this study         is to compare the predictive performances of the final models built on the filtered and the un-filtered training datasets.         A case study of software measurement data of a high assurance software project is performed. It is shown that predictive performances         of models built on the filtered fit datasets and evaluated on a noisy test dataset are generally better than those built on         the noisy (un-filtered) fit dataset. However, predictive performance based on certain aggressive filters is affected by presence         of noise in the evaluation dataset.      </content></document><document><year>2007</year><authors>Ji-Dong Chen1| 2  | Xiao-Feng Meng1| 2 </authors><title>Indexing Future Trajectories of Moving Objects in a Constrained Network      </title><content>Advances in wireless sensor networks and positioning technologies enable new applications monitoring moving objects. Some         of these applications, such as traffic management, require the possibility to query the future trajectories of the objects.         In this paper, we propose an original data access method, the ANR-tree, which supports predictive queries. We focus on real         life environments, where the objects move within constrained networks, such as vehicles on roads. We introduce a simulation-based         prediction model based on graphs of cellular automata, which makes full use of the network constraints and the stochastic         traffic behavior. Our technique differs strongly from the linear prediction model, which has low prediction accuracy and requires         frequent updates when applied to real traffic with velocity changing frequently. The data structure extends the R-tree with         adaptive units which group neighbor objects moving in the similar moving patterns. The predicted movement of the adaptive         unit is not given by a single trajectory, but instead by two trajectory bounds based on different assumptions on the traffic         conditions and obtained from the simulation. Our experiments, carried on two different datasets, show that the ANR-tree is         essentially one order of magnitude more efficient than the TPR-tree, and is much more scalable.      </content></document><document><year>2007</year><authors>Wei Wang1| 2 | Yu Hu2 | Yin-He Han2 | Xiao-Wei Li2  | You-Sheng Zhang1 </authors><title>Leakage Current Optimization Techniques During Test Based on Don&amp;#8217;t Care Bits Assignment      </title><content>It is a well-known fact that test power consumption may exceed that during functional operation. Leakage power dissipation         caused by leakage current in Complementary Metal-Oxide-Semiconductor (CMOS) circuits during test has become a significant         part of the total power dissipation. Hence, it is important to reduce leakage power to prolong battery life in portable systems         which employ periodic self-test, to increase test reliability and to reduce test cost. This paper analyzes leakage current         and presents a kind of leakage current simulator based on the transistor stacking effect. Using it, we propose techniques         based on don't care bits (denoted by Xs) in test vectors to optimize leakage current in integrated circuit (IC) test by genetic algorithm. The techniques identify         a set of don't care inputs in given test vectors and reassign specified logic values to the X inputs by the genetic algorithm to get minimum leakage vector (MLV). Experimental results indicate that the techniques can         effectually optimize leakage current of combinational circuits and sequential circuits during test while maintaining high         fault coverage.      </content></document><document><year>2007</year><authors>Jun Zhao1  | Fei-Fan Liu1 </authors><title>Linguistic Theory Based Contextual Evidence Mining for Statistical Chinese Co-Reference Resolution      </title><content>Under statistical learning framework, the paper focuses on how to use traditional linguistic findings on anaphora resolution         as a guide for mining and organizing contextual features for Chinese co-reference resolution. The main achievements are as         follows. (1) In order to simulate &amp;#8220;syntactic and semantic parallelism factor&amp;#8221;, we extract &amp;#8220;bags of word form and POS&amp;#8221; feature         and &amp;#8220;bag of semes&amp;#8221; feature from the contexts of the entity mentions and incorporate them into the baseline feature set. (2)         Because it is too coarse to use the feature of bags of word form, POS tag and seme to determine the syntactic and semantic         parallelism between two entity mentions, we propose a method for contextual feature reconstruction based on semantic similarity         computation, in order that the reconstructed contextual features could better approximate the anaphora resolution factor of         &amp;#8220;Syntactic and Semantic Parallelism Preferences&amp;#8221;. (3) We use an entity-mention-based contextual feature representation instead         of isolated word-based contextual feature representation, and expand the size of the contextual windows in addition, in order         to approximately simulate &amp;#8220;the selectional restriction factor&amp;#8221; for anaphora resolution. The experiments show that the multi-level         contextual features are useful for co-reference resolution, and the statistical system incorporated with these features performs         well on the standard ACE datasets.      </content></document><document><year>2007</year><authors>Dong-Hong Han1 | Guo-Ren Wang1 | Chuan Xiao1 | Rui Zhou1</authors><title>Load Shedding for Window Joins over Streams      </title><content>We address several load shedding techniques over sliding window joins. We first construct a dual window architectural model         including aux-windows and join-windows, and build statistics on aux-windows. With the statistics, we develop an effective         load shedding strategy producing maximum subset join outputs. In order to accelerate the load shedding process, binary indexed         trees have been utilized to reduce the cost on shedding evaluation. When streams have high arrival rates, we propose an approach         incorporating front-shedding and rear-shedding, and find an optimal trade-off between them. As for the scenarios of variable         speed ratio, we develop a plan reallocating CPU resources and dynamically resizing the windows. In addition, we prove that         load shedding is not affected during the process of reallocation. Both synthetic and real data are used in our experiments,         and the results show the promise of our strategies.      </content></document><document><year>2007</year><authors>Ozgur Sinanoglu1 </authors><title>Low Cost Scan Test by Test Correlation Utilization      </title><content>Scan-based testing methodologies remedy the testability problem of sequential circuits; yet they suffer from prolonged test         time and excessive test power due to numerous shift operations. The correlation among test data along with the high density         of the unspecified bits in test data enables the utilization of the existing test data in the scan chain for the generation         of the subsequent test stimulus, thus reducing both test time and test data volume. We propose a pair of scan approaches in         this paper; in the first approach, a test stimulus partially consists of the preceding stimulus, while in the second approach,         a test stimulus partially consists of the preceding test response bits. Both proposed scan-based test schemes access only         a subset of scan cells for loading the subsequent test stimulus while freezing the remaining scan cells with the preceding         test data, thus decreasing scan chain transitions during shift operations. The proposed scan architecture is coupled with         test data manipulation techniques which include test stimuli ordering and partitioning algorithms, boosting test time reductions.         The experimental results confirm that test time reductions exceeding 97%, and test power reductions exceeding 99% can be achieved         by the proposed scan-based testing methodologies on larger ISCAS89 benchmark circuits.      </content></document><document><year>2007</year><authors>Chaveevan Pechsiri1  | Asanee Kawtrakul2 </authors><title>Mining Causality for Explanation Knowledge from Text      </title><content>Mining causality is essential to provide a diagnosis. This research aims at extracting the causality existing within multiple         sentences or EDUs (Elementary Discourse Unit). The research emphasizes the use of causality verbs because they make explicit         in a certain way the consequent events of a cause, e.g., &amp;#8220;Aphids         suck         the sap from rice leaves. Then leaves will         shrink         . Later, they will         become         yellow and         dry.&amp;#8221;. A verb can also be the causal-verb link between cause and effect within EDU(s), e.g., &amp;#8220;Aphids suck the sap from rice leaves         causing         leaves to be shrunk&amp;#8221; (&amp;#8220;causing&amp;#8221; is equivalent to a causal-verb link in Thai). The research confronts two main problems: identifying the interesting         causality events from documents and identifying their boundaries. Then, we propose mining on verbs by using two different         machine learning techniques, NaГЇve Bayes classifier and Support Vector Machine. The resulted mining rules will be used for         the identification and the causality extraction of the multiple EDUs from text. Our multiple EDUs extraction shows 0.88 precision         with 0.75 recall from NaГЇve Bayes classifier and 0.89 precision with 0.76 recall from Support Vector Machine.      </content></document><document><year>2007</year><authors>Xiao-Bo Fan1| 2 | Ting-Ting Xie1| 2 | Cui-Ping Li1| 2  | Hong Chen1| 2 </authors><title>MRST&amp;#8212;An Efficient Monitoring Technology of Summarization on Stream Data      </title><content>Monitoring on data streams is an efficient method of acquiring the characters of data stream. However the available resources         for each data stream are limited, so the problem of how to use the limited resources to process infinite data stream is an         open challenging problem. In this paper, we adopt the wavelet and sliding window methods to design a multi-resolution summarization         data structure, the Multi-Resolution Summarization Tree (MRST) which can be updated incrementally with the incoming data and         can support point queries, range queries, multi-point queries and keep the precision of queries. We use both synthetic data         and real-world data to evaluate our algorithm. The results of experiment indicate that the efficiency of query and the adaptability         of MRST have exceeded the current algorithm, at the same time the realization of it is simpler than others.      </content></document><document><year>2007</year><authors>Jung-Hoon Lee1 </authors><title>Next High Performance and Low Power Flash Memory Package Structure      </title><content>In general, NAND flash memory has advantages in low power consumption, storage capacity, and fast erase/write performance         in contrast to NOR flash. But, main drawback of the NAND flash memory is the slow access time for random read operations.         Therefore, we proposed the new NAND flash memory package for overcoming this major drawback. We present a high performance         and low power NAND flash memory system with a dual cache memory. The proposed NAND flash package consists of two parts, i.e.,         an NAND flash memory module, and a dual cache module. The new NAND flash memory system can achieve dramatically higher performance         and lower power consumption compared with any conventional NAND-type flash memory module. Our results show that the proposed         system can reduce about 78% of write operations into the flash memory cell and about 70% of read operations from the flash         memory cell by using only additional 3KB cache space. This value represents high potential to achieve low power consumption         and high performance gain.      </content></document><document><year>2007</year><authors>Nelly Condori-FernГЎndez1 | Silvia AbrahГЈo1  | Oscar Pastor1 </authors><title>On the Estimation of the Functional Size of Software from Requirements Specifications      </title><content>This paper introduces a measurement procedure, called RmFFP, which describes a set of operations for modelling and estimating         the size of object-oriented software systems from high-level specifications using the OO-Method Requirement Model. OO-Method         is an automatic software production method. The contribution of this work is to systematically define a set of rules that         allows estimating the functional size at an early stage of the software production process, in accordance with COSMIC-FFP.         To do this, we describe the design, the application, and the analysis of the proposed measurement procedure following the         steps of a process model for software measurement. We also report initial results on the evaluation of RmFFP in terms of its         reproducibility.      </content></document><document><year>2007</year><authors>Jing-Fa Liu1| 2  | Wen-Qi Huang2</authors><title>Quasi-Physical Algorithm of an Off-Lattice Model for Protein Folding Problem      </title><content>Protein folding problem is one of the most prominent problems of bioinformatics. In this paper, we study a three-dimensional         off-lattice protein AB model with two species of monomers, hydrophobic and hydrophilic, and present a heuristic quasi-physical         algorithm. By elaborately simulating the movement of the smooth elastic balls in the physical world, the algorithm finds low-energy         configurations for a given monomer chain. A subsequent &amp;#8220;off-trap&amp;#8221; strategy is proposed to trigger a jump for a stuck situation         in order to get out of local minima. The methods have been tested in the off-lattice AB model. The computational results show         promising performance. For all sequences with 13 to 55 monomers, the algorithm finds states with lower energy than previously         proposed putative ground states. Furthermore, for the sequences with 21, 34 and 55 monomers, new putative ground states are         found, which are different from those given in present literature.      </content></document><document><year>2007</year><authors>Feng Xue1 | You-Sheng Zhang1 | Ju-Lang Jiang2 | Min Hu1 | Xin-Dong Wu1| 3  | Rong-Gui Wang1 </authors><title>Real-Time Texture Synthesis Using s-Tile Set      </title><content>This paper presents a novel method of generating a set of texture tiles from samples, which can be seamlessly tiled into arbitrary         size textures in real-time. Compared to existing methods, our approach is simpler and more advantageous in eliminating visual         seams that may exist in each tile of the existing methods, especially when the samples have elaborate features or distinct         colors. Texture tiles generated by our approach can be regarded as single-colored tiles on each orthogonal direction border,         which are easier for tiling and more suitable for sentence tiling. Experimental results demonstrate the feasibility and effectiveness         of our approach.      </content></document><document><year>2007</year><authors>Hai-Xia Xu1  | Bao Li1 </authors><title>Relationship Between a Non-Malleable Commitment Scheme and a Modified Selective Decommitment Scheme      </title><content>The notion of commitment is one of the most important primitives in cryptography. To meet various needs, there have been many         kinds of commitment schemes among which non-malleable commitment scheme and selective decommitment scheme are important and         in general use. And, the increasing security demands suggest a closer look at the relationship between the two schemes. For         the convenience of our proof, a new definition for selective decommitment scheme is proposed, which is named as modified selective         decommitment scheme. The security relation is deduced that a non-malleable commitment scheme implies a modified selective         decommitment scheme, but the reverse is not true.      </content></document><document><year>2007</year><authors>Dennis Y. W. Liu1 | Joseph K. Liu2 | Yi Mu3 | Willy Susilo3  | Duncan S. Wong1 </authors><title>Revocable Ring Signature      </title><content>Group signature allows the anonymity of a real signer in a group to be revoked by a trusted party called group manager. It         also gives the group manager the absolute power of controlling the formation of the group. Ring signature, on the other hand,         does not allow anyone to revoke the signer anonymity, while allowing the real signer to form a group (also known as a ring)         arbitrarily without being controlled by any other party. In this paper, we propose a new variant for ring signature, called Revocable Ring Signature. The signature allows a real signer to form a ring arbitrarily while allowing a set of authorities to revoke the anonymity         of the real signer. This new variant inherits the desirable properties from both group signature and ring signature in such         a way that the real signer will be responsible for what it has signed as the anonymity is revocable by authorities while the         real signer still has the freedom on ring formation. We provide a formal security model for revocable ring signature and propose         an efficient construction which is proven secure under our security model.      </content></document><document><year>2007</year><authors>Yong-Dong Zhang1 | Sheng Tang1  | Jin-Tao Li1 </authors><title>Secure and Incidental Distortion Tolerant Digital Signature for Image Authentication      </title><content>In this paper, a secure and incidental distortion tolerant signature method for image authentication is proposed. The generation         of authentication signature is based on Hotelling&amp;#8217;s T-square Statistic (HTS) via Principal Component Analysis (PCA) of block         DCT coefficients. HTS values of all blocks construct a unique and stable &amp;#8220;block-edge image&amp;#8221;, i.e., Structural and Statistical         Signature (SSS). The characteristic of SSS is that it is short, and can tolerate content-preserving manipulations while keeping         sensitive to content-changing attacks, and locate tampering easily. During signature matching, the Fisher criterion is used         to obtain optimal threshold for automatically and universally distinguishing incidental manipulations from malicious attacks.         Moreover, the security of SSS is achieved by encryption of the DCT coefficients with chaotic sequences before PCA. Experiments         show that the novel method is effective for authentication.      </content></document><document><year>2007</year><authors>Yong-Long Luo1| 2| 3 | Liu-Sheng Huang1| 3  | Hong Zhong1| 3</authors><title>Secure Two-Party Point-Circle Inclusion Problem      </title><content>Privacy-preserving computational geometry is a special secure multi-party computation and has many applications. Previous         protocols for determining whether a point is inside a circle are not secure enough. We present a two-round protocol for computing         the distance between two private points and develop a more efficient protocol for the point-circle inclusion problem based         on the distance protocol. In comparison with previous solutions, our protocol not only is more secure but also reduces the         number of communication rounds and the number of modular multiplications significantly.      </content></document><document><year>2007</year><authors>Xian-He Sun1| 2 | Surendra Byna1  | Yong Chen1 </authors><title>Server-Based Data Push Architecture for Multi-Processor Environments      </title><content>Data access delay is a major bottleneck in utilizing current high-end computing (HEC) machines. Prefetching, where data is         fetched before CPU demands for it, has been considered as an effective solution to masking data access delay. However, current         client-initiated prefetching strategies, where a computing processor initiates prefetching instructions, have many limitations.         They do not work well for applications with complex, non-contiguous data access patterns. While technology advances continue         to increase the gap between computing and data access performance, trading computing power for reducing data access delay         has become a natural choice. In this paper, we present a server-based data-push approach and discuss its associated implementation         mechanisms. In the server-push architecture, a dedicated server called Data Push Server (DPS) initiates and proactively pushes         data closer to the client in time. Issues, such as what data to fetch, when to fetch, and how to push are studied. The SimpleScalar         simulator is modified with a dedicated prefetching engine that pushes data for another processor to test DPS based prefetching.         Simulation results show that L1 Cache miss rate can be reduced by up to 97% (71% on average) over a superscalar processor         for SPEC CPU2000 benchmarks that have high cache miss rates.      </content></document><document><year>2007</year><authors>Bo Qin1| 2 | Qian-Hong Wu3 | Willy Susilo3 | Yi Mu3 | Yu-Min Wang1 | Zheng-Tao Jiang4 </authors><title>Short Group Signatures Without Random Oracles      </title><content>We propose short group signature (GS) schemes which are provably secure without random oracles. Our basic scheme is about 14 times shorter than the Boyen-Waters GS scheme at Eurocrypt 2006, and 42% shorter         than the recent GS schemes due to Ateniese et al. The security proofs are provided in the Universally Composable model, which allows the proofs of security valid not only         when our scheme is executed in isolation, but also in composition with other secure cryptographic primitives. We also present         several new computational assumptions and justify them in the generic group model. These assumptions are useful in the design         of high-level protocols and may be of independent interest.      </content></document><document><year>2007</year><authors>Zhang-Lin Cheng1| 2 | Xiao-Peng Zhang1| 2  | Bao-Quan Chen3 </authors><title>Simple Reconstruction of Tree Branches from a Single Range Image      </title><content>3D modeling of trees in real environments is a challenge in computer graphics and computer vision, since the geometric shape         and topological structure of trees are more complex than conventional artificial objects. In this paper, we present a multi-process         approach that is mainly performed in 2D space to faithfully construct a 3D model of the trunk and main branches of a real         tree from a single range image. The range image is first segmented into patches by jump edge detection based on depth discontinuity.         Coarse skeleton points and initial radii are then computed from the contour of each patch. Axis directions are estimated using         cylinder fitting in the neighborhood of each coarse skeleton point. With the help of axis directions, skeleton nodes and corresponding         radii are computed. Finally, these skeleton nodes are hierarchically connected, and improper radii are modified based on plant         knowledge. 3D models generated from single range images of real trees demonstrate the effectiveness of our method. The main         contributions of this paper are simple reconstruction by virtue of image storage order of single scan and skeleton computation         based on axis directions.      </content></document><document><year>2007</year><authors>Vincent Le Chevalier1 | Marc Jaeger2 | Xing Mei3  | Paul-Henry CournГЁde1 </authors><title>Simulation and Visualisation of Functional Landscapes: Effects of the Water Resource Competition Between Plants      </title><content>Vegetation ecosystem simulation and visualisation are challenging topics involving multidisciplinary aspects. In this paper,         we present a new generic frame for the simulation of natural phenomena through manageable and interacting models. It focuses         on the functional growth of large vegetal ecosystems, showing coherence for scales ranging from the individual plant to communities         and with a particular attention to the effects of water resource competition between plants. The proposed approach is based         on a model of plant growth in interaction with the environmental conditions. These are deduced from the climatic data (light,         temperature, rainfall) and a model of soil hydrological budget. A set of layers is used to store the water resources and to         build the interfaces between the environmental data and landscape components: temperature, rain, light, altitude, lakes, plant         positions, biomass, cycles, etc. At the plant level, the simulation is performed for each individual by a structural-functional         growth model, interacting with the plant&amp;#8217;s environment. Temperature is spatialised, changing according to altitude, and thus         locally controls plant growth speed. The competition for water is based on a soil hydrological model taking into account rainfalls,         water runoff, absorption, diffusion, percolation in soil. So far, the incoming light radiation is not studied in detail and         is supposed constant. However, competition for light between plants is directly taken into account in the plant growth model.         In our implementation, we propose a simple architecture for such a simulator and a simulation scheme to synchronise the water         resource updating (on a temporal basis) and the plant growth cycles (determined by the sum of daily temperatures). The visualisation         techniques are based on sets of layers, allowing both morphological and functional landscape views and providing interesting         tools for ecosystem management. The implementation of the proposed frame leads to encouraging results that are presented and         illustrate simple academic cases.      </content></document><document><year>2007</year><authors>Zhiyuan Li1 </authors><title>Simultaneous Minimization of Capacity and Conflict Misses      </title><content>Loop tiling (or loop blocking) is a well-known loop transformation to improve temporal locality in nested loops which perform         matrix computations. When targeting caches that have low associativities, one of the key challenges for loop tiling is to         simultaneously minimize capacity misses and conflict misses. This paper analyzes the effect of the tile size and the array-dimension         size on capacity misses and conflict misses. The analysis supports the approach of combining tile-size selection (to minimize         capacity misses) with array padding (to minimize conflict misses).      </content></document><document><year>2007</year><authors>Sin-Kyu Kim1 | Jae-Woo Choi2 | Dae-Hun Nyang2 | Gene-Beck Hahn3  | Joo-Seok Song4 </authors><title>Smart Proactive Caching Scheme for Fast Authenticated Handoff in Wireless LAN      </title><content>Handoff in IEEE 802.11 requires the repeated authentication and key exchange procedures, which will make the provision of         seamless services in wireless LAN more difficult. To reduce the overhead, the proactive caching schemes have been proposed.         However, they require too many control packets delivering the security context information to neighbor access points. Our         contribution is made in two-fold: one is a significant decrease in the number of control packets for proactive caching and         the other is a superior cache replacement algorithm.      </content></document><document><year>2007</year><authors>Juan J. Cuadrado Gallego1 | Daniel RodrГ­guez1 | Miguel ГЃngel Sicilia1 | Miguel Garre Rubio1  | Angel GarcГ­a Crespo2 </authors><title>Software Project Effort Estimation Based on Multiple Parametric Models Generated Through Data Clustering      </title><content>Parametric software effort estimation models usually consists of only a single mathematical relationship. With the advent         of software repositories containing data from heterogeneous projects, these types of models suffer from poor adjustment and         predictive accuracy. One possible way to alleviate this problem is the use of a set of mathematical equations obtained through         dividing of the historical project datasets according to different parameters into subdatasets called partitions. In turn,         partitions are divided into clusters that serve as a tool for more accurate models. In this paper, we describe the process,         tool and results of such approach through a case study using a publicly available repository, ISBSG. Results suggest the adequacy         of the technique as an extension of existing single-expression models without making the estimation process much more complex         that uses a single estimation model. A tool to support the process is also presented.      </content></document><document><year>2007</year><authors>En-Jian Bai1| 2  | Xiao-Juan Liu3 </authors><title>Some Notes on Prime-Square Sequences      </title><content>The well-known binary Legendre sequences possess good autocorrelation functions and high linear complexity, and are just special         cases of much larger families of cyclotomic sequences. Prime-square sequences are the generalization of these Legendre sequences,         but the ratio of the linear complexity to the least period of these sequences approximates to zero if the prime is infinite.         However, a relatively straightforward modification can radically improve this situation. The structure and properties, including         linear complexity, minimal polynomial, and autocorrelation function, of these modified prime-square sequences are investigated.         The hardware implementation is also considered.      </content></document><document><year>2007</year><authors>Anwar M. Mirza1 | Asmatullah Chaudhry1  | Badre Munir1</authors><title>Spatially Adaptive Image Restoration Using Fuzzy Punctual Kriging      </title><content>We present a general formulation based on punctual kriging and fuzzy concepts for image restoration in spatial domain. Gray-level         images degraded with Gaussian white noise have been considered. Based on the pixel local neighborhood, fuzzy logic has been         employed intelligently to avoid unnecessary estimation of a pixel. The intensity estimation of the selected pixels is then         carried out by employing punctual kriging in conjunction with the method of Lagrange multipliers and estimates of local semi-variances.         Application of such a hybrid technique performing both selection and intensity estimation of a pixel demonstrates substantial         improvement in the image quality as compared to the adaptive Wiener filter and existing fuzzy-kriging approaches. It has been         found that these filters achieve noise reduction without loss of structural detail information, as indicated by their higher         structure similarity indices, peak signal to noise ratios and the new variogram based quality measures.      </content></document><document><year>2007</year><authors>Chang-Xuan Wan1| 2  | Xi-Ping Liu1| 2</authors><title>Structural Join and Staircase Join Algorithms of Sibling Relationship      </title><content>The processing of XML queries can result in evaluation of various structural relationships. Efficient algorithms for evaluating         ancestor-descendant and parent-child relationships have been proposed. Whereas the problems of evaluating preceding-sibling-following-sibling         and preceding-following relationships are still open. In this paper, we studied the structural join and staircase join for         sibling relationship. First, the idea of how to filter out and minimize unnecessary reads of elements using parent&amp;#8217;s structural         information is introduced, which can be used to accelerate structural joins of parent-child and preceding-sibling-following-sibling         relationships. Second, two efficient structural join algorithms of sibling relationship are proposed. These algorithms lead         to optimal join performance: nodes that do not participate in the join can be judged beforehand and then skipped using B+-tree index. Besides, each element list joined is scanned sequentially once at most. Furthermore, output of join results is         sorted in document order. We also discussed the staircase join algorithm for sibling axes. Studies show that, staircase join         for sibling axes is close to the structural join for sibling axes and shares the same characteristic of high efficiency. Our         experimental results not only demonstrate the effectiveness of our optimizing techniques for sibling axes, but also validate         the efficiency of our algorithms. As far as we know, this is the first work addressing this problem specially.      </content></document><document><year>2007</year><authors>Xin-Li Huang1 | Fu-Tai Zou2  | Fan-Yuan Ma1 </authors><title>Targeted Local Immunization in Scale-Free Peer-to-Peer Networks      </title><content>The power-law node degree distributions of peer-to-peer overlay networks make them extremely robust to random failures whereas         highly vulnerable under intentional targeted attacks. To enhance attack survivability of these networks, DeepCure, a novel         heuristic immunization strategy, is proposed to conduct decentralized but targeted immunization. Different from existing strategies,         DeepCure identifies immunization targets as not only the highly-connected nodes but also the nodes with high availability and/or high link load, with the aim of injecting immunization information into just right targets to cure. To better trade off the cost and the efficiency, DeepCure deliberately select these targets from 2-local neighborhood, as well as topologically-remote but semantically-close friends if needed. To remedy the weakness of existing         strategies in case of sudden epidemic outbreak, DeepCure is also coupled with a local-hub oriented rate throttling mechanism to enforce proactive rate control. Extensive simulation results show that DeepCure outperforms its competitors,         producing an arresting increase of the network attack tolerance, at a lower price of eliminating viruses or malicious attacks.      </content></document><document><year>2007</year><authors>Shi-Zhu Liu1  | He-Ping Hu1</authors><title>Text Classification Using Sentential Frequent Itemsets      </title><content>Text classification techniques mostly rely on single term analysis of the document data set, while more concepts, especially         the specific ones, are usually conveyed by set of terms. To achieve more accurate text classifier, more informative feature         including frequent co-occurring words in the same sentence and their weights are particularly important in such scenarios.         In this paper, we propose a novel approach using sentential frequent itemset, a concept comes from association rule mining,         for text classification, which views a sentence rather than a document as a transaction, and uses a variable precision rough         set based method to evaluate each sentential frequent itemset&amp;#8217;s contribution to the classification. Experiments over the Reuters         and newsgroup corpus are carried out, which validate the practicability of the proposed system.      </content></document><document><year>2007</year><authors>Min Zhao1 | Su-Qing Han2  | Jue Wang3 </authors><title>Tree Expressions for Information Systems      </title><content>The discernibility matrix is one of the most important approaches to computing positive region, reduct, core and value reduct         in rough sets. The subject of this paper is to develop a parallel approach of it, called &amp;#8220;tree expression&amp;#8221;. Its computational complexity for positive region and reduct is O(m         2&amp;#8201;Г—&amp;#8201;n) instead of O(m&amp;#8201;Г—&amp;#8201;n         2) in discernibility-matrix-based approach, and is not over O(n         2) for other concepts in rough sets, where m and n are the numbers of attributes and objects respectively in a given dataset (also called an &amp;#8220;information system&amp;#8221; in rough sets). This approach suits information systems with n&amp;#8201;&amp;#8811;&amp;#8201;m and containing over one million objects.      </content></document><document><year>2007</year><authors>Ying-Han Pang1 | Andrew T. B. J.1  | David N. C. L1 </authors><title>Two-Factor Cancelable Biometrics Authenticator      </title><content>Biometrics-based authentication system offers advantages of providing high reliability and accuracy. However, the contemporary         authentication system is impuissance to compromise. If a biometrics data is compromised, it cannot be replaced and rendered         unusable. In this paper, a cancelable biometrics-based authenticator is proposed to solve this irrevocability issue. The proposed         approach is a two-factor authentication system, which requires both of the random data and facial feature in order to access         the system. In this system, tokenized pseudo-random data is coupled with moment-based facial feature via inner product algorithm.         The output of the product is then discretized to generate a set of private binary code, coined as 2factor-Hashing code, which         is acted as verification key. If this biometrics-based verification key is compromised, a new one can be issued by replacing         a different set of random number via token replacement. Then, the compromised one is rendered completely useless. This feature         offers an extra protection layer against biometrics fabrication since the verification code is replaceable. Experimental results         demonstrate that the proposed system provides zero Equal Error Rate in which there is a clear separation in between the genuine         and the imposter distribution populations.      </content></document><document><year>2007</year><authors>Chiou-Yng Lee1 | Yung-Hui Chen1 | Che-Wun Chiou2  | Jim-Min Lin3 </authors><title>Unified Parallel Systolic Multiplier Over                                  </title><content>In general, there are three popular basis representations, standard (canonical, polynomial) basis, normal basis, and dual         basis, for representing elements in                   . Various basis representations have their distinct advantages and have their different associated multiplication architectures.         In this paper, we will present a unified systolic multiplication architecture, by employing Hankel matrix-vector multiplication,         for various basis representations. For various element representation in                   , we will show that various basis multiplications can be performed by Hankel matrix-vector multiplications. A comparison with         existing and similar structures has shown that the proposed architectures perform well both in space and time complexities.      </content></document><document><year>2007</year><authors>Wen Zheng1| 2 | Jun-Hai Yong1  | Jean-Claude Paul1| 3 </authors><title>Visual Simulation of Multiple Unmixable Fluids      </title><content>We present a novel grid-based method for simulating multiple unmixable fluids moving and interacting. Unlike previous methods         that can only represent the interface between two fluids (usually between liquid and gas), this method can handle an arbitrary         number of fluids through multiple independent level sets coupled with a constrain condition. To capture the fluid surface         more accurately, we extend the particle level set method to a multi-fluid version. It shares the advantages of the particle         level set method, and has the ability to track the interfaces of multiple fluids. To handle the dynamic behavior of different         fluids existing together, we use a multiphase fluid formulation based on a smooth weight function.      </content></document><document><year>2007</year><authors>Rafiullah Chamlawi1 | Asifullah Khan1  | Adnan Idris2 </authors><title>Wavelet Based Image Authentication and Recovery      </title><content>In this paper, we propose a secure semi-fragile watermarking technique based on integer wavelet transform with a choice of         two watermarks to be embedded. A self-recovering algorithm is employed, that hides the image digest into some wavelet subbands         for detecting possible illicit object manipulation undergone in the image. The semi-fragility makes the scheme tolerant against         JPEG lossy compression with the quality factor as low as 70%, and locates the tampered area accurately. In addition, the system         ensures more security because the embedded watermarks are protected with private keys. The computational complexity is reduced         by using parameterized integer wavelet transform. Experimental results show that the proposed scheme guarantees safety of         a watermark, recovery of image and localization of tampered area.      </content></document><document><year>2005</year><authors>Ying Chen1 | Karthik Ranganathan2 | Vasudev V. Pai3 | David J. Lilja1  | Kia Bazargan1 </authors><title>A Novel Memory Structure for Embedded Systems: Flexible Sequential and Random Access Memory      </title><content>The on-chip memory performance of embedded systems directly affects the system designers' decision about how to allocate expensive         silicon area. A novel memory architecture, flexible sequential and random access memory (FSRAM), is investigated for embedded systems. To realize sequential accesses, small &amp;#8220;links&amp;#8221; are added to each row in the         RAM array to point to the next row to be prefetched. The potential cache pollution is ameliorated by a small sequential access buffer (SAB). To evaluate the architecture-level performance of FSRAM, we ran the Mediabench benchmark programs on a modified version         of the SimpleScalar simulator. Our results show that the FSRAM improves the performance of a baseline processor with a 16KB         data cache up to 55%, with an average of 9%; furthermore, the FSRAM reduces 53.1% of the data cache miss count on average         due to its prefetching effect. We also designed RTL and SPICE models of the FSRAM, which show that the FSRAM significantly         improves memory access time, while reducing power consumption, with negligible area overhead.      </content></document><document><year>2005</year><authors>Yin-Shui Xia1| 2 | Lun-Yao Wang2  | A. E. A. Almaini1 </authors><title>A Novel Multiple-Valued CMOS Flip-Flop Employing Multiple-Valued Clock</title><content>A new CMOS quaternary D flip-flop is implemented employing a multiple-valued clock. PSpice simulation shows that the proposed flip-flop has correct operation. Compared with traditional multiple-valued flip-flops, the proposed multiple-valued CMOS flip-flop is characterized by improved storage capacity, flexible logic structure and reduced power dissipation.</content></document><document><year>2005</year><authors>Cheol Hong Kim1 | Sung Woo Chung2  | Chu Shik Jhon1 </authors><title>A Power-Aware Branch Predictor by Accessing the BTB Selectively      </title><content>Microarchitects should consider power consumption, together with accuracy, when designing a branch predictor, especially in         embedded processors. This paper proposes a power-aware branch predictor, which is based on the gshare predictor, by accessing         the BTB (Branch Target Buffer) selectively. To enable the selective access to the BTB, the PHT (Pattern History Table) in         the proposed branch predictor is accessed one cycle earlier than the traditional PHT if the program is executed sequentially         without branch instructions. As a side effect, two predictions from the PHT are obtained through one access to the PHT, resulting         in more power savings. In the proposed branch predictor, if the previous instruction was not a branch and the prediction from         the PHT is untaken, the BTB is not accessed to reduce power consumption. If the previous instruction was a branch, the BTB         is always accessed, regardless of the prediction from the PHT, to prevent the additional delay/accuracy decrease. The proposed         branch predictor reduces the power consumption with little hardware overhead, not incurring additional delay and never harming         prediction accuracy. The simulation results show that the proposed branch predictor reduces the power consumption by 29&amp;#8211;47%.      </content></document></documents>