<?xml version="1.0" encoding="UTF-8" standalone="no"?><documents><document><year>2004</year><authors>Anil K. Jain1  | Chitra Dorai2 </authors><title>3D object recognition: Representation and matching</title><content>Three-dimensional object recognition entails a number of fundamental problems in computer vision: representation of a 3D object, identification of the object from its image, estimation of its position and orientation, and registration of multiple views of the object for automatic model construction. This paper surveys three of those topics, namely representation, matching, and pose estimation. It also presents an overview of the free-form surface matching problem, and describes COSMOS, our framework for representing and recognizing free-form objects. The COSMOS system recognizes arbitrarily curved 3D rigid objects from a single view using dense surface data. We present both the theoretical aspects and the experimental results of a prototype recognition system based on COSMOS.</content></document><document><year>2004</year><authors>T. Fearn| P. J. Brown | P. Besbeas</authors><title>A Bayesian decision theory approach to variable selection for discrimination</title><content>Motivated by examples in spectroscopy, we study variable selection for discrimination in problems with very many predictor variables. Assuming multivariate normal distributions with common variance for the predictor variables within groups, we develop a Bayesian decision theory approach that balances costs for variables against a loss due to classification errors. The approach is computationally intensive, requiring a simulation to approximate the intractable expected loss and a search, using simulated annealing, over a large space of possible subsets of variables. It is illustrated by application to a spectroscopic example with 3 groups, 100 variables, and 71 training cases, where the approach finds subsets of between 5 and 14 variables whose discriminatory power is comparable with that of linear discriminant analysis using principal components derived from the full 100 variables. We study both the evaluation of expected loss and the tuning of the simulated annealing for the example, and conclude that computational effort should be concentrated on the search.</content></document><document><year>2004</year><authors>Mark J. Brewer1 </authors><title>A Bayesian model for local smoothing in kernel density estimation</title><content>A new procedure is proposed for deriving variable bandwidths in univariate kernel density estimation, based upon likelihood cross-validation and an analysis of a Bayesian graphical model. The procedure admits bandwidth selection which is flexible in terms of the amount of smoothing required. In addition, the basic model can be extended to incorporate local smoothing of the density estimate. The method is shown to perform well in both theoretical and practical situations, and we compare our method with those of Abramson (The Annals of Statistics 10: 1217&amp;#x2013;1223) and Sain and Scott (Journal of the American Statistical Association 91: 1525&amp;#x2013;1534). In particular, we note that in certain cases, the Sain and Scott method performs poorly even with relatively large sample sizes.We compare various bandwidth selection methods using standard mean integrated square error criteria to assess the quality of the density estimates. We study situations where the underlying density is assumed both known and unknown, and note that in practice, our method performs well when sample sizes are small. In addition, we also apply the methods to real data, and again we believe our methods perform at least as well as existing methods.</content></document><document><year>2004</year><authors>Dipak K. Dey1| Lynn Kuo1 | Sujit K. Sahu2</authors><title>A Bayesian predictive approach to determining the number of components in a mixture distribution</title><content>This paper describes a Bayesian approach to mixture modelling and a method based on predictive distribution to determine the number of components in the mixtures. The implementation is done through the use of the Gibbs sampler. The method is described through the mixtures of normal and gamma distributions. Analysis is presented in one simulated and one real data example. The Bayesian results are then compared with the likelihood approach for the two examples.</content></document><document><year>2004</year><authors>Renata Rotondi1 | Silvia Drappo1</authors><title>A clustering method for global optimization based on thekth nearest neighbour</title><content>In this paper we describe a stochastic method for global optimization based on a uniform sampling in the search domain. After a reduction of the sample, computing the distance between the remaining points and using the distribution of the kth nearest neighbour enables clusters of points to be built up, hopefully fitting the regions of attraction of significant local optima; from each of these a local search is started. The properties of the method are analysed, and detailed computational results on standard test functions are provided.</content></document><document><year>2004</year><authors>Silvia Polettini1  | Julian St|er2</authors><title>A comment on A theoretical basis for perturbation methods by Krishnamurty Muralidhar and Rathindra Sarathy</title><content>Without Abstract</content></document><document><year>2004</year><authors>David Maxwell Chickering1  | David Heckerman2 </authors><title>A comparison of scientific and engineering criteria for Bayesian model selection</title><content>Given a set of possible models for variables X and a set of possible parameters for each model, the Bayesian estimate of the probability distribution for X given observed data is obtained by averaging over the possible models and their parameters. An often-used approximation for this estimate is obtained by selecting a single model and averaging over its parameters. The approximation is useful because it is computationally efficient, and because it provides a model that facilitates understanding of the domain. A common criterion for model selection is the posterior probability of the model. Another criterion for model selection, proposed by San Martini and Spezzafari (1984), is the predictive performance of a model for the next observation to be seen. From the standpoint of domain understanding, both criteria are useful, because one identifies the model that is most likely, whereas the other identifies the model that is the best predictor of the next observation. To highlight the difference, we refer to the posterior-probability and alternative criteria as the scientific criterion (SC) and engineering criterion (EC), respectively. When we are interested in predicting the next observation, the model-averaged estimate is at least as good as that produced by EC, which itself is at least as good as the estimate produced by SC. We show experimentally that, for Bayesian-network models containing discrete variables only, the predictive performance of the model average can be significantly better than those of single models selected by either criterion, and that differences between models selected by the two criterion can be substantial.</content></document><document><year>2004</year><authors>K. G. Russell</authors><title>A comparison of six methods of analysing row-column designs with inter-block information</title><content>Six methods of obtaining estimates of treatment effects in a row-column design are considered. Five methods use estimates of inter-row and inter-column variation, and the remaining method is Ordinary Least Squares. Using simulation, these methods are examined to see which are most appropriate for minimising the sum of the squared differences between the estimates of the elementary treatment contrasts and their true values. Recommendations are made of which methods to use.</content></document><document><year>2004</year><authors>Bruce E. Barrett1 | J. Brian Gray1</authors><title>A computational framework for variable selection in multivariate regression</title><content>Stepwise variable selection procedures are computationally inexpensive methods for constructing useful regression models for a single dependent variable. At each step a variable is entered into or deleted from the current model, based on the criterion of minimizing the error sum of squares (SSE). When there is more than one dependent variable, the situation is more complex. In this article we propose variable selection criteria for multivariate regression which generalize the univariate SSE criterion. Specifically, we suggest minimizing some function of the estimated error covariance matrix: the trace, the determinant, or the largest eigenvalue. The computations associated with these criteria may be burdensome. We develop a computational framework based on the use of the SWEEP operator which greatly reduces these calculations for stepwise variable selection in multivariate regression.</content></document><document><year>2004</year><authors>Ruggero Bellio1 | Aless|ra R. Brazzale2 </authors><title>A computer algebra package for approximate conditional inference</title><content>This paper presents a set of REDUCE procedures that make a number of existing higher-order asymptotic results available for both theoretical and practical research. Attention has been restricted to the context of exact and approximate inference for a parameter of interest conditionally either on an ancillary statistic or on a statistic partially sufficient for the nuisance parameter. In particular, the procedures apply to regression-scale models and multiparameter exponential families. Most of them support algebraic computation as well as numerical calculation for a given data set. Examples illustrate the code.</content></document><document><year>2004</year><authors>A. N. Pettitt| I. S. Weir | A. G. Hart</authors><title>A Conditional Autoregressive Gaussian Process for Irregularly Spaced Multivariate Data with Application to Modelling Large Sets of Binary Data</title><content>A Gaussian conditional autoregressive (CAR) formulation is presented that permits the modelling of the spatial dependence and the dependence between multivariate random variables at irregularly spaced sites so capturing some of the modelling advantages of the geostatistical approach. The model benefits not only from the explicit availability of the full conditionals but also from the computational simplicity of the precision matrix determinant calculation using a closed form expression involving the eigenvalues of a precision matrix submatrix. The introduction of covariates into the model adds little computational complexity to the analysis and thus the method can be straightforwardly extended to regression models. The model, because of its computational simplicity, is well suited to application involving the fully Bayesian analysis of large data sets involving multivariate measurements with a spatial ordering. An extension to spatio-temporal data is also considered. Here, we demonstrate use of the model in the analysis of bivariate binary data where the observed data is modelled as the sign of the hidden CAR process. A case study involving over 450 irregularly spaced sites and the presence or absence of each of two species of rain forest trees at each site is presented; Markov chain Monte Carlo (MCMC) methods are implemented to obtain posterior distributions of all unknowns. The MCMC method works well with simulated data and the tree biodiversity data set.</content></document><document><year>2004</year><authors>B&amp;auml rbel F. Finkenst&amp;auml dt1| Qiwei Yao2 | Howell Tong2| 3</authors><title>A Conditional Density Approach to the Order Determination of Time Series</title><content>The study focuses on the selection of the order of a general time series process via the conditional density of the latter, a characteristic of which is that it remains constant for every order beyond the true one. Using simulated time series from various nonlinear models we illustrate how this feature can be traced from conditional density estimation. We study whether two statistics derived from the likelihood function can serve as univariate statistics to determine the order of the process. It is found that a weighted version of the log likelihood function has desirable robust properties in detecting the order of the process.</content></document><document><year>2004</year><authors>Murray Jorgensen</authors><title>A dynamic EM algorithm for estimating mixture proportions</title><content>We investigate the use of a dynamic form of the EM algorithm to estimate proportions in finite mixtures of known distributions. We prove a consistency result for this algorithm, which employs only a single EM update for each new observation. Our aim is to demonstrate that the slow convergence rate of the EM algorithm in many applications is of little practical consequence in a situation when data is frequently being updated.</content></document><document><year>2004</year><authors>FRANCESCO MOLA1  | ROBERTA SICILIANO1 </authors><title>A fast splitting procedure for classification trees</title><content>This paper provides a faster method to find the best split at each node when using the CART methodology. The predictability index  is proposed as a splitting rule for growing the same classification tree as CART does when using the Gini index of heterogeneity as an impurity measure. A theorem is introduced to show a new property of the index : the  for a given predictor has a value not lower than the  for any split generated by the predictor. This property is used to make a substantial saving in the time required to generate a classification tree. Three simulation studies are presented in order to show the computational gain in terms of both the number of splits analysed at each node and the CPU time. The proposed splitting algorithm can prove computational efficiency in real data sets as shown in an example.</content></document><document><year>2004</year><authors>Murray Aitkin | Roberto Rocci</authors><title>A general maximum likelihood analysis of measurement error in generalized linear models</title><content>This paper describes an EM algorithm for maximum likelihood estimation in generalized linear models (GLMs) with continuous measurement error in the explanatory variables. The algorithm is an adaptation of that for nonparametric maximum likelihood (NPML) estimation in overdispersed GLMs described in Aitkin (Statistics and Computing 6: 251&amp;#x2013;262, 1996). The measurement error distribution can be of any specified form, though the implementation described assumes normal measurement error. Neither the reliability nor the distribution of the true score of the variables with measurement error has to be known, nor are instrumental variables or replication required.Standard errors can be obtained by omitting individual variables from the model, as in Aitkin (1996).</content></document><document><year>2004</year><authors>Murray Aitkin1</authors><title>A general maximum likelihood analysis of overdispersion in generalized linear models</title><content>This paper presents an EM algorithm for maximum likelihood estimation in generalized linear models with overdispersion. The algorithm is initially derived as a form of Gaussian quadrature assuming a normal mixing distribution, but with only slight variation it can be used for a completely unknown mixing distribution, giving a straightforward method for the fully non-parametric ML estimation of this distribution. This is of value because the ML estimates of the GLM parameters may be sensitive to the specification of a parametric form for the mixing distribution. A listing of a GLIM4 algorithm for fitting the overdispersed binomial logit model is given in an appendix.A simple method is given for obtaining correct standard errors for parameter estimates when using the EM algorithm.</content></document><document><year>2004</year><authors>Darrell Whitley1</authors><title>A genetic algorithm tutorial</title><content>This tutorial covers the canonical genetic algorithm as well as more experimental forms of genetic algorithms, including parallel island models and parallel cellular genetic algorithms. The tutorial also illustrates genetic search by hyperplane sampling. The theoretical foundations of genetic algorithms are reviewed, include the schema theorem as well as recently developed exact models of the canonical genetic algorithm.</content></document><document><year>2004</year><authors>PAUL GUSTAFSON1</authors><title>A guided walk Metropolis algorithm</title><content>The random walk Metropolis algorithm is a simple Markov chain Monte Carlo scheme which is frequently used in Bayesian statistical problems. We propose a guided walk Metropolis algorithm which suppresses some of the random walk behavior in the Markov chain. This alternative algorithm is no harder to implement than the random walk Metropolis algorithm, but empirical studies show that it performs better in terms of efficiency and convergence time.</content></document><document><year>2004</year><authors>Stephen M. S. Lee | Irene O. L. Wong</authors><title>A hybrid approach based on saddlepoint and importance sampling methods for bootstrap tail probability estimation</title><content>We propose a simple hybrid method which makes use of both saddlepoint and importance sampling techniques to approximate the bootstrap tail probability of an M-estimator. The method does not rely on explicit formula of the Lugannani-Rice type, and is computationally more efficient than both uniform bootstrap sampling and importance resampling suggested in earlier literature. The method is also applied to construct confidence intervals for smooth functions of M-estimands.</content></document><document><year>2004</year><authors>Murray Aitkin1| 2 | Irit Aitkin3</authors><title>A hybrid EM/Gauss-Newton algorithm for maximum likelihood in mixture distributions</title><content>A faster alternative to the EM algorithm in finite mixture distributions is described, which alternates EM iterations with Gauss-Newton iterations using the observed information matrix. At the expense of modest additional analytical effort in obtaining the observed information, the hybrid algorithm reduces the computing time required and provides asymptotic standard errors at convergence. The algorithm is illustrated on the two-component normal mixture.</content></document><document><year>2004</year><authors>Agostino Nobile1</authors><title>A hybrid Markov chain for the Bayesian analysis of the multinomial probit model</title><content>Bayesian inference for the multinomial probit model, using the Gibbs sampler with data augmentation, has been recently considered by some authors. The present paper introduces a modification of the sampling technique, by defining a hybrid Markov chain in which, after each Gibbs sampling cycle, a Metropolis step is carried out along a direction of constant likelihood. Examples with simulated data sets motivate and illustrate the new technique. A proof of the ergodicity of the hybrid Markov chain is also given.</content></document><document><year>2004</year><authors>F. M. Malvestuto1 </authors><title>A hypergraph-theoretic analysis of collapsibility and decomposability for extended log-linear models</title><content>Extended log-linear models (ELMs) are the natural generalization of log-linear models when the positivity assumption is relaxed. The hypergraph language, which is currently used to specify the syntax of ELMs, both provides an insight into key notions of the theory of ELMs such as collapsibility and decomposability, and allows to work out efficient algorithms to solve some problems of inference. This is the case for the three search problems addressed in this paper and referred to as the approximation problem, the selective-reduction problem and the synthesis problem. The approximation problem consists in finding the smallest decomposable ELM that contains a given ELM and is such that the given ELM is collapsible onto each of its generators. The selective-reduction problem consists in deleting the maximum number of generators of a given ELM in such a way that the resulting ELM is a submodel and none of certain variables of interest is missing. The synthesis problem consists in finding a minimal ELM containing the intersection of ELMs specified by given independence relations. We show that each of the three search problems above can be reduced to an equivalent search problem on hypergraphs, which can be solved in polynomial time.</content></document><document><year>2004</year><authors>K. A. Froeschl1</authors><title>A metadata approach to statistical query processing</title><content>Concerning the task of integrating census and survey data from different sources as it is carried out by supranational statistical agencies, a formal metadata approach is investigated which supports data integration and table processing simultaneously. To this end, a metadata model is devised such that statistical query processing is accomplished by means of symbolic reasoning on machine-readable, operative metadata. As in databases, statistical queries are stated as formal expressions specifying declaratively what the intended output is; the operations necessary to retrieve appropriate available source data and to aggregate source data into the requested macrodata are derived mechanically. Using simple mathematics, this paper focuses particularly on the metadata model devised to harmonize semantically related data sources as well as the table model providing the principal data structure of the proposed system. Only an outline of the general design of a statistical information system based on the proposed metadata model is given and the state of development is summarized briefly.</content></document><document><year>2004</year><authors>W. James Gauderman1</authors><title>A method for simulating familial disease data with variable age at onset and genetic and environmental effects</title><content>The field of genetic epidemiology is growing rapidly with the realization that many important diseases are influenced by both genetic and environmental factors. For this reason, pedigree data are becoming increasingly valuable as a means of studying patterns of disease occurrence. Analysis of pedigree data is complicated by the lack of independence among family members and by the non-random sampling schemes used to ascertain families. An additional complicating factor is the variability in age at disease onset from one person to another. In developing statistical methods for analysing pedigree data, analytic results are often intractable, making simulation studies imperative for assessing the performance of proposed methods and estimators. In this paper, an algorithm is presented for simulating disease data in pedigrees, incorporating variable age at onset and genetic and environmental effects. Computational formulas are developed in the context of a proportional hazards model and assuming single ascertainment of families, but the methods can be easily generalized to alternative models. The algorithm is computationally efficient, making multi-dataset simulation studies feasible. Numerical examples are provided to demonstrate the methods.</content></document><document><year>2004</year><authors>Murray Aitkin1| 2| Steve Finch3| 2| Nancy Mendell3| 2 | Henry Thode4| 2</authors><title>A new test for the presence of a normal mixture distribution based on the posterior Bayes factor</title><content>We present a new test for the presence of a normal mixture distribution, based on the posterior Bayes factor of Aitkin (1991). The new test has slightly lower power than the likelihood ratio test. It does not require the computation of the MLEs of the parameters or a search for multiple maxima, but requires computations based on classification likelihood assignments of observations to mixture components.</content></document><document><year>2004</year><authors>Alun Thomas1</authors><title>A note on the four-colourability of pedigrees and its consequences for probability calculations</title><content>A graph representing the probabilistic dependences between genotypes of individuals in a pedigree is shown to be four-colourable. The consequences of this for computation of probabilities, using both exact and Markov chain Monte Carlo methods, is discussed. A four-colouring and triangulation of an inbred section of a pedigree of Red-Crowned Cranes is illustrated.</content></document><document><year>2004</year><authors>Bill Shipley1 </authors><title>A permutation procedure for testing the equality of pattern hypotheses across groups involving correlation or covariance matrices</title><content>This paper describes a permutation procedure to test for the equality of selected elements of a covariance or correlation matrix across groups. It involves either centring or standardising each variable within each group before randomly permuting observations between groups. Since the assumption of exchangeability of observations between groups does not strictly hold following such transformations, Monte Carlo simulations were used to compare expected and empirical rejection levels as a function of group size, the number of groups and distribution type (Normal, mixtures of Normals and Gamma with various values of the shape parameter). The Monte Carlo study showed that the estimated probability levels are close to those that would be obtained with an exact test except at very small sample sizes (5 or 10 observations per group). The test appears robust against non-normal data, different numbers of groups or variables per group and unequal sample sizes per group. Power was increased with increasing sample size, effect size and the number of elements in the matrix and power was decreased with increasingly unequal numbers of observations per group.</content></document><document><year>2004</year><authors>Fern|o Tusell1 </authors><title>A permutation test for randomness with power against smooth variation</title><content>A permutation test for the white noise hypothesis is described, offering power against a general class of smooth alternatives. Simulation results show that it performs well, as compared with similar tests available in the literature, in terms of power. An example demonstrates its use in a particular problem in which a test for randomness was sought without any specific alternative.</content></document><document><year>2004</year><authors>Krishnamurty Muralidhar1  | Rathindra Sarathy1 </authors><title>A rejoinder to the comments by Polettini and Stander</title><content>Without Abstract</content></document><document><year>2004</year><authors>Silvia Golia1  | Marco S|ri2 </authors><title>A Resampling Algorithm for Chaotic Time Series</title><content>In the field of chaotic time series analysis, there is a lack of a distributional theory for the main quantities used to characterize the underlying data generating process (DGP). In this paper a method for resampling time series generated by a chaotic dynamical system is proposed. The basic idea is to develop an algorithm for building trajectories which lie on the same attractor of the true DGP, that is with the same dynamical and geometrical properties of the original data. We performed some numerical experiments on some short noise-free and high-noise series confirming that we are able to correctly reproduce the distribution of the largest finite-time Lyapunov exponent and of the correlation dimension.</content></document><document><year>2004</year><authors>N. M. Adams1| S. P. J. Kirby1| P. Harris1 | D. B. Clegg1</authors><title>A review of parallel processing for statistical computation</title><content>Parallel computers differ from conventional serial computers in that they can, in a variety of ways, perform more than one operation at a time. Parallel processing, the application of parallel computers, has been successfully utilized in many fields of science and technology. The purpose of this paper is to review efforts to use parallel processing for statistical computing. We present some technical background, followed by a review of the literature that relates parallel computing to statistics. The review material focuses explicitly on statistical methods and applications, rather than on conventional mathematical techniques. Thus, most of the review material is drawn from statistics publications. We conclude by discussing the nature of the review material and considering some possibilities for the future.</content></document><document><year>2004</year><authors>TREVOR F. COX1 | KIM F. PEARCE1</authors><title>A robust logistic discrimination model</title><content>Logistic discrimination is a well documented method for classifying observations to two or more groups. However, estimation of the discriminant rule can be seriously affected by outliers. To overcome this, Cox and Ferry produced a robust logistic discrimination technique. Although their method worked in practice, parameter estimation was sometimes prone to convergence problems. This paper proposes a simplified robust logistic model which does not have any such problems and which takes a generalized linear model form. Misclassification rates calculated in a simulation exercise are used to compare the new method with ordinary logistic discrimination. Model diagnostics are also presented. The newly proposed model is then used on data collected from pregnant women at two district general hospitals. A robust logistic discriminant is calculated which can be used to predict accurately which method of feeding a woman will eventually use: breast feeding or bottle feeding.</content></document><document><year>2004</year><authors>Morgan C. Wang1 | William J. Kennedy2</authors><title>A self-validating numerical method for computation of central and non-central F probabilities and percentiles</title><content>A self-validating numerical method based on interval analysis for the computation of central and non-central F probabilities and percentiles is reported. The major advantage of this approach is that there are guaranteed error bounds associated with the computed values (or intervals), i.e. the computed values satisfy the user-specified accuracy requirements. The methodology reported in this paper can be adapted to approximate the probabilities and percentiles for other commonly used distribution functions.</content></document><document><year>2004</year><authors>R. A. Rigby1 | D. M. Stasinopoulos1</authors><title>A semi-parametric additive model for variance heterogeneity</title><content>This paper presents a flexible model for variance heterogeneity in a normal error model. Specifically, both the mean and variance are modelled using semi-parametric additive models. We call this model a Mean And Dispersion Additive Model (MADAM). A successive relaxation algorithm for fitting the model is described and justified as maximizing a penalized likelihood function with penalties for lack of smoothness in the additive non-parametric functions in both mean and variance models. The algorithm is implemented in GLIM4, allowing flexible and interactive modelling of variance heterogeneity. Two data sets are used for demonstration.</content></document><document><year>2004</year><authors>Mark J. Dixon1 | Jonathan A. Tawn1</authors><title>A semi-parametric model for multivariate extreme values</title><content>Threshold methods for multivariate extreme values are based on the use of asymptotically justified approximations of both the marginal distributions and the dependence structure in the joint tail. Models derived from these approximations are fitted to a region of the observed joint tail which is determined by suitably chosen high thresholds. A drawback of the existing methods is the necessity for the same thresholds to be taken for the convergence of both marginal and dependence aspects, which can result in inefficient estimation. In this paper an extension of the existing models, which removes this constraint, is proposed. The resulting model is semi-parametric and requires computationally intensive techniques for likelihood evaluation. The methods are illustrated using a coastal engineering application.</content></document><document><year>2004</year><authors>Alan D. Hutson</authors><title>A Semi-Parametric Quantile Function Estimator for Use in Bootstrap Estimation Procedures</title><content>In this note we develop a new quantile function estimator called the tail extrapolation quantile function estimator. The estimator behaves asymptotically exactly the same as the standard linear interpolation estimator. For finite samples there is small correction towards estimating the extreme quantiles. We illustrate that by employing this new estimator we can greatly improve the coverage probabilities of the standard bootstrap percentile confidence intervals. The method does not reqiure complicated calculations and hence it should appeal to the statistical practitioner.</content></document><document><year>2004</year><authors>Claus Skaanning Jensen1</authors><title>A simple method for finding a legal configuration in complex Bayesian networks</title><content>This paper deals with an important problem with large and complex Bayesian networks. Exact inference in these networks is simply not feasible owing to the huge storage requirements of exact methods. Markov chain Monte Carlo methods, however, are able to deal with these large networks but to do this they require an initial legal configuration to set off the sampler. So far nondeterministic methods such as forward sampling have often been used for this, even though the forward sampler may take an eternity to come up with a legal configuration. In this paper a novel algorithm will be presented that allows a legal configuration in a general Bayesian network to be found in polynomial time in almost all cases. The algorithm will not be proved deterministic but empirical results will demonstrate that this holds in most cases. Also, the algorithm will be justified by its simplicity and ease of implementation.</content></document><document><year>2004</year><authors>Jeffrey S. Simonoff1</authors><title>A simple, automatic and adaptive bivariate density estimator based on conditional densities</title><content>The standard approach to non-parametric bivariate density estimation is to use a kernel density estimator. Practical performance of this estimator is hindered by the fact that the estimator is not adaptive (in the sense that the level of smoothing is not sensitive to local properties of the density). In this paper a simple, automatic and adaptive bivariate density estimator is proposed based on the estimation of marginal and conditional densities. Asymptotic properties of the estimator are examined, and guidance to practical application of the method is given. Application to two examples illustrates the usefulness of the estimator as an exploratory tool, particularly in situations where the local behaviour of the density varies widely. The proposed estimator is also appropriate for use as a pilot estimate for an adaptive kernel estimate, since it is relatively inexpensive to calculate.</content></document><document><year>2004</year><authors>M. Lavielle1  | E. Moulines2</authors><title>A simulated annealing version of the EM algorithm for non-Gaussian deconvolution</title><content>The Expectation&amp;#x2013;Maximization (EM) algorithm is a very popular technique for maximum likelihood estimation in incomplete data models. When the expectation step cannot be performed in closed form, a stochastic approximation of EM (SAEM) can be used. Under very general conditions, the authors have shown that the attractive stationary points of the SAEM algorithm correspond to the global and local maxima of the observed likelihood. In order to avoid convergence towards a local maxima, a simulated annealing version of SAEM is proposed. An illustrative application to the convolution model for estimating the coefficients of the filter is given.</content></document><document><year>2004</year><authors>MARY KATHRYN COWLES1 | JEFFREY S. ROSENTHAL2</authors><title>A simulation approach to convergence rates for Markov chain Monte Carlo algorithms</title><content>Markov chain Monte Carlo (MCMC) methods, including the Gibbs sampler and the Metropolis&amp;#x2013;Hastings algorithm, are very commonly used in Bayesian statistics for sampling from complicated, high-dimensional posterior distributions. A continuing source of uncertainty is how long such a sampler must be run in order to converge approximately to its target stationary distribution. A method has previously been developed to compute rigorous theoretical upper bounds on the number of iterations required to achieve a specified degree of convergence in total variation distance by verifying drift and minorization conditions. We propose the use of auxiliary simulations to estimate the numerical values needed in this theorem. Our simulation method makes it possible to compute quantitative convergence bounds for models for which the requisite analytical computations would be prohibitively difficult or impossible. On the other hand, although our method appears to perform well in our example problems, it cannot provide the guarantees offered by analytical proof.</content></document><document><year>2004</year><authors>David J. Allcroft | Chris A. Glasbey</authors><title>A Spectral Estimator of Arma Parameters from Thresholded Data</title><content>We consider computationally-fast methods for estimating parameters in ARMA processes from binary time series data, obtained by thresholding the latent ARMA process. All methods involve matching estimated and expected autocorrelations of the binary series. In particular, we focus on the spectral representation of the likelihood of an ARMA process and derive a restricted form of this likelihood, which uses correlations at only the first few lags. We contrast these methods with an efficient but computationally-intensive Markov chain Monte Carlo (MCMC) method. In a simulation study we show that, for a range of ARMA processes, the spectral method is more efficient than variants of least squares and much faster than MCMC. We illustrate by fitting an ARMA(2,1) model to a binary time series of cow feeding data.</content></document><document><year>2004</year><authors>W. J. Krzanowski1</authors><title>A stopping rule for structure-preserving variable selection</title><content>A stopping rule is provided for the backward elimination process suggested by Krzanowski (1987a) for selecting variables to preserve data structure. The stopping rule is based on perturbation theory for Procrustes statistics, and a small simulation study verifies its suitability. Some illustrative examples are also provided and discussed.</content></document><document><year>2004</year><authors>Merrilee Hurn1 | Christopher Jennison1</authors><title>A study of simulated annealing and a revised cascade algorithm for image reconstruction</title><content>We describe an image reconstruction problem and the computational difficulties arising in determining the maximum a posteriori (MAP) estimate. Two algorithms for tackling the problem, iterated conditional modes (ICM) and simulated annealing, are usually applied pixel by pixel. The performance of this strategy can be poor, particularly for heavily degraded images, and as a potential improvement Jubb and Jennison (1991) suggest the cascade algorithm in which ICM is initially applied to coarser images formed by blocking squares of pixels. In this paper we attempt to resolve certain criticisms of cascade and present a version of the algorithm extended in definition and implementation. As an illustration we apply our new method to a synthetic aperture radar (SAR) image. We also carry out a study of simulated annealing, with and without cascade, applied to a more tractable minimization problem from which we gain insight into the properties of cascade algorithms.</content></document><document><year>2004</year><authors>Krishnamurty Muralidhar1  | Rathindra Sarathy2 </authors><title>A theoretical basis for perturbation methods</title><content>In this paper we discuss a new theoretical basis for perturbation methods. In developing this new theoretical basis, we define the ideal measures of data utility and disclosure risk. Maximum data utility is achieved when the statistical characteristics of the perturbed data are the same as that of the original data. Disclosure risk is minimized if providing users with microdata access does not result in any additional information. We show that when the perturbed values of the confidential variables are generated as independent realizations from the distribution of the confidential variables conditioned on the non-confidential variables, they satisfy the data utility and disclosure risk requirements. We also discuss the relationship between the theoretical basis and some commonly used methods for generating perturbed values of confidential numerical variables.</content></document><document><year>2004</year><authors>Alex J. Smola1  | Bernhard Sch&amp;ouml lkopf2 </authors><title>A tutorial on support vector regression</title><content>In this tutorial we give an overview of the basic ideas underlying Support Vector (SV) machines for function estimation. Furthermore, we include a summary of currently used algorithms for training SV machines, covering both the quadratic (or convex) programming part and advanced methods for dealing with large datasets. Finally, we mention some modifications and extensions that have been applied to the standard SV algorithm, and discuss the aspect of regularization from a SV perspective.</content></document><document><year>2004</year><authors>Mary Kathryn Cowles1</authors><title>Accelerating Monte Carlo Markov chain convergence for cumulative-link generalized linear models</title><content>The ordinal probit, univariate or multivariate, is a generalized linear model (GLM) structure that arises frequently in such disparate areas of statistical applications as medicine and econometrics. Despite the straightforwardness of its implementation using the Gibbs sampler, the ordinal probit may present challenges in obtaining satisfactory convergence.We present a multivariate Hastings-within-Gibbs update step for generating latent data and bin boundary parameters jointly, instead of individually from their respective full conditionals. When the latent data are parameters of interest, this algorithm substantially improves Gibbs sampler convergence for large datasets. We also discuss Monte Carlo Markov chain (MCMC) implementation of cumulative logit (proportional odds) and cumulative complementary log-log (proportional hazards) models with latent data.</content></document><document><year>2004</year><authors>Dankmar B&amp;ouml hning1</authors><title>Acceleration techniques in fixed-point methods for finding percentage points</title><content>This contribution discusses various acceleration techniques of fixed-point methods for iteratively finding percentage points of a given distribution function. Recently, Farnum (1991) suggests a particular fixed-point method for solving this problem. In this paper, his method is discussed and some disadvantages are highlighted. Alternatives are suggested and discussed. In addition, methodology is developed which transforms a linearly convergent fixed point sequence into one which is converging quadratically. This includes the discussion of various forms of Aitken acceleration as well as a parametric form of acceleration.</content></document><document><year>2004</year><authors>A. C. Davison1| D. V. Hinkley2 | B. J. Worton3</authors><title>Accurate and efficient construction of bootstrap likelihoods</title><content>The simplest construction of bootstrap likelihoods involves two levels of bootstrapping, kernel density estimation, and non-parametric curve-smoothing. We describe more accurate and efficient constructions, based on smoothing at the first level of nested bootstraps and saddlepoint approximation to remove second-level bootstrap variation. Detailed illustrations are given.</content></document><document><year>2004</year><authors>Andrey Feuerverger1  | Jeffrey S. Rosenthal1 </authors><title>Achieving limiting distributions for Markov chains using back buttons</title><content>As a simple model for browsing the World Wide Web, we consider Markov chains with the option of moving back to the previous state. We develop an algorithm which uses back buttons to achieve essentially any limiting distribution on the state space. This corresponds to spending the desired total fraction of time at each web page. On finite state spaces, our algorithm always succeeds. On infinite state spaces the situation is more complicated, and is related to both the tail behaviour of the distributions, and the properties of convolution equations.</content></document><document><year>2004</year><authors>Umberto Amato1  | Anestis Antoniadis2 </authors><title>Adaptive wavelet series estimation in separable nonparametric regression models</title><content>It is well-known that multivariate curve estimation suffers from the curse of dimensionality. However, reasonable estimators are possible, even in several dimensions, under appropriate restrictions on the complexity of the curve. In the present paper we explore how much appropriate wavelet estimators can exploit a typical restriction on the curve such as additivity. We first propose an adaptive and simultaneous estimation procedure for all additive components in additive regression models and discuss rate of convergence results and data-dependent truncation rules for wavelet series estimators. To speed up computation we then introduce a wavelet version of functional ANOVA algorithm for additive regression models and propose a regularization algorithm which guarantees an adaptive solution to the multivariate estimation problem. Some simulations indicate that wavelets methods complement nicely the existing methodology for nonparametric multivariate curve estimation.</content></document><document><year>2004</year><authors>Jean Thioulouse1| Daniel Chessel1| Sylvain Dole&amp;acute dec | Jean-Michel Olivier1</authors><title>ADE-4: a multivariate analysis and graphical display software</title><content>We present ADE-4, a multivariate analysis and graphical display software. Multivariate analysis methods available in ADE-4 include usual one-table methods like principal component analysis and correspondence analysis, spatial data analysis methods (using a total variance decomposition into local and global components, analogous to Moran and Geary indices), discriminant analysis and within/between groups analyses, many linear regression methods including lowess and polynomial regression, multiple and PLS (partial least squares) regression and orthogonal regression (principal component regression), projection methods like principal component analysis on instrumental variables, canonical correspondence analysis and many other variants, coinertia analysis and the RLQ method, and several three-way table (k-table) analysis methods. Graphical display techniques include an automatic collection of elementary graphics corresponding to groups of rows or to columns in the data table, thus providing a very efficient way for automatic k-table graphics and geographical mapping options. A dynamic graphic module allows interactive operations like searching, zooming, selection of points, and display of data values on factor maps. The user interface is simple and homogeneous among all the programs; this contributes to making the use of ADE-4 very easy for non- specialists in statistics, data analysis or computer science.</content></document><document><year>2004</year><authors>Maurizio Rafanelli1</authors><title>Aggregate statistical data: models for their representation</title><content>The paper gives a review of a number of data models for aggregate statistical data which have appeared in the computer science literature in the last ten years.After a brief introduction to the data model in general, the fundamental concepts of statistical data are introduced. These are called statistical objects because they are complex data structures (vectors, matrices, relations, time series, etc) which may have different possible representations (e.g. tables, relations, vectors, pie-charts, bar-charts, graphs, and so on). For this reason a statistical object is defined by two different types of attribute (a summary attribute, with its own summary type and with its own instances, called summary data, and the set of category attributes, which describe the summary attribute). Some conceptual models of statistical data (CSM, SDM4S), some semantic models of statistical data (SCM, SAM*, OSAM*), and some graphical models of statistical data (SUBJECT, GRASS, STORM) are also discussed.</content></document><document><year>2004</year><authors>I. H. Dinwoodie</authors><title>Algebraic Methods for Polynomial Statistical Models</title><content>We describe applications of computational algebra to statistical problems of parameter identifiability, sufficiency, and estimation. The methods work for a family of statistical models that includes Poisson and binomial examples in network tomography.</content></document><document><year>2004</year><authors>John E. Kolassa1</authors><title>Algorithms for approximate conditional inference</title><content>This paper presents a method for listing the sample space for a conditional distribution in a discrete generalized linear model. This tabulation is used in conjunction with saddlepoint methods to approximate the associated conditional probabilities. These probabilities are used to calculate conditional p-values.</content></document><document><year>2004</year><authors>Stephen Rowe1</authors><title>An algorithm for computing principal points with respect to a loss function in the unidimensional case</title><content>In Flury (1990) the k principal points of a random vector X are defned as the points p(1),..., p(k) minimizing EX&amp;#x2013;p(i)2; i=1,..., k. We extend this concept to that of k principal points with respect to a loss function L, and present an algorithm for their computation in the univariate case.</content></document><document><year>2004</year><authors>L. J. Elliott | J. A. Eccleston  | R. J. Martin </authors><title>An algorithm for the design of factorial experiments when the data are correlated</title><content>Since the development of methods for the analysis of experiments with dependent data, see for example Gleeson and Cullis (1987), the design of such experiments has been an area of active research. We investigate the design of factorial experiments, complete and fractional, for various dependency structures. An algorithm for generating optimal or near optimal designs is presented and shown to be useful across a wide range of dependency structures.</content></document><document><year>2004</year><authors>Murray Aitkin1 | Camil Fuchs1</authors><title>An analysis of models for the dilution and adulteration of fruit juice</title><content>We present models based on the multivariate normal distribution to represent the process of dilution and adulteration of citrius juice. The models specify a common dilution parameter for those components of the juice which are affected by dilution but not adulteration.Statistical testing of the hypothesis of no dilution or adulteration presents theoretical difficulties. These difficulties are resolved by model comparisons based on averaged, rather than maximized, likelihoods.</content></document><document><year>2004</year><authors>Andrew R. Webb1</authors><title>An approach to non-linear principal components analysis using radially symmetric kernel functions</title><content>An approach to non-linear principal components using radially symmetric kernel basis functions is described. The procedure consists of two steps: a projection of the data set to a reduced dimension using a non-linear transformation whose parameters are determined by the solution of a generalized symmetric eigenvector equation. This is achieved by demanding a maximum variance transformation subject to a normalization condition (Hotelling's approach) and can be related to the homogeneity analysis approach of Gifi through the minimization of a loss function. The transformed variables are the principal components whose values define contours, or more generally hypersurfaces, in the data space. The second stage of the procedure defines the fitting surface, the principal surface, in the data space (again as a weighted sum of kernel basis functions) using the definition of self-consistency of Hastie and Stuetzle. The parameters of this principal surface are determined by a singular value decomposition and crossvalidation is used to obtain the kernel bandwidths. The approach is assessed on four data sets.</content></document><document><year>2004</year><authors>D. NILSSON1 </authors><title>An efficient algorithm for finding the M most probable configurationsin probabilistic expert systems</title><content>A probabilistic expert system provides a graphical representation of a joint probability distribution which enables local computations of probabilities. Dawid (1992) provided a flow- propagation algorithm for finding the most probable configuration of the joint distribution in such a system. This paper analyses that algorithm in detail, and shows how it can be combined with a clever partitioning scheme to formulate an efficient method for finding the M most probable configurations. The algorithm is a divide and conquer technique, that iteratively identifies the M most probable configurations.</content></document><document><year>2004</year><authors>G. R. Oskrochi1 | R. B. Davies1</authors><title>An EM-type algorithm for multivariate mixture models</title><content>This paper introduces a new approach, based on dependent univariate GLMs, for fitting multivariate mixture models. This approach is a multivariate generalization of the method for univariate mixtures presented by Hinde (1982). Its accuracy and efficiency are compared with direct maximization of the log-likelihood. Using a simulation study, we also compare the efficiency of Monte Carlo and Gaussian quadrature methods for approximating the mixture distribution. The new approach with Gaussian quadrature outperforms the alternative methods considered. The work is motivated by the multivariate mixture models which have been proposed for modelling changes of employment states at an individual level. Similar formulations are of interest for modelling movement between other social and economic states and multivariate mixture models also occur in biostatistics and epidemiology.</content></document><document><year>2004</year><authors>Jos&amp;eacute  G. Dias1  | Michel Wedel2</authors><title>An empirical comparison of EM, SEM and MCMC performance for problematic Gaussian mixture likelihoods</title><content>We compare EM, SEM, and MCMC algorithms to estimate the parameters of the Gaussian mixture model. We focus on problems in estimation arising from the likelihood function having a sharp ridge or saddle points. We use both synthetic and empirical data with those features. The comparison includes Bayesian approaches with different prior specifications and various procedures to deal with label switching. Although the solutions provided by these stochastic algorithms are more often degenerate, we conclude that SEM and MCMC may display faster convergence and improve the ability to locate the global maximum of the likelihood function.</content></document><document><year>2004</year><authors>Joan G. Staniswalis1 | Peter F. Thall2</authors><title>An explanation of generalized profile likelihoods</title><content>Let X, T, Y be random vectors such that the distribution of Y conditional on covariates partitioned into the vectors X = x and T = t is given by f(y; x, ), where  = (, (t)). Here  is a parameter vector and (t) is a smooth, real&amp;#x2013;valued function of t. The joint distribution of X and T is assumed to be independent of  and . This semiparametric model is called conditionally parametric because the conditional distribution f(y; x, ) of Y given X = x, T = t is parameterized by a finite dimensional parameter  = (, (t)). Severini and Wong (1992. Annals of Statistics 20: 1768&amp;#x2013;1802) show how to estimate  and (&amp;middot;) using generalized profile likelihoods, and they also provide a review of the literature on generalized profile likelihoods. Under specified regularity conditions, they derive an asymptotically efficient estimator of  and a uniformly consistent estimator of (&amp;middot;). The purpose of this paper is to provide a short tutorial for this method of estimation under a likelihood&amp;#x2013;based model, reviewing results from Stein (1956. Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, vol. 1, University of California Press, Berkeley, pp. 187&amp;#x2013;196), Severini (1987. Ph.D Thesis, The University of Chicago, Department of Statistics, Chicago, Illinois), and Severini and Wong (op. cit.).</content></document><document><year>2004</year><authors>Leonie Burgess | Deborah J. Street</authors><title>An interchange algorithm for four factor orthogonal main effect plans</title><content>In this paper we give a construction for four factor orthogonal main effect plans (OMEPs) and an interchange algorithm to give four factor OMEPs with various different numbers of repeated runs.</content></document><document><year>2004</year><authors>Hong-Tu Zhu | Sik-Yum Lee</authors><title>Analysis of generalized linear mixed models via a stochastic approximation algorithm with Markov chain Monte-Carlo method</title><content>In recent years much effort has been devoted to maximum likelihood estimation of generalized linear mixed models. Most of the existing methods use the EM algorithm, with various techniques in handling the intractable E-step. In this paper, a new implementation of a stochastic approximation algorithm with Markov chain Monte Carlo method is investigated. The proposed algorithm is computationally straightforward and its convergence is guaranteed. A simulation and three real data sets, including the challenging salamander data, are used to illustrate the procedure and to compare it with some existing methods. The results indicate that the proposed algorithm is an attractive alternative for problems with a large number of random effects or with high dimensional intractable integrals in the likelihood function.</content></document><document><year>2004</year><authors>Jeffrey S. Rosenthal1</authors><title>Analysis of the Gibbs sampler for a model related to James-Stein estimators</title><content>We analyse a hierarchical Bayes model which is related to the usual empirical Bayes formulation of James-Stein estimators. We consider running a Gibbs sampler on this model. Using previous results about convergence rates of Markov chains, we provide rigorous, numerical, reasonable bounds on the running time of the Gibbs sampler, for a suitable range of prior distributions. We apply these results to baseball data from Efron and Morris (1975). For a different range of prior distributions, we prove that the Gibbs sampler will fail to converge, and use this information to prove that in this case the associated posterior distribution is non-normalizable.</content></document><document><year>2004</year><authors>Radford M. Neal1 </authors><title>Annealed importance sampling</title><content>Simulated annealing&amp;#x2014;moving from a tractable distribution to a distribution of interest via a sequence of intermediate distributions&amp;#x2014;has traditionally been used as an inexact method of handling isolated modes in Markov chain samplers. Here, it is shown how one can use the Markov chain transitions for such an annealing sequence to define an importance sampler. The Markov chain aspect allows this method to perform acceptably even for high-dimensional problems, where finding good importance sampling distributions would otherwise be very difficult, while the use of importance weights ensures that the estimates found converge to the correct values as the number of annealing runs increases. This annealed importance sampling procedure resembles the second half of the previously-studied tempered transitions, and can be seen as a generalization of a recently-proposed variant of sequential importance sampling. It is also related to thermodynamic integration methods for estimating ratios of normalizing constants. Annealed importance sampling is most attractive when isolated modes are present, or when estimates of normalizing constants are required, but it may also be more generally useful, since its independent sampling allows one to bypass some of the problems of assessing convergence and autocorrelation in Markov chain samplers.</content></document><document><year>2004</year><authors>&amp;Oslash yvind Langsrud</authors><title>ANOVA for unbalanced data: Use Type II instead of Type III sums of squares</title><content>Methods for analyzing unbalanced factorial designs can be traced back to Yates (1934). Today, most major statistical programs perform, by default, unbalanced ANOVA based on Type III sums of squares (Yates's weighted squares of means). As criticized by Nelder and Lane (1995), this analysis is founded on unrealistic models&amp;#x2014;models with interactions, but without all corresponding main effects. The Type II analysis (Yates's method of fitting constants) is usually not preferred because of the underlying assumption of no interactions. This argument is, however, also founded on unrealistic models. Furthermore, by considering the power of the two methods, it is clear that Type II is preferable.</content></document><document><year>2004</year><authors>Henri Luchian1 | Daniel Stamate1</authors><title>Answer-perturbation techniques for the protection of statistical databases</title><content>This paper presents an answer-perturbation model for the protection of statistical databases. Compared to previous approaches, this model saves both space and time since neither a copy of the original database nor an extra processing of the query set is needed. The modularity of our protection mechanism (perturbation is applied after the query processing, not during it or upon data entry, as in the other approaches) facilitates the implementation. The method presented is user-oriented and since the usability (accuracy) of the statistically altered answers can be fine-tuned, the balance between usability and protection (security) is under the control of the database administrator.</content></document><document><year>2004</year><authors>OUHONG WANG1 | WILLIAM J. KENNEDY2</authors><title>Application of numerical interval analysis to obtain self-validating results for multivariate probabilities in a massively parallel environment</title><content>Conventional computations use real numbers as input and produce real numbers as results without any indication of the accuracy. Interval analysis, instead, uses interval elements throughout the computation and produces intervals as output with the guarantee that the true results are contained in them. One major use for interval analysis in statistics is to get results of high-dimensional multivariate probabilities. With the efforts to decrease the length of the intervals that contain the theoretically true answers, we can obtain results to any arbitrary accuracy, which is demonstrated by multivariate normal and multivariate t integrations. This is an advantage over the approximation methods that are currently in use. Since interval analysis is more computationally intensive than traditional computing, a MasPar parallel computer is used in this research to improve performance.</content></document><document><year>2004</year><authors>K. J. Powell1| T. Sapatinas1| T. C. Bailey1 | W. J. Krzanowski1</authors><title>Application of wavelets to the pre-processing of underwater sounds</title><content>In this paper we consider data on underwater sounds of differing types. Our objective is to filter background noise and achieve an acceptable level of reduction in the raw data, whilst at the same time maintaining the main features of the original signal. In particular, we consider data compression through the use of wavelet analysis followed by a thresholding of small coefficients in the resulting multiresolution decomposition. Various methods to threshold the wavelet representation are discussed and compared using recordings of dolphin sounds. An empirical modification to one of them is also proposed which shows promise in better preserving certain structures in our particular sound data.</content></document><document><year>2004</year><authors>Sylvia Fr&amp;uuml hwirth-Schnatter1</authors><title>Applied state space modelling of non-Gaussian time series using integration-based Kalman filtering</title><content>The main topic of the paper is on-line filtering for non-Gaussian dynamic (state space) models by approximate computation of the first two posterior moments using efficient numerical integration. Based on approximating the prior of the state vector by a normal density, we prove that the posterior moments of the state vector are related to the posterior moments of the linear predictor in a simple way. For the linear predictor Gauss-Hermite integration is carried out with automatic reparametrization based on an approximate posterior mode filter. We illustrate how further topics in applied state space modelling, such as estimating hyperparameters, computing model likelihoods and predictive residuals, are managed by integration-based Kalman-filtering. The methodology derived in the paper is applied to on-line monitoring of ecological time series and filtering for small count data.</content></document><document><year>2004</year><authors>Applying classification algorithms in practice</authors><title>In this paper we present a perspective on the overall process of developing classifiers for real-world classification problems. Specifically, we identify, categorize and discuss the various problem-specific factors that influence the development process. Illustrative examples are provided to demonstrate the iterative nature of the process of applying classification algorithms in practice. In addition, we present a case study of a large scale classification application using the process framework described, providing an end-to-end example of the iterative nature of the application process. The paper concludes that the process of developing classification applications for operational use involves many factors not normally considered in the typical discussion of classification models and algorithms.</title><content/></document><document><year>2009</year><authors>Ivan Kojadinovic1  | Jun Yan2 </authors><title>A goodness-of-fit test for multivariate multiparameter copulas based on multiplier central limit theorems      </title><content>Recent large scale simulations indicate that a powerful goodness-of-fit test for copulas can be obtained from the process         comparing the empirical copula with a parametric estimate of the copula derived under the null hypothesis. A first way to         compute approximate p-values for statistics derived from this process consists of using the parametric bootstrap procedure recently thoroughly         revisited by Genest and Rmillard. Because it heavily relies on random number generation and estimation, the resulting goodness-of-fit         test has a very high computational cost that can be regarded as an obstacle to its application as the sample size increases.         An alternative approach proposed by the authors consists of using a multiplier procedure. The study of the finite-sample performance of the multiplier version of the goodness-of-fit test for bivariate         one-parameter copulas showed that it provides a valid alternative to the parametric bootstrap-based test while being orders         of magnitude faster. The aim of this work is to extend the multiplier approach to multivariate multiparameter copulas and         study the finite-sample performance of the resulting test. Particular emphasis is put on elliptical copulas such as the normal         and the t as these are flexible models in a multivariate setting. The implementation of the procedure for the latter copulas proves         challenging and requires the extension of the Plackett formula for the t distribution to arbitrary dimension. Extensive Monte Carlo experiments, which could be carried out only because of the good         computational properties of the multiplier approach, confirm in the multivariate multiparameter context the satisfactory behavior         of the goodness-of-fit test.      </content></document><document><year>2009</year><authors>Eun-Kyung Lee1  | Dianne Cook2</authors><title>A projection pursuit index for large p small n data      </title><content>In high-dimensional data, one often seeks a few interesting low-dimensional projections which reveal important aspects of         the data. Projection pursuit for classification finds projections that reveal differences between classes. Even though projection         pursuit is used to bypass the curse of dimensionality, most indexes will not work well when there are a small number of observations         relative to the number of variables, known as a large p (dimension) small n (sample size) problem. This paper discusses the relationship between the sample size and dimensionality on classification         and proposes a new projection pursuit index that overcomes the problem of small sample size for exploratory classification.      </content></document><document><year>2009</year><authors>Frank Critchley1 | Michal Schyns2 | Gentiane Haesbroeck3 | Ccile Fauconnier4| Guobing Lu5| Richard A. Atkinson6 | Dong Qian Wang7</authors><title>A relaxed approach to combinatorial problems in robustness and diagnostics      </title><content>A range of procedures in both robustness and diagnostics require optimisation of a target functional over all subsamples of         given size. Whereas such combinatorial problems are extremely difficult to solve exactly, something less than the global optimum         can be &amp;#8216;good enough&amp;#8217; for many practical purposes, as shown by example. Again, a relaxation strategy embeds these discrete,         high-dimensional problems in continuous, low-dimensional ones. Overall, nonlinear optimisation methods can be exploited to         provide a single, reasonably fast algorithm to handle a wide variety of problems of this kind, thereby providing a certain         unity. Four running examples illustrate the approach. On the robustness side, algorithmic approximations to minimum covariance         determinant (MCD) and least trimmed squares (LTS) estimation. And, on the diagnostic side, detection of multiple multivariate         outliers and global diagnostic use of the likelihood displacement function. This last is developed here as a global complement         to Cook&amp;#8217;s (in J. R. Stat. Soc. 48:133&amp;#8211;169, 1986) local analysis. Appropriate convergence of each branch of the algorithm is guaranteed for any target functional whose relaxed         form is&amp;#8212;in a natural generalisation of concavity, introduced here&amp;#8212;&amp;#8216;gravitational&amp;#8217;. Again, its descent strategy can downweight         to zero contaminating cases in the starting position. A simulation study shows that, although not optimised for the LTS problem,         our general algorithm holds its own with algorithms that are so optimised. An adapted algorithm relaxes the gravitational         condition itself.      </content></document><document><year>2009</year><authors>David J. Nott1  | Li Jialiang1</authors><title>A sign based loss approach to model selection in nonparametric regression      </title><content>In parametric regression models the sign of a coefficient often plays an important role in its interpretation. One possible         approach to model selection in these situations is to consider a loss function that formulates prediction of the sign of a         coefficient as a decision problem. Taking a Bayesian approach, we extend this idea of a sign based loss for selection to more         complex situations. In generalized additive models we consider prediction of the sign of the derivative of an additive term         at a set of predictors. Being able to predict the sign of the derivative at some point (that is, whether a term is increasing         or decreasing) is one approach to selection of terms in additive modelling when interpretation is the main goal. For models         with interactions, prediction of the sign of a higher order derivative can be used similarly. There are many advantages to         our sign-based strategy for selection: one can work in a full or encompassing model without the need to specify priors on         a model space and without needing to specify priors on parameters in submodels. Also, avoiding a search over a large model         space can simplify computation. We consider shrinkage prior specifications on smoothing parameters that allow for good predictive         performance in models with large numbers of terms without the need for selection, and a frequentist calibration of the parameter         in our sign-based loss function when it is desired to control a false selection rate for interpretation.      </content></document><document><year>2009</year><authors>Marco S|ri1 | Paola Zuccolotto1 </authors><title>Analysis and correction of bias in Total Decrease in Node Impurity measures for tree-based algorithms      </title><content>Variable selection is one of the main problems faced by data mining and machine learning techniques. These techniques are         often, more or less explicitly, based on some measure of variable importance. This paper considers Total Decrease in Node         Impurity (TDNI) measures, a popular class of variable importance measures defined in the field of decision trees and tree-based         ensemble methods, like Random Forests and Gradient Boosting Machines. In spite of their wide use, some measures of this class         are known to be biased and some correction strategies have been proposed. The aim of this paper is twofold. Firstly, to investigate         the source and the characteristics of bias in TDNI measures using the notions of informative and uninformative splits. Secondly,         a bias-correction algorithm, recently proposed for the Gini measure in the context of classification, is extended to the entire         class of TDNI measures and its performance is investigated in the regression framework using simulated and real data.      </content></document><document><year>2009</year><authors>Gavino Puggioni1  | Alan E. Gelf|1 </authors><title>Analyzing space-time sensor network data under suppression and failure in transmission      </title><content>In this paper we present a fully model-based analysis of the effects of suppression and failure in data transmission with         sensor networks. Sensor networks are becoming an increasingly common data collection mechanism in a variety of fields. Sensors         can be created to collect data at very high temporal resolution. However, during periods when the process is following a stable         path, transmission of such high resolution data would carry little additional information with regard to the process model,         i.e., all of the data that is collected need not be transmitted. In particular, when there is cost to transmission, we find         ourselves moving to consideration of suppression in transmission. Additionally, for many sensor networks, in practice, we         will experience failures in transmission&amp;#8212;messages sent by a sensor but not received at the gateway, messages sent but arriving         corrupted. Evidently, both suppression and failure lead to information loss which will be reflected in inference associated         with our process model. Our effort here is to assess the impact of such information loss under varying extents of suppression         and varying incidence of failure. We consider two illustrative process models, presenting fully model-based analyses of suppression         and failure using hierarchical models. Such models naturally facilitate borrowing strength across nodes, leveraging all available         data to learn about local process behavior.      </content></document><document><year>2009</year><authors>Man-Lai Tang1  | Maozai Tian2</authors><title>Approximate confidence interval construction for risk difference under inverse sampling      </title><content>For studies with dichotomous outcomes, inverse sampling (also known as negative binomial sampling) is often used when the subjects arrive sequentially, when the underlying response of interest is acute, and/or when the         maximum likelihood estimators of some epidemiologic indices are undefined. Although exact unconditional inference has been         shown to be appealing, its applicability and popularity is severely hindered by the notorious conservativeness due to the         adoption of the maximization principle and by the tedious computing time due to the involvement of infinite summation. In         this article, we demonstrate how these obstacles can be overcome by the application of the constrained maximum likelihood         estimation and truncated approximation. The present work is motivated by confidence interval construction for the risk difference         under inverse sampling. Wald-type and score-type confidence intervals based on inverting two one-sided and one two-sided tests         are considered. Monte Carlo simulations are conducted to evaluate the performance of these confidence intervals with respect         to empirical coverage probability, empirical confidence width, and empirical left and right non-coverage probabilities. Two         examples from a maternal congenital heart disease study and a drug comparison study are used to demonstrate the proposed methodologies.      </content></document><document><year>2009</year><authors>Mari Myllymki1  | Antti Penttinen1</authors><title>Bayesian inference for Gaussian excursion set generated Cox processes with set-marking      </title><content>This work considers spatial Cox point processes where the random intensity is defined by a random closed set such that different         point intensities appear in the two phases formed by the random set and its complement. The point pattern is observed as a         set of point coordinates in a bounded region W&amp;#8834;&amp;#8477;            d             together with the information on the phase of the location of each point. This phase information, called set-marking, is         not a representative sample from the random set, and hence it cannot be directly used for deducing properties of the random         set. Excursion sets of continuous-parameter Gaussian random fields are applied as a flexible model for the random set. Fully         Bayesian method and Markov chain Monte Carlo (MCMC) simulation is adopted for inferring the parameters of the model and estimating         the random set. The performance of the new approach is studied by means of simulation experiments. Further, two forestry data         sets on point patterns of saplings are analysed. The saplings grow in a clear-cut forest area where, before planting and natural         seeding, the soil has been mounded forming a blotched soil structure. The tree densities tend to be different in the tilled         patches and in the area outside the patches. The coordinates of each sapling have been measured and it is known whether this         location is in a patch or outside. This example has been a motivation for the study.      </content></document><document><year>2009</year><authors>Arthur Charpentier1  | Abder Oulidi2 </authors><title>Beta kernel quantile estimators of heavy-tailed loss distributions      </title><content>In this paper we suggest several nonparametric quantile estimators based on Beta kernel. They are applied to transformed data         by the generalized Champernowne distribution initially fitted to the data. A Monte Carlo based study has shown that those estimators improve the efficiency         of the traditional ones, not only for light tailed distributions, but also for heavy tailed, when the probability level is         close to 1. We also compare these estimators with the Extreme Value Theory Quantile applied to Danish data on large fire insurance         losses.      </content></document><document><year>2009</year><authors>Brian D. Marx1 | Paul H. C. Eilers2 | Jutta Gampe3  | Rol| Rau4 </authors><title>Bilinear modulation models for seasonal tables of counts      </title><content>We propose generalized linear models for time or age-time tables of seasonal counts, with the goal of better understanding         seasonal patterns in the data. The linear predictor contains a smooth component for the trend and the product of a smooth         component (the modulation) and a periodic time series of arbitrary shape (the carrier wave). To model rates, a population         offset is added. Two-dimensional trends and modulation are estimated using a tensor product B-spline basis of moderate dimension.         Further smoothness is ensured using difference penalties on the rows and columns of the tensor product coefficients. The optimal         penalty tuning parameters are chosen based on minimization of a quasi-information criterion. Computationally efficient estimation         is achieved using array regression techniques, avoiding excessively large matrices. The model is applied to female death rate         in the US due to cerebrovascular diseases and respiratory diseases.      </content></document><document><year>2009</year><authors>F. Greselin1  | S. Ingrassia2 </authors><title>Constrained monotone EM algorithms for mixtures of multivariate t distributions      </title><content>Mixtures of multivariate t distributions provide a robust parametric extension to the fitting of data with respect to normal mixtures. In presence of         some noise component, potential outliers or data with longer-than-normal tails, one way to broaden the model can be provided         by considering t distributions. In this framework, the degrees of freedom can act as a robustness parameter, tuning the heaviness of the tails,         and downweighting the effect of the outliers on the parameters estimation. The aim of this paper is to extend to mixtures         of multivariate elliptical distributions some theoretical results about the likelihood maximization on constrained parameter         spaces. Further, a constrained monotone algorithm implementing maximum likelihood mixture decomposition of multivariate t distributions is proposed, to achieve improved convergence capabilities and robustness. Monte Carlo numerical simulations         and a real data study illustrate the better performance of the algorithm, comparing it to earlier proposals.      </content></document><document><year>2009</year><authors>Luca Scrucca1 </authors><title>Dimension reduction for model-based clustering      </title><content>We introduce a dimension reduction method for visualizing the clustering structure obtained from a finite mixture of Gaussian         densities. Information on the dimension reduction subspace is obtained from the variation on group means and, depending on         the estimated mixture model, on the variation on group covariances. The proposed method aims at reducing the dimensionality         by identifying a set of linear combinations, ordered by importance as quantified by the associated eigenvalues, of the original         features which capture most of the cluster structure contained in the data. Observations may then be projected onto such a         reduced subspace, thus providing summary plots which help to visualize the clustering structure. These plots can be particularly         appealing in the case of high-dimensional data and noisy structure. The new constructed variables capture most of the clustering         information available in the data, and they can be further reduced to improve clustering performance. We illustrate the approach         on both simulated and real data sets.      </content></document><document><year>2009</year><authors>Kwang Woo Ahn1  | Kung-Sik Chan2 </authors><title>Efficient Markov chain Monte Carlo with incomplete multinomial data      </title><content>We propose a block Gibbs sampling scheme for incomplete multinomial data. We show that the new approach facilitates maximal         blocking, thereby reducing serial dependency and speeding up the convergence of the Gibbs sampler. We compare the efficiency         of the new method with the standard, non-block Gibbs sampler via a number of numerical examples.      </content></document><document><year>2009</year><authors>Giovanna Capizzi1  | Guido Masarotto1 </authors><title>Evaluation of the run-length distribution for a combined Shewhart-EWMA control chart      </title><content>A simple algorithm is introduced for computing the run length distribution of a monitoring scheme combining a Shewhart chart         with an Exponentially Weighted Moving Average control chart. The algorithm is based on the numerical approximation of the         integral equations and integral recurrence relations related to the run-length distribution. In particular, a Clenshaw-Curtis         product-integration rule is applied for handling discontinuities in the integrand function due to the simultaneous use of         the two control schemes. The proposed algorithm, implemented in R and publicy available, compares favourably with the Markov         chain approach originally used to approximate the run length properties of the combined Shewhart-EWMA.      </content></document><document><year>2009</year><authors>Javier Roca-Pardias1 | Stefan Sperlich2 </authors><title>Feasible estimation in generalized structured models      </title><content>This article introduces a feasible estimation method for a large class of semi and nonparametric models. We present the family         of generalized structured models which we wish to estimate. After highlighting the main idea of the theoretical smooth backfitting         estimators, we introduce a general estimation procedure. We consider modifications and practical issues, and discuss inference,         cross validation, and asymptotic theory applying the theoretical framework of Mammen and Nielsen (Biometrika 90: 551&amp;#8211;566,         2003). An extensive simulation study shows excellent performance of our method. Furthermore, real data applications from environmetrics         and biometrics demonstrate its usefulness.      </content></document><document><year>2009</year><authors>Guillaume Obozinski1 | Ben Taskar2  | Michael I. Jordan3 </authors><title>Joint covariate selection and joint subspace selection for multiple classification problems      </title><content>We address the problem of recovering a common set of covariates that are relevant simultaneously to several classification         problems. By penalizing the sum of &amp;#8467;         2 norms of the blocks of coefficients associated with each covariate across different classification problems, similar sparsity         patterns in all models are encouraged. To take computational advantage of the sparsity of solutions at high regularization         levels, we propose a blockwise path-following scheme that approximately traces the regularization path. As the regularization         coefficient decreases, the algorithm maintains and updates concurrently a growing set of covariates that are simultaneously         active for all problems. We also show how to use random projections to extend this approach to the problem of joint subspace selection, where multiple predictors are found in a common low-dimensional subspace. We present theoretical results showing that this         random projection approach converges to the solution yielded by trace-norm regularization. Finally, we present a variety of         experimental results exploring joint covariate selection and joint subspace selection, comparing the path-following approach         to competing algorithms in terms of prediction accuracy and running time.      </content></document><document><year>2009</year><authors>Rudolf Beran1 | Lutz Dmbgen2 </authors><title>Least squares and shrinkage estimation under bimonotonicity constraints      </title><content>In this paper we describe active set type algorithms for minimization of a smooth function under general order constraints,         an important case being functions on the set of bimonotone rs matrices. These algorithms can be used, for instance, to estimate a bimonotone regression function via least squares or (a         smooth approximation of) least absolute deviations. Another application is shrinkage estimation in image denoising or, more         generally, regression problems with two ordinal factors after representing the data in a suitable basis which is indexed by         pairs (i,j)&amp;#8712;{1,&amp;#8230;,r}{1,&amp;#8230;,s}. Various numerical examples illustrate our methods.      </content></document><document><year>2009</year><authors>Farhat Iqbal1  | Kanchan Mukherjee1 </authors><title>M-estimators of some GARCH-type models; computation and application      </title><content>In this paper, we consider robust M-estimation of time series models with both symmetric and asymmetric forms of heteroscedasticity         related to the GARCH and GJR models. The class of estimators includes least absolute deviation (LAD), Huber&amp;#8217;s, Cauchy and         B-estimator as well as the well-known quasi maximum likelihood estimator (QMLE). Extensive simulations are used to check the         relative performance of these estimators in both models and the weighted resampling methods are used to approximate the sampling         distribution of M-estimators. Our study indicates that there are estimators that can perform better than QMLE and even outperform         robust estimator such as LAD when the error distribution is heavy-tailed. These estimators are also applied to real data sets.      </content></document><document><year>2009</year><authors>Yong Wang1 </authors><title>Maximum likelihood computation for fitting semiparametric mixture models      </title><content>Three general algorithms that use different strategies are proposed for computing the maximum likelihood estimate of a semiparametric         mixture model. They seek to maximize the likelihood function by, respectively, alternating the parameters, profiling the likelihood         and modifying the support set. All three algorithms make a direct use of the recently proposed fast and stable constrained         Newton method for computing the nonparametric maximum likelihood of a mixing distribution and employ additionally an optimization         algorithm for unconstrained problems. The performance of the algorithms is numerically investigated and compared for solving         the Neyman-Scott problem, overcoming overdispersion in logistic regression models and fitting two-level mixed effects logistic         regression models. Satisfactory results have been obtained.      </content></document><document><year>2009</year><authors>Frederico Z. Poleto1| Julio M. Singer1  | Carlos Daniel Paulino2</authors><title>Missing data mechanisms and their implications on;the;analysis of;categorical data      </title><content>We review some issues related to the implications of different missing data mechanisms on statistical inference for contingency         tables and consider simulation studies to compare the results obtained under such models to those where the units with missing         data are disregarded. We confirm that although, in general, analyses under the correct missing at random and missing completely         at random models are more efficient even for small sample sizes, there are exceptions where they may not improve the results         obtained by ignoring the partially classified data. We show that under the missing not at random (MNAR) model, estimates on         the boundary of the parameter space as well as lack of identifiability of the parameters of saturated models may be associated         with undesirable asymptotic properties of maximum likelihood estimators and likelihood ratio tests; even in standard cases         the bias of the estimators may be low only for very large samples. We also show that the probability of a boundary solution         obtained under the correct MNAR model may be large even for large samples and that, consequently, we may not always conclude         that a MNAR model is misspecified because the estimate is on the boundary of the parameter space.      </content></document><document><year>2009</year><authors>Merrill W. Liechty1  | Matthew Tibbits2</authors><title>Multivariate sufficient statistics using Kronecker;products      </title><content>We present a multivariate sufficient statistic using Kronecker products that dramatically increases computational efficiency         in evaluating likelihood functions and/or posterior distributions. In particular, we examine the example of multivariate regression         in a Bayesian setting. We compare the computation time for using the Gibbs sampler both with and without the sufficient statistic.         We show that the difference in computation time grows quadratically with the number of covariates and products and linearly         with the number of individuals. In the simulation study, speedup factors ranging from at least 20 times to almost 300 times         were observed when using the Kronecker sufficient statistic.      </content></document><document><year>2009</year><authors>Friedrich Leisch1 </authors><title>Neighborhood graphs, stripes and shadow plots for;cluster;visualization      </title><content>Centroid-based partitioning cluster analysis is a popular method for segmenting data into more homogeneous subgroups. Visualization         can help tremendously to understand the positions of these subgroups relative to each other in higher dimensional spaces and         to assess the quality of partitions. In this paper we present several improvements on existing cluster displays using neighborhood         graphs with edge weights based on cluster separation and convex hulls of inner and outer cluster regions. A new display called         shadow-stars can be used to diagnose pairwise cluster separation with respect to the distribution of the original data. Artificial         data and two case studies with real data are used to demonstrate the techniques.      </content></document><document><year>2009</year><authors>Michael G. B. Blum1  | Olivier Franois2</authors><title>Non-linear regression models for Approximate Bayesian Computation      </title><content>Approximate Bayesian inference on the basis of summary statistics is well-suited to complex problems for which the likelihood         is either mathematically or computationally intractable. However the methods that use rejection suffer from the curse of dimensionality         when the number of summary statistics is increased. Here we propose a machine-learning approach to the estimation of the posterior         density by introducing two innovations. The new method fits a nonlinear conditional heteroscedastic regression of the parameter         on the summary statistics, and then adaptively improves estimation using importance sampling. The new algorithm is compared         to the state-of-the-art approximate Bayesian methods, and achieves considerable reduction of the computational burden in two         examples of inference in statistical genetics and in a queueing model.      </content></document><document><year>2009</year><authors>Evgenia Rubinshtein1  | Anuj Srivastava2 </authors><title>Optimal linear projections for enhancing desired data statistics      </title><content>Problems involving high-dimensional data, such as pattern recognition, image analysis, and gene clustering, often require         a preliminary step of dimension reduction before or during statistical analysis. If one restricts to a linear technique for         dimension reduction, the remaining issue is the choice of the projection. This choice can be dictated by desire to maximize         certain statistical criteria, including variance, kurtosis, sparseness, and entropy, of the projected data. Motivations for         such criteria comes from past empirical studies of statistics of natural and urban images. We present a geometric framework         for finding projections that are optimal for obtaining certain desired statistical properties. Our approach is to define an         objective function on spaces of orthogonal linear projections&amp;#8212;Stiefel and Grassmann manifolds, and to use gradient techniques         to optimize that function. This construction uses the geometries of these manifolds to perform the optimization. Experimental         results are presented to demonstrate these ideas for natural and facial images.      </content></document><document><year>2009</year><authors>Youngjo Lee1  | Il Do Ha2 </authors><title>Orthodox BLUP versus h-likelihood methods for inferences about; random effects in Tweedie mixed models      </title><content>Recently, the orthodox best linear unbiased predictor (BLUP) method was introduced for inference about random effects in Tweedie         mixed models. With the use of h-likelihood, we illustrate that the standard likelihood procedures, developed for inference         about fixed unknown parameters, can be used for inference about random effects. We show that the necessary standard error         for the prediction interval of the random effect can be computed from the Hessian matrix of the h-likelihood. We also show         numerically that the h-likelihood provides a prediction interval that maintains a more precise coverage probability than the         BLUP method.      </content></document><document><year>2009</year><authors>I. Gijbels1  | A. Verhasselt1</authors><title>P-splines regression smoothing and difference type of penalty      </title><content>P-splines regression provides a flexible smoothing tool. In this paper we consider difference type penalties in a context         of nonparametric generalized linear models, and investigate the impact of the order of the differencing operator. Minimizing         Akaike&amp;#8217;s information criterion we search for a possible best data-driven value of the differencing order. Theoretical derivations         are established for the normal model and provide insights into a possible &amp;#8216;optimal&amp;#8217; choice of the differencing order and its         interrelation with other parameters. Applications of the selection procedure to non-normal models, such as Poisson models,         are given. Simulation studies investigate the performance of the selection procedure and we illustrate its use on real data         examples.      </content></document><document><year>2009</year><authors>Nicolas Wicker1 </authors><title>Perfect sampling algorithm for small mn contingency tables      </title><content>A Markov chain is proposed that uses coupling from the past sampling algorithm for sampling mn contingency tables. This method is an extension of the one proposed by Kijima and Matsui (Rand. Struct. Alg., 29:243&amp;#8211;256,         2006). It is not polynomial, as it is based upon a recursion, and includes a rejection phase but can be used for practical purposes         on small contingency tables as illustrated in a classical 44 example.      </content></document><document><year>2009</year><authors>M. Sperrin1 | T. Jaki1  | E. Wit2 </authors><title>Probabilistic relabelling strategies for the label switching problem in Bayesian mixture models      </title><content>The label switching problem is caused by the likelihood of a Bayesian mixture model being invariant to permutations of the         labels. The permutation can change multiple times between Markov Chain Monte Carlo (MCMC) iterations making it difficult to         infer component-specific parameters of the model. Various so-called &amp;#8216;relabelling&amp;#8217; strategies exist with the goal to &amp;#8216;undo&amp;#8217;         the label switches that have occurred to enable estimation of functions that depend on component-specific parameters. Existing         deterministic relabelling algorithms rely upon specifying a loss function, and relabelling by minimising its posterior expected         loss. In this paper we develop probabilistic approaches to relabelling that allow for estimation and incorporation of the         uncertainty in the relabelling process. Variants of the probabilistic relabelling algorithm are introduced and compared to         existing deterministic relabelling algorithms. We demonstrate that the idea of probabilistic relabelling can be expressed         in a rigorous framework based on the EM algorithm.      </content></document><document><year>2009</year><authors>Jinfeng Xu1 | Chenlei Leng1  | Zhiliang Ying2 </authors><title>Rank-based variable selection with censored data      </title><content>A rank-based variable selection procedure is developed for the semiparametric accelerated failure time model with censored         observations where the penalized likelihood (partial likelihood) method is not directly applicable.                     The new method penalizes the rank-based Gehan-type loss function with the &amp;#8467;               1 penalty. To correctly choose the tuning parameters, a novel likelihood-based &amp;#967;               2-type criterion is proposed. Desirable properties of the estimator such as the oracle properties are established through the               local quadratic expansion of the Gehan loss function.            </content></document><document><year>2009</year><authors>Tsung-I Lin1 </authors><title>Robust mixture modeling using multivariate skew t;distributions      </title><content>This paper presents a robust mixture modeling framework using the multivariate skew t distributions, an extension of the multivariate Student&amp;#8217;s t family with additional shape parameters to regulate skewness. The proposed model results in a very complicated likelihood.         Two variants of Monte Carlo EM algorithms are developed to carry out maximum likelihood estimation of mixture parameters.         In addition, we offer a general information-based method for obtaining the asymptotic covariance matrix of maximum likelihood         estimates. Some practical issues including the selection of starting values as well as the stopping criterion are also discussed.         The proposed methodology is applied to a subset of the Australian Institute of Sport data for illustration.      </content></document><document><year>2009</year><authors>Christian H. Wei1 </authors><title>Rule generation for categorical time series with;Markov;assumptions      </title><content>Several procedures of sequential pattern analysis are designed to detect frequently occurring patterns in a single categorical         time series (episode mining). Based on these frequent patterns, rules are generated and evaluated, for example, in terms of         their confidence. The confidence value is commonly interpreted as an estimate of a conditional probability, so some kind of         stochastic model has to be assumed. The model is identified as a variable length Markov model. With this assumption, the usual confidences are maximum likelihood estimates of the transition probabilities of the Markov         model. We discuss possibilities of how to efficiently fit an appropriate model to the data. Based on this model, rules are         formulated. It is demonstrated that this new approach generates noticeably less and more reliable rules.      </content></document><document><year>2009</year><authors>Simon Rogers1 | Mark Girolami1  | Tamara Polajnar1 </authors><title>Semi-parametric analysis of multi-rater data      </title><content>Datasets that are subjectively labeled by a number of experts are becoming more common in tasks such as biological text annotation         where class definitions are necessarily somewhat subjective. Standard classification and regression models are not suited         to multiple labels and typically a pre-processing step (normally assigning the majority class) is performed. We propose Bayesian         models for classification and ordinal regression that naturally incorporate multiple expert opinions in defining predictive         distributions. The models make use of Gaussian process priors, resulting in great flexibility and particular suitability to         text based problems where the number of covariates can be far greater than the number of data instances. We show that using         all labels rather than just the majority improves performance on a recent biological dataset.      </content></document><document><year>2009</year><authors>Francesco Audrino1  | Dominik Colangelo2 </authors><title>Semi-parametric forecasts of the implied volatility surface using regression trees      </title><content>We present a new semi-parametric model for the prediction of implied volatility surfaces that can be estimated using machine         learning algorithms. Given a reasonable starting model, a boosting algorithm based on regression trees sequentially minimizes         generalized residuals computed as differences between observed and estimated implied volatilities. To overcome the poor predictive         power of existing models, we include a grid in the region of interest, and implement a cross-validation strategy to find an         optimal stopping value for the boosting procedure. Back testing the out-of-sample performance on a large data set of implied         volatilities from S&amp;amp;P 500 options, we provide empirical evidence of the strong predictive power of our model.      </content></document><document><year>2009</year><authors>M. Bevilacqua1| J. Mateu2 | E. Porcu2| H. Zhang3 | A. Zini4</authors><title>Weighted composite likelihood-based tests for space-time separability of covariance functions      </title><content>Testing for separability of space-time covariance functions is of great interest in the analysis of space-time data. In this         paper we work in a parametric framework and consider the case when the parameter identifying the case of separability of the         associated space-time covariance lies on the boundary of the parametric space. This situation is frequently encountered in         space-time geostatistics. It is known that classical methods such as likelihood ratio test may fail in this case.                     We present two tests based on weighted composite likelihood estimates and the bootstrap method, and evaluate their performance               through an extensive simulation study as well as an application to Irish wind speeds. The tests are performed with respect               to a new class of covariance functions, which presents some desirable mathematical features and has margins of the Generalized               Cauchy type. We also apply the test on a element of the Gneiting class, obtaining concordant results.            </content></document><document><year>2008</year><authors>Yungtai Lo1 </authors><title>A likelihood ratio test of a homoscedastic normal mixture against a heteroscedastic normal mixture      </title><content>It is generally assumed that the likelihood ratio statistic for testing the null hypothesis that data arise from a homoscedastic         normal mixture distribution versus the alternative hypothesis that data arise from a heteroscedastic normal mixture distribution         has an asymptotic &amp;#967;         2 reference distribution with degrees of freedom equal to the difference in the number of parameters being estimated under         the alternative and null models under some regularity conditions. Simulations show that the &amp;#967;         2 reference distribution will give a reasonable approximation for the likelihood ratio test only when the sample size is 2000         or more and the mixture components are well separated when the restrictions suggested by Hathaway (Ann. Stat. 13:795&amp;#8211;800,         1985) are imposed on the component variances to ensure that the likelihood is bounded under the alternative distribution. For         small and medium sample sizes, parametric bootstrap tests appear to work well for determining whether data arise from a normal         mixture with equal variances or a normal mixture with unequal variances.      </content></document><document><year>2008</year><authors>E. Di Nardo1 | G. Guarino2  | D. Senato1 </authors><title>A new method for fast computing unbiased estimators of;cumulants      </title><content>We propose new algorithms for generating k-statistics, multivariate k-statistics, polykays and multivariate polykays. The resulting computational times are very fast compared with procedures         existing in the literature. Such speeding up is obtained by means of a symbolic method arising from the classical umbral calculus.         The classical umbral calculus is a light syntax that involves only elementary rules to managing sequences of numbers or polynomials.         The cornerstone of the procedures here introduced is the connection between cumulants of a random variable and a suitable         compound Poisson random variable. Such a connection holds also for multivariate random variables.      </content></document><document><year>2008</year><authors>Djalil Chafa1| 2  | Didier Concordet1| 2 </authors><title>A new method for the estimation of variance matrix with;prescribed zeros in nonlinear mixed effects models      </title><content>We propose a new method for the Maximum Likelihood Estimator (MLE) of nonlinear mixed effects models when the variance matrix         of Gaussian random effects has a prescribed pattern of zeros (PPZ). The method consists of coupling the recently developed         Iterative Conditional Fitting (ICF) algorithm with the Expectation Maximization (EM) algorithm. It provides positive definite         estimates for any sample size, and does not rely on any structural assumption concerning the PPZ. It can be easily adapted         to many versions of EM.      </content></document><document><year>2008</year><authors>Henghsiu Tsai1  | Kung-Sik Chan2 </authors><title>A note on the non-negativity of continuous-time ARMA and GARCH processes      </title><content>A general approach for modeling the volatility process in continuous-time is based on the convolution of a kernel with a non-decreasing         Lvy process, which is non-negative if the kernel is non-negative. Within the framework of Continuous-time Auto-Regressive         Moving-Average (CARMA) processes, we derive a necessary condition for the kernel to be non-negative, and propose a numerical         method for checking the non-negativity of a kernel function. These results can be lifted to solving a similar problem with         another approach to modeling volatility via the COntinuous-time Generalized Auto-Regressive Conditional Heteroscedastic (COGARCH)         processes.      </content></document><document><year>2008</year><authors>Chun-Xia Zhang1  | Jiang-She Zhang1 </authors><title>A novel method for constructing ensemble classifiers      </title><content>This paper presents a novel ensemble classifier generation method by integrating the ideas of bootstrap aggregation and Principal         Component Analysis (PCA). To create each individual member of an ensemble classifier, PCA is applied to every out-of-bag sample         and the computed coefficients of all principal components are stored, and then the principal components calculated on the         corresponding bootstrap sample are taken as additional elements of the original feature set. A classifier is trained with         the bootstrap sample and some features randomly selected from the new feature set. The final ensemble classifier is constructed         by majority voting of the trained base classifiers. The results obtained by empirical experiments and statistical tests demonstrate         that the proposed method performs better than or as well as several other ensemble methods on some benchmark data sets publicly         available from the UCI repository. Furthermore, the diversity-accuracy patterns of the ensemble classifiers are investigated         by kappa-error diagrams.      </content></document><document><year>2008</year><authors>Rosa Arboretti Giancristofaro1 | Stefano Bonnini1 | Fortunato Pesarin2</authors><title>A permutation approach for testing heterogeneity in two-sample categorical variables      </title><content>In many sciences researchers often meet the problem of establishing if the distribution of a categorical variable is more         concentrated, or less heterogeneous, in population P         1 than in population P         2. An approximate nonparametric solution to this problem is discussed within the permutation context. Such a solution has similarities         to that of testing for stochastic dominance, that is, of testing under order restrictions, for ordered categorical variables.         Main properties of given solution and a Monte Carlo simulation in order to evaluate its degree of approximation and its power         behaviour are examined. Two application examples are also discussed.      </content></document><document><year>2008</year><authors>Frdric J. P. Richard1 | Adeline M. M. Samson1  | Charles A. Cunod2 </authors><title>A SAEM algorithm for the estimation of template and deformation parameters in medical image sequences      </title><content>This paper is about object deformations observed throughout a sequence of images. We present a statistical framework in which         the observed images are defined as noisy realizations of a randomly deformed template image. In this framework, we focus on         the problem of the estimation of parameters related to the template and deformations. Our main motivation is the construction         of estimation framework and algorithm which can be applied to short sequences of complex and highly-dimensional images. The         originality of our approach lies in the representations of the template and deformations, which are defined on a common triangulated         domain, adapted to the geometry of the observed images. In this way, we have joint representations of the template and deformations         which are compact and parsimonious. Using such representations, we are able to drastically reduce the number of parameters         in the model. Besides, we adapt to our framework the Stochastic Approximation EM algorithm combined with a Markov Chain Monte         Carlo procedure which was proposed in 2004 by Kuhn and Lavielle. Our implementation of this algorithm takes advantage of some         properties which are specific to our framework. More precisely, we use the Markovian properties of deformations to build an         efficient simulation strategy based on a Metropolis-Hasting-Within-Gibbs sampler. Finally, we present some experiments on         sequences of medical images and synthetic data.      </content></document><document><year>2008</year><authors>Antonello Maruotti1  | Tobias Rydn2</authors><title>A semiparametric approach to hidden Markov models under longitudinal observations      </title><content>We propose a hidden Markov model for longitudinal count data where sources of unobserved heterogeneity arise, making data         overdispersed. The observed process, conditionally on the hidden states, is assumed to follow an inhomogeneous Poisson kernel,         where the unobserved heterogeneity is modeled in a generalized linear model (GLM) framework by adding individual-specific         random effects in the link function.                     Due to the complexity of the likelihood within the GLM framework, model parameters may be estimated by numerical maximization               of the log-likelihood function or by simulation methods; we propose a more flexible approach based on the Expectation Maximization               (EM) algorithm. Parameter estimation is carried out using a non-parametric maximum likelihood (NPML) approach in a finite               mixture context. Simulation results and two empirical examples are provided.            </content></document><document><year>2008</year><authors>Christophe Andrieu1  | Johannes Thoms2</authors><title>A tutorial on adaptive MCMC      </title><content>We review adaptive Markov chain Monte Carlo algorithms (MCMC) as a mean to optimise their performance. Using simple toy examples         we review their theoretical underpinnings, and in particular show why adaptive MCMC algorithms might fail when some fundamental         properties are not satisfied. This leads to guidelines concerning the design of correct algorithms. We then review criteria         and the useful framework of stochastic approximation, which allows one to systematically optimise generally used criteria,         but also analyse the properties of adaptive MCMC algorithms. We then propose a series of novel adaptive algorithms which prove         to be robust and reliable in practice. These algorithms are applied to artificial and high dimensional scenarios, but also         to the classic mine disaster dataset inference problem.      </content></document><document><year>2008</year><authors>Marina I. Knight1  | Guy P. Nason1</authors><title>A &amp;#8216;nondecimated&amp;#8217; lifting transform      </title><content>Classical nondecimated wavelet transforms are attractive for many applications. When the data comes from complex or irregular         designs, the use of second generation wavelets in nonparametric regression has proved superior to that of classical wavelets.         However, the construction of a nondecimated second generation wavelet transform is not obvious. In this paper we propose a         new &amp;#8216;nondecimated&amp;#8217; lifting transform, based on the lifting algorithm which removes one coefficient at a time, and explore         its behavior. Our approach also allows for embedding adaptivity in the transform, i.e. wavelet functions can be constructed         such that their smoothness adjusts to the local properties of the signal. We address the problem of nonparametric regression         and propose an (averaged) estimator obtained by using our nondecimated lifting technique teamed with empirical Bayes shrinkage.         Simulations show that our proposed method has higher performance than competing techniques able to work on irregular data.         Our construction also opens avenues for generating a &amp;#8216;best&amp;#8217; representation, which we shall explore.      </content></document><document><year>2008</year><authors>Yuan Ren1| Yu Ding1  | Faming Liang2 </authors><title>Adaptive evolutionary Monte Carlo algorithm for optimization with;applications to sensor placement problems      </title><content>In this paper, we present an adaptive evolutionary Monte Carlo algorithm (AEMC), which combines a tree-based predictive model         with an evolutionary Monte Carlo sampling procedure for the purpose of global optimization. Our development is motivated by         sensor placement applications in engineering, which requires optimizing certain complicated &amp;#8220;black-box&amp;#8221; objective function.         The proposed method is able to enhance the optimization efficiency and effectiveness as compared to a few alternative strategies.         AEMC falls into the category of adaptive Markov chain Monte Carlo (MCMC) algorithms and is the first adaptive MCMC algorithm         that simulates multiple Markov chains in parallel. A theorem about the ergodicity property of the AEMC algorithm is stated         and proven. We demonstrate the advantages of the proposed method by applying it to a sensor placement problem in a manufacturing         process, as well as to a standard Griewank test function.      </content></document><document><year>2008</year><authors>Olivier Capp1| R|al Douc2| Arnaud Guillin3| Jean-Michel Marin4| 5  | Christian P. Robert6| 7</authors><title>Adaptive importance sampling in general mixture classes      </title><content>In this paper, we propose an adaptive algorithm that iteratively updates both the weights and component parameters of a mixture         importance sampling density so as to optimise the performance of importance sampling, as measured by an entropy criterion.         The method, called M-PMC, is shown to be applicable to a wide class of importance sampling densities, which includes in particular         mixtures of multivariate Student t distributions. The performance of the proposed scheme is studied on both artificial and real examples, highlighting in particular         the benefit of a novel Rao-Blackwellisation device which can be easily incorporated in the updating scheme.      </content></document><document><year>2008</year><authors>Jonathan M. Keith1 | Dirk P. Kroese2  | George Y. Sofronov3 </authors><title>Adaptive independence samplers      </title><content>Markov chain Monte Carlo (MCMC) is an important computational technique for generating samples from non-standard probability         distributions. A;major challenge in the design of practical MCMC samplers is to achieve efficient convergence and mixing properties.         One way to accelerate convergence and mixing is to adapt the proposal distribution in light of previously sampled points,         thus increasing the probability of acceptance. In this paper, we propose two new adaptive MCMC algorithms based on the Independent         Metropolis&amp;#8211;Hastings algorithm. In the first, we adjust the proposal to minimize an estimate of the cross-entropy between the         target and proposal distributions, using the experience of pre-runs. This approach provides a general technique for deriving         natural adaptive formulae. The second approach uses multiple parallel chains, and involves updating chains individually, then         updating a proposal density by fitting a Bayesian model to the population. An important feature of this approach is that adapting         the proposal does not change the limiting distributions of the chains. Consequently, the adaptive phase of the sampler can         be continued indefinitely. We include results of numerical experiments indicating that the new algorithms compete well with         traditional Metropolis&amp;#8211;Hastings algorithms. We also demonstrate the method for a realistic problem arising in Comparative         Genomics.      </content></document><document><year>2008</year><authors>Julien Cornebise1 | ric Moulines1  | Jimmy Olsson2 </authors><title>Adaptive methods for sequential importance sampling with;application to state space models      </title><content>In this paper we discuss new adaptive proposal strategies for sequential Monte Carlo algorithms&amp;#8212;also known as particle filters&amp;#8212;relying         on criteria evaluating the quality of the proposed particles. The choice of the proposal distribution is a major concern and         can dramatically influence the quality of the estimates. Thus, we show how the long-used coefficient of variation (suggested         by Kong et;al. in J. Am. Stat. Assoc. 89(278&amp;#8211;288):590&amp;#8211;599, 1994) of the weights can be used for estimating the chi-square distance between the target and instrumental distributions of the         auxiliary particle filter. As a by-product of this analysis we obtain an auxiliary adjustment multiplier weight type for which         this chi-square distance is minimal. Moreover, we establish an empirical estimate of linear complexity of the Kullback-Leibler         divergence between the involved distributions. Guided by these results, we discuss adaptive designing of the particle filter         proposal distribution and illustrate the methods on a numerical example.      </content></document><document><year>2008</year><authors>T. J. Heaton1 </authors><title>Adaptive thresholding of sequences with locally variable strength      </title><content>This paper addresses, via thresholding, the estimation of a;possibly sparse signal observed subject to Gaussian noise. Conceptually,         the optimal threshold for such problems depends upon the strength of the underlying signal. We propose two new methods that         aim to adapt to potential local variation in this signal strength and select a;variable threshold accordingly. Our methods         are based upon an;empirical Bayes approach with a;smoothly variable mixing weight chosen via either spline or kernel based         marginal maximum likelihood regression. We demonstrate the excellent performance of our methods in both one and two-dimensional         estimation when compared to various alternative techniques. In addition, we consider the application to wavelet denoising         where reconstruction quality is significantly improved with local adaptivity.      </content></document><document><year>2008</year><authors>Colin Chen1  | Keming Yu2 </authors><title>Automatic Bayesian quantile regression curve fitting      </title><content>Quantile regression, including median regression, as a more completed statistical model than mean regression, is now well         known with its wide spread applications. Bayesian inference on quantile regression or Bayesian quantile regression has attracted         much interest recently. Most of the existing researches in Bayesian quantile regression focus on parametric quantile regression,         though there are discussions on different ways of modeling the model error by a parametric distribution named asymmetric Laplace         distribution or by a nonparametric alternative named scale mixture asymmetric Laplace distribution. This paper discusses Bayesian         inference for nonparametric quantile regression. This general approach fits quantile regression curves using piecewise polynomial         functions with an unknown number of knots at unknown locations, all treated as parameters to be inferred through reversible         jump Markov chain Monte Carlo (RJMCMC) of Green (Biometrika 82:711&amp;#8211;732, 1995). Instead of drawing samples from the posterior, we use regression quantiles to create Markov chains for the estimation of         the quantile curves. We also use approximate Bayesian factor in the inference. This method extends the work in automatic Bayesian         mean curve fitting to quantile regression. Numerical results show that this Bayesian quantile smoothing technique is competitive         with quantile regression/smoothing splines of He and Ng (Comput. Stat. 14:315&amp;#8211;337, 1999) and P-splines (penalized splines) of Eilers and de Menezes (Bioinformatics 21(7):1146&amp;#8211;1153, 2005).      </content></document><document><year>2008</year><authors>Y. Fan1 | G. W. Peters1 | S. A. Sisson1</authors><title>Automating and evaluating reversible jump MCMC proposal distributions      </title><content>The reversible jump Markov chain Monte Carlo (MCMC) sampler (Green in Biometrika 82:711&amp;#8211;732, 1995) has become an invaluable device for Bayesian practitioners. However, the primary difficulty with the sampler lies with the         efficient construction of transitions between competing models of possibly differing dimensionality and interpretation. We         propose the use of a marginal density estimator to construct between-model proposal distributions. This provides both a step         towards black-box simulation for reversible jump samplers, and a tool to examine the utility of common between-model mapping         strategies. We compare the performance of our approach to well established alternatives in both time series and mixture model         examples.      </content></document><document><year>2008</year><authors>Gerhard Koekemoer1  | Jan W. H. Swanepoel1</authors><title>A;semi-parametric method for transforming data to;normality      </title><content>A;non-parametric transformation function is introduced to transform data to any continuous distribution. When transformation         of data to normality is desired, the use of a suitable parametric pre-transformation function improves the performance of         the proposed non-parametric transformation function. The resulting semi-parametric transformation function is shown empirically,         via a Monte Carlo study, to perform at least as well as any parametric transformation currently available in the literature.      </content></document><document><year>2008</year><authors>Helen Armstrong1| Christopher K. Carter2| Kin Foon Kevin Wong3 | Robert Kohn2 </authors><title>Bayesian covariance matrix estimation using a mixture of;decomposable graphical models      </title><content>We present a Bayesian approach to estimating a;covariance matrix by using a prior that is a mixture over all decomposable         graphs, with the probability of each graph size specified by the user and graphs of equal size assigned equal probability.         Most previous approaches assume that all graphs are equally probable. We show empirically that the prior that assigns equal         probability over graph sizes outperforms the prior that assigns equal probability over all graphs in more efficiently estimating         the covariance matrix. The;prior requires knowing the number of decomposable graphs for each graph size and we give a simulation         method for estimating these counts. We also present a Markov chain Monte Carlo method for estimating the posterior distribution         of the covariance matrix that is much more efficient than current methods. Both the prior and the simulation method to evaluate         the prior apply generally to any decomposable graphical model.      </content></document><document><year>2008</year><authors>D. Allingham1 | R. A. R. King2 | K. L. Mengersen3</authors><title>Bayesian estimation of;quantile distributions      </title><content>Use of Bayesian modelling and analysis has become commonplace in many disciplines (finance, genetics and image analysis, for         example). Many complex data sets are collected which do not readily admit standard distributions, and often comprise skew         and kurtotic data. Such data is well-modelled by the very flexibly-shaped distributions of the quantile distribution family,         whose members are defined by the inverse of their cumulative distribution functions and rarely have analytical likelihood         functions defined. Without explicit likelihood functions, Bayesian methodologies such as Gibbs sampling cannot be applied         to parameter estimation for this valuable class of distributions without resorting to numerical inversion. Approximate Bayesian         computation provides an alternative approach requiring only a sampling scheme for the distribution of interest, enabling easier         use of quantile distributions under the Bayesian framework. Parameter estimates for simulated and experimental data are presented.      </content></document><document><year>2008</year><authors>Richard Gerlach1 | Cathy W. S. Chen2 </authors><title>Bayesian inference and model comparison for asymmetric smooth transition heteroskedastic models      </title><content>Inference, quantile forecasting and model comparison for an asymmetric double smooth transition heteroskedastic model is investigated.         A;Bayesian framework in employed and an adaptive Markov chain Monte Carlo scheme is designed. A;mixture prior is proposed         that alleviates the usual identifiability problem as the speed of transition parameter tends to zero, and an informative prior         for this parameter is suggested, that allows for reliable inference and a proper posterior, despite the non-integrability         of the likelihood function. A;formal Bayesian posterior model comparison procedure is employed to compare the proposed model         with its two limiting cases: the double threshold GARCH and symmetric ARX GARCH models. The proposed methods are illustrated         using both simulated and international stock market return series. Some illustrations of the advantages of an adaptive sampling         scheme for these models are also provided. Finally, Bayesian forecasting methods are employed in a Value-at-Risk study of         the international return series. The results generally favour the proposed smooth transition model and highlight explosive         and smooth nonlinear behaviour in financial markets.      </content></document><document><year>2008</year><authors>Cinzia Viroli1 </authors><title>Bayesian inference in non-Gaussian factor analysis      </title><content>Non-Gaussian factor analysis differs from ordinary factor analysis because of the distribution assumption on the factors which         are modelled by univariate mixtures of Gaussians thus relaxing the classical normal hypothesis. From this point of view, the         model can be thought of as a generalization of ordinary factor analysis and its estimation problem can still be solved via         the maximum likelihood method. The focus of this work is to introduce, develop and explore a Bayesian analysis of the model         in order to provide an answer to unresolved questions about the number of latent factors and simultaneously the number of         mixture components to model each factor. The effectiveness of the proposed method is explored in a simulation study and in         a real example of international exchange rates.      </content></document><document><year>2008</year><authors>Luke Akong&amp;#8217 o Orawo1  | J. Andrs Christen2 </authors><title>Bayesian sequential analysis for multiple-arm clinical trials      </title><content>Use of full Bayesian decision-theoretic approaches to obtain optimal stopping rules for clinical trial designs typically requires         the use of Backward Induction. However, the implementation of Backward Induction, apart from simple trial designs, is generally         impossible due to analytical and computational difficulties. In this paper we present a numerical approximation of Backward         Induction in a multiple-arm clinical trial design comparing k experimental treatments with a standard treatment where patient response is binary. We propose a novel stopping rule, denoted         by &amp;#964;                     p            , as an approximation of the optimal stopping rule, using the optimal stopping rule of a single-arm clinical trial obtained         by Backward Induction. We then present an example of a double-arm (k=2) clinical trial where we use a simulation-based algorithm together with &amp;#964;                     p             to estimate the expected utility of continuing and compare our estimates with exact values obtained by an implementation         of Backward Induction. For trials with more than two treatment arms, we evaluate &amp;#964;                     p             by studying its operating characteristics in a three-arm trial example. Results from these examples show that our approximate         trial design has attractive properties and hence offers a relevant solution to the problem posed by Backward Induction.      </content></document><document><year>2008</year><authors>Andrea Cerioli1 | Marco Riani1  | Anthony C. Atkinson2 </authors><title>Controlling the size of multivariate outlier tests with the MCD estimator of scatter      </title><content>Multivariate outlier detection requires computation of robust distances to be compared with appropriate cut-off points. In         this paper we propose a new calibration method for obtaining reliable cut-off points of distances derived from the MCD estimator         of scatter. These cut-off points are based on a more accurate estimate of the extreme tail of the distribution of robust distances.         We show that our procedure gives reliable tests of outlyingness in almost all situations of practical interest, provided that         the sample size is not much smaller than 50. Therefore, it is a considerable improvement over all the available MCD procedures,         which are unable to provide good control over the size of multiple outlier tests for the data structures considered in this         paper.      </content></document><document><year>2008</year><authors>Ralph dos Santos Silva1  | Hedibert Freitas Lopes2 </authors><title>Copula, marginal distributions and model selection: a;Bayesian;note      </title><content>Copula functions and marginal distributions are combined to produce multivariate distributions. We show advantages of estimating         all parameters of these models using the Bayesian approach, which can be done with standard Markov chain Monte Carlo algorithms.         Deviance-based model selection criteria are also discussed when applied to copula models since they are invariant under monotone         increasing transformations of the marginals. We focus on the deviance information criterion. The joint estimation takes into         account all dependence structure of the parameters&amp;#8217; posterior distributions in our chosen model selection criteria. Two Monte         Carlo studies are conducted to show that model identification improves when the model parameters are jointly estimated. We         study the Bayesian estimation of all unknown quantities at once considering bivariate copula functions and three known marginal         distributions.      </content></document><document><year>2008</year><authors>J. Q. Shi1  | B. Wang2</authors><title>Curve prediction and clustering with mixtures of Gaussian process functional regression models      </title><content>Shi, Wang, Murray-Smith and Titterington (Biometrics 63:714&amp;#8211;723, 2007) proposed a Gaussian process functional regression (GPFR)         model to model functional response curves with a set of functional covariates. Two main problems are addressed by their method:         modelling nonlinear and nonparametric regression relationship and modelling covariance structure and mean structure simultaneously.         The method gives very good results for curve fitting and prediction but side-steps the problem of heterogeneity. In this paper         we present a new method for modelling functional data with &amp;#8216;spatially&amp;#8217; indexed data, i.e., the heterogeneity is dependent         on factors such as region and individual patient&amp;#8217;s information. For data collected from different sources, we assume that         the data corresponding to each curve (or batch) follows a Gaussian process functional regression model as a lower-level model,         and introduce an allocation model for the latent indicator variables as a higher-level model. This higher-level model is dependent         on the information related to each batch. This method takes advantage of both GPFR and mixture models and therefore improves         the accuracy of predictions. The mixture model has also been used for curve clustering, but focusing on the problem of clustering         functional relationships between response curve and covariates, i.e. the clustering is based on the surface shape of the functional         response against the set of functional covariates. The model is examined on simulated data and real data.      </content></document><document><year>2008</year><authors>Cajo J. F. ter Braak1  | Jasper A. Vrugt2</authors><title>Differential Evolution Markov Chain with snooker updater and;fewer chains      </title><content>Differential Evolution Markov Chain (DE-MC) is an adaptive MCMC algorithm, in which multiple chains are run in parallel. Standard         DE-MC requires at least N=2d chains to be run in parallel, where d is the dimensionality of the posterior. This paper extends DE-MC with a snooker updater and shows by simulation and real         examples that DE-MC can work for d up to 50&amp;#8211;100 with fewer parallel chains (e.g.         N=3) by exploiting information from their past by generating jumps from differences of pairs of past states. This approach         extends the practical applicability of DE-MC and is shown to be about 5&amp;#8211;26 times more efficient than the optimal Normal random         walk Metropolis sampler for the 97.5% point of a variable from a 25&amp;#8211;50 dimensional Student t         3 distribution. In a nonlinear mixed effects model example the approach outperformed a block-updater geared to the specific         features of the model.      </content></document><document><year>2008</year><authors>Paul Fearnhead1 </authors><title>Editorial: Special issue on adaptive Monte Carlo methods      </title><content>Without Abstract</content></document><document><year>2008</year><authors>C. J. Prez1 | J. Martn1| C. Rojano2 | F. J. Girn2</authors><title>Efficient generation of random vectors by using the;ratio-of-uniforms method with ellipsoidal envelopes      </title><content>         Stochastic simulation is widely used to validate procedures and provide guidance for both theoretical and practical problems.         Random variate generation is the basis of stochastic simulation. Applying the ratio-of-uniforms method to generate random         vectors requires the ability to generate points uniformly in a suitable region of the space. Starting from the observation         that, for many multivariate distributions, the multidimensional objective region can be covered by a hyper-ellipsoid more         tightly than by a hyper-rectangle, a new algorithm to generate from multivariate distributions is proposed. Due to the computational         saving it can produce, this method becomes an appealing statistical tool to generate random vectors from families of standard         and nonstandard multivariate distributions. It is particularly interesting to generate from densities known up to a multiplicative         constant, for example, from those arising in Bayesian computation. The proposed method is applied and its efficiency is shown         for some classes of distributions.               </content></document><document><year>2008</year><authors>Hsiuying Wang1| 2 </authors><title>Exact average coverage probabilities and confidence coefficients of;confidence intervals for discrete distributions      </title><content>For a confidence interval (L(X),U(X)) of a parameter &amp;#952; in one-parameter discrete distributions, the coverage probability is a variable function of &amp;#952;. The confidence coefficient is the infimum of the coverage probabilities, inf&amp;#8201;            &amp;#952;                     P                     &amp;#952;            (&amp;#952;&amp;#8712;(L(X),U(X))). Since we do not know which point in the parameter space the infimum coverage probability occurs at, the exact confidence         coefficients are unknown. Beside confidence coefficients, evaluation of a confidence intervals can be based on the average         coverage probability. Usually, the exact average probability is also unknown and it was approximated by taking the mean of         the coverage probabilities at some randomly chosen points in the parameter space. In this article, methodologies for computing         the exact average coverage probabilities as well as the exact confidence coefficients of confidence intervals for one-parameter         discrete distributions are proposed. With these methodologies, both exact values can be derived.      </content></document><document><year>2008</year><authors>Ian L. Dryden1 | Li Bai2| Christopher J. Brignell1 | Linlin Shen3</authors><title>Factored principal components analysis, with applications to face recognition      </title><content>A dimension reduction technique is proposed for matrix data, with applications to face recognition from images. In particular,         we propose a factored covariance model for the data under study, estimate the parameters using maximum likelihood, and then         carry out eigendecompositions of the estimated covariance matrix. We call the resulting method factored principal components         analysis. We also develop a method for classification using a likelihood ratio criterion, which has previously been used for         evaluating the strength of forensic evidence. The methodology is illustrated with applications in face recognition.      </content></document><document><year>2008</year><authors>S. Saha1 | P. K. M|al1 | Y. Boers2 | H. Driessen2  | A. Bagchi1 </authors><title>Gaussian proposal density using moment matching in SMC methods      </title><content>In this article we introduce a new Gaussian proposal distribution to be used in conjunction with the sequential Monte Carlo         (SMC) method for solving non-linear filtering problems. The proposal, in line with the recent trend, incorporates the current         observation. The introduced proposal is characterized by the exact moments obtained from the dynamical system. This is in         contrast with recent works where the moments are approximated either numerically or by linearizing the observation model.         We show further that the newly introduced proposal performs better than other similar proposal functions which also incorporate         both state and observations.      </content></document><document><year>2008</year><authors>Caroline Bernard-Michel1| Laurent Gardes1 | Stphane Girard1 </authors><title>Gaussian Regularized Sliced Inverse Regression      </title><content>Sliced Inverse Regression (SIR) is an effective method for dimension reduction in high-dimensional regression problems. The         original method, however, requires the inversion of the predictors covariance matrix. In case of collinearity between these         predictors or small sample sizes compared to the dimension, the inversion is not possible and a regularization technique has         to be used. Our approach is based on a Fisher Lecture given by R.D. Cook where it is shown that SIR axes can be interpreted         as solutions of an inverse regression problem. We propose to introduce a Gaussian prior distribution on the unknown parameters         of the inverse regression problem in order to regularize their estimation. We show that some existing SIR regularizations         can enter our framework, which permits a global understanding of these methods. Three new priors are proposed leading to new         regularizations of the SIR method. A comparison on simulated data as well as an application to the estimation of Mars surface         physical properties from hyperspectral images are provided.      </content></document><document><year>2008</year><authors>M. S. Ridout1 </authors><title>Generating random numbers from a distribution specified by;its;Laplace transform      </title><content>This paper discusses simulation from an absolutely continuous distribution on the positive real line when the Laplace transform         of the distribution is known but its density and distribution functions may not be available. We advocate simulation by the         inversion method using a modified Newton-Raphson method, with values of the distribution and density functions obtained by         numerical transform inversion. We show that this algorithm performs well in a series of increasingly complex examples. Caution         is needed in some situations when the numerical Laplace transform inversion becomes unreliable. In particular the algorithm         should not be used for distributions with finite range. But otherwise, except for rather pathological distributions, the approach         offers a rapid way of generating random samples with minimal user effort. We contrast our approach with an alternative algorithm         due to Devroye (Comput. Math. Appl. 7, 547&amp;#8211;552, 1981).      </content></document><document><year>2008</year><authors>David J. Lunn1 | Nicky Best2 | John C. Whittaker3</authors><title>Generic reversible jump MCMC using graphical models      </title><content>Markov chain Monte Carlo techniques have revolutionized the field of Bayesian statistics. Their power is so great that they         can even accommodate situations in which the structure of the statistical model itself is uncertain. However, the analysis         of such trans-dimensional (TD) models is not easy and available software may lack the flexibility required for dealing with the complexities of real         data, often because it does not allow the TD model to be simply part of some bigger model. In this paper we describe a class         of widely applicable TD models that can be represented by a generic graphical model, which may be incorporated into arbitrary         other graphical structures without significantly affecting the mechanism of inference. We also present a decomposition of         the reversible jump algorithm into abstract and problem-specific components, which provides infrastructure for applying the         method to all models in the class considered. These developments represent a first step towards a context-free method for implementing         TD models that will facilitate their use by applied scientists for the practical exploration of model uncertainty. Our approach         makes use of the popular WinBUGS framework as a sampling engine and we illustrate its use via two simple examples in which         model uncertainty is a key feature.      </content></document><document><year>2008</year><authors>Debasish Roy1 | Geoff Nicholls2 | Colin Fox1</authors><title>Imaging convex quadrilateral inclusions in uniform conductors from electrical boundary measurements      </title><content>We demonstrate simulation-based Bayesian imaging from electrical impedance tomographic data, by summarizing the set of conductance         images which could give rise to the data. The forward map from conductance image to data requires the solution of a partial         differential equation subject to boundary conditions. We develop the example of recovering an unknown convex polygonal insulating         inclusion within an object made of otherwise uniformly conducting material, and illustrate our methods with noisy synthetic         data. Sampling is carried out using Markov chain Monte Carlo with the efficiency of the algorithm investigated over a range         of noise levels.      </content></document><document><year>2008</year><authors>Robert Gramacy1 | Richard Samworth1  | Ruth King2 </authors><title>Importance tempering      </title><content>Simulated tempering (ST) is an established Markov chain Monte Carlo (MCMC) method for sampling from a multimodal density &amp;#960;(&amp;#952;). Typically, ST involves introducing an auxiliary variable k taking values in a finite subset of [0,1] and indexing a set of tempered distributions, say &amp;#960;                     k            (&amp;#952;)&amp;#8733;         &amp;#960;(&amp;#952;)            k            . In this case, small values of k encourage better mixing, but samples from &amp;#960; are only obtained when the joint chain for (&amp;#952;,k) reaches k=1. However, the entire chain can be used to estimate expectations under &amp;#960; of functions of interest, provided that importance sampling (IS) weights are calculated. Unfortunately this method, which         we call importance tempering (IT), can disappoint. This is partly because the most immediately obvious implementation is nave         and can lead to high variance estimators. We derive a new optimal method for combining multiple IS estimators and prove that         the resulting estimator has a highly desirable property related to the notion of effective sample size. We briefly report         on the success of the optimal combination in two modelling scenarios requiring reversible-jump MCMC, where the nave approach         fails.      </content></document><document><year>2008</year><authors>Sylvia Frhwirth-Schnatter1 | Rudolf Frhwirth2| Leonhard Held3 | Hvard Rue4</authors><title>Improved auxiliary mixture sampling for hierarchical models of;non-Gaussian data      </title><content>The article considers Bayesian analysis of hierarchical models for count, binomial and multinomial data using efficient MCMC         sampling procedures. To this end, an improved method of auxiliary mixture sampling is proposed. In contrast to previously         proposed samplers the method uses a bounded number of latent variables per observation, independent of the intensity of the         underlying Poisson process in the case of count data, or of the number of experiments in the case of binomial and multinomial         data. The bounded number of latent variables results in a more general error distribution, which is a negative log-Gamma distribution         with arbitrary integer shape parameter. The required approximations of these distributions by Gaussian mixtures have been         computed. Overall, the improvement leads to a substantial increase in efficiency of auxiliary mixture sampling for highly         structured models. The method is illustrated for finite mixtures of generalized linear models and an epidemiological case         study.      </content></document><document><year>2008</year><authors>P. Besbeas1  | B. J. T. Morgan1</authors><title>Improved estimation of the stable laws      </title><content>Fitting general stable laws to data by maximum likelihood is important but difficult. This is why much research has considered         alternative procedures based on empirical characteristic functions. Two problems then are how many values of the characteristic         function to select, and how to position them. We provide recommendations for both of these topics. We propose an arithmetic         spacing of transform variables, coupled with a recommendation for the location of the variables. It is shown that arithmetic         spacing, which is far simpler to implement, closely approximates optimum spacing. The new methods that result are compared         in simulation studies with existing methods, including maximum-likelihood. The main conclusion is that arithmetic spacing         of the values of the characteristic function, coupled with appropriately limiting the range for these values, improves the         overall performance of the regression-type method of Koutrouvelis, which is the standard procedure for estimating general         stable law parameters.      </content></document><document><year>2008</year><authors>Willi Sauerbrei1 | Norbert Hollnder1 | Anika Buchholz1</authors><title>Investigation about a screening step in model selection      </title><content>         In many studies a large number of variables is measured and the identification of relevant variables influencing an outcome         is an important task. For variable selection several procedures are available. However, focusing on one model only neglects         that there usually exist other equally appropriate models. Bayesian or frequentist model averaging approaches have been proposed         to improve the development of a predictor. With a larger number of variables (say more than ten variables) the resulting class         of models can be very large. For Bayesian model averaging Occam&amp;#8217;s window is a popular approach to reduce the model space.         As;this approach may not eliminate any variables, a variable screening step was proposed for a;frequentist model averaging         procedure. Based on the results of selected models in bootstrap samples, variables are eliminated before deriving a model         averaging predictor. As a simple alternative screening procedure backward elimination can be used.                                             Through two examples and by means of simulation we investigate some properties of the screening step. In the simulation study               we consider situations with fifteen and 25 variables, respectively, of which seven have an influence on the outcome. With               the screening step most of the uninfluential variables will be eliminated, but also some variables with a weak effect. Variable               screening leads to more applicable models without eliminating models, which are more strongly supported by the data. Furthermore,               we give recommendations for important parameters of the screening step.                           </content></document><document><year>2008</year><authors>Nicolas Chopin1 </authors><title>Jim Albert: Bayesian computation with R         Springer, 2007. ISBN: 978-0-387-71384-7</title><content>Without Abstract</content></document><document><year>2008</year><authors>Pedro Delicado1  | Marcelo Smrekar1</authors><title>Measuring non-linear dependence for two random variables distributed along a curve      </title><content>We propose new dependence measures for two real random variables not necessarily linearly related. Covariance and linear correlation         are expressed in terms of principal components and are generalized for variables distributed along a curve. Properties of         these measures are discussed. The new measures are estimated using principal curves and are computed for simulated and real         data sets. Finally, we present several statistical applications for the new dependence measures.      </content></document><document><year>2008</year><authors>Bo Cai1 | Renate Meyer2 | Franois Perron3</authors><title>Metropolis&amp;#8211;Hastings algorithms with adaptive proposals      </title><content>Different strategies have been proposed to improve mixing and convergence properties of Markov Chain Monte Carlo algorithms.         These are mainly concerned with customizing the proposal density in the Metropolis&amp;#8211;Hastings algorithm to the specific target         density and require a detailed exploratory analysis of the stationary distribution and/or some preliminary experiments to         determine an efficient proposal. Various Metropolis&amp;#8211;Hastings algorithms have been suggested that make use of previously sampled         states in defining an adaptive proposal density. Here we propose a general class of adaptive Metropolis&amp;#8211;Hastings algorithms         based on Metropolis&amp;#8211;Hastings-within-Gibbs sampling. For the case of a one-dimensional target distribution, we present two         novel algorithms using mixtures of triangular and trapezoidal densities. These can also be seen as improved versions of the         all-purpose adaptive rejection Metropolis sampling (ARMS) algorithm to sample from non-logconcave univariate densities. Using         various different examples, we demonstrate their properties and efficiencies and point out their advantages over ARMS and         other adaptive alternatives such as the Normal Kernel Coupler.      </content></document><document><year>2008</year><authors>Dimitris Karlis1  | Anais Santourian1</authors><title>Model-based clustering with non-elliptically contoured distributions      </title><content>The majority of the existing literature on model-based clustering deals with symmetric components. In some cases, especially         when dealing with skewed subpopulations, the estimate of the number of groups can be misleading; if symmetric components are         assumed we need more than one component to describe an asymmetric group. Existing mixture models, based on multivariate normal         distributions and multivariate t distributions, try to fit symmetric distributions, i.e. they fit symmetric clusters. In the present paper, we propose the         use of finite mixtures of the normal inverse Gaussian distribution (and its multivariate extensions). Such finite mixture         models start from a density that allows for skewness and fat tails, generalize the existing models, are tractable and have         desirable properties. We examine both the univariate case, to gain insight, and the multivariate case, which is more useful         in real applications. EM type algorithms are described for fitting the models. Real data examples are used to demonstrate         the potential of the new model in comparison with existing ones.      </content></document><document><year>2008</year><authors>Martin L. Hazelton1  | Berwin A. Turlach2| 3 </authors><title>Nonparametric density deconvolution by weighted kernel estimators      </title><content>Nonparametric density estimation in the presence of measurement error is considered. The usual kernel deconvolution estimator         seeks to account for the contamination in the data by employing a modified kernel. In this paper a new approach based on a         weighted kernel density estimator is proposed. Theoretical motivation is provided by the existence of a weight vector that         perfectly counteracts the bias in density estimation without generating an excessive increase in variance. In practice a data         driven method of weight selection is required. Our strategy is to minimize the discrepancy between a standard kernel estimate         from the contaminated data on the one hand, and the convolution of the weighted deconvolution estimate with the measurement         error density on the other hand. We consider a direct implementation of this approach, in which the weights are optimized         subject to sum and non-negativity constraints, and a regularized version in which the objective function includes a ridge-type         penalty. Numerical tests suggest that the weighted kernel estimation can lead to tangible improvements in performance over         the usual kernel deconvolution estimator. Furthermore, weighted kernel estimates are free from the problem of negative estimation         in the tails that can occur when using modified kernels. The weighted kernel approach generalizes to the case of multivariate         deconvolution density estimation in a very straightforward manner.      </content></document><document><year>2008</year><authors>Juan A. Cuesta-Albertos1 | Antonio Cuevas2 | Ricardo Fraiman3| 4</authors><title>On projection-based tests for directional and compositional data      </title><content>A new class of nonparametric tests, based on random projections, is proposed. They can be used for several null hypotheses         of practical interest, including uniformity for spherical (directional) and compositional data, sphericity of the underlying         distribution and homogeneity in two-sample problems on the sphere or the simplex.                     The proposed procedures have a number of advantages, mostly associated with their flexibility (for example, they also work               to test &amp;#8220;partial uniformity&amp;#8221; in a subset of the sphere), computational simplicity and ease of application even in high-dimensional               cases.            </content></document><document><year>2008</year><authors>Ali Baharev1  | Sndor Kemny1 </authors><title>On the computation of the noncentral F and noncentral beta distribution      </title><content>Unfortunately many of the numerous algorithms for computing the comulative distribution function (cdf) and noncentrality parameter         of the noncentral F and beta distributions can produce completely incorrect results as demonstrated in the paper by examples. Existing algorithms         are scrutinized and those parts that involve numerical difficulties are identified. As a result, a pseudo code is presented         in which all the known numerical problems are resolved. This pseudo code can be easily implemented in programming language         C or FORTRAN without understanding the complicated mathematical background.                     Symbolic evaluation of a finite and closed formula is proposed to compute exact cdf values. This approach makes it possible               to check quickly and reliably the values returned by professional statistical packages over an extraordinarily wide parameter               range without any programming knowledge.            </content></document><document><year>2008</year><authors>A. Kume1  | S. G. Walker1 </authors><title>On the Fisher&amp;#8211;Bingham distribution      </title><content>This paper primarily is concerned with the sampling of the Fisher&amp;#8211;Bingham distribution and we describe a slice sampling algorithm         for doing this. A by-product of this task gave us an infinite mixture representation of the Fisher&amp;#8211;Bingham distribution; the         mixing distributions being based on the Dirichlet distribution. Finite numerical approximations are considered and a sampling         algorithm based on a finite mixture approximation is compared with the slice sampling algorithm.      </content></document><document><year>2008</year><authors>David Bremner1| Dan Chen2| John Iacono3| Stefan Langerman4 | Pat Morin5 </authors><title>Output-sensitive algorithms for Tukey depth and related problems      </title><content>The Tukey depth (Proceedings of the International Congress of Mathematicians, vol.;2, pp.;523&amp;#8211;531, 1975) of a point p with respect to a finite set S of points is the minimum number of elements of S contained in any closed halfspace that contains p. Algorithms for computing the Tukey depth of a point in various dimensions are considered. The running times of these algorithms         depend on the value of the output, making them suited to situations, such as outlier removal, where the value of the output         is typically small.      </content></document><document><year>2008</year><authors>Florent Chatelain1 | Sophie Lambert-Lacroix2  | Jean-Yves Tourneret1 </authors><title>Pairwise likelihood estimation for multivariate mixed Poisson models generated by Gamma intensities      </title><content>Estimating the parameters of multivariate mixed Poisson models is an important problem in image processing applications, especially         for active imaging or astronomy. The classical maximum likelihood approach cannot be used for these models since the corresponding         masses cannot be expressed in a simple closed form. This paper studies a maximum pairwise likelihood approach to estimate         the parameters of multivariate mixed Poisson models when the mixing distribution is a multivariate Gamma distribution. The         consistency and asymptotic normality of this estimator are derived. Simulations conducted on synthetic data illustrate these         results and show that the proposed estimator outperforms classical estimators based on the method of moments. An application         to change detection in low-flux images is also investigated.      </content></document><document><year>2008</year><authors>A. Berlinet1  | C. Rol|2 </authors><title>Parabolic acceleration of the EM algorithm      </title><content>A new acceleration scheme for optimization procedures is defined through geometric considerations and applied to the EM algorithm.         In many cases it is able to circumvent the problem of stagnation. No modification of the original algorithm is required. It         is simply used as a software component. Thus the new scheme can be easily implemented to accelerate a fixed point algorithm         maximizing some objective function. Some practical examples and simulations are presented to show its ability to accelerate         EM-type algorithms converging slowly.      </content></document><document><year>2008</year><authors>Paul David McNicholas1  | Thomas Brendan Murphy2 </authors><title>Parsimonious Gaussian mixture models      </title><content>Parsimonious Gaussian mixture models are developed using a latent Gaussian model which is closely related to the factor analysis         model. These models provide a unified modeling framework which includes the mixtures of probabilistic principal component         analyzers and mixtures of factor of analyzers models as special cases.                     In particular, a class of eight parsimonious Gaussian mixture models which are based on the mixtures of factor analyzers model               are introduced and the maximum likelihood estimates for the parameters in these models are found using an AECM algorithm.               The class of models includes parsimonious models that have not previously been developed.            </content></document><document><year>2008</year><authors>Gerhard Tutz1  | Jan Ulbricht1 </authors><title>Penalized regression with correlation-based penalty      </title><content>A new regularization method for regression models is proposed. The criterion to be minimized contains a penalty term which         explicitly links strength of penalization to the correlation between predictors. Like the elastic net, the method encourages         a grouping effect where strongly correlated predictors tend to be in or out of the model together. A boosted version of the         penalized estimator, which is based on a new boosting method, allows to select variables. Real world data and simulations         show that the method compares well to competing regularization techniques. In settings where the number of predictors is smaller         than the number of observations it frequently performs better than competitors, in high dimensional settings prediction measures         favor the elastic net while accuracy of estimation and stability of variable selection favors the newly proposed method.      </content></document><document><year>2008</year><authors>Ying Yang1 </authors><title>Penalized semiparametric density estimation      </title><content>In this article we propose a penalized likelihood approach for the semiparametric density model with parametric and nonparametric         components. An efficient iterative procedure is proposed for estimation. Approximate generalized maximum likelihood criterion         from Bayesian point of view is derived for selecting the smoothing parameter. The finite sample performance of the proposed         estimation approach is evaluated through simulation. Two real data examples, suicide study data and Old Faithful geyser data,         are analyzed to demonstrate use of the proposed method.      </content></document><document><year>2008</year><authors>Joseph Ryan G. Lansangan1  | Erniel B. Barrios1 </authors><title>Principal components analysis of nonstationary time series data      </title><content>The effect of nonstationarity in time series columns of input data in principal components analysis is examined. Nonstationarity         are very common among economic indicators collected over time. They are subsequently summarized into fewer indices for purposes         of monitoring. Due to the simultaneous drifting of the nonstationary time series usually caused by the trend, the first component         averages all the variables without necessarily reducing dimensionality. Sparse principal components analysis can be used,         but attainment of sparsity among the loadings (hence, dimension-reduction is achieved) is influenced by the choice of parameter(s)         (&amp;#955;         1,i            ). Simulated data with more variables than the number of observations and with different patterns of cross-correlations and         autocorrelations were used to illustrate the advantages of sparse principal components analysis over ordinary principal components         analysis. Sparse component loadings for nonstationary time series data can be achieved provided that appropriate values of         &amp;#955;         1,j             are used. We provide the range of values of &amp;#955;         1,j             that will ensure convergence of the sparse principal components algorithm and consequently achieve sparsity of component         loadings.      </content></document><document><year>2008</year><authors>Hee-Seok Oh1 | Donghoh Kim2  | Yongdai Kim1 </authors><title>Robust wavelet shrinkage using robust selection of thresholds      </title><content>This paper considers the problem of selecting a robust threshold of wavelet shrinkage. Previous approaches reported in literature         to handle the presence of outliers mainly focus on developing a robust procedure for a given threshold; this is related to         solving a nontrivial optimization problem. The drawback of this approach is that the selection of a robust threshold, which         is crucial for the resulting fit is ignored. This paper points out that the best fit can be achieved by a robust wavelet shrinkage         with a robust threshold. We propose data-driven selection methods for a robust threshold. These approaches are based on a         coupling of classical wavelet thresholding rules with pseudo data. The concept of pseudo data has influenced the implementation         of the proposed methods, and provides a fast and efficient algorithm. Results from a simulation study and a real example demonstrate         the promising empirical properties of the proposed approaches.      </content></document><document><year>2008</year><authors>W. Gonzlez-Manteiga1| M. D. Martnez-Mir|a2  | R. Raya-Mir|a2</authors><title>SiZer Map for inference with additive models      </title><content>Sizer Map is proposed as a graphical tool for assistance in nonparametric additive regression testing problems. Four problems         have been analyzed by using SiZer Map: testing for additivity, testing the components significance, testing parametric models         for the components and testing for interactions. The simplicity and flexibility of SiZer Map for our purposes are highlighted         from the performed empirical study with several real datasets. With these data, we compare the conclusions derived from SiZer         analysis with the global results derived from standard tests, previously proposed in the literature.      </content></document><document><year>2008</year><authors>Riccardo Gatto1 </authors><title>Some computational aspects of the generalized von Mises distribution      </title><content>This article deals with some important computational aspects of the generalized von Mises distribution in relation with parameter         estimation, model selection and simulation. The generalized von Mises distribution provides a flexible model for circular         data allowing for symmetry, asymmetry, unimodality and bimodality. For this model, we show the equivalence between the trigonometric         method of moments and the maximum likelihood estimators, we give their asymptotic distribution, we provide bias-corrected         estimators of the entropy, the Akaike information criterion and the measured entropy for model selection, and we implement         the ratio-of-uniforms method of simulation.      </content></document><document><year>2008</year><authors>Eric Renshaw1  | Carlos Comas2</authors><title>Space-time generation of high intensity patterns using growth-interaction processes      </title><content>We describe a novel spatial-temporal algorithm for generating packing structures of disks and spheres, which not only incorporates         all the attractive features of existing algorithms but is also more flexible in defining spatial interactions and other control         parameters. The advantage of this approach lies in the ability of marks to exploit to best advantage the space available to         them by changing their size in response to the interaction pressure of their neighbours. Allowing particles to move in response         to such pressure results in high-intensity packing. Indeed, since particles may temporarily overlap, even under hard-packing         scenarios, they possess a greater potential for rearranging themselves, and thereby creating even higher packing intensities         than exist under other strategies. Non-overlapping pattern structures are achieved simply by allowing the process to &amp;#8216;burn-out&amp;#8217;         at the end of its development period. A variety of different growth-interaction regimes are explored, both symmetric and asymmetric,         and the convergence issues that they raise are examined. We conjecture that not only may this algorithm be easily generalised         to cover a large variety of situations across a wide range of disciplines, but that appropriately targeted generalisations         may well include established packing algorithms as special cases.      </content></document><document><year>2008</year><authors>Reinhard Furrer1  | Stephan R. Sain2 </authors><title>Spatial model fitting for large datasets with applications to climate and microarray problems      </title><content>Many problems in the environmental and biological sciences involve the analysis of large quantities of data. Further, the         data in these problems are often subject to various types of structure and, in particular, spatial dependence. Traditional         model fitting often fails due to the size of the datasets since it is difficult to not only specify but also to compute with         the full covariance matrix describing the spatial dependence. We propose a very general type of mixed model that has a random         spatial component. Recognizing that spatial covariance matrices often exhibit a large number of zero or near-zero entries,         covariance tapering is used to force near-zero entries to zero. Then, taking advantage of the sparse nature of such tapered         covariance matrices, backfitting is used to estimate the fixed and random model parameters. The novelty of the paper is the         combination of the two techniques, tapering and backfitting, to model and analyze spatial datasets several orders of magnitude         larger than those datasets typically analyzed with conventional approaches. Results will be demonstrated with two datasets.         The first consists of regional climate model output that is based on an experiment with two regional and two driver models         arranged in a two-by-two layout. The second is microarray data used to build a profile of differentially expressed genes relating         to cerebral vascular malformations, an important cause of hemorrhagic stroke and seizures.      </content></document><document><year>2008</year><authors>C. A. Glasbey1 </authors><title>Two-dimensional generalisations of dynamic programming for;image;analysis      </title><content>Dynamic programming (DP) is a fast, elegant method for solving many one-dimensional optimisation problems but, unfortunately,         most problems in image analysis, such as restoration and warping, are two-dimensional. We consider three generalisations of         DP. The first is iterated dynamic programming (IDP), where DP is used to recursively solve each of a sequence of one-dimensional         problems in turn, to find a local optimum. A second algorithm is an empirical, stochastic optimiser, which is implemented         by adding progressively less noise to IDP. The final approach replaces DP by a more computationally intensive Forward-Backward         Gibbs Sampler, and uses a simulated annealing cooling schedule. Results are compared with existing pixel-by-pixel methods         of iterated conditional modes (ICM) and simulated annealing in two applications: to restore a synthetic aperture radar (SAR)         image, and to warp a pulsed-field electrophoresis gel into alignment with a reference image. We find that IDP and its stochastic         variant outperform the remaining algorithms.      </content></document><document><year>2008</year><authors>C. A. McGrory1 | D. M. Titterington2| R. Reeves1 | A. N. Pettitt1</authors><title>Variational Bayes for estimating the parameters of a hidden Potts model      </title><content>Hidden Markov random field models provide an appealing representation of images and other spatial problems. The drawback is         that inference is not straightforward for these models as the normalisation constant for the likelihood is generally intractable         except for very small observation sets. Variational methods are an emerging tool for Bayesian inference and they have already         been successfully applied in other contexts. Focusing on the particular case of a hidden Potts model with Gaussian noise,         we show how variational Bayesian methods can be applied to hidden Markov random field inference. To tackle the obstacle of         the intractable normalising constant for the likelihood, we explore alternative estimation approaches for incorporation into         the variational Bayes algorithm. We consider a pseudo-likelihood approach as well as the more recent reduced dependence approximation         of the normalisation constant. To illustrate the effectiveness of these approaches we present empirical results from the analysis         of simulated datasets. We also analyse a real dataset and compare results with those of previous analyses as well as those         obtained from the recently developed auxiliary variable MCMC method and the recursive MCMC method. Our results show that the         variational Bayesian analyses can be carried out much faster than the MCMC analyses and produce good estimates of model parameters.         We also found that the reduced dependence approximation of the normalisation constant outperformed the pseudo-likelihood approximation         in our analysis of real and synthetic datasets.      </content></document><document><year>2006</year><authors>Robert G. Aykroyd1  | Brain A. Cattle2 </authors><title>A flexible statistical and efficient computational approach to object location applied to electrical tomography      </title><content>In electrical tomography, multiple measurements of voltage are taken between electrodes on the boundary of a region with the         aim of investigating the electrical conductivity distribution within the region. The relationship between conductivity and         voltage is governed by an elliptic partial differential equation derived from Maxwell&amp;#8217;s equations. Recent statistical approaches,         combining Bayesian methods with Markov chain Monte Carlo (MCMC) algorithms, allow to greater flexibility than classical inverse         solution approaches and require only the calculation of voltages from a conductivity distribution. However, solution of this         forward problem still requires the use of the Finite Difference Method (FDM) or the Finite Element Method (FEM) and many thousands         of forward solutions are needed which strains practical feasibility.                     Many tomographic applications involve locating the perimeter of some homogeneous conductivity objects embedded in a homogeneous               background. It is possible to exploit this type of structure using the Boundary Element Method (BEM) to provide a computationally               efficient alternative forward solution technique. A geometric model is then used to define the region boundary, with priors               on boundary smoothness and on the range of feasible conductivity values. This paper investigates the use of a BEM/MCMC approach               for electrical resistance tomography (ERT) data. The efficiency of the boundary-element method coupled with the flexibility               of the MCMC technique gives a promising new approach to object identification in electrical tomography. Simulated ERT data               are used to illustrate the procedures.            </content></document><document><year>2006</year><authors>Cajo J. F. Ter Braak1 </authors><title>A Markov Chain Monte Carlo version of the genetic algorithm Differential Evolution: easy Bayesian computing for real parameter         spaces      </title><content>Differential Evolution (DE) is a simple genetic algorithm for numerical optimization in real parameter spaces. In a statistical         context one would not just want the optimum but also its uncertainty. The uncertainty distribution can be obtained by a Bayesian         analysis (after specifying prior and likelihood) using Markov Chain Monte Carlo (MCMC) simulation. This paper integrates the         essential ideas of DE and MCMC, resulting in Differential Evolution Markov Chain (DE-MC). DE-MC is a population MCMC algorithm,         in which multiple chains are run in parallel. DE-MC solves an important problem in MCMC, namely that of choosing an appropriate         scale and orientation for the jumping distribution. In DE-MC the jumps are simply a fixed multiple of the differences of two         random parameter vectors that are currently in the population. The selection process of DE-MC works via the usual Metropolis         ratio which defines the probability with which a proposal is accepted. In tests with known uncertainty distributions, the         efficiency of DE-MC with respect to random walk Metropolis with optimal multivariate Normal jumps ranged from 68% for small         population sizes to 100% for large population sizes and even to 500% for the 97.5% point of a variable from a 50-dimensional         Student distribution. Two Bayesian examples illustrate the potential of DE-MC in practice. DE-MC is shown to facilitate multidimensional         updates in a multi-chain &amp;#8220;Metropolis-within-Gibbs&amp;#8221; sampling approach. The advantage of DE-MC over conventional MCMC are simplicity,         speed of calculation and convergence, even for nearly collinear parameters and multimodal densities.      </content></document><document><year>2006</year><authors>Matthew A. Nunes1 | Marina I. Knight1 | Guy P. Nason1</authors><title>Adaptive lifting for nonparametric regression      </title><content>Many wavelet shrinkage methods assume that the data are observed on an equally spaced grid of length of the form 2J for some J. These methods require serious modification or preprocessed data to cope with irregularly spaced data. The lifting scheme         is a recent mathematical innovation that obtains a multiscale analysis for irregularly spaced data.                     A key lifting component is the &amp;#8220;predict&amp;#8221; step where a prediction of a data point is made. The residual from the prediction               is stored and can be thought of as a wavelet coefficient. This article exploits the flexibility of lifting by adaptively choosing               the kind of prediction according to a criterion. In this way the smoothness of the underlying &amp;#8216;wavelet&amp;#8217; can be adapted to               the local properties of the function. Multiple observations at a point can readily be handled by lifting through a suitable               choice of prediction. We adapt existing shrinkage rules to work with our adaptive lifting methods.            </content></document><document><year>2006</year><authors>Barry R. Cobb1 | Prakash P. Shenoy2  | Rafael Rum3 </authors><title>Approximating probability density functions in hybrid Bayesian networks with mixtures of truncated exponentials      </title><content>Mixtures of truncated exponentials (MTE) potentials are an alternative to discretization and Monte Carlo methods for solving         hybrid Bayesian networks. Any probability density function (PDF) can be approximated by an MTE potential, which can always         be marginalized in closed form. This allows propagation to be done exactly using the Shenoy-Shafer architecture for computing         marginals, with no restrictions on the construction of a join tree. This paper presents MTE potentials that approximate standard         PDF&amp;#8217;s and applications of these potentials for solving inference problems in hybrid Bayesian networks. These approximations         will extend the types of inference problems that can be modelled with Bayesian networks, as demonstrated using three examples.      </content></document><document><year>2006</year><authors>Mike K. P. So1 </authors><title>Bayesian analysis of nonlinear and non-Gaussian state space models via multiple-try sampling methods      </title><content>We develop in this paper three multiple-try blocking schemes for Bayesian analysis of nonlinear and non-Gaussian state space         models. To reduce the correlations between successive iterates and to avoid getting trapped in a local maximum, we construct         Markov chains by drawing state variables in blocks with multiple trial points. The first and second methods adopt autoregressive         and independent kernels to produce the trial points, while the third method uses samples along suitable directions. Using         the time series structure of the state space models, the three sampling schemes can be implemented efficiently. In our multimodal         examples, the three multiple-try samplers are able to generate the desired posterior sample, whereas existing methods fail         to do so.      </content></document><document><year>2006</year><authors>G. J. Gibson1 | W. Otten2| J. A. N. Filipe2| 3| A. Cook1| 4| G. Marion4 | C. A. Gilligan2</authors><title>Bayesian estimation for percolation models of disease spread in plant populations      </title><content>Statistical methods are formulated for fitting and testing percolation-based, spatio-temporal models that are generally applicable         to biological or physical processes that evolve in spatially distributed populations. The approach is developed and illustrated         in the context of the spread of Rhizoctonia solani, a fungal pathogen, in radish but is readily generalized to other scenarios. The particular model considered represents processes         of primary and secondary infection between nearest-neighbour hosts in a lattice, and time-varying susceptibility of the hosts.         Bayesian methods for fitting the model to observations of disease spread through space and time in replicate populations are         developed. These use Markov chain Monte Carlo methods to overcome the problems associated with partial observation of the         process. We also consider how model testing can be achieved by embedding classical methods within the Bayesian analysis. In         particular we show how a residual process, with known sampling distribution, can be defined. Model fit is then examined by         generating samples from the posterior distribution of the residual process, to which a classical test for consistency with         the known distribution is applied, enabling the posterior distribution of the P-value of the test used to be estimated. For the Rhizoctonia-radish system the methods confirm the findings of earlier non-spatial analyses regarding the dynamics of disease transmission         and yield new evidence of environmental heterogeneity in the replicate experiments.      </content></document><document><year>2006</year><authors>Jukka Cor|er1 | Mats Gyllenberg1 | Timo Koski1| 2</authors><title>Bayesian model learning based on a parallel MCMC strategy      </title><content>We introduce a novel Markov chain Monte Carlo algorithm for estimation of posterior probabilities over discrete model spaces.         Our learning approach is applicable to families of models for which the marginal likelihood can be analytically calculated,         either exactly or approximately, given any fixed structure. It is argued that for certain model neighborhood structures, the         ordinary reversible Metropolis-Hastings algorithm does not yield an appropriate solution to the estimation problem. Therefore,         we develop an alternative, non-reversible algorithm which can avoid the scaling effect of the neighborhood. To efficiently         explore a model space, a finite number of interacting parallel stochastic processes is utilized. Our interaction scheme enables         exploration of several local neighborhoods of a model space simultaneously, while it prevents the absorption of any particular         process to a relatively inferior state. We illustrate the advantages of our method by an application to a classification model.         In particular, we use an extensive bacterial database and compare our results with results obtained by different methods for         the same data.      </content></document><document><year>2006</year><authors>Andrew Golightly1  | Darren J. Wilkinson1</authors><title>Bayesian sequential inference for nonlinear multivariate diffusions      </title><content>In this paper, we adapt recently developed simulation-based sequential algorithms to the problem concerning the Bayesian analysis         of discretely observed diffusion processes. The estimation framework involves the introduction of m&amp;#8722;1 latent data points between every pair of observations. Sequential MCMC methods are then used to sample the posterior distribution         of the latent data and the model parameters on-line. The method is applied to the estimation of parameters in a simple stochastic         volatility model (SV) of the U.S. short-term interest rate. We also provide a simulation study to validate our method, using         synthetic data generated by the SV model with parameters calibrated to match weekly observations of the U.S. short-term interest         rate.      </content></document><document><year>2006</year><authors>Enrique Alba1  | Enrique Domnguez1 </authors><title>Comparative analysis of modern optimization tools for the p-median problem      </title><content>This paper develops a study on different modern optimization techniques to solve the p-median problem. We analyze the behavior of a class of evolutionary algorithm (EA) known as cellular EA (cEA), and compare         it against a tailored neural network model and against a canonical genetic algorithm for optimization of the p-median problem. We also compare against existing approaches including variable neighborhood search and parallel scatter search,         and show their relative performances on a large set of problem instances. Our conclusions state the advantages of using a         cEA: wide applicability, low implementation effort and high accuracy. In addition, the neural network model shows up as being         the more accurate tool at the price of a narrow applicability and larger customization effort.      </content></document><document><year>2006</year><authors>Nikolaos Demiris1  | Philip D. O&amp;#8217 Neill2</authors><title>Computation of final outcome probabilities for the generalised stochastic epidemic      </title><content>This paper is concerned with methods for the numerical calculation of the final outcome distribution for a well-known stochastic         epidemic model in a closed population. The model is of the SIR (Susceptible&amp;#8594;Infected&amp;#8594; Removed) type, and the infectious period         can have any specified distribution. The final outcome distribution is specified by the solution of a triangular system of         linear equations, but the form of the distribution leads to inherent numerical problems in the solution. Here we employ multiple         precision arithmetic to surmount these problems. As applications of our methodology, we assess the accuracy of two approximations         that are frequently used in practice, namely an approximation for the probability of an epidemic occurring, and a Gaussian         approximation to the final number infected in the event of an outbreak. We also present an example of Bayesian inference for         the epidemic threshold parameter.      </content></document><document><year>2006</year><authors>A. W. Bowman1 | A. Pope2  | B. Ismail3 </authors><title>Detecting discontinuities in nonparametric regression curves and surfaces      </title><content>The existence of a discontinuity in a regression function can be inferred by comparing regression estimates based on the data         lying on different sides of a point of interest. This idea has been used in earlier research by Hall and Titterington (1992),         Mller (1992) and later authors. The use of nonparametric regression allows this to be done without assuming linear or other         parametric forms for the continuous part of the underlying regression function. The focus of the present paper is on assessing         the evidence for the presence of a discontinuity within a regression function through examination of the standardised differences         of &amp;#8216;left&amp;#8217; and &amp;#8216;right&amp;#8217; estimators at a variety of covariate values. The calculations for the test are carried out through distributional         results on quadratic forms. A graphical method in the form of a reference band to highlight the sources of the evidence for         discontinuities is proposed. The methods are also developed for the two covariate case where there are additional issues associated         with the presence of a jump location curve. Methods for estimating this curve are also developed. All the techniques, for         the one and two covariate situations, are illustrated through applications.      </content></document><document><year>2006</year><authors>Onno Zoeter1  | Tom Heskes1 </authors><title>Deterministic approximate inference techniques for conditionally Gaussian state space models      </title><content>We describe a novel deterministic approximate inference technique for conditionally Gaussian state space models, i.e. state         space models where the latent state consists of both multinomial and Gaussian distributed variables. The method can be interpreted         as a smoothing pass and iteration scheme symmetric to an assumed density filter. It improves upon previously proposed smoothing         passes by not making more approximations than implied by the projection onto the chosen parametric form, the assumed density.         Experimental results show that the novel scheme outperforms these alternative deterministic smoothing passes. Comparisons         with sampling methods suggest that the performance does not degrade with longer sequences.      </content></document><document><year>2006</year><authors>Heikki Haario1 | Marko Laine1| Antonietta Mira2 | Eero Saksman3</authors><title>DRAM: Efficient adaptive MCMC      </title><content>We propose to combine two quite powerful ideas that have recently appeared in the Markov chain Monte Carlo literature: adaptive         Metropolis samplers and delayed rejection. The ergodicity of the resulting non-Markovian sampler is proved, and the efficiency         of the combination is demonstrated with various examples. We present situations where the combination outperforms the original         methods: adaptation clearly enhances efficiency of the delayed rejection algorithm in cases where good proposal distributions         are not available. Similarly, delayed rejection provides a systematic remedy when the adaptation process has a slow start.      </content></document><document><year>2006</year><authors>Wolfgang Jank1 </authors><title>Efficient simulated maximum likelihood with an application to online retailing      </title><content>Simulated maximum likelihood estimates an analytically intractable likelihood function with an empirical average based on         data simulated from a suitable importance sampling distribution. In order to use simulated maximum likelihood in an efficient         way, the choice of the importance sampling distribution as well as the mechanism to generate the simulated data are crucial.         In this paper we develop a new heuristic for an automated, multistage implementation of simulated maximum likelihood which,         by adaptively updating the importance sampler, approximates the (locally) optimal importance sampling distribution. The proposed         approach also allows for a convenient incorporation of quasi-Monte Carlo methods. Quasi-Monte Carlo methods produce simulated         data which can significantly increase the accuracy of the likelihood-estimate over regular Monte Carlo methods. Several examples         provide evidence for the potential efficiency gain of this new method. We apply the method to a computationally challenging         geostatistical model of online retailing.      </content></document><document><year>2006</year><authors>Paul Fearnhead1 </authors><title>Exact and efficient Bayesian inference for multiple changepoint problems      </title><content>We demonstrate how to perform direct simulation from the posterior distribution of a class of multiple changepoint models         where the number of changepoints is unknown. The class of models assumes independence between the posterior distribution of         the parameters associated with segments of data between successive changepoints. This approach is based on the use of recursions,         and is related to work on product partition models. The computational complexity of the approach is quadratic in the number         of observations, but an approximate version, which introduces negligible error, and whose computational cost is roughly linear         in the number of observations, is also possible. Our approach can be useful, for example within an MCMC algorithm, even when         the independence assumptions do not hold. We demonstrate our approach on coal-mining disaster data and on well-log data. Our         method can cope with a range of models, and exact simulation from the posterior distribution is possible in a matter of minutes.      </content></document><document><year>2006</year><authors>Donald B. Percival1  | William L. B. Constantine2</authors><title>Exact simulation of Gaussian Time Series from Nonparametric Spectral Estimates with Application to Bootstrapping      </title><content>The circulant embedding method for generating statistically exact simulations of time series from certain Gaussian distributed         stationary processes is attractive because of its advantage in computational speed over a competitive method based upon the         modified Cholesky decomposition. We demonstrate that the circulant embedding method can be used to generate simulations from         stationary processes whose spectral density functions are dictated by a number of popular nonparametric estimators, including         all direct spectral estimators (a special case being the periodogram), certain lag window spectral estimators, all forms of         Welch's overlapped segment averaging spectral estimator and all basic multitaper spectral estimators. One application for         this technique is to generate time series for bootstrapping various statistics. When used with bootstrapping, our proposed         technique avoids some &amp;#8211; but not all &amp;#8211; of the pitfalls of previously proposed frequency domain methods for simulating time         series.      </content></document><document><year>2006</year><authors>Youngjo Lee1  | John A. Nelder2 </authors><title>Fitting via alternative random-effect models      </title><content>When there are two alternative random-effect models leading to the same marginal model, inferences from one model can be used         for the other model. We illustrate how a likelihood method for fitting models with independent random effects can be applied         to seemingly very different models with correlated random effects. We also discuss some merits of using these alternative         models.      </content></document><document><year>2006</year><authors>Yongtao Guan1 | Rol| Fleiner1 | Paul Joyce1| 2  | Stephen M. Krone1 </authors><title>Markov Chain Monte Carlo in small worlds      </title><content>As the number of applications for Markov Chain Monte Carlo (MCMC) grows, the power of these methods as well as their shortcomings         become more apparent. While MCMC yields an almost automatic way to sample a space according to some distribution, its implementations         often fall short of this task as they may lead to chains which converge too slowly or get trapped within one mode of a multi-modal         space. Moreover, it may be difficult to determine if a chain is only sampling a certain area of the space or if it has indeed         reached stationarity.                     In this paper, we show how a simple modification of the proposal mechanism results in faster convergence of the chain and               helps to circumvent the problems described above. This mechanism, which is based on an idea from the field of &amp;#8220;small-world&amp;#8221;               networks, amounts to adding occasional &amp;#8220;wild&amp;#8221; proposals to any local proposal scheme. We demonstrate through both theory and               extensive simulations, that these new proposal distributions can greatly outperform the traditional local proposals when it               comes to exploring complex heterogenous spaces and multi-modal distributions. Our method can easily be applied to most, if               not all, problems involving MCMC and unlike many other remedies which improve the performance of MCMC it preserves the simplicity               of the underlying algorithm.            </content></document><document><year>2006</year><authors>Jan Pol|1  | Marcus Hutter2 </authors><title>MDL convergence speed for Bernoulli sequences      </title><content>The Minimum Description Length principle for online sequence estimation/prediction in a proper learning setup is studied.         If the underlying model class is discrete, then the total expected square loss is a particularly interesting performance measure:         (a) this quantity is finitely bounded, implying convergence with probability one, and (b) it additionally specifies the convergence         speed. For MDL, in general one can only have loss bounds which are finite but exponentially larger than those for Bayes mixtures.         We show that this is even the case if the model class contains only Bernoulli distributions. We derive a new upper bound on         the prediction error for countable Bernoulli classes. This implies a small bound (comparable to the one for Bayes mixtures)         for certain important model classes. We discuss the application to Machine Learning tasks such as classification and hypothesis         testing, and generalization to countable classes of i.i.d. models.      </content></document><document><year>2006</year><authors>Kenny Y. F. Chan1| Stephen M. S. Lee1  | Kai W. Ng1</authors><title>Minimum variance unbiased estimation based on bootstrap iterations      </title><content>Practical computation of the minimum variance unbiased estimator (MVUE) is often a difficult, if not impossible, task, even         though general theory assures its existence under regularity conditions. We propose a new approach based on iterative bootstrap         bias correction of the maximum likelihood estimator to accurately approximate the MVUE. Viewing bootstrap iteration as a Markov         process, we develop a computational algorithm for bias correction based on arbitrarily many bootstrap iterations. The algorithm,         when applied parametrically to finite sample spaces, does not involve Monte Carlo simulation. For infinite sample spaces,         a nonparametric version of the algorithm is combined with a preliminary round of Monte Carlo simulation to yield an approximate         MVUE. Both algorithms are computationally more efficient and stable than conventional simulation-based bootstrap iterations.         Examples are given of both finite and infinite sample spaces to illustrate the effectiveness of our new approach.      </content></document><document><year>2006</year><authors>T. Bernholt1 | R. Fried2 | U. Gather3  | I. Wegener1 </authors><title>Modified repeated median filters      </title><content>We discuss moving window techniques for fast extraction of a signal composed of monotonic trends and abrupt shifts from a         noisy time series with irrelevant spikes. Running medians remove spikes and preserve shifts, but they deteriorate in trend         periods. Modified trimmed mean filters use a robust scale estimate such as the median absolute deviation about the median         (MAD) to select an adaptive amount of trimming. Application of robust regression, particularly of the repeated median, has         been suggested for improving upon the median in trend periods. We combine these ideas and construct modified filters based         on the repeated median offering better shift preservation. All these filters are compared w.r.t. fundamental analytical properties         and in basic data situations. An algorithm for the update of the MAD running in time O(log n) for window width n is presented as well.      </content></document><document><year>2006</year><authors>Petros Dellaportas1  | Ioulia Papageorgiou1</authors><title>Multivariate mixtures of normals with unknown number of components      </title><content>We present full Bayesian analysis of finite mixtures of multivariate normals with unknown number of components. We adopt reversible         jump Markov chain Monte Carlo and we construct, in a manner similar to that of Richardson and Green (1997), split and merge         moves that produce good mixing of the Markov chains. The split moves are constructed on the space of eigenvectors and eigenvalues         of the current covariance matrix so that the proposed covariance matrices are positive definite. Our proposed methodology         has applications in classification and discrimination as well as heterogeneity modelling. We test our algorithm with real         and simulated data.      </content></document><document><year>2006</year><authors>Jon D. McAuliffe1 | David M. Blei2 | Michael I. Jordan3</authors><title>Nonparametric empirical Bayes for the Dirichlet process mixture model      </title><content>The Dirichlet process prior allows flexible nonparametric mixture modeling. The number of mixture components is not specified         in advance and can grow as new data arrive. However, analyses based on the Dirichlet process prior are sensitive to the choice         of the parameters, including an infinite-dimensional distributional parameter G         0. Most previous applications have either fixed G         0 as a member of a parametric family or treated G         0 in a Bayesian fashion, using parametric prior specifications. In contrast, we have developed an adaptive nonparametric method         for constructing smooth estimates of G         0. We combine this method with a technique for estimating &amp;#945;, the other Dirichlet process parameter, that is inspired by an         existing characterization of its maximum-likelihood estimator. Together, these estimation procedures yield a flexible empirical         Bayes treatment of Dirichlet process mixtures. Such a treatment is useful in situations where smooth point estimates of G         0 are of intrinsic interest, or where the structure of G         0 cannot be conveniently modeled with the usual parametric prior families. Analysis of simulated and real-world datasets illustrates         the robustness of this approach.      </content></document><document><year>2006</year><authors>Jo Eidsvik1  | HKon Tjelmel|1</authors><title>On directional Metropolis&amp;#8211;Hastings algorithms      </title><content>New Metropolis&amp;#8211;Hastings algorithms using directional updates are introduced in this paper. Each iteration of a directional         Metropolis&amp;#8211;Hastings algorithm consists of three steps (i) generate a line by sampling an auxiliary variable, (ii) propose         a new state along the line, and (iii) accept/reject according to the Metropolis&amp;#8211;Hastings acceptance probability. We consider         two classes of directional updates. The first uses a point in                                        n             as auxiliary variable, the second an auxiliary direction vector. The proposed algorithms generalize previous directional         updating schemes since we allow the distribution of the auxiliary variable to depend on properties of the target at the current         state. By letting the proposal distribution along the line depend on the density of the auxiliary variable, we identify proposal         mechanisms that give unit acceptance rate. When we use direction vector as auxiliary variable, we get the advantageous effect         of large moves in the Markov chain and hence the autocorrelation length of the samples is small. We apply the directional         Metropolis&amp;#8211;Hastings algorithms to a Gaussian example, a mixture of Gaussian densities, and a Bayesian model for seismic data.      </content></document><document><year>2006</year><authors>Nizar Bouguila1 | Djemel Ziou1  | Ernest Monga1 </authors><title>Practical Bayesian estimation of a finite beta mixture through gibbs sampling and its applications      </title><content>This paper deals with a Bayesian analysis of a finite Beta mixture model. We present approximation method to evaluate the         posterior distribution and Bayes estimators by Gibbs sampling, relying on the missing data structure of the mixture model.         Experimental results concern contextual and non-contextual evaluations. The non-contextual evaluation is based on synthetic         histograms, while the contextual one model the class-conditional densities of pattern-recognition data sets. The Beta mixture         is also applied to estimate the parameters of SAR images histograms.      </content></document><document><year>2006</year><authors>Alfred Kume1  | Stephen G. Walker1 </authors><title>Sampling from compositional and directional distributions      </title><content>This paper describes a method for sampling from a non-standard distribution which is important in both population genetics         and directional statistics. Current approaches rely on complicated procedures which do not work well, if at all, in high dimensions         and usual parameter set-ups. We use a Gibbs sampler which seems necessary in practical situations of high dimensions.      </content></document><document><year>2006</year><authors>Martin J. Wolfsegger1  | Thomas Jaki2</authors><title>Simultaneous confidence intervals by iteratively adjusted alpha for relative effects in the one-way layout      </title><content>A bootstrap based method to construct 1&amp;#8722;&amp;#945; simultaneous confidence intervals for relative effects in the one-way layout is         presented. This procedure takes the stochastic correlation between the test statistics into account and results in narrower         simultaneous confidence intervals than the application of the Bonferroni correction. Instead of using the bootstrap distribution         of a maximum statistic, the coverage of the confidence intervals for the individual comparisons are adjusted iteratively until         the overall confidence level is reached. Empirical coverage and power estimates of the introduced procedure for many-to-one         comparisons are presented and compared with asymptotic procedures based on the multivariate normal distribution.      </content></document><document><year>2006</year><authors>Marina Meil&amp;#259 1  | Tommi Jaakkola2 </authors><title>Tractable Bayesian learning of tree belief networks      </title><content>In this paper we present decomposable priors, a family of priors over structure and parameters of tree belief nets for which Bayesian learning with complete observations         is tractable, in the sense that the posterior is also decomposable and can be completely determined analytically in polynomial         time. Our result is the first where computing the normalization constant and averaging over a super-exponential number of         graph structures can be performed in polynomial time. This follows from two main results: First, we show that factored distributions         over spanning trees in a graph can be integrated in closed form. Second, we examine priors over tree parameters and show that         a set of assumptions similar to Heckerman, Geiger and Chickering (1995) constrain the tree parameter priors to be a compactly         parametrized product of Dirichlet distributions. Besides allowing for exact Bayesian learning, these results permit us to         formulate a new class of tractable latent variable models in which the likelihood of a data point is computed through an ensemble         average over tree structures.      </content></document><document><year>2006</year><authors>Marco Alf1  | Murray Aitkin2| 3</authors><title>Variance component models for longitudinal count data with baseline information: epilepsy data revisited      </title><content>Random effect models have often been used in longitudinal data analysis since they allow for association among repeated measurements         due to unobserved heterogeneity. Various approaches have been proposed to extend mixed models for repeated count data to include         dependence on baseline counts. Dependence between baseline counts and individual-specific random effects result in a complex         form of the (conditional) likelihood. An approximate solution can be achieved ignoring this dependence, but this approach         could result in biased parameter estimates and in wrong inferences. We propose a computationally feasible approach to overcome         this problem, leaving the random effect distribution unspecified. In this context, we show how the EM algorithm for nonparametric         maximum likelihood (NPML) can be extended to deal with dependence of repeated measures on baseline counts.      </content></document><document><year>2006</year><authors>Umberto Amato1| Anestis Antoniadis2 | Marianna Pensky3 </authors><title>Wavelet kernel penalized estimation for non-equispaced design regression      </title><content>The paper considers regression problems with univariate design points. The design points are irregular and no assumptions         on their distribution are imposed. The regression function is retrieved by a wavelet based reproducing kernel Hilbert space         (RKHS) technique with the penalty equal to the sum of blockwise RKHS norms. In order to simplify numerical optimization, the         problem is replaced by an equivalent quadratic minimization problem with an additional penalty term. The computational algorithm         is described in detail and is implemented with both the sets of simulated and real data. Comparison with existing methods         showed that the technique suggested in the paper does not oversmooth the function and is superior in terms of the mean squared         error. It is also demonstrated that under additional assumptions on design points the method achieves asymptotic optimality         in a wide range of Besov spaces.      </content></document><document><year>2007</year><authors>Harald Binder1  | Gerhard Tutz2</authors><title>A comparison of methods for the fitting of generalized additive models      </title><content>         There are several procedures for fitting generalized additive models, i.e. regression models for an exponential family response         where the influence of each single covariates is assumed to have unknown, potentially non-linear shape. Simulated data are         used to compare a smoothing parameter optimization approach for selection of smoothness and of covariates, a stepwise approach,         a mixed model approach, and a procedure based on boosting techniques. In particular it is investigated how the performance         of procedures is linked to amount of information, type of response, total number of covariates, number of influential covariates,         and extent of non-linearity. Measures for comparison are prediction performance, identification of influential covariates,         and smoothness of fitted functions. One result is that the mixed model approach returns sparse fits with frequently over-smoothed         functions, while the functions are less smooth for the boosting approach and variable selection is less strict. The other         approaches are in between with respect to these measures. The boosting procedure is seen to perform very well when little         information is available and/or when a large number of covariates is to be investigated. It is somewhat surprising that in         scenarios with low information the fitting of a linear model, even with stepwise variable selection, has not much advantage         over the fitting of an additive model when the true underlying structure is linear. In cases with more information the prediction         performance of all procedures is very similar. So, in difficult data situations the boosting approach can be recommended,         in others the procedures can be chosen conditional on the aim of the analysis.               </content></document><document><year>2007</year><authors>S. A. Sisson1  | Y. Fan1 </authors><title>A distance-based diagnostic for trans-dimensional Markov chains      </title><content>         Over the last decade the use of trans-dimensional sampling algorithms has become endemic in the statistical literature. In         spite of their application however, there are few reliable methods to assess whether the underlying Markov chains have reached         their stationary distribution. In this article we present a distance-based method for the comparison of trans-dimensional         Markov chain sample output for a broad class of models. This diagnostic will simultaneously assess deviations between and         within chains. Illustration of the analysis of Markov chain sample-paths is presented in simulated examples and in two common         modelling situations: a finite mixture analysis and a change-point problem.               </content></document><document><year>2007</year><authors>Marco Alf1 | Luciano Nieddu2 | Donatella Vicari1</authors><title>A finite mixture model for image segmentation      </title><content>         In this paper, we propose a model for image segmentation based on a finite mixture of Gaussian distributions. For each pixel         of the image, prior probabilities of class memberships are specified through a Gibbs distribution, where association between         labels of adjacent pixels is modeled by a class-specific term allowing for different interaction strengths across classes.         We show how model parameters can be estimated in a maximum likelihood framework using Mean Field theory. Experimental performance         on perturbed phantom and on real benchmark images shows that the proposed method performs well in a wide variety of empirical         situations.               </content></document><document><year>2007</year><authors>David S. Leslie1| Robert Kohn2  | David J. Nott3</authors><title>A general approach to heteroscedastic linear regression      </title><content>Our article presents a general treatment of the linear regression model, in which the error distribution is modelled nonparametrically         and the error variances may be heteroscedastic, thus eliminating the need to transform the dependent variable in many data         sets. The mean and variance components of the model may be either parametric or nonparametric, with parsimony achieved through         variable selection and model averaging. A Bayesian approach is used for inference with priors that are data-based so that         estimation can be carried out automatically with minimal input by the user. A Dirichlet process mixture prior is used to model         the error distribution nonparametrically; when there are no regressors in the model, the method reduces to Bayesian density         estimation, and we show that in this case the estimator compares favourably with a well-regarded plug-in density estimator.         We also consider a method for checking the fit of the full model. The methodology is applied to a number of simulated and         real examples and is shown to work well.      </content></document><document><year>2007</year><authors>J.-J. Daudin1| F. Picard1 | S. Robin1 </authors><title>A mixture model for random graphs      </title><content>         The Erds&amp;#8211;Rnyi model of a network is simple and possesses many explicit expressions for average and asymptotic properties,         but it does not fit well to real-world networks. The vertices of those networks are often structured in unknown classes (functionally         related proteins or social communities) with different connectivity properties. The stochastic block structures model was         proposed for this purpose in the context of social sciences, using a Bayesian approach. We consider the same model in a frequentest         statistical framework. We give the degree distribution and the clustering coefficient associated with this model, a variational         method to estimate its parameters and a model selection criterion to select the number of classes. This estimation procedure         allows us to deal with large networks containing thousands of vertices. The method is used to uncover the modular structure         of a network of enzymatic reactions.               </content></document><document><year>2007</year><authors>Marc Lavielle1  | Cristian Meza2 </authors><title>A parameter expansion version of the SAEM algorithm      </title><content>The EM algorithm and its extensions are very popular tools for maximum likelihood estimation in incomplete data setting. However,         one of the limitations of these methods is their slow convergence. The PX-EM (parameter-expanded EM) algorithm was proposed         by Liu, Rubin and Wu to make EM much faster. On the other hand, stochastic versions of EM are powerful alternatives of EM         when the E-step is untractable in a closed form. In this paper we propose the PX-SAEM which is a parameter expansion version         of the so-called SAEM (Stochastic Approximation version of EM). PX-SAEM is shown to accelerate SAEM and improve convergence         toward the maximum likelihood estimate in a parametric framework. Numerical examples illustrate the behavior of PX-SAEM in         linear and nonlinear mixed effects models.      </content></document><document><year>2007</year><authors>Ulrike von Luxburg1 </authors><title>A tutorial on spectral clustering      </title><content>         In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement,         can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms         such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it         works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe         different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive         those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering         algorithms are discussed.               </content></document><document><year>2007</year><authors>Radu V. Craiu1  | Christiane Lemieux2 </authors><title>Acceleration of the Multiple-Try Metropolis algorithm using antithetic and stratified sampling      </title><content>The Multiple-Try Metropolis is a recent extension of the Metropolis algorithm in which the next state of the chain is selected         among a set of proposals. We propose a modification of the Multiple-Try Metropolis algorithm which allows for the use of correlated         proposals, particularly antithetic and stratified proposals. The method is particularly useful for random walk Metropolis         in high dimensional spaces and can be used easily when the proposal distribution is Gaussian. We explore the use of quasi         Monte Carlo (QMC) methods to generate highly stratified samples. A series of examples is presented to evaluate the potential         of the method.      </content></document><document><year>2007</year><authors>Juliana Gambini1 | Marta E. Mejail1 | Julio Jacobo-Berlles1  | Alej|ro C. Frery2 </authors><title>Accuracy of edge detection methods with local information in;speckled imagery      </title><content>         We compare the accuracy of five approaches for contour detection in speckled imagery. Some of these methods take advantage         of the statistical properties of speckled data, and all of them employ active contours using B-spline curves. Images obtained         with coherent illumination are affected by a noise called speckle, which is inherent to the imaging process. These data have         been statistically modeled by a multiplicative model using the G0 distribution, under which regions with different degrees         of roughness can be characterized by the value of a parameter. We use this information to find boundaries between regions         with different textures. We propose and compare five strategies for boundary detection: three based on the data (maximum discontinuity         on raw data, fractal dimension and maximum likelihood) and two based on estimates of the roughness parameter (maximum discontinuity         and anisotropic smoothed roughness estimates). In order to compare these strategies, a Monte Carlo experience was performed         to assess the accuracy of fitting a curve to a region. The probability of finding the correct edge with less than a specified         error is estimated and used to compare the techniques. The two best procedures are then compared in terms of their computational         cost and, finally, we show that the maximum likelihood approach on the raw data using the G0 law is the best technique.               </content></document><document><year>2007</year><authors>Allou Sam1 | Christophe Ambroise2  | Grard Govaert2 </authors><title>An online classification EM algorithm based on the mixture model      </title><content>         Mixture model-based clustering is widely used in many applications. In certain real-time applications the rapid increase of         data size with time makes classical clustering algorithms too slow. An online clustering algorithm based on mixture models         is presented in the context of a real-time flaw-diagnosis application for pressurized containers which uses data from acoustic         emission signals. The proposed algorithm is a stochastic gradient algorithm derived from the classification version of the         EM algorithm (CEM). It provides a model-based generalization of the well-known online k-means algorithm, able to handle non-spherical         clusters. Using synthetic and real data sets, the proposed algorithm is compared with the batch CEM algorithm and the online         EM algorithm. The three approaches generate comparable solutions in terms of the resulting partition when clusters are relatively         well separated, but online algorithms become faster as the size of the available observations increases.               </content></document><document><year>2007</year><authors>&#152;ivind Skare1| Jesper Mller2 | Eva B. Vedel Jensen3 </authors><title>Bayesian analysis of spatial point processes in the neighbourhood of Voronoi networks      </title><content>         A model for an inhomogeneous Poisson process with high intensity near the edges of a Voronoi tessellation in 2D or 3D is proposed.         The model is analysed in a Bayesian setting with priors on nuclei of the Voronoi tessellation and other model parameters.         An MCMC algorithm is constructed to sample from the posterior, which contains information about the unobserved Voronoi tessellation         and the model parameters. A major element of the MCMC algorithm is the reconstruction of the Voronoi tessellation after a         proposed local change of the tessellation. A simulation study and examples of applications from biology (animal territories)         and material science (alumina grain structure) are presented.               </content></document><document><year>2007</year><authors>Agostino Nobile1  | Alastair T. Fearnside1</authors><title>Bayesian finite mixtures with an unknown number of components: The allocation sampler      </title><content>A new Markov chain Monte Carlo method for the Bayesian analysis of finite mixture distributions with an unknown number of         components is presented. The sampler is characterized by a state space consisting only of the number of components and the         latent allocation variables. Its main advantage is that it can be used, with minimal changes, for mixtures of components from         any parametric family, under the assumption that the component parameters can be integrated out of the model analytically.         Artificial and real data sets are used to illustrate the method and mixtures of univariate and of multivariate normals are         explicitly considered. The problem of label switching, when parameter inference is of interest, is addressed in a post-processing         stage.      </content></document><document><year>2007</year><authors>R. J. Boys1 | D. J. Wilkinson1 | T. B. L. Kirkwood1</authors><title>Bayesian inference for a discretely observed stochastic kinetic model      </title><content>         The ability to infer parameters of gene regulatory networks is emerging as a key problem in systems biology. The biochemical         data are intrinsically stochastic and tend to be observed by means of discrete-time sampling systems, which are often limited         in their completeness. In this paper we explore how to make Bayesian inference for the kinetic rate constants of regulatory         networks, using the stochastic kinetic Lotka-Volterra system as a model. This simple model describes behaviour typical of         many biochemical networks which exhibit auto-regulatory behaviour. Various MCMC algorithms are described and their performance         evaluated in several data-poor scenarios. An algorithm based on an approximating process is shown to be particularly efficient.               </content></document><document><year>2007</year><authors>Loukia Meligkotsidou1 </authors><title>Bayesian multivariate Poisson mixtures with an unknown number of components      </title><content>In this paper we present Bayesian analysis of finite mixtures of multivariate Poisson distributions with an unknown number         of components. The multivariate Poisson distribution can be regarded as the discrete counterpart of the multivariate normal         distribution, which is suitable for modelling multivariate count data. Mixtures of multivariate Poisson distributions allow         for overdispersion and for negative correlations between variables. To perform Bayesian analysis of these models we adopt         a reversible jump Markov chain Monte Carlo (MCMC) algorithm with birth and death moves for updating the number of components.         We present results obtained from applying our modelling approach to simulated and real data. Furthermore, we apply our approach         to a problem in multivariate disease mapping, namely joint modelling of diseases with correlated counts.      </content></document><document><year>2007</year><authors>Sylvia Frhwirth-Schnatter1  | Regina Tchler2 </authors><title>Bayesian parsimonious covariance estimation for hierarchical linear mixed models      </title><content>         We consider a non-centered parameterization of the standard random-effects model, which is based on the Cholesky decomposition         of the variance-covariance matrix. The regression type structure of the non-centered parameterization allows us to use Bayesian         variable selection methods for covariance selection. We search for a parsimonious variance-covariance matrix by identifying         the non-zero elements of the Cholesky factors. With this method we are able to learn from the data for each effect whether         it is random or not, and whether covariances among random effects are zero. An application in marketing shows a substantial         reduction of the number of free elements in the variance-covariance matrix.               </content></document><document><year>2007</year><authors>Woncheol Jang1  | Martin Hendry2</authors><title>Cluster analysis of massive datasets in astronomy      </title><content>         Clusters of galaxies are a useful proxy to trace the distribution of mass in the universe. By measuring the mass of clusters         of galaxies on different scales, one can follow the evolution of the mass distribution (Martnez and Saar, Statistics of the Galaxy Distribution, 2002). It can be shown that finding galaxy clusters is equivalent to finding density contour clusters (Hartigan, Clustering Algorithms, 1975): connected components of the level set S                     c            &amp;#8801;{f&amp;gt;c} where f is a probability density function. Cuevas et;al. (Can. J. Stat. 28, 367&amp;#8211;382, 2000; Comput. Stat. Data Anal. 36, 441&amp;#8211;459, 2001) proposed a nonparametric method for density contour clusters, attempting to find density contour clusters by the minimal         spanning tree. While their algorithm is conceptually simple, it requires intensive computations for large datasets. We propose         a more efficient clustering method based on their algorithm with the Fast Fourier Transform (FFT). The method is applied to         a study of galaxy clustering on large astronomical sky survey data.               </content></document><document><year>2007</year><authors>Adelchi Azzalini1  | Nicola Torelli2 </authors><title>Clustering via nonparametric density estimation      </title><content>Although Hartigan (1975) had already put forward the idea of connecting identification of subpopulations with regions with         high density of the underlying probability distribution, the actual development of methods for cluster analysis has largely         shifted towards other directions, for computational convenience. Current computational resources allow us to reconsider this         formulation and to develop clustering techniques directly in order to identify local modes of the density. Given a set of         observations, a nonparametric estimate of the underlying density function is constructed, and subsets of points with high         density are formed through suitable manipulation of the associated Delaunay triangulation. The method is illustrated with         some numerical examples.      </content></document><document><year>2007</year><authors>Pierre Hansen1  | Nenad Mladenovi&amp;#263 2 </authors><title>Complement to a comparative analysis of heuristics for;the;p-median problem      </title><content>         A recent comparison of evolutionary, neural network, and scatter search heuristics for solving the p-median problem is completed by (i) gathering or obtaining exact optimal values in order to evaluate errors precisely, and         (ii) including results obtained with several variants of a variable neighborhood search (VNS) heuristic. For a first, well-known,         series of instances, the average errors of the evolutionary and neural network heuristics are over 10% and more than 1000         times larger than that of VNS. For a second series, this error is about 3% while the errors of the parallel VNS and of a hybrid         heuristic are about 0.01% and that of parallel scatter search even smaller.               </content></document><document><year>2007</year><authors>Paul Fearnhead1 </authors><title>Computational methods for complex stochastic systems: a review of some alternatives to MCMC      </title><content>         We consider analysis of complex stochastic models based upon partial information. MCMC and reversible jump MCMC are often         the methods of choice for such problems, but in some situations they can be difficult to implement; and suffer from problems         such as poor mixing, and the difficulty of diagnosing convergence. Here we review three alternatives to MCMC methods: importance         sampling, the forward-backward algorithm, and sequential Monte Carlo (SMC). We discuss how to design good proposal densities         for importance sampling, show some of the range of models for which the forward-backward algorithm can be applied, and show         how resampling ideas from SMC can be used to improve the efficiency of the other two methods. We demonstrate these methods         on a range of examples, including estimating the transition density of a diffusion and of a discrete-state continuous-time         Markov chain; inferring structure in population genetics; and segmenting genetic divergence data.               </content></document><document><year>2007</year><authors>W. S. Kendall1| J.-M. Marin2  | C. P. Robert3</authors><title>Confidence bands for Brownian motion and applications to Monte Carlo simulation      </title><content>Minimal area regions are constructed for Brownian paths and perturbed Brownian paths. While the theoretical optimal region         cannot be obtained in closed form, we provide practical confidence regions based on numerical approximations and local time         arguments. These regions are used to provide informal convergence assessments for both Monte Carlo and Markov Chain Monte         Carlo experiments, via the Brownian asymptotic approximation of cumulative sums.      </content></document><document><year>2007</year><authors>James P. McDermott1 | G. Jogesh Babu1| John C. Liechty2 | Dennis K. J. Lin3</authors><title>Data skeletons: simultaneous estimation of multiple quantiles for massive streaming datasets with applications to density         estimation      </title><content>         We consider the problem of density estimation when the data is in the form of a continuous stream with no fixed length. In         this setting, implementations of the usual methods of density estimation such as kernel density estimation are problematic.         We propose a method of density estimation for massive datasets that is based upon taking the derivative of a smooth curve         that has been fit through a set of quantile estimates. To achieve this, a low-storage, single-pass, sequential method is proposed         for simultaneous estimation of multiple quantiles for massive datasets that form the basis of this method of density estimation.         For comparison, we also consider a sequential kernel density estimator. The proposed methods are shown through simulation         study to perform well and to have several distinct advantages over existing methods.               </content></document><document><year>2007</year><authors>John S. Preisser1  | Jamie Perin1</authors><title>Deletion diagnostics for marginal mean and correlation model parameters in estimating equations      </title><content>         Regression diagnostics are introduced for parameters in marginal association models for clustered binary outcomes in an implementation         of generalized estimating equations. Estimating equations for intracluster correlations facilitate computational formulae         for one-step deletion diagnostics in an extension of earlier work on diagnostics for parameters in the marginal mean model.         The proposed diagnostics measure the influence of an observation or a cluster of observations on the estimated regression         parameters and on the overall fit of the model. The diagnostics are applied to data from four research studies from public         health and medicine.               </content></document><document><year>2007</year><authors>Richard J. Stevens1  | Trevor J. Sweeting2</authors><title>Estimation across multiple models with application to Bayesian computing and software development      </title><content>         Statistical models are sometimes incorporated into computer software for making predictions about future observations. When         the computer model consists of a single statistical model this corresponds to estimation of a function of the model parameters.         This paper is concerned with the case that the computer model implements multiple, individually-estimated statistical sub-models.         This case frequently arises, for example, in models for medical decision making that derive parameter information from multiple         clinical studies. We develop a method for calculating the posterior mean of a function of the parameter vectors of multiple         statistical models that is easy to implement in computer software, has high asymptotic accuracy, and has a computational cost         linear in the total number of model parameters. The formula is then used to derive a general result about posterior estimation         across multiple models. The utility of the results is illustrated by application to clinical software that estimates the risk         of fatal coronary disease in people with diabetes.               </content></document><document><year>2007</year><authors>M. Bock1 | A. W. Bowman1  | B. Ismail2 </authors><title>Estimation and inference for error variance in bivariate nonparametric regression      </title><content>In nonparametric regression, principal interest usually lies in the estimation of a smooth curve or surface defining the mean         response at particular values of the co-variates. However, in order to move beyond descriptive use an estimate of the underlying         error variance is required as an essential step in constructing interval estimates or comparing models. In the case of a single         covariate, a variety of methods is available for estimating error variance, based on local differencing techniques. These         methods are investigated in the important case of two covariates. An estimator constructed from residuals based on a nonparametric         regression surface using a very small value of smoothing parameter is proposed as the most effective approach. A specific         proposal based on nearest neighbour distances is investigated for choice of the smoothing parameter. Quadratic form techniques         are used to construct confidence intervals and tests based on estimates of error variance. These include comparisons of error         variances from different groups of data and the assessment of the assumption of constant variance across a regression surface.         The techniques are illustrated on examples with real data.      </content></document><document><year>2007</year><authors>Peter K. Dunn1  | Gordon K. Smyth2</authors><title>Evaluation of Tweedie exponential dispersion model densities by;Fourier inversion      </title><content>         The Tweedie family of distributions is a family of exponential dispersion models with power variance functions V(&amp;#956;)=&amp;#956;                     p             for                   . These distributions do not generally have density functions that can be written in closed form. However, they have simple         moment generating functions, so the densities can be evaluated numerically by Fourier inversion of the characteristic functions.         This paper develops numerical methods to make this inversion fast and accurate. Acceleration techniques are used to handle         oscillating integrands. A range of analytic results are used to ensure convergent computations and to reduce the complexity         of the parameter space. The Fourier inversion method is compared to a series evaluation method and the two methods are found         to be complementary in that they perform well in different regions of the parameter space.               </content></document><document><year>2007</year><authors>Dimitris Karlis1  | Panagiotis Tsiamyrtzis1 </authors><title>Exact Bayesian modeling for bivariate Poisson data and extensions      </title><content>         Bivariate count data arise in several different disciplines (epidemiology, marketing, sports statistics just to name a few)         and the bivariate Poisson distribution being a generalization of the Poisson distribution plays an important role in modelling         such data. In the present paper we present a Bayesian estimation approach for the parameters of the bivariate Poisson model         and provide the posterior distributions in closed forms. It is shown that the joint posterior distributions are finite mixtures         of conditionally independent gamma distributions for which their full form can be easily deduced by a recursively updating         scheme. Thus, the need of applying computationally demanding MCMC schemes for Bayesian inference in such models will be removed,         since direct sampling from the posterior will become available, even in cases where the posterior distribution of functions         of the parameters is not available in closed form. In addition, we define a class of prior distributions that possess an interesting         conjugacy property which extends the typical notion of conjugacy, in the sense that both prior and posteriors belong to the         same family of finite mixture models but with different number of components. Extension to certain other models including         multivariate models or models with other marginal distributions are discussed.               </content></document><document><year>2007</year><authors>B. Ganguli1  | M. P. W|2</authors><title>Feature significance in generalized additive models      </title><content>This paper develops inference for the significance of features such as peaks and valleys observed in additive modeling through         an extension of the SiZer-type methodology of Chaudhuri and Marron (1999) and Godtliebsen et al. (2002, 2004) to the case         where the outcome is discrete. We consider the problem of determining the significance of features such as peaks or valleys         in observed covariate effects both for the case of additive modeling where the main predictor of interest is univariate as         well as the problem of studying the significance of features such as peaks, inclines, ridges and valleys when the main predictor         of interest is geographical location. We work with low rank radial spline smoothers to allow to the handling of sparse designs         and large sample sizes. Reducing the problem to a Generalised Linear Mixed Model (GLMM) framework enables derivation of simulation-based         critical value approximations and guards against the problem of multiple inferences over a range of predictor values. Such         a reduction also allows for easy adjustment for confounders including those which have an unknown or complex effect on the         outcome. A simulation study indicates that our method has satisfactory power. Finally, we illustrate our methodology on several         data sets.      </content></document><document><year>2007</year><authors>A. Delaigle1| 2 | I. Gijbels3 </authors><title>Frequent problems in calculating integrals and optimizing objective functions: a case study in density deconvolution      </title><content>         Many statistical procedures involve calculation of integrals or optimization (minimization or maximization) of some objective         function. In practical implementation of these, the user often has to face specific problems such as seemingly numerical instability         of the integral calculation, choices of grid points, appearance of several local minima or maxima, etc. In this paper we provide         insights into these problems (why and when are they happening?), and give some guidelines of how to deal with them. Such problems         are not new, neither are the ways to deal with them, but it is worthwhile to devote serious considerations to them. For a         transparant and clear discussion of these issues, we focus on a particular statistical problem: nonparametric estimation of         a density from a sample that contains measurement errors. The discussions and guidelines remain valid though in other contexts.         In the density deconvolution setting, a kernel density estimator has been studied in detail in the literature. The estimator         is consistent and fully data-driven procedures have been proposed. When implemented in practice however, the estimator can         turn out to be very inaccurate if no adequate numerical procedures are used. We review the steps leading to the calculation         of the estimator and in selecting parameters of the method, and discuss the various problems encountered in doing so.               </content></document><document><year>2007</year><authors>Youngjo Lee1 | John A. Nelder2 | Maengseok Noh3</authors><title>H-likelihood: problems and solutions      </title><content>In recent issues of this journal it has been asserted in two papers that the use of h-likelihood is wrong, in the sense of         giving unsatisfactory estimates of some parameters for binary data (Kuk and Cheng, 1999; Waddington and Thompson, 2004) or         theoretically unsound (Kuk and Cheng, 1999). We wish to refute both these assertions.      </content></document><document><year>2007</year><authors>Martin Neil1| 2 | Manesh Tailor2 | David Marquez1</authors><title>Inference in hybrid Bayesian networks using dynamic discretization      </title><content>         We consider approximate inference in hybrid Bayesian Networks (BNs) and present a new iterative algorithm that efficiently         combines dynamic discretization with robust propagation algorithms on junction trees. Our approach offers a significant extension         to Bayesian Network theory and practice by offering a flexible way of modeling continuous nodes in BNs conditioned on complex         configurations of evidence and intermixed with discrete nodes as both parents and children of continuous nodes. Our algorithm         is implemented in a commercial Bayesian Network software package, AgenaRisk, which allows model construction and testing to         be carried out easily. The results from the empirical trials clearly show how our software can deal effectively with different         type of hybrid models containing elements of expert judgment as well as statistical inference. In particular, we show how         the rapid convergence of the algorithm towards zones of high probability density, make robust inference analysis possible         even in situations where, due to the lack of information in both prior and data, robust sampling becomes unfeasible.               </content></document><document><year>2007</year><authors>Pierre Pinson1 | Henrik Aa. Nielsen1 | Henrik Madsen1  | Torben S. Nielsen2 </authors><title>Local linear regression with adaptive orthogonal fitting for the wind power application      </title><content>         Short-term forecasting of wind generation requires a model of the function for the conversion of meteorological variables         (mainly wind speed) to power production. Such a power curve is nonlinear and bounded, in addition to being nonstationary.         Local linear regression is an appealing nonparametric approach for power curve estimation, for which the model coefficients         can be tracked with recursive Least Squares (LS) methods. This may lead to an inaccurate estimate of the true power curve,         owing to the assumption that a noise component is present on the response variable axis only. Therefore, this assumption is         relaxed here, by describing a local linear regression with orthogonal fit. Local linear coefficients are defined as those         which minimize a weighted Total Least Squares (TLS) criterion. An adaptive estimation method is introduced in order to accommodate         nonstationarity. This has the additional benefit of lowering the computational costs of updating local coefficients every         time new observations become available. The estimation method is based on tracking the left-most eigenvector of the augmented         covariance matrix. A robustification of the estimation method is also proposed. Simulations on semi-artificial datasets (for         which the true power curve is available) underline the properties of the proposed regression and related estimation methods.         An important result is the significantly higher ability of local polynomial regression with orthogonal fit to accurately approximate         the target regression, even though it may hardly be visible when calculating error criteria against corrupted data.               </content></document><document><year>2007</year><authors>Jouni Kerman1  | Andrew Gelman2 </authors><title>Manipulating and summarizing posterior simulations using random variable objects      </title><content>         Practical Bayesian data analysis involves manipulating and summarizing simulations from the posterior distribution of the         unknown parameters. By manipulation we mean computing posterior distributions of functions of the unknowns, and generating         posterior predictive distributions. The results need to be summarized both numerically and graphically.                                             We introduce, and implement in R, an object-oriented programming paradigm based on a random variable object type that is implicitly               represented by simulations. This makes it possible to define vector and array objects that may contain both random and deterministic               quantities, and syntax rules that allow to treat these objects like any numeric vectors or arrays, providing a solution to               various problems encountered in Bayesian computing involving posterior simulations.                           </content></document><document><year>2007</year><authors>Hongtu Zhu1 | Minggao Gu2 | Bradley Peterson3</authors><title>Maximum likelihood from spatial random effects models via the stochastic approximation expectation maximization algorithm      </title><content>We introduce a class of spatial random effects models that have Markov random fields (MRF) as latent processes. Calculating         the maximum likelihood estimates of unknown parameters in SREs is extremely difficult, because the normalizing factors of         MRFs and additional integrations from unobserved random effects are computationally prohibitive. We propose a stochastic approximation         expectation-maximization (SAEM) algorithm to maximize the likelihood functions of spatial random effects models. The SAEM         algorithm integrates recent improvements in stochastic approximation algorithms; it also includes components of the Newton-Raphson         algorithm and the expectation-maximization (EM) gradient algorithm. The convergence of the SAEM algorithm is guaranteed under         some mild conditions. We apply the SAEM algorithm to three examples that are representative of real-world applications: a         state space model, a noisy Ising model, and segmenting magnetic resonance images (MRI) of the human brain. The SAEM algorithm         gives satisfactory results in finding the maximum likelihood estimate of spatial random effects models in each of these instances.      </content></document><document><year>2007</year><authors>Alicja Jokiel-Rokita1 | Ryszard Magiera1 </authors><title>Minimax estimation of a probability of success under LINEX loss      </title><content>         Minimax estimation of a binomial probability under LINEX loss function is considered. It is shown that no equalizer estimator         is available in the statistical decision problem under consideration. It is pointed out that the problem can be solved by         determining the Bayes estimator with respect to a least favorable distribution having finite support. In this situation, the         optimal estimator and the least favorable distribution can be determined only by using numerical methods. Some properties         of the minimax estimators and the corresponding least favorable prior distributions are provided depending on the parameters         of the loss function. The properties presented are exploited in computing the minimax estimators and the least favorable distributions.         The results obtained can be applied to determine minimax estimators of a cumulative distribution function and minimax estimators         of a survival function.               </content></document><document><year>2007</year><authors>J.-H. Zhao1| 2 | Philip L. H. Yu1  | Qibao Jiang3 </authors><title>ML estimation for factor analysis: EM or non-EM?      </title><content>         To obtain maximum likelihood (ML) estimation in factor analysis (FA), we propose in this paper a novel and fast conditional maximization (CM) algorithm, which has quadratic and monotone         convergence, consisting of a sequence of CM log-likelihood (CML) steps. The main contribution of this algorithm is that the         closed form expression for the parameter to be updated in each step can be obtained explicitly, without resorting to any numerical         optimization methods. In addition, a new ECME algorithm similar to Liu&amp;#8217;s (Biometrika 81, 633&amp;#8211;648, 1994) one is obtained as a by-product, which turns out to be very close to the simple iteration algorithm proposed by Lawley (Proc.         R. Soc. Edinb. 60, 64&amp;#8211;82, 1940) but our algorithm is guaranteed to increase log-likelihood at every iteration and hence to converge. Both algorithms inherit         the simplicity and stability of EM but their convergence behaviors are much different as revealed in our extensive simulations:         (1);In most situations, ECME and EM perform similarly; (2);CM outperforms EM and ECME substantially in all situations, no         matter assessed by the CPU time or the number of iterations. Especially for the case close to the well known Heywood case, it accelerates EM by factors of around 100 or more. Also, CM is much more insensitive to the choice of starting values         than EM and ECME.               </content></document><document><year>2007</year><authors>Sourabh Bhattacharya1 | Alan E. Gelf|2 | Kent E. Holsinger3</authors><title>Model fitting and inference under latent equilibrium processes      </title><content>This paper presents a methodology for model fitting and inference in the context of Bayesian models of the type f(Y         |         X,&amp;#952;)f(X|&amp;#952;)f(&amp;#952;), where Y is the (set of) observed data, &amp;#952; is a set of model parameters and X is an unobserved (latent) stationary stochastic process induced by the first order transition model f(X         (t+1)|X         (t),&amp;#952;), where X         (t) denotes the state of the process at time (or generation) t. The crucial feature of the above type of model is that, given &amp;#952;, the transition model f(X         (t+1)|X         (t),&amp;#952;) is known but the distribution of the stochastic process in equilibrium, that is f(X|&amp;#952;), is, except in very special cases, intractable, hence unknown. A further point to note is that the data Y has been assumed to be observed when the underlying process is in equilibrium. In other words, the data is not collected         dynamically over time.                     We refer to such specification as a latent equilibrium process (LEP) model. It is motivated by problems in population genetics               (though other applications are discussed), where it is of interest to learn about parameters such as mutation and migration               rates and population sizes, given a sample of allele frequencies at one or more loci. In such problems it is natural to assume               that the distribution of the observed allele frequencies depends on the true (unobserved) population allele frequencies, whereas               the distribution of the true allele frequencies is only indirectly specified through a transition model.            </content></document><document><year>2007</year><authors>Zheng Su1 | Jiaqiao Hu2 | Wei Zhu2</authors><title>Multi-step variance minimization in sequential tests      </title><content>         We introduce a multi-step variance minimization algorithm for numerical estimation of Type;I and Type;II error probabilities         in sequential tests. The algorithm can be applied to general test statistics and easily built into general design algorithms         for sequential tests. Our simulation results indicate that the proposed algorithm is particularly useful for estimating tail         probabilities, and may lead to significant computational efficiency gains over the crude Monte Carlo method.               </content></document><document><year>2007</year><authors>Gopi Goswami1  | Jun S. Liu1 </authors><title>On learning strategies for evolutionary Monte Carlo      </title><content>The real-parameter evolutionary Monte Carlo algorithm (EMC) has been proposed as an effective tool both for sampling from         high-dimensional distributions and for stochastic optimization (Liang and Wong, 2001). EMC uses a temperature ladder similar         to that in parallel tempering (PT; Geyer, 1991). In contrast with PT, EMC allows for crossover moves between parallel and         tempered MCMC chains. In the context of EMC, we introduce four new moves, which enhance its efficiency as measured by the         effective sample size. Secondly, we introduce a practical strategy for determining the temperature range and placing the temperatures         in the ladder used in EMC and PT. Lastly, we prove the validity of the conditional sampling step of the snooker algorithm,         a crossover move in EMC, which extends a result of Roberts and Gilks (1994).      </content></document><document><year>2007</year><authors>Ajay Jasra1 | David A. Stephens2 | Christopher C. Holmes3</authors><title>On population-based simulation for static inference      </title><content>         In this paper we present a review of population-based simulation for static inference problems. Such methods can be described as generating a collection of random variables {X                     n            }            n=1,&amp;#8230;,N             in parallel in order to simulate from some target density &amp;#960; (or potentially sequence of target densities). Population-based simulation is important as many challenging sampling problems         in applied statistics cannot be dealt with successfully by conventional Markov chain Monte Carlo (MCMC) methods. We summarize         population-based MCMC (Geyer, Computing Science and Statistics: The 23rd Symposium on the Interface, pp.;156&amp;#8211;163, 1991; Liang and Wong, J.;Am. Stat. Assoc. 96, 653&amp;#8211;666, 2001) and sequential Monte Carlo samplers (SMC) (Del Moral, Doucet and Jasra, J.;Roy. Stat. Soc. Ser. B 68, 411&amp;#8211;436, 2006a), providing a comparison of the approaches. We give numerical examples from Bayesian mixture modelling (Richardson and Green,         J.;Roy. Stat. Soc. Ser.;B 59, 731&amp;#8211;792, 1997).               </content></document><document><year>2007</year><authors>David J. H|1| 3| Wojtek J. Krzanowski2  | Martin J. Crowder1</authors><title>Optimal predictive partitioning      </title><content>In many situations, one wishes to group objects into well-defined classes on the basis of one set of descriptor variables,         and then predict the classes of new objects from a different set of variables. For example, a bank may categorise customers         into distinct financial behaviour pattern classes by observing how they have behaved over a period of years, and then seek         to assign new customers to future behaviour classes using information captured when they open an account. Such situations         require the striking of a compromise between the compactness and integrity of the cluster structure, and the accuracy of the         predictive assignment to clusters. We describe two algorithms for achieving such a compromise, discuss some of their features,         and illustrate their performance in a simulation study and in a liver transplant problem.      </content></document><document><year>2007</year><authors>Leena Choi1 | Brian Caffo1 | Charles Rohde1</authors><title>Optimal sampling times in bioequivalence studies using a simulated annealing algorithm      </title><content>         In pharmacokinetic (PK) studies, blood samples are taken over time on subjects after the administration of a drug to measure         the time-course of the plasma drug concentration. In bioequivalence studies, the trapezoidal rule on the sampled time points         is often used to estimate the area under the plasma concentration-time curve, a quantity of principal interest. This article         investigates the choice of sampling time points to estimate the area under the curve. In particular, we explore the relative         merits of several objective functions, those functions which are minimized with respect to the sampling times to obtain an         optimal study design. Consequently, we propose an objective function which overcomes some of the deficits of existing choices.         We also present a simulated annealing algorithm to perform the minimization. The main benefits of the simulated annealing         algorithm are the ease in which it can handle constraints on the sampling schedules and its ability to accommodate a variety         of models and objective functions. The manuscript presents optimal sampling times for some key examples of true underlying         models.               </content></document><document><year>2007</year><authors>Jun Yan1 | Mary Kathryn Cowles1| 2| Shaowen Wang3 | Marc P. Armstrong4| 5</authors><title>Parallelizing MCMC for Bayesian spatiotemporal geostatistical models      </title><content>         When MCMC methods for Bayesian spatiotemporal modeling are applied to large geostatistical problems, challenges arise as a         consequence of memory requirements, computing costs, and convergence monitoring. This article describes the parallelization         of a reparametrized and marginalized posterior sampling (RAMPS) algorithm, which is carefully designed to generate posterior         samples efficiently. The algorithm is implemented using the Parallel Linear Algebra Package (PLAPACK). The scalability of         the algorithm is investigated via simulation experiments that are implemented using a cluster with 25 processors. The usefulness         of the method is illustrated with an application to sulfur dioxide concentration data from the Air Quality System database         of the U.S. Environmental Protection Agency.               </content></document><document><year>2007</year><authors>Adam M. Johansen1 | Arnaud Doucet2  | Manuel Davy3 </authors><title>Particle methods for maximum likelihood estimation in latent variable models      </title><content>         Standard methods for maximum likelihood parameter estimation in latent variable models rely on the Expectation-Maximization         algorithm and its Monte Carlo variants. Our approach is different and motivated by similar considerations to simulated annealing;         that is we build a sequence of artificial distributions whose support concentrates itself on the set of maximum likelihood         estimates. We sample from these distributions using a sequential Monte Carlo approach. We demonstrate state-of-the-art performance         for several applications of the proposed approach.               </content></document><document><year>2007</year><authors>Irne Gannaz1 </authors><title>Robust estimation and wavelet thresholding in partially linear models      </title><content>         This paper is concerned with a;semiparametric partially linear regression model with unknown regression coefficients, an unknown         nonparametric function for the non-linear component, and unobservable Gaussian distributed random errors. We present a;wavelet         thresholding based estimation procedure to estimate the components of the partial linear model by establishing a;connection         between an l         1-penalty based wavelet estimator of the nonparametric component and Huber&amp;#8217;s M-estimation of a;standard linear model with outliers.         Some general results on the large sample properties of the estimates of both the parametric and the nonparametric part of         the model are established. Simulations are used to illustrate the general results and to compare the proposed methodology         with other methods available in the recent literature.               </content></document><document><year>2007</year><authors>Tsung I. Lin1 | Jack C. Lee2 | Wan J. Hsieh3</authors><title>Robust mixture modeling using the skew t distribution      </title><content>A finite mixture model using the Student's t distribution has been recognized as a robust extension of normal mixtures. Recently, a mixture of skew normal distributions         has been found to be effective in the treatment of heterogeneous data involving asymmetric behaviors across subclasses. In         this article, we propose a robust mixture framework based on the skew t distribution to efficiently deal with heavy-tailedness, extra skewness and multimodality in a wide range of settings. Statistical         mixture modeling based on normal, Student's t and skew normal distributions can be viewed as special cases of the skew t mixture model. We present analytically simple EM-type algorithms for iteratively computing maximum likelihood estimates.         The proposed methodology is illustrated by analyzing a real data example.      </content></document><document><year>2007</year><authors>Christian H. Wei1 </authors><title>Statistical mining of interesting association rules      </title><content>         This article utilizes stochastic ideas for reasoning about association rule mining, and provides a formal statistical view         of this discipline. A simple stochastic model is proposed, based on which support and confidence are reasonable estimates         for certain probabilities of the model. Statistical properties of the corresponding estimators, like moments and confidence         intervals, are derived, and items and itemsets are observed for correlations.                                             After a brief review of measures of interest of association rules, with the main focus on interestingness measures motivated               by statistical principles, two new measures are described. These measures, called &amp;#945;- and &amp;#963;-precision, respectively, rely on statistical properties of the estimators discussed before. Experimental results demonstrate               the effectivity of both measures.                           </content></document><document><year>2007</year><authors>Sinjini Mitra1 | Nicole A. Lazar2 | Yanxi Liu3</authors><title>Understanding the role of facial asymmetry in human face identification      </title><content>Face recognition has important applications in forensics (criminal identification) and security (biometric authentication).         The problem of face recognition has been extensively studied in the computer vision community, from a variety of perspectives.         A relatively new development is the use of facial asymmetry in face recognition, and we present here the results of a statistical         investigation of this biometric. We first show how facial asymmetry information can be used to perform three different face         recognition tasks&amp;#8212;human identification (in the presence of expression variations), classification of faces by expression,         and classification of individuals according to sex. Initially, we use a simple classification method, and conduct a feature         analysis which shows the particular facial regions that play the dominant role in achieving these three entirely different         classification goals. We then pursue human identification under expression changes in greater depth, since this is the most         important task from a practical point of view. Two different ways of improving the performance of the simple classifier are         then discussed: (i) feature combinations and (ii) the use of resampling techniques (bagging and random subspaces). With these         modifications, we succeed in obtaining near perfect classification results on a database of 55 individuals, a statistically         significant improvement over the initial results as seen by hypothesis tests of proportions.      </content></document><document><year>2005</year><authors>Feng Zhang1 | Bani Mallick2 | Zhujun Weng3</authors><title>A Bayesian method for identifying independent sources of non-random spatial patterns      </title><content>A Bayesian blind source separation (BSS) algorithm is proposed in this paper to recover independent sources from observed         multivariate spatial patterns. As a widely used mechanism, Gaussian mixture model is adopted to represent the sources for         statistical description and machine learning. In the context of linear latent variable BSS model, some conjugate priors are         incorporated into the hyperparameters estimation of mixing matrix. The proposed algorithm then approximates the full posteriors         over model structure and source parameters in an analytical manner based on variational Bayesian treatment. Experimental studies         demonstrate that this Bayesian source separation algorithm is appropriate for systematic spatial pattern analysis by modeling         arbitrary sources and identify their effects on high dimensional measurement data. The identified patterns will serve as diagnosis         aids for gaining insight into the nature of physical process for the potential use of statistical quality control.      </content></document><document><year>2005</year><authors>Peter Neal1  | Gareth Roberts2 </authors><title>A case study in non-centering for data augmentation: Stochastic epidemics      </title><content>In this paper, we introduce non-centered and partially non-centered MCMC algorithms for stochastic epidemic models. Centered         algorithms previously considered in the literature perform adequately well for small data sets. However, due to the high dependence         inherent in the models between the missing data and the parameters, the performance of the centered algorithms gets appreciably         worse when larger data sets are considered. Therefore non-centered and partially non-centered algorithms are introduced and         are shown to out perform the existing centered algorithms.      </content></document><document><year>2005</year><authors>Salvatore Ingrassia1</authors><title>A comparison between the simulated annealing and the EM algorithms in normal mixture decompositions</title><content>We compare the performances of the simulated annealing and the EM algorithms in problems of decomposition of normal mixtures according to the likelihood approach. In this case the likelihood function has multiple maxima and singularities, and we consider a suitable reformulation of the problem which yields an optimization problem having a global solution and at least a smaller number of spurious maxima. The results are compared considering some distance measures between the estimated distributions and the true ones. No overwhelming superiority of either method has been demonstrated, though in one of our cases simulated annealing achieved better results.</content></document><document><year>2005</year><authors>Joseph G. Hirschberg1</authors><title>A computationally efficient method for bootstrapping systems of demand equations: A comparison to traditional techniques</title><content>The solution to a Liapunov matrix equation (LME) has been proposed to estimate the parameters of the demand equations derived from the Translog, the Almost Ideal Demand System and the Rotterdam demand models. When compared to traditional scemingly unrelated regression (SUR) methods the LME approach saves both computer time and space, and it provides parameter estimates that are less likely to suffer from round-off error. However, the LME method is difficult to implement without the use of specially written computer programs and, unlike traditional SUR methods, it does not automatically provide an estimate of the covariance of the parameters. This paper solves these two problems, the first by providing a simplified solution to the Liapunov matrix equation which can be written in a few lines of code in computer languages such as SAS PROC MATRIX/IMLTM or GAUSSTM; the second, by bootstrapping the parameter covariance matrix.</content></document><document><year>2005</year><authors>Katsuhiko Tsujino1 | Shogo Nishida1</authors><title>A knowledge acquisition inductive system driven by empirical interpretation of derived results</title><content>This paper describes the design philosophy of and current issues concerning a knowledge acquisition system namedkaiser. This system is an intelligent workbench for construction of knowledge bases for classification tasks by domain experts themselves. It first learns classification knowledge inductively from the examples given by a human expert, then analyzes the result and process based on abstract domain knowledge which is also given by the expert. Based on this analysis, it asks sophisticated questions for acquiring new knowledge. The queries stimulate the human expert and help him to revise the learned results, control the learning process and prepare new examples and domain knowledge. Viewed from an AI aspect, it aims at integrating similarity-based inductive learning and explanation-based deductive reasoning by guiding inductive inference with theoretical and/or heuristic knowledge about the domain. This interactive induce-evaluate-ask cycle produces a rational interview which promotes incremental acquisition of domain knowledge as well as efficient induction of operational and reasonable knowledge proved by the domain knowledge.</content></document><document><year>2005</year><authors>A. C. Atkinson1</authors><title>A segmented algorithm for simulated annealing</title><content>The properties of a parameterized form of generalized simulated annealing for function minimization are investigated by studying the properties of repeated minimizations from random starting points. This leads to the comparison of distributions of function values and of numbers of function evaluations. Parameter values which yield searches repeatedly terminating close to the global minimum may require unacceptably many function evaluations. If computational resources are a constraint, the total number of function evaluations may be limited. A sensible strategy is then to restart at a random point any search which terminates, until the total allowable number of function evaluations has been exhausted. The response is now the minimum of the function values obtained. This strategy yields a surprisingly stable solution for the parameter values of the simulated annealing algorithm. The algorithm can be further improved by segmentation in which each search is limited to a maximum number of evaluations, perhaps no more than a fifth of the total available. The main tool for interpreting the distributions of function values is the boxplot. The application is to the optimum design of experiments.</content></document><document><year>2005</year><authors>Judea Pearl1 | Thomas S. Verma1</authors><title>A statistical semantics for causation</title><content>We propose a model-theoretic definition of causation, and show that, contrary to common folklore, genuine causal influences can be distinguished from spurious covariations following standard norms of inductive reasoning. We also establish a sound characterization of the conditions under which such a distinction is possible. Finally, we provide a proof-theoretical procedure for inductive causation and show that, for a large class of data and structures, effective algorithms exist that uncover the direction of causal influences as defined above.</content></document><document><year>2005</year><authors>Accurate ARL computation for EWMA-S2 control charts      </authors><title>Originally, the exponentially weighted moving average (EWMA) control chart was developed for detecting changes in the process         mean. The average run length (ARL) became the most popular performance measure for schemes with this objective. When monitoring         the mean of independent and normally distributed observations the ARL can be determined with high precision. Nowadays, EWMA         control charts are also used for monitoring the variance. Charts based on the sample variance S2 are an appropriate choice. The usage of ARL evaluation techniques known from mean monitoring charts, however, is difficult.         The most accurate method&amp;#8212;solving a Fredholm integral equation with the Nystrm method&amp;#8212;fails due to an improper kernel in the         case of chi-squared distributions. Here, we exploit the collocation method and the product Nystrm method. These methods are         compared to Markov chain based approaches. We see that collocation leads to higher accuracy than currently established methods.      </title><content/></document></documents>