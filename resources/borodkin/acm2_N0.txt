GUID:2B0BEE8B-FC04-4E7D-A889-D30009851694
LCount:700
CCount:140
ClCount:7
ClNames: control systems synthesis; data structures; failure analysis; image analysis; information retrieval; robots; Search Engines;
L:1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 833 834 835 836 837 838 839 840 
C:693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 
ID:1
CLASS:1
Title: A comprehensive high-level synthesis system for control-flow intensive behaviors
Abstract: In this paper, we describe a comprehensive high-level synthesis system for control-flow intensive as well as data-dominated behaviors. We propose a new control-data flow graph model to preserve the parallelism inherent in the application, as well as to facilitate high-level synthesis. Our algorithm, which is based on an iterative improvement strategy, performs clock selection, scheduling, module selection, resource allocation and assignment simultaneously to fully derive the benefits of design space exploration at the behavior level. The system can be used to optimize area, power or energy, by selecting the cost function accordingly. Experimental results show that for energy-optimized designs, energy is reduced by up to 79.4% (an average of 42.2%), with an average of 24.8% area overhead, compared to area-optimized designs. For power-optimized designs, power is reduced by up to 70.8% (an average of 56.7%), with an average of 25.2% area overhead, compared to area-optimized designs. No Vdd scaling is performed to obtain the above results.
ID:2
CLASS:1
Title: IMPACT: a high-level synthesis system for low power control-flow intensive circuits
Abstract: In this paper, we present a comprehensive high-level synthesis system that is geared towards reducing power consumption in control-flow intensive circuits. An iterative improvement algorithm is at the heart of the system. The algorithm searches the design space by handling scheduling, module selection, resource sharing and multiplexer network restructuring simultaneously. The scheduler performs concurrent loop optimization and implicit loop unrolling. It minimizes the expected number of cycles of the schedule without compromising on the minimum and maximum schedule lengths. A fast simulation technique based on trace manipulation aids power estimation in driving synthesis in the right direction. Experimental results demonstrate power reduction of up to 85% with minimal overhead in area over area-optimized designs operating at 5V.
ID:3
CLASS:1
Title: Phoan: An intelligent system for distributed control synthesis
Abstract: Phoan is an experimental system for the specification and synthesis of control software over suitably restricted domains; its initial application domain is control software for distributed telephone systems. Phoan provides the developer with an example-based specification language, which it translates into an internal representation language based on rules and constraints. Phoan reasons at the representation level to ensure that the system definition is coherent and consistent with independent models of the target domain. Phoan synthesizes the final control structures and their associated protocols by transformation from the internal representation. The implementation of Phoan is currently in progress.
ID:4
CLASS:1
Title: Automatic correct scheduling of control flow intensive behavioral descriptions in formal synthesis
Abstract: Formal synthesis ensures the correctness of hardware synthesis by automatically deriving the circuit implementation by behavior preserving transformations within a theorem prover. In this paper, we present a new approach to formal synthesis that is able to handle control flow intensive descriptions. In particular, we consider here the scheduling phase, which is a central task in high-level synthesis. We describe a methodology employing a new control flow oriented representation which allows the fully automatic scheduling of control flow intensive descriptions in formal synthesis. To obtain scheduled circuits of high quality, the scheduling is guided by conventional scheduling algorithms.
ID:5
CLASS:1
Title: A domain-specific language for task handlers generation, applying discrete controller synthesis
Abstract: We propose a simple programming language, called Nemo, specific to the domain of multi-task real-time embedded systems, such as in robotic, automotive or avionics systems. It can be used to specify a set of resources with usage constraints, a set of tasks that consume them according to various modes, and applications sequencing the tasks. We obtain automatically an application-specific task handler that correctly manages the constraints (if there exists one), through a compilation-like process including a phase of discrete controller synthesis. This way, this formal technique contributes to the safety of the designed systems, while being encapsulated in a tool that makes it usable by end-users and application experts. Our approach is based on the synchronous modelling techniques, languages and tools.
ID:6
CLASS:1
Title: Dynamic scheduling and synchronization synthesis of concurrent digital systems under system-level constraints
Abstract: We present in this paper a novel control synthesis technique for system-level specifications that are better described as a set of concurrent synchronous descriptions, their synchronizations and constraints. The proposed synthesis technique considers the degrees of freedom introduced by the concurrent models and by the environment in order to satisfy the design constraints.Synthesis is divided in two phases. In the first phase, the original specification is translated into an algebraic system, for which complex control-flow constraints and quantifiers of the design are determined. This algebraic system is then analyzed and the design space of the specification is represented by a finite-state machine, from which a set of Boolean formulas is generated and manipulated in order to obtain a solution. This method contrasts with usual high-level synthesis methods in that it can handle arbitrarily complex control-flow structures, concurrency and synchronization by allowing the scheduling of the operations to change dynamically over time.
ID:7
CLASS:1
Title: Multi-thread graph: a system model for real-time embedded software synthesis
Abstract: Software synthesis is a new approach which focuses on the support of real-time embedded multi-tasking software without the use of operating systems. A software synthesis system starts from a concurrent process system specification and maps this description automatically onto one or more processors. In this paper the internal system-level model which captures the embedded software and which is the backbone of our software synthesis methodology, is presented. The model captures the fine-grain behaviour of a system, and supports multiple threads of control (concurrency), synchronisation, data communication, hierarchy and timing constraints.
ID:8
CLASS:1
Title: Multiagent Planning as Control Synthesis
Abstract: This paper proposes a new multiagent planning approach to coordination synthesis that views distributed agents as discrete-event processes. The connection between discrete-event control synthesis and coordination planning is first established, thereby enabling the exploitation of the vast body of knowledge and associated software synthesis tools from ýThe Supervisory Control of Discrete-Event Systemsý for automatic coordination synthesis of distributed agents. Importantly, these coordinating agents designed collectively generate a behaviour guaranteed not to contradict any specified inter-agent constraint, is nonblocking and optimal. A simple planning methodology is proposed in terms of procedures supported by CTCT, an existing, freely available design tool developed based on the control synthesis framework.Asimple example illustrates the use of the CTCT-based methodology to synthesize coordination modules for distributed agents. Discussions in relation to previous work examine the relative significance of the new multiagent planning framework.
ID:9
CLASS:1
Title: Synthesis of low-power selectively-clocked systems from high-level specification
Abstract: In this paper we propose a technique for synthesizing low-power systems from a high-level specification. We analyze the control flow of the specification to detect mutually exclusive sections of the computation. A selectively-clocked interconnection of interacting FSMs is automatically generated and optimized where each FSM controls the execution of one section of computation. Only one of the interacting FSMs is active at any given clock cycle, while all the others are idle and their clock is stopped. Our interacting FSM implementation achieves consistently lower power dissipation savings are obtained with a 30% area overhead.
ID:10
CLASS:1
Title: Bridge: a versatile behavioral synthesis system
Abstract: Bridge is a behavioral synthesis system being developed at AT&amp;T Bell Laboratories. Two slicing techniques are implemented in this system to drive structural allocation; one is local slicing and the other is global slicing. Global slicing supports the synthesis of concurrent processes with a centralized control. A variable in a behavioral description can be either a storage element or a signal. The impacts of treating a variable as a signal on data flow scheduling, control flow scheduling, and lifetime analysis are discussed. Intelligent bindings of the variables in a behavioral description to registers and signals not only reduce the implementation cost but also improve the circuit performance. Using global slicing and signal variables, a mircoarchitecture model can sometimes be reduced to the view of a finite state machine or a combinational circuit. Experimental data for the behavioral descriptions of the Intel 8251 are presented.
ID:11
CLASS:1
Title: eFASE: expressive facial animation synthesis and editing with phoneme-isomap controls
Abstract: This paper presents a novel data-driven system for expressive facial animation synthesis and editing. Given novel phoneme-aligned speech input and its emotion modifiers (specifications), this system automatically generates expressive facial animation by concatenating captured motion data while animators establish constraints and goals. A constrained dynamic programming algorithm is used to search for best-matched captured motion nodes by minimizing a cost function. Users optionally specify "hard constraints" (motion-node constraints for expressing phoneme utterances) and "soft constraints" (emotion modifiers) to guide the search process. Users can also edit the processed facial motion node database by inserting and deleting motion nodes via a novel phoneme-Isomap interface. Novel facial animation synthesis experiments and objective trajectory comparisons between synthesized facial motion and captured motion demonstrate that this system is effective for producing realistic expressive facial animations.
ID:12
CLASS:1
Title: Synthesis of low-power selectively-clocked systems from high-level specification
Abstract: We propose a technique for synthesizing low-power systems from behavioral specifications. We analyze the control flow of the specification model to detect mutually exclusive sections of the computation. A selectively-clocked interconnection of interacting FSMs is automatically generated and optimized, where each FSM controls the execution of one section of computation. Only one of the interacting FSMs is active for a high fraction of the operation time, while the others are idle and their clocks are stopped. Periodically, the active machine releases the control of the system to another FSM and stops. Our interacting FSM implementation achieves consistently lower power dissipation than the functionally equivalent monolithic implementation. On average, 37% power savings  and 12% speedup are obtained, despite a 30% area overhead.
ID:13
CLASS:1
Title: The use of a virtual instruction set for the software synthesis of Hw/Sw embedded systems
Abstract: The application range of embedded computing is going to cover the majority of market products spanning from consumer electronic, automotive, telecom and process control. For such applications, typically there is strong cooperation between dedicated hardware modules and software systems. An important issue toward a fully automated system-level implementation is represented by the software development process. The basic requirements are: accurate timing characterization to be used during the early stages of the design to compare alternative architectures and reliable synthesis techniques to ensure the respect of the correct functionality by avoiding, as much as possible, the direct designer's intervention during the development process. This paper describes a novel methodology to address the needs of concurrently synthesizing the software component of a control-dominated hardware-software system, possibly under real-time constraints. An intermediate model (Virtual Instruction Set) for the software is presented, suitable for both for synthesis and analysis purposes. The overall system synthesis is presented with particular emphasis on the problem of low level performance estimation, static scheduling of the software process and retargetable code synthesis.
ID:14
CLASS:1
Title: An Approach to Mixed Systems Co-Synthesis
Abstract: The paper presents an extension of co-synthesis for data dominated applications to include reactive processes. The extension allows for rate constraints as used in data dominated applications as well as minimum and maximum time constraints for communication and I/O which is required to define reactive behavior of control tasks. A co-synthesis approach is proposed which differentiates global process and communication scheduling, which is non preemptive, and local scheduling which includes a restricted interrupt controlled process invocation to extend the design space. Several user parameters allow design space exploration. The approach includes buffering, process pipelining and parallelization for control as well as for data dominated tasks on different levels of granularity. It supports inter process time constraints which span processes with different periods. The target architectures are heterogeneous systems consisting of multiple processors, hardware components, memories and different types of communication media.
ID:15
CLASS:1
Title: <italic>SYMCAD</italic>: synthesis of microprogrammed control for automated VLSI design
Abstract: This paper discusses a software package named SYMCAD, a retargetable microcode synthesizer which forms part of a microprogrammed control synthesis in the VLSI CAD system IDEAS (Integrated Design Automation System) [1]. The objective is to synthesize a microprogrammed control for a specified data part and microsequencer to realize the behavior expressed in a hardware description language. A distinctive feature of this approach is the synthesis of a customized control unit suitable for VLSI realization.Retargetable microcode generation comprises symbolic microprogram synthesis, microinstruction format generation and binary microcode generation, to map the required behavior on the specified micro-architecture.
ID:16
CLASS:1
Title: High-level scheduling model and control synthesis for a broad range of design applications
Abstract: This paper presents a versatile scheduling model and an efficient control synthesis methodology which enables architectural (high-level) design/synthesis systems to seamlessly support a broad range of architectural design applications from datapath-dominated digital signal processing (DSP) to micro-processors/controllers and control-dominated peripherals, utilizing multi-phase clocking schemes, multiple threading, data-dependent delays, pipelining, and combinations of the above. The work presented in this paper is an enabling technology for high-level synthesis to go beyond traditional datapath-dominated DSP applications and to start becoming a viable and cost-effective design methodology for commodity ICs such as micro-processors/controllers and control-dominated peripherals.
ID:17
CLASS:1
Title: Synthesis of application-specific highly efficient multi-mode cores for embedded systems
Abstract: In this paper, we present a novel design methodology for synthesizing multiple configurations (or modes) into a single programmable core that can be used in embedded systems. Recent portable applications require reconfigurability of a system along with efficiency in terms of power, performance, and area. The field programmable gate arrays (FPGAs) provide a reconfigurable platform; however, they are slower in speed with significantly higher power and area than achievable by a customized application-specific integrated circuits (ASIC). Implementation of a system in either FPGA or ASIC represents a trade-off between programmability and design efficiency. In this work, we have developed techniques to realize efficient reconfigurable cores for a set of user-specified applications. The resultant system, named as multimode system, can easily switch configurations throughout the set of configurations it is designed for. A data flow graph transformation method coupled with efficient scheduling and allocation is used to automatically synthesize a Multi-Mode system from its behavior-level specifications. Experimental results on several applications demonstrate that our implementations can achieve about 60X power reduction on average and run 3.5X faster over corresponding FPGA implementations.
ID:18
CLASS:1
Title: An enhanced GA to improve the search process reliability in tuning of control systems
Abstract: Evolutionary Algorithms (EAs) have been largely applied to optimisation and synthesis of controllers. In spite of several successful applications and competitive solutions, the stochastic nature of EAs and the uncertainty of the results have considerably hindered their use in industrial applications. In this paper we propose a Genetic Algorithm (GA) for tuning controllers for classical first and second order plants with actuator nonlinearities. To increase the robustness of the algorithm we introduce two features: 1) genetic operators that perform directional mutations, 2) selection tournaments organized by genome vicinity. The experiment results show that the proposed GA is able to guarantee high performance and low variance in the results from different runs. The increased reliability, compared to the results from a classical GA, seems to favour particularly the application of Evolutionary Computation (EC) in tuning of control systems, where, thanks to this approach, a large search space can be searched repeatedly with high consistency in the solutions.
ID:19
CLASS:1
Title: A Burst-Mode Oriented Back-End for the Balsa Synthesis System
Abstract: This paper introduces several new component clustering techniquesfor the optimization of asynchronous systems. In particular, novel"Burst-Mode aware" restrictions are imposed to limit the cluster sizesand to ensure synthesizability. A new control specification language,CH, is also introduced which facilitates the manipulation and optimizationof handshake control components. The new method has beenfully integrated into a comprehensive asynchronous synthesis package,Balsa. Experimental results on several substantial design examples,including an 32-bit microprocessor core, indicate significantperformance improvements for the optimized circuits.
ID:20
CLASS:1
Title: Synthesis of concurrent system interface modules with automatic protocol conversion generation
Abstract: We describe a new high-level compiler called Integral for designing system interface modules. The input is a high-level concurrent algorithmic specification that can model complex concurrent control flow, logical and arithmetic computations, abstract communication, and low-level behavior. For abstract communication between two communicating modules that obey different I/O protocols, the necessary protocol conversion behaviors are automatically synthesized using a Petri net theoretic approach. We present a synthesis trajectory that can synthesize the necessary hardware resources, control circuitry, and protocol conversion behaviors for implementing system interface modules.
ID:21
CLASS:1
Title: A formal approach to fault tree synthesis for the analysis of distributed fault tolerant systems
Abstract: Designing cost-sensitive real-time control systems for safety-critical applications requires a careful analysis of both performance versus cost aspects and fault coverage of fault tolerant solutions. This further complicates the difficult task of deploying the embedded software that implements the control algorithms on a possibly distributed execution platform (for instance in automotive applications). In this paper, we present a novel technique for constructing a fault tree that models how component faults may lead to system failure. The fault tree enables us to use existing commercial analysis tools to assess a number of dependability metrics of the system. Our approach is centered on a model of computation, Fault Tolerant Data Flow (FTDF), that enables the integration of formal verification techniques. This new analysis capability is added to an existing design framework, also based on FTDF, that enables a synthesis-based, correct-by-construction, design methodology for the deployment of real-time feedback control systems in safety critical applications.
ID:22
CLASS:1
Title: Interactive package for analysis and synthesis of multivariable control systems
Abstract: This paper describes an interactive package for designing multivariabl e control systems. It is assumed that a model of the system, either in the form of a matrix of transfer functions or in state-space form is known. The design procedure is by successive analysis. In each iteration the designer checks the stability and transient response of the system. The package is implemented on a PDP-9 computer with precision display, disk, line printer and teletype.
ID:23
CLASS:1
Title: Highly flexible multi-mode system synthesis
Abstract: Multi-mode systems have emerged as an area- and power-efficient approach to implementing multiple time-wise mutually exclusive algorithms and applications in a single hardware space. These systems have limited flexibility and temporal separation between modes is achieved by changing only the dataflow between components. This paper presents a synthesis methodology for integrating flexible components and controllers into primarily fixed logic multi-mode systems thereby increasing their overall flexibility and efficiency. The components are built using a technique called small-scale reconfigurability that provides the necessary flexibility without the penalties associated with general-purpose reconfigurable logic. The reconfiguration latency is small enabling both inter-mode and intra-mode reconfiguration of components. Datapath and controller area and power consumption are reduced beyond what is provided in current multi-mode systems using this methodology, without sacrificing performance. The results show an average 7% reduction in datapath component area, 26% reduction in register area, 36% reduction in interconnect MUX cost, and a 68% reduction in the number of controller signals for a set of benchmark 32-bit signal processing applications. There is also an average 38% increase in component utilization.
ID:24
CLASS:1
Title: Synthesis of VLSI systems with the CAMAD design aid
Abstract: CAMAD is a high level design tool which helps designers to model, analyze, and design VLSI systems. This design aid system is based on a unified design representation model derived from timed Petri nets and consisting of separate but related models of control and data parts. The present paper describes the automatic synthesis package of the CAMAD system which takes a high level behavioral description as its input and synthesizes it into an implementation structure. This implementation structure may then be partitioned into several quasi-independent modules with well-defined interfaces, which allows potentially asynchronous operation of the designed systems as well as physical distribution of the modules.
ID:25
CLASS:1
Title: System architectures for computer music
Abstract: Computer music is a relatively new field. While a large proportion of the public is aware of computer music in one form or another, there seems to be a need for a better understanding of its capabilities and limitations in terms of synthesis, performance, and recording hardware. This article addresses that need by surveying and discussing the architecture of existing computer music systems.System requirements vary according to what the system will be used for. Common uses for computer music systems include composition, performance, research, home entertainment, and studio recording/mixing. This paper outlines system components with this wide diversity of possible uses in mind.Current synthesis and analysis techniques, and the different way in which these  techniques can be implemented in special-purpose hardware, are comprehensively reviewed. Design specifications are given for certain digital-to-analog (and analog-to-digital) converters, disk interfaces, system organization, control hardware and software, and numerical precision.Several synthesis systems are described in detail, with an emphasis on theoretical developments and innovative design. Commercial synthesizers and other architectures are also briefly mentioned.
ID:26
CLASS:1
Title: Multiple-process behavioral synthesis for mixed hardware-software systems
Abstract: Abstract: Systems composed of microprocessors interacting with ASICs are necessarily multiple-process systems, since the controller in the microprocessor is separate from any controllers on the ASIC. For this reason, the design of such systems offers an opportunity to exploit not only hardware-software tradeoffs, but concurrency tradeoffs as well. The paper describes an automated iterative improvement technique for performing concurrency optimization and hardware-software tradeoffs simultaneously. Experimental results illustrate that addressing these two issues simultaneously enables us to identify a number of interesting cost/performance points that would not have been found otherwise.
ID:27
CLASS:1
Title: Speculation techniques for high level synthesis of control intensive designs
Abstract: The quality of synthesis results for most high level synthesis approaches is strongly affected by the choice of control flow (through conditions and loops) in the input description.  In this paper, we explore the effectiveness of various types of code motions, such as moving operations across conditionals, out of conditionals (speculation) and into conditionals (reverse speculation), and how they can be effectively directed by heuristics so as to lead to improved synthesis results in terms of fewer execution cycles and fewer number of states in the finite state machine controller.  We also study the effects of the code motions on the area and latency of the final synthesized netlist.  Based on speculative code motions, we present a novel way to perform early condition execution that leads to significant improvements in highly control-intensive designs.  Overall, reductions of up to 38 \% in execution cycles are obtained with all the code motions enabled.
ID:28
CLASS:1
Title: Synthesis and optimization of coordination controllers for distributed embedded systems
Abstract: A main advantage of control composition with modal processes [4] is the enhanced retargetability of the composed behavior over a wide variety of target architectures. Unlike previous component models that hardwire the coordination behavior either explicitly in the components or implicitly in the underlying model of computation, modal processes decouple component functionality and coordination protocols. Retargetability is achieved through the synthesis of distributed mode managers, which abstract away low-level synchronization and control communication details that would otherwise be exposed to the component designer. This paper presents an algorithm for the synthesis and optimization of distributed coordination controllers by computing an optimal projection of the global state space onto each processor. It not only minimizes interprocessor communication traffic for coordination but also reduces controller complexity by minimizing replication.
ID:29
CLASS:1
Title: Hardware/software synthesis of formal specifications in codesign of embedded systems
Abstract: CoDesign aims to integrate the design techniques of hardware and software. In this work, we present a CoDesign methodology based on a formal approach to embedded system specification. This methodology uses the Templated T-LOTOS language to specify the system during all design phases. Templated T-LOTOS is a formal language based on CCS and CSP models. Using Templated T-LOTOS, a system can be specified by observing the temporal ordering in which the events occur from the outside. In this paper we focus on the synthesis of system specified by Templated T-LOTOS. The proposed synthesis algorithm takes advantage of peculiarities of Templates T-LOTOS. Hardware modules are translated into a register transfer-level language that manages some signals in order to drive synchronization, while the software models are translated into C according to a finite state model  whose operations are controlled by a scheduler. The synthesis of the Templated T-LOTOS specification is based on the direct translation of the language operators to ensure that the implemented system is the same as the specified one.
ID:30
CLASS:1
Title: Software Architecture Synthesis for Retargetable Real-time Embedded Systems
Abstract: Retargetability of embedded system descriptions not only enables better exploration of the design space and evaluation of cost/performance tradeoffs but also enhances design maintainability and adaptivity to new technologies. Unfortunately, the traditional boundary between run-time support and user-code encourages use of ad hoc architecture-specific features that lack the structure to permit automatic code synthesis for the satisfaction of timing constraints. This work proposes a specification style for control-dominated embedded systems that can be easily retargeted via automatic synthesis of the software architecture and run-time support. Unlike previous work, user-specified modes are an integral part of the run-time system and isolate architecture-specific details while scoping timing constraints to enable more efficient scheduling.
ID:31
CLASS:1
Title: Conflict analysis in multiprocess synthesis for optimized system integration
Abstract: This paper presents a novel approach for multiprocess synthesis supporting well-tailored module integration at system level. The goal is to extend the local scope of existing architectural synthesis approaches in order to apply global optimization techniques across process bounds for shared system resources (e.g. memories, busses, global ALUs) during scheduling and binding. This allows an area efficient implementation of un-timed or cycle-fixed multiprocess specifications at RT or algorithmic level of abstraction. Furthermore, this approach supports environment-oriented synthesis for optimized module integration by scheduling accesses to global resources with respect to the access schedules of other modules communicating to the same global resources. As a result, dynamic access conflicts can be avoided by construction, and hence, there is no need for dynamic arbitration of bus and memory accesses with potentially unpredictable timing behavior.
ID:32
CLASS:1
Title: Audio-based head motion synthesis for Avatar-based telepresence systems
Abstract: In this paper, a data-driven audio-based head motion synthesis technique is presented for avatar-based telepresence systems. First, head motion of a human subject speaking a custom corpus is captured, and the accompanying audio features are extracted. Based on the aligned pairs between audio features and head motion (audio-headmotion), a K-Nearest Neighbors (KNN) based dynamic programming algorithm is used to synthesize novel head motion given new audio input. This approach also provides optional intuitive keyframe (key head poses) control: after key head poses are specified, this method will synthesize appropriate head motion sequences that maximally meet the requirements of both the speech and key head poses.
ID:33
CLASS:1
Title: Optimization and synthesis for complex reactive embedded systems by incremental collapsing
Abstract: We propose a software synthesis procedure for reactive real-time embedded systems. In our approach, control parts of the system are represented in a decomposed form enabling more complex control structures to be represented. We propose a synthesis procedure for this representation that incrementally aggregates elements of the representation while keeping the resulting code size under tight control. This method combined with heuristic strategies works very well on real-life designs and demonstrates the potential to produce results that challenge or beat hand-written implementations.
ID:34
CLASS:1
Title: An overview of logic synthesis systems
Abstract: The term logic synthesis is used to describe systems that range from relatively simple mapping schemes to tools with sophisticated logic optimizations. In this tutorial, the requirements on logic synthesis systems will be discussed and the advantages and disadvantages of different approaches to logic synthesis will be presented.
ID:35
CLASS:1
Title: Communication Synthesis for Embedded Systems with Global Considerations
Abstract: Designers of distributed embedded systems require communication synthesis to more effectively explore the design space. Communication synthesis creates or instantiates the necessary software and hardware required to allow system components to exchange data. This work examines the problem of mapping a high-level specification to an arbitrary, but fixed architecture that uses particular bus protocols for interprocessor communication. The approach detailed in this paper illustrates that global considerations are necessary to achieve a correct implementation. A communication model is presented that allows for easy retargeting to different bus topologies and protocols. The effectiveness of this approach is demonstrated by mapping a high-level specification to different architectures.
ID:36
CLASS:1
Title: Thread-based software synthesis for embedded system design
Abstract: We propose a thread-based software synthesis technique to reduce communication overhead incurred by the hardware-software interface in a system. We start from a CDFG that models the system. The CDFG is analyzed and partitioned into a set of threads. Then we generate a mixed static-dynamic thread scheduler. The scheduler statically schedules as many threads as possible to maximize the scheduling overhead. Then the scheduler dynamically schedules the remaining threads. Reduction of the total execution time including the communication overhead is demonstrated with some examples.
ID:37
CLASS:1
Title: State assignment for hardwired VLSI control units
Abstract: Finding a binary encoding of symbolic control states, such that the implementation area of a digital control unit is minimized is well known to be NP-complete. Many heuristic algorithms have been proposed for this state assignment problem. The objective of this article is to present a comprehensive survey and systematic categorization of the various techniques, in particular, for synchronous sequential circuits with nonmicroprogrammed implementations. The problem is partitioned into the generation and the satisfaction of coding constraints. Three types of coding constraints&mdash;adjacency, covering, and disjunctive constraints&mdash;are widely used. The constraint satisfaction algorithms are classified into column-based, row-based, tree-based, dichotomy-based, and global minimization approaches. All of them are illustrated with examples. Special coding requirements and testability-related aspects of state assignment are considered in a separate section. Different implementations of the algorithms presented are also compared.
ID:38
CLASS:1
Title: Cost Sensitive Modulo Scheduling in a Loop Accelerator Synthesis System
Abstract: Scheduling algorithms used in compilers traditionally focus on goals such as reducing schedule length and register pressure or producing compact code. In the context of a hardware synthesis system where the schedule is used to determine various components of the hardware, including datapath, storage, and interconnect, the goals of a scheduler change drastically. In addition to achieving the traditional goals, the scheduler must proactively make decisions to ensure efficient hardware is produced. This paper proposes two exact solutions for cost sensitive modulo scheduling, one based on an integer linear programming formulation and another based on branch-and-bound search. To achieve reasonable compilation times, decomposition techniques to break down the complex scheduling problem into phase ordered sub-problems are proposed. The decomposition techniques work either by partitioning the dataflow graph into smaller subgraphs and optimally scheduling the subgraphs, or by splitting the scheduling problem into two phases, time slot and resource assignment. The effectiveness of cost sensitive modulo scheduling in minimizing the costs of function units, register structures, and interconnection wires are evaluated within a fully automatic synthesis system for loop accelerators. The cost sensitive modulo scheduler increases the efficiency of the resulting hardware significantly compared to both traditional cost unaware and greedy cost aware modulo schedulers.
ID:39
CLASS:1
Title: Transformations for the synthesis and optimization of asynchronous distributed control
Abstract: Asynchronous design has been the focus of renewed interest.  However, a key bottleneck is the lack of high-quality CAD tools for the synthesis of large-scale systems which also allow design-space exploration.  This paper proposes a new synthesis method to address this issue, based on transformations.The method starts with a scheduled and resource-bounded Control-Data Flow Graph (CDFG). Global transformations are first applied to the entire CDFG, unoptimized controllers are then extracted, and, finally, local transforms are applied to the individual controllers. The result is a highly-optimized set of interacting distributed controllers. The new transforms include aggressive timing- and area-oriented optimizations, several of which have not been previously supported by existing asynchronous CAD tools.As a case study, the method is applied to the well-knowndifferential equation solversynthesis benchmark. Results comparable to a highly-optimized manual design by Yun et al.[26] can be obtained by applying the new automated transformations. Such an implementation cannot be obtained using existing asynchronous CAD tools.
ID:40
CLASS:1
Title: Fast high-level power estimation for control-flow intensive design
Abstract: In this paper, we present a power estimation technique for control-flow intensive designs that is tailored towards driving iterative high-level synthesis systems, where hundreds of architectural trade-offs are explored and compared. Our method is fast and relatively accurate. The algorithm utilizes the behavioral information to extract branch probabilities, and uses these in conjunction with switching activity and circuit capacitance information, to estimate the power consumption of a given architecture.We test our algorithm using a series of experiments, each geared towards measuring a different indicator. The first set of experiments measures the algorithm's accuracy when compared to the actual circuit power. The second set of experiments measures the average tracking index, and tracking index fidelity for a series of architectures. This index measures how well the algorithm makes decisions when comparing the relative power consumption of two architectures contending as low-power candidates.Results indicate that our algorithm achieved an average estimation error of 11.8% and an average tracking index of 0.95 over all examples.
ID:41
CLASS:1
Title: Synthesis of Application-Specific Highly-Efficient Multi-Mode Systems for Low-Power Applications
Abstract: We present a novel design methodology for synthesizing multiple configurations (or modes) into a single programmable system. Many DSP and multimedia applications require reconfigurability of a system along with efficiency in terms of power, performance and area. FPGAs provide a reconfigurable platform, however, they are slower in speed with significantly higher power consumption than achievable by a customized ASIC. In this work, we have developed techniques to realize an efficient reconfigurable system for a set of user-specified configurations. A data flow graph transformation method coupled with efficient scheduling and allocation are used to automatically synthesize the system from its behavioral level specifications. Experimental results on several applications demonstrate that we can achieve about 60X power reduction on average with about 4X improvement in performance over corresponding FPGA implementations.
ID:42
CLASS:1
Title: The FSM network model for behavioral synthesis of control-dominated machines
Abstract: Since many ASICs are dominated by control functions, control-dominated architectures form an important domain for behavioral synthesis. We propose modeling control-dominated architectures during behavioral synthesis as networks of communicating FSMs&mdash;the model more directly reflects behavior and allows more accurate cost estimation, especially for control, than do traditional data-directed representations for control-dominated machines. We show how to implement a number of important compiler optimizations on the FSM network model.
ID:43
CLASS:1
Title: Loop Shifting and Compaction for the High-Level Synthesis of Designs with Complex Control Flow
Abstract: Emerging embedded system applications in multimedia and image processing are characterized by complex control flow consisting of deeply nested conditionals and loops. Wepresent a technique called loop shifting that incrementally exploits loop level parallelism across iterations by shifting and compacting operations across loop iterations. Our experimental results show that loop shifting is particularly effective for the synthesis of designs with complex control especially when resource utilization is already high and/or under tight resource constraints. In situations when further loop unrolling (or initiating another iteration of the loop body) leads to a sharp increase in the longest combinational path in the circuit and the circuit area, loop shifting is able to achieve up to 20 % reduction in the input-to-output delay in the synthesized circuit. We implemented loop shifting within the SPARK parallelizing high-level synthesis framework and present results for experiments on designs derived from multimedia and image processing applications.
ID:44
CLASS:1
Title: Dual Flow Nets: Modeling the control/data-flow relation in embedded systems
Abstract: This paper addresses the interrelation between control and data flow in embedded system models through a new design representation, called Dual Flow Net (DFN). A modeling formalism with a very close-fitting control and data flow is achieved by this representation, as a consequence of enhancing its underlying Petri net structure. The work presented in this paper does not only tackle the modeling side in embedded systems design, but also the validation of embedded system models through formal methods. Various introductory examples illustrate the applicability of the DFN principles, whereas the capability of the model to with complex designs is demonstrated through the design and verification of a real-life Ethernet coprocessor.
ID:45
CLASS:1
Title: Recent developments in high-level synthesis
Abstract: We survey recent developments in high level synthesis technology for VLSI design. The need for higher-level design automation tools are discussed first. We then describe some basic techniques for various subtasks of high-level synthesis. Techniques that have been proposed in the past few years (since 1994) for various subtasks of high-level synthesis are surveyed. We also survey some new synthesis objectives including testability, power efficiency, and reliability.
ID:46
CLASS:1
Title: Application-driven synthesis of core-based systems
Abstract: We developed a new hierarchical modular approach for synthesis of area-minimal core-based data-intensive systems. The optimization approach employs a novel global least-constraining most-constrained heuristic to minimize the instruction cache misses for a given application, instruction cache size and organization. Based on this performance optimization technique, we constructed a strategy to search for a minimal-area processor core, and an instruction and data cache which satisfy the performance characteristics of a set of target applications. The synthesis platform integrates the existing modeling, profiling, and simulation tools with the developed system-level synthesis tools. The effectiveness of the approach is demonstrated on a variety of modern real-life multimedia and communication applications.
ID:47
CLASS:1
Title: Real-time multi-tasking in software synthesis for information processing systems
Abstract: Abstract: Software synthesis is a new approach which focuses on the support of embedded systems without the use of operating systems. Compared to traditional design practices, a better utilization of the available time and hardware resources can be achieved, because the static information provided by the system specification is fully exploited and an application-specific solution is automatically generated. On-going research on a software synthesis approach for real-time information processing systems is presented which starts from a concurrent process system specification and tries to automate the mapping of this description to a single processor. An internal representation model which is well-suited for the support of concurrency and timing constraints is proposed, together with flexible execution models for multi-tasking with real-time constraints. The method is illustrated on a personal terminal receiver demodulator for mobile satellite communication.
ID:48
CLASS:1
Title: Bus-Based Communication Synthesis on System-Level
Abstract: We present an approach to automatic generation of communication topologies on system-level. Given a set of processes communicating via abstract send and receive functions and detailed information about the communication requirements of each process, we first perform a clustering of data transfers. This results in groups of transfers suited to share a common bus. For each of these clusters we execute a bus generation algorithm which schedules bus accesses in order to minimize the total communication costs. Other than previous approaches, we infer RAM, if necessary, and consider data-dependencies as well as periodic execution of processes, like in VHDL. An example demonstrates the efficiency of the developed algorithm.
ID:49
CLASS:1
Title: Design of heterogeneous ICs for mobile and personal communication systems
Abstract: Mobile and personal communication systems form key market areas for the electronics industry of the nineties. Stringent requirements in terms of flexibility, performance and power dissipation, are driving the development of integrated circuits into the direction of heterogeneous single-chip solutions. New IC architectures are emerging which contain the core of a powerful programmable processor, complemented with dedicated hardware, memory and interface structures. In this tutorial we will discuss the real-life design of a heterogeneous IC for an industrial telecom application: a reconfigurable mobile terminal for satellite communication. Based on this practical design experience, we will subsequently discuss a methodology for the design of heterogeneous ICs. Design steps that will be addressed include: system specification and refinement, data path and communication synthesis, and code generation for embedded processor cores.
ID:50
CLASS:1
Title: An overview of embedded system design education at berkeley
Abstract: Embedded systems have been a traditional area of strength in the research agenda of the University of California at Berkeley. In parallel to this effort, a pattern of graduate and undergraduate classes has emerged that is the result of a distillation process of the research results. In this paper, we present the considerations that are driving our curriculum development and we review our undergraduate and graduate program. In particular, we describe in detail a graduate class (EECS249: Design of Embedded Systems: Modeling, Validation and Synthesis) that has been taught for six years. A common feature of our education agenda is the search for fundamentals of embedded system science rather than embedded system design techniques, an approach that today is rather unique.
ID:51
CLASS:1
Title: Tutorial on high-level synthesis
Abstract: High-level synthesis takes an abstract behavioral specification of a digital system and finds a register-transfer level structure that realizes the given behavior. In this tutorial we will examine the high-level synthesis task, showing how it can be decomposed into a number of distinct but not independent subtasks. Then we will present the techniques that have been developed for solving those subtasks. Finally, we will note those areas related to high-level synthesis that are still open problems.
ID:52
CLASS:1
Title: Programming languages for computer music synthesis, performance, and composition
Abstract: The development of formal, descriptive, and procedural notations has become a practical concern within the field of music now that computers are being applied to musical tasks. Music combines the real-time demands of performance with the intellectual demands of highly developed symbolic systems that are quite different from natural language. The richness and variety of these demands makes the programming language paradigm a natural one in the musical application of computers. This paradigm provides musicians with a fresh perspective on their work. At the same time, music is a very advanced form of human endeavor, making computer music applications a worthy challenge for computer scientists. In this paper we outline the traditional tasks and forms of representation in music, then proceed with a survey of languages that deal with music programming.
ID:53
CLASS:1
Title: Synthesis of system-level communication by an allocation-based approach
Abstract: Abstract: Communication synthesis aims to transform a system with processes that communicate via high level primitives through channels into interconnected processes that communicate via signals and share communication control. We present a new algorithm that performs binding/allocation of communication units. This algorithm makes use of a cost function to evaluate different allocation alternatives. The proposed communication synthesis approach deals with both protocol selection and interface synthesis and is based on binding/allocation of communication units. We illustrate through an example the usefulness of the algorithm for allocating automatically different protocols within the same system.
ID:54
CLASS:1
Title: An application of L systems to local microcode synthesis
Abstract: In this paper we present a formal language model of register-transfer (RT) design, and apply the model to local microcode synthesis. We transform an RT design's data-path to a DTOL system&mdash;a parallel formal grammar&mdash;and formulate the design of the control part as a parsing problem of the corresponding DTOL language.The initial specification is based on the proposed SDC model of register-transfer design. We discuss the necessary steps for transforming a legal SDC-based data-path to its equivalent DTOL system. We then propose an efficient, goal-oriented, search algorithm to parse a behavioral specification, and use this method to generate the microcode for the corresponding control part. We close by presenting two examples, using a benchmark data-path.
ID:55
CLASS:1
Title: Synthesis of Complex Control Structures from Behavioral SystemC Models
Abstract: In this paper we present the results of a set of experiments we conducted in order to evaluate the viability of the behavioral synthesis, relying on the tools available at the moment in EDA market. To accomplish this we modelled a complex PCI bus interface in SystemC using a behavioral style of description. Then we tried to synthesize it by means of the Synopsis CoCentric SystemC Compiler tool. The problems arisen during synthesis, in particular those concerned with the cycle-accurate timing behavior of the synthesized circuit, were addressed. After analyzing them, possible solutions were proposed, were possible. Finally, a summary of the pros and cons of the behavioral synthesis in SystemC is presented.
ID:56
CLASS:1
Title: Modal processes: towards enhanced retargetability through control composition of distributed embedded systems
Abstract: To explore different points in the design space of an embeddedsystem, it is important to be able to compose a designfrom reusable design components, and then map the resultingsystem description onto several possible target architectureswith different partitionings of functionality. Today's specificationmodels support composition styles that work well fordata communication but not for control communication betweenconcurrent processes to be mapped onto a distributedarchitecture. We propose a new retargetable system specificationmodel that combines the best properties of process-basedand hierarchical-FSM-based methods for modular compositionof data and control. The model lends itself to automatedsynthesis of the run-time system for coordinating tasks ondifferent processors in the system. The model and synthesismethod are illustrated with several examples of embeddedsystems.
ID:57
CLASS:1
Title: Automatic Generation of a Real-Time Operating System for Embedded Systems
Abstract: Embedded systems are typically implemented as a set of communicating components some of which are implemented in hardware and some of which are implemented in software. Usually many software components share a processor. A real-time operating system (RTOS) is used to enable sharing and provide a communication mechanism between components. Commercial RTOSs are available for many popular micro-controllers. Using them provides significant reduction in design time and often leads to better structured and more maintainable systems. However, since they have to be quite general, they are not efficient enough for many applications, either in memory usage or in run times. Thus, it is often the case that RTOSs are hand coded by an expert for a particular application. This approach is obviously slow, expensive and error-prone.In this paper we propose an alternative where a RTOS is automatically generated based on a high-level description of the system. RTOSs created in our approach offer an ease of use comparable to commercial RTOSs, and yet since they are generated for a specific example, they can be optimized based on the same information used to optimize hand-written code. We have implemented our approach within POLIS, a system for HW/SW co-design of embedded system. To evaluate the POLIS-generated RTOS we have developed a prototyping environment which we use to compare POLIS against a commercial operating system.
ID:58
CLASS:1
Title: Memory, control and communications synthesis for scheduled algorithms
Abstract: This paper explores a method of grouping individual memory requirements from a hardware-constrained schedule of an algorithm, such that control and communications may be optimised. A new representation of memory requirements is introduced to explain the method. The technique may also be used to allocate operations to hardware resources. This, and control and communication optimisation are illustrated with an example.
ID:59
CLASS:1
Title: POSE: a parallel object-oriented synthesis environment
Abstract: Design automation tools and methodologies always encounter a problem of how systems may be designed efficiently, including issues such as static modeling and dynamic manipulation of system parts. With the rapid progress of design technology, the continuously increasing number of different choices per system part and the growing complexity of today's systems, the efficiency of the design environment is not only a major concern now, but will also be a demanding problem in the near future. In contrast to heuristic methods, a novel environment called POSE is proposed that increases efficiency during design without losing optimality in the final design results. System parts are modeled using the popular object-oriented modeling technique and are dynamically manipulated using the parallel design technique. A complete integration of object-oriented and parallel techniques is one of the major feature of POSE. Common problems related to parallel design such as emptiness and deadlock are also elegantly solved within POSE. Experimental results and formal analysis based on POSE all show its practical and theoretical usefulness. POSE can be used at any level of synthesis as long as off-the-shelf building-blocks manipulation is required. POSE can be applied especially to system-level synthesis, whose targets can be parallel computer architectures, systems-on-chip, or embedded systems. We will show how POSE has been applied to ICOS, a recently proposed synthesis methodology. Furthermore, POSE can be easily integrated with other heuristic design methodologies to allow increased design efficiency.
ID:60
CLASS:1
Title: A two-layer library-based approach to synthesis of analog systems from VHDL-AMS specifications
Abstract: This paper presents a synthesis methodology for analog systems described using VHDL-AMS language. Synthesis produces net-lists of analog components that are selected from a library, and sized so that specified objectives (like AC response, signal to noise ratio, dynamic range, area) are optimized. The gap between abstract specifications and implementations is bridged using a two-layered methodology. The first layer is architecture generation. The second layer is component synthesis and constraint transformation. Architecture generation employs the branch-and-bound algorithm to create architectural alternatives for a system. Component synthesis and constraint transformation use a directed interval based genetic algorithm that operates on parameter ranges. The performance estimation engine embeds technology process parameters, SPICE models for basic circuits, and symbolic composition equations for basic structural configurations. The paper discusses the VHDL-AMS subset for synthesis. The subset offers the composition semantics. As a result, specifications offer sufficient insight into the system structure to allow automated architecture generation. To justify the flexibility of the methodology, the paper presents results for three case studies, a signal conditioning system, a filter, and an analog to digital converter. Experiments show that constraint-satisfying designs can be synthesized in a short time, at a low cost, and without requesting broad knowledge on analog circuits.
ID:61
CLASS:1
Title: Synthesis of asynchronous control circuits with automatically generated relative timing assumptions
Abstract: This paper describes a method of synthesis of asynchronous circuits with relative timing. Asynchronous communication between gates and modules typically utilizes handshakes to ensure functionality. Relative timing assumptions in the form &ldquo;event a occurs before event b&rdquo; can be used to remove redundant handshakes and associated logic. This paper presents a method for automatic generation of relative timing assumptions from the untimed specification. These assumptions can be used for area and delay optimization of the circuit. A set of relative timing constraints sufficient for the correct operation of the circuit is back-annotated to the designer. Experimental results for control circuits of a prototype iA32 instruction length decoding and steering unit called RAPPID (&ldquo;Revolving Asynchronous Pentium&reg; Processor Instruction Decoder&rdquo;) shows significant improvements in area and delay over speed-independent circuits.
ID:62
CLASS:1
Title: Global hardware synthesis from behavioral dataflow descriptions
Abstract: This paper reports on a new bottom-up synthesis technique for general behavioral descriptions. Our technique extends traditional straight-line code synthesis by allowing hierarchical, block-structured dataflow graphs with block-level parallelism. Program path probabilities are taken into account; and both high-level synthesis and design-space exploration are addressed.
ID:63
CLASS:1
Title: A software synthesis tool for distributed embedded system design
Abstract: We present a design tool for automated synthesis of embedded systems on distributed COTS-based platforms. Our synthesis tool consists of (1) a graphical user interface for input of software layouts, which maps tasks to resources and (2) a constraints solving engine, which allocates local resources to tasks, all with the goal of meeting specified performance criteria. Our tool differs from previous work in that it allows (a) use of stochastic (rather than worst-case) models of resource usage and (b) resource sharing among components. Our approach uses analytical approximate solutions for quick estimates of the desired performance measures. These estimates permit an efficient search of the possible design space. Once candidate designs are determined, they are validated through a simulation model. We demonstrate the efficiency and robustness of this tool on a synthetic aperture radar benchmark.
ID:64
CLASS:1
Title: ILP Models for the Synthesis of Asynchronous Control Circuits
Abstract: A new technique for the logic synthesis of asynchronous circuits is presented. It is based on the structural theory of Petri nets and integer linear programming. The techniqueis capable of checking implementability conditions, such as,complete state coding, and deriving a gate netlist to implement the specified behavior. This technique can synthesize specifications with few thousands of transitions in the Petrinet, providing a speed-up of several orders of magnitudewith regard to other existing techniques.
ID:65
CLASS:1
Title: Generic netlist representation for system and PE level design exploration
Abstract: Designer productivity and design predictability are vital factors for successful embedded system design. Shrinking time-to-market and increasing complexity of these systems require more productive design approaches starting from high-level languages such as C. On the other hand, tight constraints of embedded systems require careful design exploration at system level (coarse grained exploration) and at the processing-element (PE) level (fine grained exploration).In this paper we presented GNR, a formal modeling approach, developed to improve productivity of designing systems and processing elements, the same way that traditional ADLs improved productivity for designing processors. The GNR is an order of magnitude shorter than state-of-the-art ADLs with RTL generation capabilities and yet can capture any structural details that affect the implementation quality. Using relatively short GNR description, we explored several designs for implementing an MP3 decoder and achieved 3.25 speedup compared to MicroBlaze processor. We have also developed a web-based interface for our tools, so that users can upload and evaluate new architectures described in GNR. Our toolset and GNR is an intermediate step towards synthesis of TLM to RTL.
ID:66
CLASS:1
Title: Automatic synthesis of microprogrammed control units from behavioral descriptions
Abstract: This paper presents an approach for automatic synthesis of a microprogrammed control unit from a behavioral description, incorporating two new features:a) A rule based program to automatically design a microprogram sequencer most suited for the given behavior, considering the speed/cost requirements.b) A powerful microinstrunction format optimizer which considers a vast space of alternative formats and a novel encoding scheme.
ID:67
CLASS:1
Title: Gesture-based control of highly articulated biomechatronic systems
Abstract: A robotic puppet is developed for studying motion generation and control of highly articulated biomimic mechatronic systems with anatomical motion data of human in real time. The system is controlled by a pair of data gloves tracking human fingers' actions. With the primitives designed in a multilayered motion synthesis structure, the puppet can realize some complex human-like actions. Continuous full body movements are produced on the robotic puppet by combining and sequencing the actions on different body parts using temporal and spatial information provided by the data gloves. Human is involved in the interactive design of the coordination and timing of the body movements of the robotic puppet in a natural and intuitive manner. The methods of motion generation exhibited on the robotic puppet may be applied to the interactive media, entertainment and biomedical engineering.
ID:68
CLASS:1
Title: Adaptive trust negotiation and access control
Abstract: Electronic transactions regularly occur between business partners in separate security domains. Trust negotiation is an approach that provides an open authentication and access-control environment for such transactions, but it is vulnerable to malicious attacks leading to denial of service or leakage of sensitive information. This paper introduces an Adaptive Trust Negotiation and Access Control (ATNAC) framework to solve these problems. The framework combines two existing systems, TrustBuilder and GAA-API, to create a system with more flexibility and responsiveness to attack than either system currently provides.
ID:69
CLASS:1
Title: Automatic synthesis of microcontrollers
Abstract: A method is proposed for automating the design of a microcontroller from a register transfer level description of a digital system. This method designs the format of the control word, determines the timing of branch decisions, and specifies the content of the microprogram. A data structure is introduced which supports some heuristic optimization of the design. The goal of the design method is to produce correct designs that are partially optimized with a practical amount of computing effort. An example is given to show how the method works.
ID:70
CLASS:1
Title: Efficient scheduling of conditional behaviors for high-level synthesis
Abstract: As hardware designs get increasingly complex and time-to-market constraints get tighter there is strong motivation for high-level synthesis (HLS). HLS must efficiently handle both dataflow-dominated and controlflow-dominated designs as well as designs of a mixed nature. In the past efficient tools for the former type have been developed but so far HLS of conditional behaviors lags behind. To bridge this gap an efficient scheduling heuristic for conditional behaviors is presented. Our heuristic and the techniques it utilizes are based on a unifying design representation appropriate for both types of behavioral descriptions, enabling the proposed heuristic to exploit under the same framework several well-established techniques (chaining, multicycling) as well as conditional resource sharing and speculative execution which are essential in efficiently scheduling conditional behaviors. Preliminary experiments confirm the effectiveness of our approach and prompted the development of the CODESIS HLS tool for further experimentation.
ID:71
CLASS:1
Title: Extended quasi-static scheduling for formal synthesis and code generation of embedded software
Abstract: With the computerization of most daily-life amenities such as home appliances, the software in a real-time embedded system now accounts for as much as 70% of a system design. On one hand, this increase in software has made embedded systems more accessible and easy to use, while on the other hand, it has also necessitated further research on how complex embedded software can be designed automatically and correctly. Enhancing recent advances in this research, we propose an Extended Quasi-Static Scheduling (EQSS) method for formally synthesizing and automatically generating code for embedded software, using the Complex-Choice Petri Nets (CCPN) model. Our method improves on previous work in three ways: (1) by removing model restrictions to cover a much wider range of applications, (2) by proposing an extended algorithm to schedule the more unrestricted model, and (3) by implementing a code generator that can produce multi-threaded embedded software programs. The requirements of an embedded software are specified by a set of CCPN, which is scheduled using EQSS such that the schedules satisfy limited embedded memory requirements and task precedence constraints. Finally, a POSIX-based multi-threaded embedded software program is generated in the C programming language. Through an example, we illustrate the feasibility and advantages of the proposed EQSS method.
ID:72
CLASS:1
Title: HW/SW partitioning and code generation of embedded control applications on a reconfigurable architecture platform
Abstract: This paper studies the use of a reconfigurable architecture platform for embedded control applications aimed at improving real time performance. The hw/sw codesign methodology from POLIS is used. It starts from high-level specifications, optimizes an intermediate model of computation (Extended Finite State Machines) and derives both hardware and software, based on performance constraints. We study a particular architecture platform, which consists of a general purpose processor core, augmented with a reconfigurable function unit and data-path to improve run time performance. A new mapping flow and algorithms to partition hardware and software are proposed to generate implementations that best utilize this architecture. Encouraging preliminary results are shown for automotive electronic control examples.
ID:73
CLASS:1
Title: Using speculative computation and parallelizing techniques to improve scheduling of control based designs
Abstract: Recent research results have seen the application of parallelizing techniques to high-level synthesis. In particular, the effect of speculative code transformations on mixed control-data flow designs has demonstrated effective results on schedule lengths. In this paper we first analyze the use of the control and data dependence graph as an intermediate representation that provides the possibility of extracting the maximum parallelism. Then we analyze the scheduling problem by formulating an approach based on Integer Linear Programming (ILP) to minimize the number of control steps given the amount of resources. We improve the already proposed ILP scheduling approaches by introducing a new conditional resource sharing constraint which is then extended to the case of speculative computation. The ILP formulation has been solved by using a Branch and Cut framework which provides better results than standard branch and bound techniques.
ID:74
CLASS:1
Title: High-level synthesis: current status and future directions
Abstract: The third High-Level Synthesis Workshop was held in January of this year at Orcas Island, Washington*. What distinguished this workshop from its predecessors was that participants were asked to submit papers describing the application of their systems and algorithms to a set of benchmarks. This proved to be a successful method for comparing the extensive research being conducted in this area of CAD. In this paper, we will briefly describe the benchmarks and use them as a foundation for outlining the major themes in the research work presented at the workshop. The paper concludes with a summary of the discussions held during the course of the workshop on the future development of the high-level synthesis benchmark suite.
ID:75
CLASS:1
Title: A case study in embedded system design: an engine control unit
Abstract: A number of techniques and software tools for embedded system design have been recently proposed. However, the current practice in the designer community is heavily based on manual techniques and on past experience rather than on a rigorous approach to design. To advance the state of the art it is important to address a number of relevant design problems and solve them to demonstrate the power of the new approaches.We chose an industrial example in automotive electronics to validate our design methodology: an existing commercially available Engine Control Unit. We discuss in detail the specification, the implementation philosophy, and the architectural trade-off analysis. We analyze the results obtained with our approach and compare them with the existing design underlining the advantages offered by a systematic approach to embedded system design in terms of performance and design time.
ID:76
CLASS:1
Title: An intermediate representation for behavioral synthesis
Abstract: This paper describes an intermediate representation for behavioral and structural designs that is based on annotated state tables. It facilitates user control of the synthesis process by allowing specification of partially design structures, and a mixture of behavior, structure and user specified bindings between the abstract behavior and the structure. The format's general model allows the capture of synchronous and asynchronous behavior, and permits hierarchical descriptions with concurrency. The format is easily translated to VHDL for simulation at each stage of the design process. It therefore complements a good simulation language (VHDL) by providing an excellent input path for behavioral and register-transfer synthesis. The format's simple and uniform syntax allows it to be used both as an intermediate exchange format for various behavioral synthesis tools, and as a graphical tabular interface for the user, thereby allowing a natural medium for automatic or manual refinement of the design.
ID:77
CLASS:1
Title: ProCEED: an expert system for multivariate process control systems design
Abstract: Design and implementation of effective control systems is vital to the competitiveness of the process industry. Process control system design requires knowledge based systems which are capable of efficient numeric computing along with the symbolic reasoning. A brief survey presented in this paper shows that many industrial and academic research groups, are actively developing such knowledge based tools, in the U.S. and other countries.ProCEED, the expert system described in this paper has been developed on Digital's A.I. work-station and using Intellicorp's Knowledge Engineering Environment (KEE). This ideally combines the KEE and LISP based reasoning and Fortran based number crunching. The modular structure and the user interface permit easier integration of additional knowledge about existing systems, as well as incorporation of completely new controllers and processes.A controller design session begins with process modeling. Then the extent of interaction in the multivariable process is determined using a variety of techniques, including relative gain analysis and singular value decomposition, and conflicts among them are resolved. Based on the apparent interaction and controller implementation resources available to the user, multi-loop or completely multivariable forms of one or more control strategies, including proportional-integra-derivative mode control and simplified model predictive control, are chosen. The control systems are designed to achieve the specified performance criteria, utilizing the available numeric software in Fortran. The expert system presents the user with the design results and the simulation of the controlled system, in a graphical form.
ID:78
CLASS:1
Title: Scheduling and binding algorithms for high-level synthesis
Abstract: New algorithms for high-level synthesis are presented. The first performs scheduling under hardware resource constraints and improves on commonly used list scheduling techniques by making use of a global priority function. A new design-space exploration technique, which combines this algorithm with an existing one based on time constraints, is also presented.A second algorithm is used for register and bus allocation to satisfy two criteria: the minimization of interconnect costs as well as the final register (bus) cost. A clique partitioning approach is used where the clique graph is pruned using interconnect affinities between register (bus) pairs. Examples from current literature were chosen to illustrate the algorithms and to compare them with four existing systems.
ID:79
CLASS:1
Title: Combining supervisor synthesis and model checking
Abstract: Model checking and supervisor synthesis have been successful in solving different design problems related to discrete systems in the last decades. In this paper, we analyze some advantages and drawbacks of these approaches and combine them for mutual improvement. We achieve this through a generalization of the supervisory control problem proposed by Ramadge and Wonham. The objective of that problem is to synthesize a supervisor which constrains a system's behavior according to a given specification, ensuring controllability and coaccessibility. By introducing a new representation of the solution using systems of &mu;-calculus equations, we are able to handle these two conditions separately and thus to exchange the coaccessibility requirement by any condition that could be used in model checking. Well-known results on &mu;-calculus model checking allow us to easily assess the computational complexity of any generalization. Moreover, the model checking approach also delivers algorithms to solve the generalized synthesis problem. We include an example in which the coaccessibility requirement is replaced by fairness constraints. The paper also contains an analysis of related work by several authors.
ID:80
CLASS:1
Title: A synthesis rule for concurrent systems
Abstract: Concurrent (hardware and software) systems can become extremely complex due to the existence of multiple loci of control. Posteriori analysis of such systems is very difficult. This paper presents a systematic bottom-up modular approach to synthesis. The synthesis procedure at each stage yields all invariants (of a certain kind) of the system. These invariants can be used as an aid to proving certain properties of the system such as boundedness, conservativeness, mutual exclusion, absence of deadlock, etc. The use of the synthesis rule and the utility of the invariants are illustrated by examples.
ID:81
CLASS:1
Title: CASPER: concurrent hardware-software co-synthesis of hard real-time aperiodic and periodic specifications of embedded system architectures
Abstract: Hardware-software co-synthesis of an embedded system requires mapping of its specifications into hardware and software modules such that its real-time and other constraints are met. Embedded system specifications are generally represented by acyclic task graphs. Many embedded system applications are characterized by aperiodic as well as periodic task graphs. Aperiodic task graphs can arrive for execution at any time and their resource requirements vary depending on how their constituent tasks and edges are allocated. Traditional approaches based on a fixed architecture coupled with slack stealing and/or on-line determination of how to serve aperiodic task graphs are not suitable for embedded systems with hard real-time constraints, since they cannot guarantee that such constraints would always be met. In this paper, we address the problem of concurrent co-synthesis of aperiodic and periodic specifications of embedded systems. We estimate the resource requirements of aperiodic task graphs and allocate execution slots on processing elements and communication links for executing them. Our approach guarantees that the deadlines of both aperiodic and periodic task graphs are always met. We have observed that simultaneous consideration of aperiodic task graphs while performing co-synthesis of periodic task graphs is vital for achieving superior results compared to the traditional slack stealing and dynamic scheduling approaches. To the best of our knowledge, this is the first co-synthesis algorithm which provides simultaneous support of periodic and aperiodic task graphs with hard real-time constraints. Application of the proposed algorithm to several examples from real-life telecom transport systems shows that up to 28% and 34% system cost savings are possible over co-synthesis algorithms which employ slack stealing and rate-monotonic scheduling, respectively.
ID:82
CLASS:1
Title: Performance animation from low-dimensional control signals
Abstract: This paper introduces an approach to performance animation that employs video cameras and a small set of retro-reflective markers to create a low-cost, easy-to-use system that might someday be practical for home use. The low-dimensional control signals from the user's performance are supplemented by a database of pre-recorded human motion. At run time, the system automatically learns a series of local models from a set of motion capture examples that are a close match to the marker locations captured by the cameras. These local models are then used to reconstruct the motion of the user as a full-body animation. We demonstrate the power of this approach with real-time control of six different behaviors using two video cameras and a small set of retro-reflective markers. We compare the resulting animation to animation from commercial motion capture equipment with a full set of markers.
ID:83
CLASS:1
Title: The Chinook hardware/software co-synthesis system
Abstract: Abstract: Designers of embedded systems are facing ever tighter constraints on design time, but computer-aided design tools for embedded systems have not kept pace with these trends. The Chinook co-synthesis system addresses the automation of the most time-consuming and error-prone tasks in embedded controller design, namely the synthesis of interface hardware and software needed to integrate system components, the migration of functions between processors or custom logic, and the co-simulation of the design before, during and after synthesis. This paper describes the principal elements of Chinook and discuss its application to a variety of embedded designs.
ID:84
CLASS:1
Title: High-level synthesis for large bit-width multipliers on FPGAs: a case study
Abstract: In this paper, we present the analysis, design and implementation of an estimator to realize large bit width unsigned integer multiplier units. Larger multiplier units are required for cryptography and error correction circuits for more secure and reliable transmissions over highly insecure and/or noisy channels in networking and multimedia applications. The design space for these circuits is very large when integer multiplication on large operands is carried out hierarchically. In this paper, we explore automated synthesis of high bit-width unsigned integer multiplier circuits by defining and validating an estimator function used in search and analysis of the design space of such circuits. We focus on analysis of a hybrid hierarchical multiplier scheme that combines the throughput advantages of parallel multipliers and the resource cost-effectiveness of serial ones. We present an analytical model that rapidly predicts timing and resource usage for selected model candidates. We evaluate the estimator model in the design of a practical application, a 256-bit elliptic curve adder implemented on a Xilinx FPGA fabric. We show that our estimator allows implementation of fast, efficient circuits, where resultant designs provide order-of-magnitude performance improvements when compared with that of software implementations on a high performance computing platform.
ID:85
CLASS:1
Title: Fault-Tolerant Deployment of Embedded Software for Cost-Sensitive Real-Time Feedback-Control Applications
Abstract: Designing cost-sensitive real-time control systems for safety-critical applications requires a careful analysis of the cost/coverage trade-offs of fault-tolerant solutions. This further complicates the difficult task of deploying the embedded software that implements the control algorithms on the execution platform that is often distributed around the plant (as it is typical, for instance, in automotive applications). We propose a synthesis-based design methodology that relieves the designers from the burden of specifying detailed mechanisms for addressing platform faults, while involving them in the definition of the overall fault-tolerance strategy. Thus, they can focus on addressing plant faults within their control algorithms, selecting the best components for the execution platform, and de.ning an accurate fault model. Our approach is centered on a new model of computation, Fault Tolerant Data Flows (FTDF), that enables the integration of formal validation techniques.
ID:86
CLASS:1
Title: A unified model for co-simulation and co-synthesis of mixed hardware/software systems
Abstract: This paper presents a methodology for a unified co-simulation and co-synthesis of hardware-software systems. This approach addresses the modeling of communication between the hardware and software modules at different abstraction levels and for different design tools. The main contribution is the use of a multi-view library concept in order to hide specific hardware/software implementation details and communication schemes. A system is viewed as a set of communicating hardware (VHDL) and software (C) sub-systems. The same C, VHDL descriptions can be used for both co-simulation and hardware-software co-synthesis. This approach is illustrated by an example.
ID:87
CLASS:1
Title: An evolutionary approach to system-level synthesis
Abstract: Considers system-level synthesis as the problem of optimally mapping a task-level specification onto a heterogeneous hardware/software architecture. This problem requires: (1) the selection of the architecture (allocation), including general-purpose and dedicated processors, ASICs, buses and memories; (2) the mapping of the algorithm onto the selected architecture in space (binding) and time (scheduling); and (3) the design space exploration, with the goal of finding a set of implementations that satisfy a number of constraints on cost and performance. In this paper, a new graph-based mapping model is introduced to specify the task of system-level synthesis as an optimization problem. An evolutionary algorithm is adapted to solve this problem and is applied to explore the design space of video-codec implementations.
ID:88
CLASS:1
Title: Automatic Synthesis of Control Software for an Industrial Automation Control System
Abstract: We present a case study on automatic synthesis of control software from formal specifications for an industrial automation control system. Our aim is to compare the effectiveness (i.e. design effort and controller quality) of automatic controller synthesis from closed loop formal specifications with that of manual controller design followed by automatic verification. The system to be controlled (plant) models a metal processing facility near Karlsruhe.We succeeded in automatically generating C code implementing a (correct by construction) controller for such a plant from closed loop formal specifications. Our experimental results show that for industrial automation control systems automatic synthesis is a viable and profitable (especially as far as design effort is concerned) alternative to manual design followed by automatic verification.
ID:89
CLASS:1
Title: The system architect's workbench
Abstract: This paper presents an overview of The System Architect's Workbench, a behavioral synthesis system under development at Carnegie Mellon University. This system converts an abstract behavioral description of a piece of hardware into a set of register-transfer components and a control sequence table. Two-synthesis methodologies are supported: one is tuned specifically to design microprocessors, and the other supports a more general design style. Results are presented here for both approaches.
ID:90
CLASS:1
Title: Experience with ADAM synthesis system
Abstract: The ADAM synthesis system consists of two major subsystems: the program tools which synthesize RTL designs from behavioral descriptions and the prediction tools which guide the designer in exploring the design space for a good design. In this paper, we demonstrate the necessity for predictions in narrowing the search space. With the aid of an example, we describe the interaction of a designer with the two subsystems in designing an RTL implementation which maximizes performance while meeting a given area constraint.
ID:91
CLASS:1
Title: Resynthesis and peephole transformations for the optimization of large-scale asynchronous systems
Abstract: Several approaches have been proposed for the syntax-directed compilation of asynchronous circuits from high-level specification languages, such as Balsa and Tangram. Both compilers have been successfully used in large real-world applications; however, in practice, these methods suffer from significant performance overheads due to their reliance on straightforward syntax-directed translation.This paper introduces a powerful new set of transformations, and an extended channel-based language to support them, which can be used an optimizing back-end for Balsa. The transforms described in this paper fall into two categories: resynthesis and peephole. The proposed optimization techniques have been fully integrated into a comprehensive asynchronous CAD package, Balsa. Experimental results on several substantial design examples indicate significant performance improvements. supported by NSF ITR Award No. NSF-CCR-0086036 and NSF Award No. CCR-99-88241, and by a grant from the New York State Microelectronics Design Center.
ID:92
CLASS:1
Title: Extensible control architectures
Abstract: Architectural advances of modern systems has often been at odds with control complexity, requiring significant effort in both design and verification. This is particularly true for sequential controllers, where machine complexity can quickly surpass designer ability. Traditional solutions to this problem require elaborate specifications that are difficult to maintain and extend. Further, the logic generated from these specifications bares no resemblance to the intended behavior and often fails to meet design performance constraints. In the process of designing a multi-threaded, dynamically-pipelined microcontroller, we encountered a number of common difficulties that arise from the inadequacies of traditional pipeline design methodologies. Through the use of a novel nondeterministic finite automata (NFA) specification model, we were able to implement an extensible control structure with minimal design effort. In this paper we present a viable pipeline controller specification methodology using the pyPBS language, which enables minimal effort control partitioning and compact behavioral representation. The structure of the language encourages design decisions that promote efficient modular constructions which can be easily integrated and extended. We present an overview of the our methodology including background on the pyPBS synthesis model, an architectural overview of our multi-threaded microcontroller, and implementation details for the control structure of the design including the complete control specifications. In addition, we show that the applicative nature of the pyPBS language allows for addition of a multi-cycle multiplication unit with minimal effort.
ID:93
CLASS:1
Title: Coordinated parallelizing compiler optimizations and high-level synthesis
Abstract: We present a high-level synthesis methodology that applies a coordinated set of coarse-grain and fine-grain parallelizing transformations. The transformations are applied both during a pre-synthesis phase and during scheduling, with the objective of optimizing the results of synthesis and reducing the impact of control flow constructs on the quality of results. We first apply a set of source level presynthesis transformations that include common sub-expression elimination (CSE), copy propagation, dead code elimination and loop-invariant code motion, along with more coarse-level code restructuring transformations such as loop unrolling. We then explore scheduling techniques that use a set of aggressive speculative code motions to maximally parallelize the design by re-ordering, speculating and sometimes even duplicating operations in the design. In particular, we present a new technique called "Dynamic CSE" that dynamically coordinates CSE and code motions such as speculation and conditional speculation during scheduling. We implemented our parallelizing high-level synthesis in the &#60;i>SPARK&#60;/i> framework. This framework takes a behavioral description in ANSI-C as input and generates synthesizable register-transfer level VHDL. Our results from computationally expensive portions of three moderately complex design targets, namely, MPEG-1, MPEG-2 and the GIMP image processing tool, validate the utility of our approach to the behavioral synthesis of designs with complex control flows.
ID:94
CLASS:1
Title: Eliminating false loops caused by sharing in control path
Abstract: In high level synthesis, resource sharing may result in a circuit containing false loops that pose great difficulty in timing validation during design sign-off phase. It is hence desirable to avoid generating any false loops in a synthesized circuit. Previous work considered mainly data path sharing for false loop elimination. However, for a complete circuit with both data path and control path, false loops can be created due to control logic sharing, even though the loops caused by data path sharing have all been removed. In this paper we present a novel approach to detect and eliminate the false loops caused by control logic sharing. An effective filter is devised to reduce the computation complexity of false loop detection, which is based on checking the level numbers that are propagated from data path operators to inputs/outputs of the control path. Only the input/output pairs of the control path identified by the filter are further investigated by traversing into the data path for false loop detection. A removal algorithm is then applied to eliminate the detected false loops, followed by logic minimization to further optimize the circuit. Experimental results show that for nine example circuits we tested, the final designs after false loop removal and logic minimization give only slightly larger area than the original ones that contain false loops.
ID:95
CLASS:1
Title: Hierarchical conditional dependency graphs as a unifying design representation in the CODESIS high-level synthesis system
Abstract: In high-level hardware synthesis (HLS) there is a gap on the quality of the synthesized results between data-flow and control-flow dominated behavioral descriptions. Heuristics destined for the former usually perform poorly on the latter. To close this gap, the CODESIS interactive HLS tool relies on a unifying intermediate design representation and adapted heuristics that are able to accommodate both types of designs as well as designs of a mixed data-flow and control-flow nature. Preliminary experimental results in mutual exclusiveness detection and in efficiently scheduling conditional behaviors, are encouraging and prompt for more extensive experimentation.
ID:96
CLASS:1
Title: A scheduling algorithm for optimization and early planning in high-level synthesis
Abstract: Complexities of applications implemented on embedded and programmable systems grow with the advances in capacities and capabilities of these systems. Mapping applications onto them manually is becoming a very tedious task. This draws attention to using high-level synthesis within design flows. Meanwhile, it is essential to provide a flexible formulation of optimization objectives as well as to perform efficient planning for various design objectives early on in the design flow. In this work, we address these issues in the context of data flow graph (DFG) scheduling, which is an essential element within the high-level synthesis flow. We present an algorithm that schedules a chain of operations with data dependencies among consecutive operations at a single step. This local problem is repeated to generate the schedule for the whole DFG. The local problem is formulated as a maximum weight noncrossing bipartite matching. We use a technique from the computational geometry domain to solve the matching problem. This technique provides a theoretical guarantee on the solution quality for scheduling a single chain of operations. Although still being local, this provides a relatively wider perspective on the global scheduling objectives. In our experiments we compared the latencies obtained using our algorithm with the optimal latencies given by the exact solution to the integer linear programming (ILP) formulation of the problem. In 9 out of 14 DFGs tested, our algorithm found the optimal solution, while generating latencies comparable to the optimal solution in the remaining five benchmarks. The formulation of the objective function in our algorithm provides flexibility to incorporate different optimization goals. We present examples of how to exploit the versatility of our algorithm with specific examples of objective functions and experimental results on the ability of our algorithm to capture these objectives efficiently in the final schedules.
ID:97
CLASS:1
Title: Automatic PLA synthesis from a DDL-P description
Abstract: This paper describes an automatic PLA synthesis (APLAS) system which automatically generates a PLA for the control function of a design from a DDL-P description of a digital system. APLAS can also minimize and partition the PLA to meet the design constraints. This is a very convenient tool for designing finite state machines. The control circuit of any digital system for which a state diagram can be drawn can be designed easily using this system.
ID:98
CLASS:1
Title: Computer-music interfaces: a survey
Abstract: This paper is a study of the unique problems posed by the use of computers by composers and performers of music. The paper begins with a presentation of the basic concepts involved in the musical interaction with computer devices, followed by a detailed discussion of three musical tasks: music manuscript preparation, music language interfaces for composition, and real-time performance interaction. Fundamental design principles are exposed through an examination of several early computer music systems, especially the Structured Sound Synthesis Project. A survey of numerous systems, based on the following categories, is presented: compositions and synthesis languages, graphics score editing, performance instruments, digital audio processing tools, and computer-aided instruction in music systems. An extensive reference list is provided for further study in the field.
ID:99
CLASS:1
Title: Functional partitioning improvements over structural partitioning for packaging constraints and synthesis: tool performance
Abstract: Incorporating functional partitioning into a synthesis methodology leads to several important advantages. In functional partitioning, we first partition a functional specification into smaller subspecifications and then synthesize structure for each, in contrast to the current approach of first synthesizing structure for the entire specification and then partitioning that structure. One advantage is the improvement in I/O performance and package count, when partitioning among hardware blocks with size and I/O constraints, such as FPGAs or blocks within an ASIC. A second advantage is reduction in synthesis runtimes. We describe these important advantages, concluding that further research on functional partitioning can lead to inproved results from synthesis environments.
ID:100
CLASS:1
Title: Interface co-synthesis techniques for embedded systems
Abstract: Abstract: A key aspect of the synthesis of embedded systems is the automatic integration of system components. This entails the derivation of both the hardware and software interfaces that will bind these elements together and permit them to communicate correctly and efficiently. Without the automatic synthesis of these interfaces, designers are not able to fully simulate and evaluate their systems. Frequently, they are discouraged from exploring the design space of different hardware/software partitions because practical concerns mandate minimizing changes late in the design cycle, thus leading to more costly implementations than necessary. This paper presents a set of techniques that form the basis of a comprehensive solution to the synthesis of hardware/software interfaces. Software drivers and glue logic are generated to connect processors to peripheral devices, hardware co-processors, or communication interfaces while meeting bandwidth and performance requirements. We use as examples a set of devices that communicate over an infrared local communications network (highlighting a video wrist-watch display) to explain our techniques and the need for design space exploration tools for embedded systems.
ID:101
CLASS:2
Title: A data structure for a sequence of string accesses in external memory
Abstract: We introduce a new paradigm for querying strings in external memory, suited to the execution of sequences of operations. Formally, given a dictionary of n strings S1, &hellip;, Sn, we aim at supporting a search sequence for m not necessarily distinct strings T1, T2, &hellip;, Tm, as well as inserting and deleting individual strings. The dictionary is stored on disk, where each access to a disk page fetches B items, the cost of an operation is the number of pages accessed (I/Os), and efficiency must be attained on entire sequences of string operations rather than on individual ones.Our approach relies on a novel and conceptually simple self-adjusting data structure (SASL) based on skip lists, that is also interesting per se. The search for the whole sequence T1, T2, &hellip;, Tm can be done in an expected number of I/Os:O(&sum;j&equals;1m &verbar;Tj&verbar;/B &plus; &sum;i&equals;1nn (ni logB m/ni)),where each Tj may or may not be present in the dictionary, and ni is the number of times Si is queried (i.e., the number of Tjs equal to Si). Moreover, inserting or deleting a string Si takes an expected amortized number O(&verbar;Si&verbar;/B &plus; logB n) of I/Os. The term &sum;j&equals;1m &verbar;Tj&verbar;/B in the search formula is a lower bound for reading the input, and the term &sum;i&equals;1n ni logB m/ni (entropy of the query sequence) is a standard information-theoretic lower bound. We regard this result as the static optimality theorem for external-memory string access, as compared to Sleator and Tarjan's classical theorem for numerical dictionaries [Sleator and Tarjan 1985]. Finally, we reformulate the search bound if a cache is available, taking advantage of common prefixes among the strings examined in the search.
ID:102
CLASS:2
Title: Efficient indexing data structures for flash-based sensor devices
Abstract: Flash memory is the most prevalent storage medium found on modern wireless sensor devices (WSDs). In this article we present two external memory index structures for the efficient retrieval of records stored on the local flash memory of a WSD. Our index structures, MicroHash and MicroGF (micro grid files), exploit the asymmetric read/write and wear characteristics of flash memory in order to offer high-performance indexing and searching capabilities in the presence of a low-energy budget, which is typical for the devices under discussion. Both structures organize data and index pages on the flash media using a sorted by timestamp file organization. A key idea behind these index structures is that expensive random access deletions are completely eliminated. MicroHash enables equality searches by value in constant time and equality searches by timestamp in logarithmic time at a small cost of storing index pages on the flash media. Similarly, MicroGF enables spatial equality and proximity searches in constant time. We have implemented these index structures in nesC, the programming language of the TinyOS operating system. Our trace-driven experimentation with several real datasets reveals that our index structures offer excellent search performance at a small cost of constructing and maintaining the index.
ID:103
CLASS:2
Title: Efficient software model checking of data structure properties
Abstract: This paper presents novel language and analysis techniques that significantly speed up software model checking of data structure properties. Consider checking a red-black tree implementation. Traditional software model checkers systematically generate all red-black tree states (within some given bounds) and check every red-black tree operation (such as insert, delete, or lookup) on every red-black tree state. Our key idea is as follows. As our checker checks a red-black tree operation o on a red-black tree state s, it uses program analysis techniques to identify other red-black tree states s'1, s'2, ..., s'k on which the operation o behaves similarly. Our analyses guarantee that if o executes correctly on s, then o will execute correctly on every s'i. Our checker therefore does not need to check o on any s'i once it checks o on s. It thus safely prunes those state transitions from its search space, while still achieving complete test coverage within the bounded domain. Our preliminary results show orders of magnitude improvement over previous approaches. We believe our techniques can make model checking significantly faster, and thus enable checking of much larger programs and complex program properties than currently possible.
ID:104
CLASS:2
Title: A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data
Abstract: One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.
ID:105
CLASS:2
Title: Graphical data structures
Abstract: A graphical data structure is essentially a mathematical model of the data in a picture. This model describes the properties of that data and therefore it describes relationships which exist in that data. In this paper a method is described for constructing graphical data structures so that the relationships expressed in those structures correspond to relationships which exist in the "real" world. These relationships can then be exploited so that any data which has that structure can be manipulated in a "real" world manner. This method is based on an extended schema for relational databases which is manipulated using an extended APL notation.
ID:106
CLASS:2
Title: A general framework for prefetch scheduling in linked data structures and its application to multi-chain prefetching
Abstract: Pointer-chasing applications tend to traverse composite data structures consisting of multiple independent pointer chains. While the traversal of any single pointer chain leads to the serialization of memory operations, the traversal of independent pointer chains provides a source of memory parallelism. This article investigates exploiting such interchain memory parallelism for the purpose of memory latency tolerance, using a technique called multi--chain prefetching. Previous works [Roth et al. 1998;Roth and Sohi 1999] have proposed prefetching simple pointer-based structures in a multi--chain fashion. However, our work enables multi--chain prefetching for arbitrary data structures composed of lists, trees, and arrays.This article makes five contributions in the context of multi--chain prefetching. First, we introduce a framework for compactly describing linked data structure (LDS) traversals, providing the data layout and traversal code work information necessary for prefetching. Second, we present an off-line scheduling algorithm for computing a prefetch schedule from the LDS descriptors that overlaps serialized cache misses across separate pointer-chain traversals. Our analysis focuses on static traversals. We also propose using speculation to identify independent pointer chains in dynamic traversals. Third, we propose a hardware prefetch engine that traverses pointer-based data structures and overlaps multiple pointer chains according to the computed prefetch schedule. Fourth, we present a compiler that extracts LDS descriptors via static analysis of the application source code, thus automating multi--chain prefetching. Finally, we conduct an experimental evaluation of compiler-instrumented multi--chain prefetching and compare it against jump pointer prefetching [Luk and Mowry 1996], prefetch arrays [Karlsson et al. 2000], and predictor-directed stream buffers (PSB) [Sherwood et al. 2000].Our results show compiler-instrumented multi--chain prefetching improves execution time by 40&percnt; across six pointer-chasing kernels from the Olden benchmark suite [Rogers et al. 1995], and by 3&percnt; across four SPECint2000 benchmarks. Compared to jump pointer prefetching and prefetch arrays, multi--chain prefetching achieves 34&percnt; and 11&percnt; higher performance for the selected Olden and SPECint2000 benchmarks, respectively. Compared to PSB, multi--chain prefetching achieves 27&percnt; higher performance for the selected Olden benchmarks, but PSB outperforms multi--chain prefetching by 0.2&percnt; for the selected SPECint2000 benchmarks. An ideal PSB with an infinite Markov predictor achieves comparable performance to multi--chain prefetching, coming within 6&percnt; across all benchmarks. Finally, speculation can enable multi--chain prefetching for some dynamic traversal codes, but our technique loses its effectiveness when the pointer-chain traversal order is highly dynamic.
ID:107
CLASS:2
Title: The hyperring: a low-congestion deterministic data structure for distributed environments
Abstract: In this paper we study the problem of designing searchable concurrent data structures with performance guarantees that can be used in a distributed environment where data elements are stored in a dynamically changing set of nodes. Searchable data structures are data structures that provide three basic operations: INSERT, DELETE, and SEARCH. In addition to searching for an exact match, we demand that for a data structure to be called "searchable", Search also has to be able to search for the closest successor or predecessor of a data item. Such a property has a tremendous advantage over just exact match, because it would allow to implement many data base applications.We are interested in finding a searchable concurrent data structure that has (1) a low degree, (2) requires a small amount of work for INSERT and DELETE operations, and (3) is able to handle concurrent search requests with low congestion and dilation.We present the first deterministic concurrent data structure, called Hyperring, that can fulfill all of these objectives in a polylogarithmic way. In fact, the Hyperring has a degree of O(log n), requires O(log3 n) work for INSERT and DELETE operations, and can handle concurrent search requests to random destinations, one request per node, with congestion and dilation O(log n) w.h.p.Most of the previous solutions for distributed environments are not searchable (in our sense) but only provide exact lookup, and those that are searchable do not have proofs about the congestion caused by concurrent search requests.
ID:108
CLASS:2
Title: Retroactive data structures
Abstract: We introduce a new data structuring paradigm in which operations can be performed on a data structure not only in the present but also in the past. In this new paradigm, called retroactive data structures, the historical sequence of operations performed on the data structure is not fixed. The data structure allows arbitrary insertion and deletion of operations at arbitrary times, subject only to consistency requirements. We initiate the study of retroactive data structures by formally defining the model and its variants. We prove that, unlike persistence, efficient retroactivity is not always achievable, so we go on to present several specific retroactive data structures.
ID:109
CLASS:2
Title: An extensible framework for providing dynamic data structure visualizations in a lightweight IDE
Abstract: A framework for producing dynamic data structure visualizations within the context of a lightweight IDE is described. Multiple synchronized visualizations of a data structure can be created with minimal coding through the use of an external viewer model. The framework supplies a customizable viewer template as well as high-level APIs to a graph drawing library and the Java Debugger Interface. Initial classroom use has demonstrated the framework's ease of use as well as its potential to as an aid to student learning.
ID:110
CLASS:2
Title: XRules: an effective structural classifier for XML data
Abstract: XML documents have recently become ubiquitous because of their varied applicability in a number of applications. Classification is an important problem in the data mining domain, but current classification methods for XML documents use IR-based methods in which each document is treated as a bag of words. Such techniques ignore a significant amount of information hidden inside the documents. In this paper we discuss the problem of rule based classification of XML data by using frequent discriminatory substructures within XML documents. Such a technique is more capable of finding the classification characteristics of documents. In addition, the technique can also be extended to cost sensitive classification. We show the effectiveness of the method with respect to other classifiers. We note that the methodology discussed in this paper is applicable to any kind of semi-structured data.
ID:111
CLASS:2
Title: Automatic detection and repair of errors in data structures
Abstract: We present a system that accepts a specification of key data structure consistency constraints, then dynamically detects and repairs violations of these constraints, enabling the program to continue to execute productively even in the face of otherwise crippling errors. Our experience using our system indicates that the specifications are relatively easy to develop once one understands the data structures. Furthermore, for our set of benchmark applications, our system can effectively repair inconsistent data structures and enable the program to continue to operate successfully.
ID:112
CLASS:2
Title: A concise b-rep data structure for stratified subanalytic objects
Abstract: Current geometric kernels suffer from poor abstraction and design of their data structures. In part, this is due to the lack of a general mathematical framework for geometric modelling and processing. As a result, there is a proliferation of ad hoc solutions, say external data structures, whenever new problems arise in industry, causing serious difficulties in software maintenance. This paper proposes such a framework based on subanalytic geometry and theory of stratifications, as well as a concise data structure for it, called DiX (Data in Xtratus). Basically, this is a non-manifold b-rep (boundary representation) data structure without oriented cells (e.g. half-edges, coedges or so). Thus, it is more concise than the traditional b-rep data structures provided that no oriented cells (e.g. half-edges, half-faces, etc.) are used at all. Nevertheless, all the adjacency and incidence data we need is retrieved by a single operator, called mask operator. Besides, the DiX data structure includes shape descriptors, as generalizations of loops and shells, to support shape reasoning as needed in the design and implementation of shape operators such as, for example, Euler operators.
ID:113
CLASS:2
Title: A scalable data structure for three-dimensional non-manifold objects
Abstract: In this paper, we address the problem of representing and manipulating non-manifold, mixed-dimensional objects described by three-dimensional simplicial complexes embedded in the 3D Euclidean space. We describe the design and the implementation of a new data structure, that we call the non-manifold indexed data structure with adjacencies (NMIA), which can represent any three-dimensional Euclidean simplicial complex compactly, since it encodes only the vertices and the top simplexes of the complex plus a restricted subset of topological relations among simplexes. The NMIA structure supports efficient traversal algorithms which retrieve topological relations in optimal time, and it scales very well to the manifold case. Here, we sketch traversal algorithms, and we compare the NMIA structure with data structures for manifold and regular 3D simplicial complexes.
ID:114
CLASS:2
Title: D(k)-index: an adaptive structural summary for graph-structured data
Abstract: To facilitate queries over semi-structured data, various structural summaries have been proposed. Structural summaries are derived directly from the data and serve as indices for evaluating path expressions on semi-structured or XML data. We introduce the D(k) index, an adaptive structural summary for general graph structured documents. Building on previous work, 1-index and A(k) index, the D(k)-index is also based on the concept of bisimilarity. However, as a generalization of the 1-index and A(k)-index, the D(k) index possesses the adaptive ability to adjust its structure according to the current query load. This dynamism also facilitates efficient update algorithms, which are crucial to practical applications of structural indices, but have not been adequately addressed in previous index proposals. Our experiments show that the D(k) index is a more effective structural summary than previous static ones, as a result of its query load sensitivity. In addition, update operations on the D(k) index can be performed more efficiently than on its predecessors.
ID:115
CLASS:2
Title: ViST: a dynamic index method for querying XML data by tree structures
Abstract: With the growing importance of XML in data exchange, much research has been done in providing flexible query facilities to extract data from structured XML documents. In this paper, we propose ViST, a novel index structure for searching XML documents. By representing both XML documents and XML queries in structure-encoded sequences, we show that querying XML data is equivalent to finding subsequence matches. Unlike index methods that disassemble a query into multiple sub-queries, and then join the results of these sub-queries to provide the final answers, ViST uses tree structures as the basic unit of query to avoid expensive join operations. Furthermore, ViST provides a unified index on both content and structure of the XML documents, hence it has a performance advantage over methods indexing either just content or structure. ViST supports dynamic index update, and it relies solely on B+ Trees without using any specialized data structures that are not well supported by DBMSs. Our experiments show that ViST is effective, scalable, and efficient in supporting structural queries.
ID:116
CLASS:2
Title: Techniques for the automatic selection of data structures
Abstract: We are all aware of the development of increasingly sophisticated, elaborate, and expensive computer programs, particularly in the fields of artificial intelligence, data base management, and intelligent systems. The need for techniques to deal with such complexity has renewed interest in programming language research. Recent work on structured programming, intelligent compilers, automatic program generation and verification, and high-level optimization has resulted. A pattern of approach similar to that of earlier research on programming languages is emerging. The work divides naturally into two parts: the search for good linguistic tools for expressing algorithms and data, and the development of practical methods for translating these to working computer programs. Our emphasis in this paper is in the latter.
ID:117
CLASS:2
Title: A study of order transformations of hierarchical structures in IMS data bases
Abstract: Hierarchical structures are widely used in information processing. An application program written to traverse a hierarchical structure will not work properly, if at all, when the order of the structure (in the sense of tree order) is altered. This paper presents a method in the context of IMS systems to intercept and interpret data base manipulation commands issued by the application program to eliminate the necessity of reprogramming when a hierarchical structure is subject to an order transformation. Algorithms involving command substitution rules for various situations have been derived to insure the correct execution of application programs irrespective of the order of the structure.
ID:118
CLASS:2
Title: The structure of \&ldquo;data structures\&rdquo;
Abstract: A data structure is defined to be a 4-tuple &lt;&Dbarbelow;, &Fbarbelow;, &Sbarbelow;, &Abarbelow;&gt;. &Dbarbelow; and &Fbarbelow; are Domain and Function definitions which define the externally observable behavior; &Sbarbelow; and &Abarbelow; are a Storage Structure and Algorithms which implement the functions. It is shown that this definition helps organize the field of data structures for presentation to students. In particular, the fact that the Storage Structure is a (lower level) data structure leads to a hierarchy of data structures. Following this hierarchy, a suitable order for presentation of the material is bits, words, arrays and records, lists, trees, and search tables. Students deserve structured presentations. In a course on data structures this is especially important because otherwise the subject can seem like a series of unrelated tricks. Moreover, the material is on structure and will be used to build programs with structure, so the student should be given as many examples of good structure as possible. There are a number of questions about how the material on data structures ought to be organized. Is the order arrays, stacks, records, lists appropriate with each subject being motivated by its predecessor? Or should stacks be last so their multiple implementations are derived from the alternate implementations of lists? Where should list node representations be described since one alternative uses lists themselves? This paper answers these questions by referring to the data structure definition presented in Section 1. This definition distinguishes between the externally observable behavior of the data structure and its implementation. Since the latter must be in terms of lower level data structures, the definition induces on data structures the hierarchy discussed in Section 2. Finally, Section 3 describes various topics that are omitted by this organization and why these include some of the more formal appraoches.
ID:119
CLASS:2
Title: The design of a template structure for a generalized data structure definition facility
Abstract: A template structure capable of defining the runtime configuration of general data structures, e.g. arrays (homogeneous and non-homogeneous), cells, stacks, queues, trees, and general lists (graphs), for a generalized data structure definition facility that has practical utility in applications where thousands of data structures can be in existence at any given time is described. An important aspect of this template structure organization is that like instances of a data structure allocated at runtime share a common template rather than each allocated instance of a data structure having its own individual template. The motivation for sharing templates is derived from the fact that large numbers of data structures can be active at runtime, and templates can occupy a considerable amount of storage (in some instances a template can occupy as much or more storage than the data structure elements themselves). The importance of template sharing in a paging system, and the capability of the template structure for facilitating the design and implementation of general operations such as insert and delete are discussed.
ID:120
CLASS:2
Title: ATLAS: A geographic database system data structure and language design for geographic information
Abstract: The design concepts and languages of a geographic information system ATLAS (Administration and Total Landuse Analysis Support system) are proposed. The database structure is designed based on the geographic information structure concepts which contain semantic structure, topological structure and location structure. For a flexible user interface, the system provides two languages) IGL (Interactive Geographic Language) and GDDL (Geographic Data Definition Language), whose functions are designed based on the geographic information structure concepts. IGL is an interactive end user language, which facilitates information retrieval under geographic and/or statistic conditions and thematic map production. GDDL is a language for a system manager who constructs and maintains the database.
ID:121
CLASS:2
Title: From information requirements to DBTG-data structures
Abstract: The problem of determining, analysis and description of a particular application's information structure (and relations) and the process of mapping the information structure to a &ldquo;good&rdquo; data structure (in this case a DBTG-type structure) is considered. The applicability of a top-down oriented design procedure to a relatively large practical data base design case is demonstrated. A conceptual framework and a notation to be used for determining and definition of information requirements, information structure and information relations is suggested. A systematic and partly formalized approach to map an information structure to a DBTG-type data structure is discussed. The problem of analysis and evaluation of alternative DBTG-type structures is also considered.
ID:122
CLASS:2
Title: Selection of representations for data structures
Abstract: The process of selecting representations for data structures is considered. The model of the selection process we suggest is centered around a base of known abstract data structures and their representations. The abstract data structure for which a representation is required would not necessarily be in the base, but should be a combination of base data structures. After describing this model of selection and its motivation, two aspects of the process are examined in more detail: a) The interaction with the user is treated by defining a language for the natural description of data structure requirements and b) two main types of combinations&mdash;hierarchical and cross-product&mdash;are analyzed, clarifying the relation between representations for component data structures and a representation for the combination.
ID:123
CLASS:2
Title: Compound data structure for computer aided design; a survey
Abstract: The aim of Computer Aided Design is to create in the computer a model of the design problem. For example, an electronic circuit may be being designed; the engineer will use an environment consisting of standard circuit parts, with the laws that govern the operation, and will use this environment, together with the constraints on performance, to build a model which is his proposed solution to the design problem. This model may now be tested against the specification and will generally be modified iteratively until the design goal is achieved, in this case a layout with the required characteristics. It may also be that the design problem is being tackled by a team, in which case several users of the design system may wish to access and transform the model, for instance to display views and projections of it, or check on how it interfaces with a parallel project. The rest of this paper is concerned with the basic requirements of the data structure package, and with a survey of those packages which have been implemented and about which information is available.
ID:124
CLASS:2
Title: Graphical data structures in APL
Abstract: The continuing reduction in the price of graphics terminals has led to greater interest in graphics and computer aided design. This trend has in turn focussed attention on methods of modelling graphical data using programming languages and databases. This paper presents a method of defining, analyzing and transforming graphical data using an extended APL notation. The fundamental extension this notation makes to APL is to allow the items in an APL array to have a user defined data structure. A consequence of this extension is that the items in an array can now represent graphical entities such as circles, lines, points etc. In conventional APL an item in an array must be either a number or a character. In this extended notation an item in an array might be a structure designed to represent a point, or a circle, or any other entity.
ID:125
CLASS:2
Title: Recursive data structures and related control mechanisms in APL
Abstract: A brief description of a recursive data structure extension for APL is given which allows the construction of tree-like data structures with no explicit sharing. The proposal includes new and extended primitive functions for constructing data structures and for selecting substructures and two operators which may be used to apply a function at every, node on the first level or at the leaves of a tree respectively. Examples are drawn from algebraic manipulation, computer graphics and natural language manipulation.
ID:126
CLASS:2
Title: Data structure architectures - a major operational principle
Abstract: Computer architectures may be characterized by their operational principle and their physical structure. The paper defines these two characteristics for the novel concept of data structure architectures (DSAs). The representation and processing of arbitrary data structures in such a DSA is demonstrated by examples. It is shown how the functional requirements of a DSA can be satisfied by the specific information structure and the physical structure of the STARLET architecture introduced in preceding publications.
ID:127
CLASS:2
Title: Active data structures
Abstract: Data structures have traditionally been regarded as passive software objects implemented by procedures and as having only one view, i.e., only a single set of access functions for manipulation. The advent of microprocessor technology suggests that we rethink our previous models of data structures. This leads to a different approach in which data structures are active and provide multiple views to users. Both these ideas imply that data structures will be implemented by processes. This makes it feasible to implement each data structure on its own processor. In this paper, we describe the active data structures/multiple views model and provide two examples of its utility: a file directory system and a graphics application.
ID:128
CLASS:2
Title: Data structures for CAD object description
Abstract: Designing databases allowing the inter-application communication of designs in integrated CAD systems is a very complex task. A way of breaking this complexity consists in clearly separating the logical and physical concerns in database design. A data model well-suited to the logical description of elaborate objects such as those handled in CAD in electronics is presented. It is argued to be more adapted to the CAD needs than the hierarchic, network and relational data models.
ID:129
CLASS:2
Title: The heap/substitution concept - an implementation of functional operations on data structures for a reduction machine
Abstract: At first the concept of reduction by direct substitution is described. The syntax of a subset of Berkling's reduction language is given and then its semantics is defined by a set of substitution rules. An example - a program operating on a data structure - will demonstrate two aspects: firstly, it shows how reduction using substitution works, and secondly, it points out the n2-problem, i.e. the time taken to run the program is proportional to n2, where n is the number of items in the data structure. We discuss the problem and then give a solution, which we call the heap/substitution concept. The solution is an extension of the substitution concept and we get it by adding a restricted use of an environment. All advantages of reduction by substitution remain in effect but the time taken to evaluate programs operating on large data structures is now proportional to n. It is also shown that the heap/substitution concept can be used to implement an efficient implementation of recursive function calls.
ID:130
CLASS:2
Title: Using Jackson diagrams to classify and define data structures
Abstract: A modified set of Jackson diagrams together with a classification scheme is proposed as a means for unifying the study of data structures. The diagrams have proven to be very useful for presenting complex concepts and relationships.
ID:131
CLASS:2
Title: Proximate planar point location
Abstract: A new data structure is presented for planar point location that executes a point location query quickly if it is spatially near the previous query. Given a triangulation T of size n and a sequence of point location queries A=q1, qm, the structure presented executes qi in time O(log d(qi-1,qi)). The distance function, d, that is used is a two dimensional generalization of rank distance that counts the number of triangles in a region from qi-1 to qi. The data structure uses O(n log log n) space.
ID:132
CLASS:2
Title: Declarative definition of group indexed data structures and approximation of their domains
Abstract: We introduce a new high-level programming abstraction which extends the concept of data collection. The new construct, called GBF (for Group Based Data-Field), is based on an algebra of index sets, called a shape, and a functional extension of the array type, the field type. Shape constructions are based on group theory and put the emphasis on the logical neighborhood of the data structure elements. A field is a function from a shape to some set of values. In this study, we focus on regular neigh borhood structures and we show that arrays of any dimensions, cyclic array and trees are special kind of GBF.The recursive definitions of a GBF are then studied and we provide some elements for an implementation and some computability results in the case of recursive definition.
ID:133
CLASS:2
Title: Automatic pool allocation for disjoint data structures
Abstract: This paper presents an analysis technique and a novel program transformation that can enable powerful optimizations for entire linked data structures. The fully automatic transformation converts ordinary programs to use pool (aka region) allocation for heap-based data structures. The transformation relies on an efficient link-time interprocedural analysis to identify disjoint data structures in the program, to check whether these data structures are accessed in a type-safe manner, and to construct a Disjoint Data Structure Graph that describes the connectivity pattern within such structures. We present preliminary experimental results showing that the data structure analysis and pool allocation are effective for a set of pointer intensive programs in the Olden benchmark suite. To illustrate the optimizations that can be enabled by these techniques, we describe a novel pointer compression transformation and briefly discuss several other optimization possibilities for linked data structures.
ID:134
CLASS:2
Title: UnQL: a query language and algebra for semistructured data based on structural recursion
Abstract: This paper presents structural recursion as the basis of the syntax and semantics of query languages for semistructured data and XML. We describe a simple and powerful query language based on pattern matching and show that it can be expressed using structural recursion, which is introduced as a top-down, recursive function, similar to the way XSL is defined on XML trees. On cyclic data, structural recursion can be defined in two equivalent ways: as a recursive function which evaluates the data top-down and remembers all its calls to avoid infinite loops, or as a bulk evaluation which processes the entire data in parallel using only traditional relational algebra operators. The latter makes it possible for optimization techniques in relational queries to be applied to structural recursion. We show that the composition of two structural recursion queries can be expressed as a single such query, and this is used as the basis of an optimization method for mediator systems. Several other formal properties are established: structural recursion can be expressed in first-order logic extended with transitive closure; its data complexity is PTIME; and over relational data it is a conservative extension of the relational calculus. The underlying data model is based on value equality, formally defined with bisimulation. Structural recursion is shown to be invariant with respect to value equality.
ID:135
CLASS:2
Title: The LHAM log-structured history data access method
Abstract: Numerous applications such as stock market or medical information systems require that both historical and current data be logically integrated into a temporal database. The underlying access method must support different forms of &ldquo;time-travel&rdquo; queries, the migration of old record versions onto inexpensive archive media, and high insertion and update rates. This paper presents an access method for transaction-time temporal data, called the log-structured history data access method (LHAM) that meets these demands. The basic principle of LHAM is to partition the data into successive components based on the timestamps of the record versions. Components are assigned to different levels of a storage hierarchy, and incoming data is continuously migrated through the hierarchy. The paper discusses the LHAM concepts, including concurrency control and recovery, our full-fledged LHAM implementation, and experimental performance results based on this implementation. A detailed comparison with the TSB-tree, both analytically and based on experiments with real implementations, shows that LHAM is highly superior in terms of insert performance, while query performance is in almost all cases at least as good as for the TSB-tree; in many cases it is much better.
ID:136
CLASS:2
Title: I-structures: data structures for parallel computing
Abstract: It is difficult to achieve elegance, efficiency, and parallelism simultaneously in functional programs that manipulate large data structures. We demonstrate this through careful analysis of program examples using three common functional data-structuring approaches-lists using Cons, arrays using Update (both fine-grained operators), and arrays using make-array (a &ldquo;bulk&rdquo; operator). We then present I-structure as an alternative and show elegant, efficient, and parallel solutions for the program examples in Id, a language with I-structures. The parallelism in Id is made precise by means of an operational semantics for Id as a parallel reduction system. I-structures make the language nonfunctional, but do not lose determinacy. Finally, we show that even in the context of purely functional languages, I-structures are invaluable for implementing functional data abstractions.
ID:137
CLASS:2
Title: Addressing operations for automatic data structure accessing
Abstract: Although present computers often provide excellent operations for data manipulation, data accessing is generally performed by manipulation of addresses with little recognition of the actual data structure of programs. In an attempt to overcome this deficiency, a technique is presented whereby all of the data-accessing algorithms are separated out into separate code streams which run as coroutines in a special address processor. The operations of this address processor are appropriate to the accessing of arrays and data structures. The resultant system is able to handle, in hardware, most of the data structures which are found in present programming languages.
ID:138
CLASS:2
Title: Adapting a data organization to the structure of stored information
Abstract: A data organization for information retrieval (IR) systems is described which uses the structures imposed on the stored information. Trees are used as the main structure of data as information contents are often hierarchically structured (e.g. classifications, thesauri). However, these trees have been expanded to pseudo networks by so-called cross connecting paths. So-called data connecting paths link the information structures and the main data file. Terms occurring in the query formulation may be weighted. These weights are interpreted and then used by both the retrieval and ranking algorithm. One of the paramount problems is how to combine weighted query terms. Since the well-known IR schemes (Boolean retrieval, fuzzy retrieval etc.) do not work in our environment, a specific IR model was developed which allows to deduct suitable query and ranking evaluation algorithms.
ID:139
CLASS:2
Title: Adjacency and incidence framework: a data structure for efficient and fast management of multiresolution meshes
Abstract: This paper introduces a concise and responsiveness data structure, called AIF (Adjacency and Incidence Framework), for multiresolution meshes, as well as a new simplification algorithm based on the planarity of neighboring faces. It is an optimal data structure for polygonal meshes, manifold and non-manifold, which means that a minimal number of direct and indirect accesses are required to retrieve adjacency and incidence information from it. These querying tools are necessary for dynamic multiresolution meshing algorithms (e.g. refinement and simplification operations). AIF is an orientable, but not oriented, data structure, i.e. an orientation can be topologically induced as needed in many computer graphics and geometric modelling applications. On the other hand, the simplification algorithm proposed in this paper is "memoryless" in the sense that only the current approximation counts to compute the next one; no information about the original shape or previous approximations is considered.
ID:140
CLASS:2
Title: Circular incident edge lists: a data structure for rendering complex unstructured grids
Abstract: We present the Circular Incident Edge Lists (CIEL), a new data structure and a high-performance algorithm for generating a series of iso-surfaces in a highly unstructured grid. Slicing-based volume rendering is also considered. The CIEL data structure represents all the combinatorial information of the grid, making it possible to optimize the classical propagation from local minima paradigm. The usual geometric structures are replaced by a more efficient combinatorial structure. An active edges list is maintained, and iteratively propagated from an iso-surface to the next one in a very efficient way. The intersected cells incident to each active edge are retrieved, and the intersection polygons are generated by circulating around their facets. This latter feature enables arbitrary irregular cells to be treated, such as those encountered in certain computational fluid dynamics (CFD) simulations. Since the CIEL data structure solely depends on the connections between the cells, it is possible to take into account dynamic changes in the geometry of the mesh and in property values, which only requires the sorted extrema list to be updated. Experiments have shown that our approach is significantly faster than classical methods. The major drawback of our method is its memory consumption, higher than most classical methods. However, experimental results show that it stays within a practical range.
ID:141
CLASS:2
Title: The 2-3TR-tree, a trajectory-oriented index structure for fully evolving valid-time spatio-temporal datasets
Abstract: Supporting large volumes of multi-dimensional data is an inherent characteristic of modern database applications, such as Geographical Information Systems (GIS), Computer Aided design (CAD), and Image and Multimedia Databases. Such databases need underlying systems with extended features like query languages, data models, and indexing methods, as compared to traditional databases, mainly because of the complexity of representing and retrieving data. The presented work deals with access methods for databases that accurately model the real world. More precisely, the focus is on index structures that can capture the time varying nature of moving objects, namely spatio-temporal structures. A new taxonomy to classify these structures has been defined according to dataset characteristics and query requirements. Then, a new spatio-temporal access method, the 2-3TR-tree, has been designed to process specific datasets and fulfill specific query requirements that no other existing spatio-temporal index could handle.
ID:142
CLASS:2
Title: Analyzing energy friendly steady state phases of dynamic application execution in terms of sparse data structures
Abstract: In the past decades, data structure analysis was mainly done at a high level of abstraction in the computer science community. For instance, choosing a linked list as a data structure as opposed to an array for a specific situation, was mainly motivated from a performance point of view under the implicit assumption that the computer platform (that had to run the software) consisted out of one monolithical, physical memory. In the context of mobile, embedded devices, energy consumption is as important as performance. In addition to this, the assumption of one monolithical memory is outdated for many (if not all) current-day platforms! Clearly, there is a need to improve the choices that are made during data structure analysis given specific knowledge of the memory hierarchy of the platform under investigation. We show how memory related energy consumption can heavily be reduced by taking into account the access behaviour of the application on the one hand and the available on-chip and off-chip memory space on the other hand. We do this by exploiting the sparseness that is present in onesteady state of the data structure under investigation. Analytical results show that energy reductions of a factor of 8.7 are feasible in comparison to common data structure implementations. We trade these gains off with on-chip memory space consumption of a custom memory architecture.
ID:143
CLASS:2
Title: Automatic data structure selection in SETL
Abstract: SETL is a very high level programming language supporting set theoretical syntax and semantics. It allows algorithms to be programmed rapidly and succinctly without requiring data structure declarations to be supplied, though such declarations can be manually specified later, without recoding the program, to improve the efficiency of program execution. We describe a new technique for automatic selection of appropriate data representations during compile-time for undeclared, or partially declared programs,and present an efficient data structure selection algorithm, whose complexity is comparable with those of the fastest known general data-flow algorithms of Tarjan [TA2] and Reif [RE].
ID:144
CLASS:2
Title: Performance of data structures for small sets of strings
Abstract: Fundamental structures such as trees and hash tables are used for managing data in a huge variety of circumstances. Making the right choice of structure is essential to efficiency. In previous work we have explored the performance of a range of data structures---different forms of trees, tries, and hash tables---for the task of managing sets of millions of strings, and have developed new variants of each that are more efficient for this task than previous alternatives. In this paper we test the performance of the same data structures on small sets of strings, in the context of document processing for index construction. Our results show that the new structures, in particular our burst trie, are the most efficient choice for this task, thus demonstrating that they are suitable for managing sets of hundreds to millions of distinct strings, and for input of hundreds to billions of occurrences.
ID:145
CLASS:2
Title: SKA: supporting algorithm and data structure discussion
Abstract: Algorithm animation system design has focused primarily on providing advanced graphical capabilities. However, a fundamental mismatch exists between the needs of instructors and the features of existing algorithm animation systems. This mismatch has reduced the rate of adoption of algorithm animation tools. We describe a system, SKA (Support Kit for Animation), whose design is based on an examination of the tasks performed in the process of discussing algorithms and data structures. SKA attempts to support and enhance time-consuming instructional tasks such as tracing and data structure diagram manipulation, while requiring minimal preparation or authoring time.
ID:146
CLASS:2
Title: Automatic generation of graphic displays of data structures through a preprocessor
Abstract: Recent attention has been given to graphic display routines that allow the programmer to observe the effects of applications programs on various data structures. Much of the work reported in the literature has involved the animation of specific algorithms and has necessitated manual effort by programmers on an application by application basis. Results of initial work in developing a general purpose tool for the display of data structures have already been published. In order to make the tool more widely used and more flexible it is necessary that some type of automatic processing be provided to allow for the generation of the graphic display routines themselves. Initial work in this area and further avenues of research are discussed.
ID:147
CLASS:2
Title: Burst tries: a fast, efficient data structure for string keys
Abstract: Many applications depend on efficient management of large sets of distinct strings in memory. For example, during index construction for text databases a record is held for each distinct word in the text, containing the word itself and information such as counters. We propose a new data structure, the burst trie, that has significant advantages over existing options for such applications: it uses about the same memory as a binary search tree; it is as fast as a trie; and, while not as fast as a hash table, a burst trie maintains the strings in sorted or near-sorted order. In this paper we describe burst tries and explore the parameters that govern their performance. We experimentally determine good choices of parameters, and compare burst tries to other structures used for the same task, with a variety of data sets. These experiments show that the burst trie is particularly effective for the skewed frequency distributions common in text collections, and dramatically outperforms all other data structures for the task of managing strings while maintaining sort order.
ID:148
CLASS:2
Title: Automatic generation of physical data base structures
Abstract: This paper addresses a problem which arises during the design of an integrated data base: this is to generate a set of physical data structures capable of supporting a desired set of logical data structures. A prototype design aid which generates physical data structures for IMS is described. A state diagram is used to represent the constraints imposed by IMS, and a modified depth first tree search is used to find the physical data structures. One can force the solution to either satisfy bounds on one or more objective functions, and/or optimize a single objective function.
ID:149
CLASS:2
Title: File-level operations on network data structures
Abstract: The time and cost of implementing data processing applications can be greatly reduced through the use of software systems which provide language for the expression of file-level operations on data, i.e., operations whose operands are sets of records or entire files, such as report generation and sorting. Such systems, which have been referred to as self-contained systems or generalized file management systems, have characteristically been restricted to logical data structure classes no richer than hierarchies. This paper explores the extension of the concept of file-level operations to network structures, as exemplified by the CODASYL DDLC data structure class, on the assumption that such a facility will prove useful to certain types of users (e.g., data administrators) for certain types of data manipulation (e.g., data base creation and updating). The paper first outlines the general requirements which must be met in such a facility, and then describes a specific approach to the development of a language for such a facility.
ID:150
CLASS:2
Title: A population analysis for hierarchical data structures
Abstract: A new method termed population analysis is presented for approximating the distribution of node occupancies in hierarchical data structures which store a variable number of geometric data items per node. The basic idea is to describe a dynamic data structure as a set of populations which are permitted to transform into one another according to certain rules. The transformation rules are used to obtain a set of equations describing a population distribution which is stable under insertion of additional information into the structure. These equations can then be solved, either analytically or numerically, to obtain the population distribution. Hierarchical data structures are modeled by letting each population represent the nodes of a given occupancy. A detailed analysis of quadtree data structures for storing point data is presented, and the results are compared to experimental data. Two phenomena referred to as aging and phasing are defined and shown to account for the differences between the experimental results and those predicted by the model. The population technique is compared with statistical methods of analyzing similar data structures.
ID:151
CLASS:2
Title: External memory algorithms and data structures: dealing with <b>massive data</b>
Abstract: Data sets in large applications are often too massive to fit completely inside the computers internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this article we survey the state of the art in the design and analysis of external memory (or EM) algorithms and data structures, where the goal is to exploit locality in order to reduce the I/O costs. We consider a variety of EM paradigms for solving batched and online problems efficiently in external memory. For the batched problem of sorting and related problems such as permuting and fast Fourier transform, the key paradigms include distribution and merging. The paradigm of disk striping offers an elegant way to use multiple disks in parallel. For sorting, however,  disk striping can be nonoptimal with respect to I/O, so to gain further improvements we discuss distribution and merging techniques for using the disks independently. We also consider useful techniques for batched EM problems involving matrices (such as matrix multiplication and transposition), geometric data (such as finding intersections and constructing convex hulls), and graphs (such as list ranking, connected components, topological sorting, and shortest paths). In the online domain, canonical EM applications include dictionary lookup and range searching. The two important classes of indexed data structures are based upon extendible hashing and B-trees. The paradigms of filtering and bootstrapping provide a convenient means in online data structures to make effective use of the data accessed from disk. We also reexamine some of the above EM problems in slightly different settings, such as when the data items are moving, when the data items are variable-length (e.g., text strings), or when the allocated amount of internal memory can change dynamically. Programming tools and environments are available for simplifying the EM programming task. During the course of the survey, we report on some experiments in the domain of spatial databases using the TPIE system (transparent parallel I/O programming environment). The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.
ID:152
CLASS:2
Title: Adding range restriction capability to dynamic data structures
Abstract: A database is said to allow range restrictions if one may request that only records with some specified field in a specified range be considered when answering a given query. A transformation is presented that enables range restrictions to be added to an arbitrary dynamic data structure on n elements, provided that the problem satisfies a certain decomposability condition and that one is willing to allow increases by a factor of O(log n) in the worst-case time for an operation and in the space used. This is a generalization of a known transformation that works for static structures. This transformation is then used to produce a data structure for range queries in k dimensions with worst-case times of O(logk n) for each insertion, deletion, or query operation.
ID:153
CLASS:2
Title: An introductory course in data structures with applications
Abstract: This paper describes a two semester introductory course in data (information) structures for the undergraduate computer science student that has evolved at the University of Saskatchewan, Saskatoon. The philosophy and organization of such a course are discussed. A comparison is made between the course described and data structure courses proposed by two commitees 'on curricula.
ID:154
CLASS:2
Title: Anti-presistence: history independent data structures
Abstract: Many data structures give away much more information than they were intended to. Whenever privacy is important, we need to be concerned that it might be possible to infer information from the memory representation of a data structure that is not available through its &ldquo;legitimate&rdquo; interface. Word processors that quietly maintain old versions of a document are merely the most egregious example of a general problem.We deal with data structures whose current memory representation does not reveal their history. We focus on dictionaries, where this means revealing nothing about the order of insertions or deletions. Our first algorithm is a hash table based on open addressing, allowing O(1) insertion and search.  We also present a history independent dynamic  perfect hash table that uses space linear in the number of elements inserted and has expected amortized insertion and deletion time O(1).  To solve the dynamic perfect hashing problem we devise a general scheme for history independent memory allocation. For fixed-size records this is quite efficient, with insertion and deletion both linear in the size of the record.  Our variable-size record scheme is efficient enough for dynamic perfect hashing but not for general use.  The main open problem we leave is whether it is possible to implement a variable-size record scheme with low overhead.
ID:155
CLASS:2
Title: Soft kinetic data structures
Abstract: We introduce the framework of soft kinetic data structures (SKDS). A soft kinetic data structure is an approximate data structure that can be used to answer queries on a set of moving objects with unpredictable motion. We analyze the quality of a soft kinetic data structure by giving a competitive analysis with respect to the dynamics of the system.
ID:156
CLASS:2
Title: Making data structures confluently persistent
Abstract: We address a longstanding open problem of [8, 7], and present a general transformation that takes any data structure and transforms it to a confluently persistent data structure. We model this general problem using the concepts of a version DAG (Directed Acyclic Graph) and an instantiation of a version DAG. We introduce the concept of the effective depth of a vertex in the version DAG and use it to derive information theoretic lower bounds on the space expansion of any such transformation for this DAG. We then give a confluently persistent data structure, such that for any version DAG, the time slowdown and space expansion match the information theoretic lower bounds to within a factor of &Ogr;(log2(&brvbar;V&brvbar;)).
ID:157
CLASS:2
Title: Toward an understanding of data structures
Abstract: This paper presents a notation and formalism for describing the semantics of data structures. This is based on directed graphs with named edges and transformations on these graphs. In addition, and implementation facility is described which could be part of a programming language, which allows a programmer who has expressed the semantics of an algorithm in terms of the graphs to then specify the implementation of some of his data structures in order to gain efficiency.
ID:158
CLASS:2
Title: Structured data structures
Abstract: Programming systems which permit arbitrary linked list structures enable the user to create complicated structures without sufficient protection. Deletions can result in unreachable data elements, and there is no guarantee that additions will be performed properly. To remedy this situation, this paper proposes a Data Structure Description and Manipulation Language which provides for the creation of a restricted class of data structures but ensures the correctness of the program. This is accomplished by an explicit structure declaration facility, a restriction on the permissible operations, and execution-time checks.
ID:159
CLASS:2
Title: A preliminary system for the design of DBTG data structures
Abstract: The functional approach to database design is introduced. In this approach the goal of design is to derive a data structure which is capable of supporting a set of anticipated queries rather than a structure which &ldquo;models the business&rdquo; in some other way. An operational computer program is described which utilizes the functional approach to design data structures conforming to the Data Base Task Group specifications. The automatic programming technology utilized by this program, although typically used to generate procedure, is here used to generate declaratives.
ID:160
CLASS:2
Title: Automatic data structure selection: an example and overview
Abstract: The use of several levels of abstraction has proved to be very helpful in constructing and maintaining programs. When programs are designed with abstract data types such as sets and lists, programmer time can be saved by automating the process of filling in low-level implementation details. In the past, programming systems have provided only a single general purpose implementation for an abstract type. Thus the programs produced using abstract types were often inefficient in space or time. In this paper a system for automatically choosing efficient implementations for abstract types from a library of implementations is discussed. This process is discussed in detail for an example program. General issues in data structure selection are also reviewed.
ID:161
CLASS:2
Title: Visibility aspects of programmed dynamic data structures
Abstract: Unlike static structures, dynamic Pascal-like data structures often suffer visibility problems due to the unrestricted use of the general pointer mechanism. By classifying these structures and identifying the different kinds of pointers, a methodology is proposed for achieving improved visibility.
ID:162
CLASS:2
Title: A Survey of Data Structures for Computer Graphics Systems
Abstract: This is a survey of a data structures and their use in computer graphics systems. First, the reasons for using data structures are given. Then the sequential, random, and list organizations are discussed, and it is shown how they may be used to build complex data structures. Representative samples of languages specifically designed for creating and manipulating data structures are described next. Finally some typical computer graphics systems and their data structures are described. It is also pointed out that much work remains to be done to develop a satisfactory theoretical foundation for designing data structures.
ID:163
CLASS:2
Title: A facility for defining and manipulating generalized data structures
Abstract: A data structure definition facility (DSDF) is described that provides definitions for several primitive data types, homogeneous and heterogeneous arrays, cells, stacks, queues, trees, and general lists. Each nonprimitive data structure consists of two separate entities&mdash;a head and a body. The head contains the entry point(s) to the body of the structure; by treating the head like a cell, the DSDF operations are capable of creating and manipulating very general data structures. A template structure is described that permits data structures to share templates.The primary objectives of the DSDF are: (1) to develop a definition facility that permits the programmer to explicitly define and manipulate generalized data structures in a consistent manner, (2) to detect mistakes and prevent the programmer from creating (either inadvertently or intentionally) undesirable (or illegal) data structures, (3) to provide a syntactic construction mechanism that separates the implementation of a data structure from its use in the program in which it is defined, and (4) to facilitate the development of reliable software.
ID:164
CLASS:2
Title: On the efficiency of pairing heaps and related data structures
Abstract: The pairing heap is well regarded as an efficient data structure for implementing priority queue operations. It is included in the GNU C++ library. Strikingly simple in design, the pairing heap data structure nonetheless seems difficult to analyze, belonging to the genre of self-adjusting  data structures. With its design originating as a self-adjusting analogue of the Fibonacci heap, it has been previously conjectured that the pairing heap provides constrant amortized time decrease-key operations, and experimental studies have supported this conjecture. This paper demonstrates, contrary to conjecture, that the pairing heap requires more than constant amortized time to perform decrease-key operations. Moreover, new experimental findings are presented that reveal detectable growth in  the amortized cost of the decrease-key operation.Second, a unifying framework is developed that includes both pairing heaps and Fibonacci heaps. The parameter of interest in this framework is the storage capacity available in the nodes of the data structure for auxiliary balance information fields. In this respect Fibonacci heaps require log log n bits per node when n items are present. This is shown to be asymptotically optimal for data structures that achieve the same asymptotic performance bounds as Fibonacci heaps and fall within this framework.
ID:165
CLASS:2
Title: Extracting semi-structured data through examples
Abstract: In this paper, we describe an innovative approach to extracting semi-structured data from Web sources. The idea is to collect a couple of example objects from the user and to use this information to extract new objects from new pages or texts. To perform the extraction of new objects, we introduce a bottom-up extration strategy and, through experimentation, demonstrate that it works quite effectively with distinct Web sources, even if only a few examples are provided by the user.
ID:166
CLASS:2
Title: Automatically extracting structure and data from business reports
Abstract: A considerable amount of clean semistructured data is internally available to companies in the form of business reports. However, business reports are untapped for data mining, data warehousing, and querying because they are not in relational form. Business reports have a regular structure that can be reconstructed. We present algorithms that automatically infer the regular structure underlying business reports and automatically generate wrappers to extract relational data.
ID:167
CLASS:2
Title: Deriving initial data warehouse structures from the conceptual data models of the underlying operational information systems
Abstract: In recent years the construction of large scale data schemes for operational systems has been the major problem of conceptual data modeling for business needs. Multidimensional data structures used for decision support applications in data warehouses have rather different requirements to data modeling techniques. In case of operational systems the data models are created from application specific requirements. The data models in data warehouses base on the analytical requirements of the users. Furthermore, the development of data warehouse structures implicates the consideration of user-defined information requirements as well as the underlying operational source systems. In this paper we show that the conceptual data models of the underlying operational information systems can support the construction of multidimensional structures. We would like to point out that the special features of the Structured Entity Relationship Model (SERM) are not only useful for the development of big operational systems but can also help with the derivation of data warehouse structures. The SERM is an extension of the conventional Entity Relationship Model (ERM) and the conceptual basis of the data modeling technique used by the SAP Corporation. To illustrate the usefulness of this approach we explain the derivation of the warehouse structures from the conceptual data model of a flight reservation system.
ID:168
CLASS:2
Title: Intererence analysis tools for parallelizing programs with recursive data structures
Abstract: Interference estimation is a useful tool in developing parallel programs and is a key aspect of automatically parallelizing sequential programs. Interference analysis and disambiguation mechanisms for programs with simple data types and arrays have become a standard part of parallelizing and vectorizing compilers. However, efficient and implementable techniques for interference analysis in the presence of dynamic data-structures have yet to be developed. In this paper we study the problem of estimating interference in an imperative language with dynamic data-structures. We focus on developing efficient and implementable methods for regular recursive data-structures. We illustrate the approach by presenting a method for analysing trees and DAGs. In particular, we develop a structural flow-analysis technique that allows us to estimate whether two statements affect disjoint sub-trees of a forest of dynamically-allocated trees and DAGs. The method is based on a regular-expression-like representation of the relationships between accessible nodes in the forest. We have implemented our analysis and have obtained some very promising preliminary results.
ID:169
CLASS:2
Title: Providing activities for students to apply data structures concepts
Abstract: This paper will describe possible types of activities that can be used in a data structures course to give students experience applying the concepts being taught. It is suggested that problems be presented within a real context and in situations where there is more than one reasonable solution. Having students develop possible data structure solutions for a problem, determine appropriate criteria for comparison of the solutions, evaluate the solutions, and select a solution for a particular problem will provide them with valuable experience. In order to successfully do this, students need to have some experience using their analysis and synthesis skills to solve problems involving data structures. Many real life problems require not just one data structure but a combination of several data structures. Students will benefit from designing data structures for both simple and complex problems. They will not only have learned what each data structure is and how to manipulate it, but also when to use each particular data structure.
ID:170
CLASS:2
Title: The string B-tree: a new data structure for string search in external memory and its applications
Abstract: We introduce a new text-indexing data structure, the String B-Tree, that can be seen as a link between some traditional external-memory and string-matching data structures. In a short phrase, it is a combination of B-trees and Patricia tries for internal-node indices that is made more effective by adding extra pointers to speed up search and update operations. Consequently, the String B-Tree overcomes the theoretical limitations of inverted files, B-trees, prefix B-trees, suffix arrays, compacted tries and suffix trees. String B-trees have the same worst-case performance as B-trees but they manage unbounded-length strings and perform much more powerful search operations such as the ones supported by suffix trees. String B-trees are also effective in main memory (RAM  model) because they improve the online suffix tree search on a dynamic set of strings. They also can be successfully applied to database indexing and software duplication.
ID:171
CLASS:2
Title: Testers and visualizers for teaching data structures
Abstract: We present two tools to support the teaching of data structures and algorithms: Visualizers, which provide interactive visualizations of user-written data structures, and Testers, which check the functionality of user-written data structures. We outline a prototype implementation of visualizers and testers for data structures written in Java, and report on classroom use of testers and visualizers in an introductory Data Structures and Algorithms (CS2) course.
ID:172
CLASS:2
Title: Estimating the storage requirements of the rectangular and L-shaped corner stitching data structures
Abstract: This paper proposes a technique for estimating the storage requirements of the Rectangular Corner Stitching (RCS) data structure [Ousterhout 1984] and the  L-shaped Corner Stitching (LCS) data structure [Mehta and Blust 1997] on a given circuit by studying its (the circuit's) geometric properties. This provides a method for estimating the storage requirements of a circuit without having to implement the corner stitching data structure, which is a tedious and time-consuming task. This technique can also be used to estimate the amount of space saved by employing the LCS data structure over the RCS data structure on a given circuit.
ID:173
CLASS:2
Title: The role of lazy evaluation in amortized data structures
Abstract: Traditional techniques for designing and analyzing amortized data structures in an imperative setting are of limited use in a functional setting because they apply only to single-threaded data structures, yet functional data structures can be non-single-threaded. In earlier work, we showed how lazy evaluation supports functional amortized data structures and described a technique (the banker's method) for analyzing such data structures. In this paper, we present a new analysis technique (the physicist's method) and show how one can sometimes derive a worst-case data structure from an amortized data structure by appropriately scheduling the premature execution of delayed components. We use these techniques to develop new implementations of FIFO queues and binomial queues.
ID:174
CLASS:2
Title: The effect of data structures on the logical complexity of programs
Abstract: The logical complexity of a program is a measure of the effort required to understand it. We hypothesize that the logical complexity of a program increases with the increase in the opaqueness of the relationship between the physical data structures used in the program and their corresponding abstract data types. The results of an experiment conducted to investigate this hypothesis are reported. Documentation techniques for making programs easier to understand using complex data structures are discussed. Data structure diagrams, data structure invariants, stepwise transformation of data structures, and formal specification of the mapping between abstract and concrete data structures are illustrated using two nontrivial examples.
ID:175
CLASS:2
Title: Supporting dynamic data structures on distributed-memory machines
Abstract: Compiling for distributed-memory machines has been a very active research area in recent years. Much of this work has concentrated on programs that use arrays as their primary data structures. To date, little work has been done to address the problem of supporting programs that use pointer-based dynamic data structures. The techniques developed for supporting SPMD execution of array-based programs rely on the fact that arrays are statically defined and directly addressable. Recursive data structures do not have these properties, so new techniques must be developed. In this article, we describe an execution model for supporting programs that use pointer-based dynamic data structures. This model uses a simple mechanism for migrating a thread of control based on the layout of heap-allocated data and introduces parallelism using a technique based on futures and lazy task creation. We intend to exploit this execution model using compiler analyses and automatic parallelization techniques. We have implemented a prototype system, which we call Olden, that runs on the Intel iPSC/860 and the Thinking Machines CM-5. We discuss our implementation and report on experiments with five benchmarks.
ID:176
CLASS:2
Title: A general data dependence test for dynamic, pointer-based data structures
Abstract: Optimizing compilers require accurate dependence testing to enable numerous, performance-enhancing transformations. However, data dependence testing is a difficult problem, particularly in the presence of pointers. Though existing approaches work well for pointers to named memory locations (i.e. other variables), they are overly conservative in the case of pointers to unnamed memory locations. The latter occurs in the context of dynamic, pointer-based data structures, used in a variety of applications ranging from system software to computational geometry to N-body and circuit simulations.In this paper we present a new technique for performing more accurate data dependence testing in the presence of dynamic, pointer-based data structures. We will demonstrate its effectiveness by breaking false dependences that existing approaches cannot, and provide results which show that removing these dependences enables significant parallelization of a real application.
ID:177
CLASS:2
Title: Stored data structures on the Manchester dataflow machine
Abstract: Experience with the Manchester Dataflow Machine has highlighted the importance of efficient handling of stored data structures in a practical parallel machine. It has proved necessary to add a special-purpose structure store to the machine, and this paper describes the role of this structure store and the software which uses it. Some key issues in data structure handling for parallel machines are raised.
ID:178
CLASS:2
Title: A logical approach to data structures
Abstract: The Galois project at the University of Texas is building a programming environment that supports the formal development and verification of data structure programs. This programming environment supports features such as pointer manipulation and destructive update that often make formal treatment difficult.
ID:179
CLASS:2
Title: Abstract description of pointer data structures: an approach for improving the analysis and optimization of imperative programs
Abstract: Even though impressive progress has been made in the area of optimizing and parallelizing array-based programs, the application of similar techniques to programs using pointer data structures has remained difficult. Unlike arrays which have a small number of well-defined properties, pointers can be used to implement a wide variety of structures which exhibit a much larger set of properties. The diversity of these structures implies that programs with pointer data structures cannot be effectively analyzed by traditional optimizing and parallelizing compilers.In this paper we present a new approach that leads to the improved analysis and transformation of programs with recursively defined pointer data structures. Our approach is based on a mechanism for the Abstract Description of Data Structures (ADDS). ADDS is a simple extension to existing imperative languages that allows the programmer to explicitly describe the important properties of a large class of data structures. These abstract descriptions may be used by the compiler to achieve more accurate program analysis in the presence of pointers, which in turn enables and improves the application of numerous optimizing and parallelizing transformations. We present ADDS by describing various data structures; we discuss how such descriptions can be used to improve analysis and debugging; and we supply three important transformations enabled by ADDS.
ID:180
CLASS:2
Title: Abstractions for recursive pointer data structures: improving the analysis and transformation of imperative programs
Abstract: Even though impressive progress has been made in the area of optimizing and parallelizing programs with arrays, the application of similar techniques to programs with pointer data structures has remained difficult. In this paper we introduce a new approach that leads to improved analysis and transformation of programs with recursively-defined pointer data structures.We discuss how an abstract data structure description can improve program analysis by presenting an analysis approach that combines an alias analysis technique, path matrix, with information available from an ADDS declaration. Given this improved alias analysis technique, we provide a concrete example of applying a software pipelining transformation to loops involving pointer data structures.
ID:181
CLASS:2
Title: A qualitative comparison study of data structures for large line segment databases
Abstract: A qualitative comparative study is performed of the performance of three popular spatial indexing methods - the R-tree, R+-tree, and the PMR quadtree-in the context of processing spatial queries in large line segment databases. The data is drawn from the TIGER/Line files used by the Bureau of the Census to deal with the road networks in the US. The goal is not to find the best data structure as this is not generally possible. Instead, their comparability is demonstrated and an indication is given as to when and why their performance differs. Tests are conducted with a number of large datasets and performance is tabulated in terms of the complexity of the disk activity in building them, their storage requirements, and the complexity of the disk activity for a number of tasks that include point and window queries, as well as finding the nearest line segment to a given point and an enclosing polygon.
ID:182
CLASS:2
Title: Interpolation for data structures
Abstract: Interpolation based automatic abstraction is a powerful and robust technique for the automated analysis of hardware and software systems. Its use has however been limited to control-dominated applications because of a lack of algorithms for computing interpolants for data structures used in software programs. We present efficient procedures to construct interpolants for the theories of arrays, sets, and multisets using the reduction approach for obtaining decision procedures for complex data structures. The approach taken is that of reducing the theories of such data structures to the theories of equality and linear arithmetic for which efficient interpolating decision procedures exist. This enables interpolation based techniques to be applied to proving properties of programs that manipulate these data structures.
ID:183
CLASS:2
Title: Strongly typed memory areas programming systems-level data structures in a functional language
Abstract: Modern functional languages offer several attractive features to support development of reliable and secure software. However, in our efforts to use Haskell for systems programming tasks-including device driver and operating system construction-we have also encountered some significant gaps in functionality. As a result, we have been forced, either to code some non-trivial components in more traditional but unsafe languages like C or assembler, or else to adopt aspects of the foreign function interface that compromise on strong typing and type safety.In this paper, we describe how we have filled one of these gaps by extending a Haskell-like language with facilities for working directly with low-level, memory-based data structures. Using this extension, we are able to program a wide range of examples, including hardware interfaces, kernel data structures, and operating system APIs. Our design allows us to address concerns about representation, alignment, and placement (in virtual or physical address spaces) that are critical in some systems applications, but clearly beyond the scope of most existing functional languages.Our approach leverages type system features that are wellknown and widely supported in existing Haskell implementations, including kinds, multiple parameter type classes, functional dependencies, and improvement. One interesting feature is the use of a syntactic abbreviation that makes it easy to define and work with functions at the type level.
ID:184
CLASS:2
Title: Programming with heterogeneous structures: manipulating XML data using bondi
Abstract: Manipulating semistructured data, such as XML, does not fit well within conventional programming languages. A typical manipulation requires finding all occurrences of a structure matching a structured search pattern, whose context may be different in different places, and both aspects cause difficulty. If a special-purpose query language is used to manipulate XML, an interface to a more general programming environment is required, and this interface typically creates runtime overhead for type conversion. However, adding XML manipulation to a general-purpose programming language has proven difficult because of problems associated with expressiveness and typing.We show an alternative approach that handles many kinds of patterns within an existing strongly-typed general-purpose programming language called bondi. The key ideas are to express complex search patterns as structures of simple patterns, pass these complex patterns as parameters to generic data-processing functions and traverse heterogeneous data structures by a generalized form of pattern matching. These ideas are made possible by the language's support for pattern calculus, whose typing on structures and patterns enables path and pattern polymorphism. With this approach, adding a new kind of pattern is just a matter of programming, not language design.
ID:185
CLASS:2
Title: A framework for visual data mining of structures
Abstract: Visual data mining has been established to effectively analyze large, complex numerical data sets. Especially, the extraction and visualization of inherent structures such as hierarchies and networks has made a signi ffcant leap forward. However, it is still a challenging task for users to explore explicitly given large structures. In this paper, we approach this task by tightly coupling visualization and graph-theoretical methods. Therefore, we investigate if and how visualization can benefft from common graph-theoretical methods - mainly developed for the investigation of social networks - and vice versa. To accomplish this close integration, we introduce a design of a general framework for visual data mining of complex structures. Especially, this design includes an appropriate processing order of different mining and visualization algorithms and their mining results. Furthermore, we discuss some important implementation details of our framework to ensure fast structure processing. Finally, we examine the applicability of the framework for a large real-world data set.
ID:186
CLASS:2
Title: Power-efficient sensor placement and transmission structure for data gathering under distortion constraints
Abstract: We consider the joint optimization of sensor placement and transmission structure for data gathering, where a given number of nodes need to be placed in a field such that the sensed data can be reconstructed at a sink within specified distortion bounds while minimizing the energy consumed for communication. We assume that the nodes use either joint entropy coding based on explicit communication between sensor nodes, where coding is done when side information is available, or Slepian-Wolf coding where nodes have knowledge of network correlation statistics. We consider both maximum and average distortion bounds. We prove that this optimization is NP-complete since it involves an interplay between the spaces of possible transmission structures given radio reachability limitations, and feasible placements satisfying distortion bounds.We address this problem by first looking at the simplified problem of optimal placement in the one-dimensional case. An analytical solution is derived for the case when there is a simple aggregation scheme, and numerical results are provided for the cases when joint entropy encoding is used. We use the insight from our 1-D analysis to extend our results to the 2-D case and compare it to typical uniform random placement and shortest-path tree. Our algorithm for two-dimensional placement and transmission structure provides two to three fold reduction in total power consumption and between one to two orders of magnitude reduction in bottleneck power consumption. We perform an exhaustive performance analysis of our scheme under varying correlation models and model parameters and demonstrate that the performance improvement is typical over a range of data correlation models and parameters. We also study the impact of performing computationally-efficient data conditioning over a local scope rather than the entire network. Finally, we extend our explicit placement results to a randomized placement scheme and show that such a scheme can be effective when deployment does not permit exact node placement.
ID:187
CLASS:2
Title: Inference and enforcement of data structure consistency specifications
Abstract: Corrupt data structures are an important cause of unacceptable program execution. Data structure repair (which eliminates inconsistencies by updating corrupt data structures to conform to consistency constraints) promises to enable many programs to continue to execute acceptably in the face of otherwise fatal data structure corruption errors. A key issue is obtaining an accurate and comprehensive data structure consistency specification. We present a new technique for obtaining data structure consistency specifications for data structure repair. Instead of requiring the developer to manually generate such specifications, our approach automatically generates candidate data structure consistency properties using the Daikon invariant detection tool. The developer then reviews these properties, potentially rejecting or generalizing overly specific properties to obtain a specification suitable for automatic enforcement via data structure repair. We have implemented this approach and applied it to three sizable benchmark programs: CTAS (an air-traffic control system), BIND (a widely-used Internet name server) and Freeciv (an interactive game). Our results indicate that (1) automatic constraint generation produces constraints that enable programs to execute successfully through data structure consistency errors, (2) compared to manual specification, automatic generation can produce more comprehensive sets of constraints that cover a larger range of data structure consistency properties, and (3) reviewing the properties is relatively straightforward and requires substantially less programmer effort than manual generation, primarily because it reduces the need to examine the program text to understand its operation and extract the relevant consistency constraints. Moreover, when evaluated by a hostile third party "Red Team" contracted to evaluate the effectiveness of the technique, our data structure inference and enforcement tools successfully prevented several otherwise fatal attacks.
ID:188
CLASS:2
Title: Speeding up search in peer-to-peer networks with a multi-way tree structure
Abstract: Peer-to-Peer systems have recently become a popular means to share resources. Effective search is a critical requirement in such systems, and a number of distributed search structures have been proposed in the literature. Most of these structures provide "log time search" capability, where the logarithm is taken base 2. That is, in a system with N nodes, the cost of the search is O(log2N).In database systems, the importance of large fanout index structures has been well recognized. In P2P search too, the cost could be reduced considerably if this logarithm were taken to a larger base. In this paper, we propose a multi-way tree search structure, which reduces the cost of search to O(logmN), where m is the fanout. The penalty paid is a larger update cost, but we show how to keep this penalty to be no worse than linear in m. We experimentally explore this tradeoff between search and update cost as a function of m, and suggest how to find a good trade-off point.The multi-way tree structure we propose, BATON*, is derived from the BATON structure that has recently been suggested. In addition to multi-way fanout, BATON* also adds support for multi-attribute queries to BATON.
ID:189
CLASS:2
Title: Processing queries on tree-structured data efficiently
Abstract: This is a survey of algorithms, complexity results, and general solution techniques for efficiently processing queries on tree-structured data. I focus on query languages that compute nodes or tuples of nodes&#8212;conjunctive queries, first-order queries, datalog, and XPath. I also point out a number of connections among previous results that have not been observed before.
ID:190
CLASS:2
Title: Decomposing memory performance: data structures and phases
Abstract: The memory hierarchy continues to have a substantial effect on application performance. This paper explores the potential of high-level application understanding in improving the performance of modern memory hierarchies, decomposing the often-chaotic address stream of an application into multiple more regular streams. We present two orthogonal methodologies. The first is a system called DTrack that decomposes the dynamic reference stream of a C program by tagging each reference with its global variable or heap call-site name. The second is a technique to determine the correct granularity at which to study the global phase behavior of applications. Applying these twin analysis methods to twelve CSPEC2000 benchmarks, we demonstrate that they reveal data structure interactions that remain obscured with traditional aggregation-based analysis methods. Such a characterization creates a rich profile of an application's memory behavior that highlights the most memory-intensive data structures and program phases, and we illustrate how this profile can lead system and application designers to a deeper understanding of the applications they study.
ID:191
CLASS:2
Title: Glift: Generic, efficient, random-access GPU data structures
Abstract: This article presents Glift, an abstraction and generic template library for defining complex, random-access graphics processor (GPU) data structures. Like modern CPU data structure libraries, Glift enables GPU programmers to separate algorithms from data structure definitions; thereby greatly simplifying algorithmic development and enabling reusable and interchangeable data structures. We characterize a large body of previously published GPU data structures in terms of our abstraction and present several new GPU data structures. The structures, a stack, quadtree, and octree, are explained using simple Glift concepts and implemented using reusable Glift components. We also describe two applications of these structures not previously demonstrated on GPUs: adaptive shadow maps and octree three-dimensional paint. Last, we show that our example Glift data structures perform comparably to handwritten implementations while requiring only a fraction of the programming effort.
ID:192
CLASS:2
Title: Cylinders: a relational data structure
Abstract: A form of list structure is described which permits an efficient representation of relational data structures. The general notions of the PLEX and of ring structures, because of their proven value, have been used as a basis; but by systematically treating the array structure of the PLEX as an implicit form of linkage which is complementary to the explicit links carried by pointers, a new form of linked data structure emerges, which is called CYLINDER. While CYLINDERS are built up from two or more simple rings of pointers, they characteristically exhibit a multiplicity of closed search paths, which are usable in the construction of data representations. Examples of CYLINDER applications are discussed.
ID:193
CLASS:2
Title: Generalized data structures in Madcap VI
Abstract: The data structures proposed for the Madcap VI programming language are described. The declaration, value specification, and referencing of these structures are defined formally and their implementation using "codewords" is discussed. A structure declaration has the form of a directed tree, and a structure itself, since it can contain references to other structures, including itself, has the form of a directed graph. Levels of the tree may be ordered or unordered. Variables of primitive data-type (real, complex, etc.) are naturally considered as empty structures. The possibility for both multi-and fractional-word representation of structures is evident, but, of course the language itself is implementation independent. Thus a &lt;u&gt;field&lt;/u&gt; is equivalent to a sub-structure. The Madcap VI generalized data structures are compared to data structure concepts in PL/1.
ID:194
CLASS:2
Title: Efficient data accessing in the programming language Bliss
Abstract: The specification of data structure in higher-level languages is isolated from the related specifications of data allocation and data type. Structure specification is claimed to be the definition of the accessing (addressing) function for items having the structure. Conventional techniques for data structure isolation in higher-level languages are examined and are found to suffer from a lack of clarity and efficiency.The means by which data structure accessors may be defined in Bliss, the specification of their association with named, allocated storage, and their automatic invocation by reference to the named storage only, are discussed. An example is presented which illustrates their efficient implementation and their utility for separating the activities of data structure programming and algorithmic programming.
ID:195
CLASS:2
Title: Data structures for computer graphics
Abstract: This paper introduces data structures as applied to computer graphics. Design criteria for computer graphics data structures are discussed, followed by a comparison of general-purpose and tailored graphic data structures. A general graphic data structure is introduced as an example of a structure meeting the preceding criteria. The L6 language is then examined as a tool for implementing the above data structure, and is compared to a few other language systems.
ID:196
CLASS:2
Title: Data and storage structures for interactive graphics
Abstract: This is a tutorial paper that shows the relationship between the data structure and the rest of an interactive graphics system. The distinction between data structures and storage structures is emphasized, as is the problem of data structure segmentation. The implementation of typical graphical applications is described, along with analysis of various trade-off decisions. Finally, some high-level data structure specification languages are discussed.
ID:197
CLASS:2
Title: Data structure models for programming languages
Abstract: This paper introduces a class of models (information structure models) for characterizing computations in terms of the data structures to which they give rise during execution, shows how such models can be used to characterize automata, digital computers and programming languages, considers in some detail the data structures generated during the execution of programs in block structure languages, develops a model for a non-block structure language (SNOBOL 4) and indicates how information structure models may be used in the semantic definition and formal characterization of programming languages. Sections 1 and 2 discuss the reasons for studying the relation between data structures and programming languages, section 3 introduces the notion of an information structure model and considers the classification of interpreters, and section 4 shows how automata, computers and programming languages may be characterized as sequential information structure models. Section 5 underlines the importance of introducing cells and references as semantic primitives of computational models. Section 6 develops models of implementation of block structure languages, section 7 considers the limitations of stack structure, and section 8 considers the hardware realization of block structure implementation of the Burroughs B6500. Section 9 develops an information structure model for the non-block structure language SNOBOL 4, while section 10 briefly discusses information structure models of language definition and the use of information structure models in proofs that programs have certain property. A final subsection considers the relative merits of axiomatic definition versus implementation-dependent definition of programming languages.
ID:198
CLASS:2
Title: Transparent pointer compression for linked data structures
Abstract: 64-bit address spaces are increasingly important for modern applications, but they come at a price: pointers use twice as much memory, reducing the effective cache capacity and memory bandwidth of the system (compared to 32-bit address spaces). This paper presents a sophisticated, automatic transformation that shrinks pointers from 64-bits to 32-bits. The approach is "macroscopic," i.e., it operates on an entire logical data structure in the program at a time. It allows an individual data structure instance or even a subset thereof to grow up to 232 bytes in size, and can compress pointers to some data structures but not others. Together, these properties allow efficient usage of a large (64-bit) address space. We also describe (but have not implemented) a dynamic version of the technique that can transparently expand the pointers in an individual data structure if it exceeds the 4GB limit. For a collection of pointer-intensive benchmarks, we show that the transformation reduces peak heap sizes substantially by (20% to 2x) for several of these benchmarks, and improves overall performance significantly in some cases.
ID:199
CLASS:2
Title: Recursive data structure profiling
Abstract: As the processor-memory performance gap increases, so does the need for aggressive data structure optimizations to reduce memory access latencies. Such optimizations require a better understanding of the memory behavior of programs. We propose a profiling technique called Recursive Data Structure Profiling to help better understand the memory access behavior of programs that use recursive data structures (RDS) such as lists, trees, etc. An RDS profile captures the runtime behavior of the individual instances of recursive data structures. RDS profiling differs from other memory profiling techniques in its ability to aggregate information pertaining to an entire data structure instance, rather than merely capturing the behavior of individual loads and stores, thereby giving a more global view of a program's memory accesses.This paper describes a method for collecting RDS profile without requiring any high-level program representation or type information. RDS profiling achieves this with manageable space and time overhead on a mixture of pointer intensive benchmarks from the SPEC, Olden and other benchmark suites. To illustrate the potential of the RDS profile in providing a better understanding of memory accesses, we introduce a metric to quantify the notion of stability of an RDS instance. A stable RDS instance is one that undergoes very few changes to its structure between its initial creation and final destruction, making it an attractive candidate to certain data structure optimizations.
ID:200
CLASS:2
Title: Squeezing succinct data structures into entropy bounds
Abstract: Consider a sequence S of n symbols drawn from an alphabet A = {1, 2,. . .,&sigma;}, stored as a binary string of nlog &sigma; bits. A succinct data structure on S supports a given set of primitive operations on S using just f (n) = o(n log &sigma;) extra bits. We present a technique for transforming succinct data structures (which do not change the binary content of S) into compressed data structures using nHk + f(n) + O(n log &sigma; + log log&sigma; n + k)/ log&sigma; n) bits of space, where Hk &le; log &sigma; is the kth-order empirical entropy of S. When k + log &sigma; = o(log n), we improve the space complexity of the succinct data structure from n log &sigma; + o(n log &sigma;) to n Hk + o(nlog &sigma;) bits by keeping S in compressed format, so that any substring of O(log &sigma; n) symbols in S (i.e. O(log n) bits) can be decoded on the fly in constant time. Thus, the time complexity of the supported operations does not change asymptotically. Namely, if an operation takes t(n) time in the succinct data structure, it requires O(t(n)) time in the resulting compressed data structure. Using this simple approach we improve the space complexity of some of the best known results on succinct data structures We extend our results to handle another definition of entropy.
ID:201
CLASS:3
Title: High level failure analysis for Integrated Modular Avionics
Abstract: Integrated Modular Avionics (IMA) is the term used for common computer network aboard an aircraft. In order to gain full benefit from this technology a strategy is required to allow the separate development and safety analysis of applications and the computing platform. This paper presents the results of high level failure analysis of an IMA computing platform as a separate system and shows how the analysis can be used as part of an overall certification strategy for IMA. For the analysis six high level functions were constructed which described the functionality provided to applications and devices using the IMA platform. Lower level IMA services, such as scheduling and communications, are used to meet one or more of the functions. Deviations in service provision were considered using a number of guide words to suggest possible failure modes. The analysis revealed a number of weaknesses in the design which will require further consideration.
ID:202
CLASS:3
Title: PSE: explaining program failures via postmortem static analysis
Abstract: In this paper, we describe PSE (Postmortem Symbolic Evaluation), a static analysis algorithm that can be used by programmers to diagnose software failures. The algorithm requires minimal information about a failure, namely its kind (e.g. NULL dereference), and its location in the program's source code. It produces a set of execution traces along which the program can be driven to the given failure.
ID:203
CLASS:3
Title: A method and tool support for model-based semi-automated failure modes and effects analysis of engineering designs
Abstract: Limitations in scope but also difficulties with the efficiency and scalability of present algorithms seem to have so far limited the industrial uptake of existing automated FMEA technology. In this paper, we describe a new tool for the automatic synthesis of FMEAs which builds upon our earlier work on fault tree synthesis. The tool constructs FMEAs from engineering diagrams (e.g. developed in Matlab-Simulink) that have been augmented with information about component failures. To generate a system FMEA, the tool first generates a "forest" of interconnected system fault trees by traversing the system model. This "forest" is then mechanically translated into a simple table of direct relationships between component and system failures, effectively a system FMEA. We describe the architecture of the tool and demonstrate its application on a steer-by-wire prototype. We also discuss its performance and show that this approach could lead to efficient ways of generating useful analyses from design representations.
ID:204
CLASS:3
Title: FAME: A Fault-Pattern Based Memory Failure Analysis Framework
Abstract: A memory failure analysis framework is developed-the FailureAnalyzer for MEmories (FAME). The FAME integrates the MemoryError Catch and Analysis (MECA) system and the Memory Defect Diagnostics(MDD) system. The fault-type based diagnostics approachused by MECA can improve the efficiency of the test and diagnosticalgorithms. The fault-pattern based diagnostics approach used byMDD further improves the defect identification capability. The FAMEalso comes with a powerful viewer for inspecting the failure patternsand fault patterns. It provides an easy way to narrow down the potentialcause of failures and identify possible defects more accuratelyduring the memory product development and yield ramp-up stage.An experiment has been done on an industrial case, demonstratingvery accurate results in a much shorter time as compared with theconventional way.
ID:205
CLASS:3
Title: Analysis of link failures in an IP backbone
Abstract: Today's IP backbones are provisioned to provide excellent performance in terms of loss, delay and availability. However, performance degradation and service disruption are likely in the case of failure, such as fiber cuts, router crashes, etc. In this paper, we investigate the occurence of failures in Sprint's IP backbone and their potential impact on emerging services such as Voice-over-IP (VoIP). We first examine the frequency and duration of failure events derived from IS-IS routing updates collected from three different points in the Sprint IP backbone. We observe that link failures occur as part of everyday operation, and the majority of them are short-lived (less than 10 minutes). We also discuss various statistics such as the distribution of inter-failure time, distribution of link failure durations, etc. which are essential for constructing a realistic link failure model. Next, we present an analysis of routing and service reconvergence time during a controlled link failure scenario in our backbone. Our results indicate that disruption to packet forwarding after link failures depends not only on routing protocol dynamics, but also on the design of routers' architectures and control planes. Thus our results offer insights into two basic components for defining network-wide availability, which we consider a more appropriate metric for service-level agreements to support emerging applications.
ID:206
CLASS:3
Title: Noise propagation and failure criteria for VLSI designs
Abstract: Noise analysis has become a critical concern in advanced chip designs. Traditional methods suffer from two common issues. First, noise that is propagated through the driver of a net is combined with noise injected by capacitively coupled aggressor nets using linear summation. Since this ignores the non-linear behavior of the driver gate the noise that develops on a net can be significantly underestimated. We therefore propose a new linear model that accurately combines propagated and injected noise on a net and which maintains the efficiency of linear simulation. After the propagated and injected noise are correctly combined on a victim net, it is necessary to determine if the noise can result in a functional failure. This is the second issue that we discuss in this paper. Traditionally, noise failure criteria have been based on unity gain points of the DC or AC transfer curves. However, we will show that for digital designs, these approaches can result in a pessimistic analysis in some cases, while in other cases, they allow circuit operation that is extremely close to regions that are unstable and do not allow sufficient margin for error in the analysis. In this paper, we compare the effectiveness of the discussed noise failure criteria and also present a propagation based method, which is intended to overcome these drawbacks. The proposed methods were implemented in a noise analysis tool and we demonstrate results on industrial circuits.
ID:207
CLASS:3
Title: Analysis of timing failures due to random AC defects in VLSI modules
Abstract: This paper presents an analytical model for projecting the yield loss due to random delay defects for modules or VLSI packages containing multiple semiconductor chips. A module to be analyzed is characterized by distribution of path delays. Statistical analysis is applied to obtain the distribution of delays caused by defects in logic circuits of LSI chips. The model uses these two distributions to calculate the probability that a module contains a path that does not meet the system timing requirements. All inputs to the model can be obtained much earlier than the availability of modules for actual testing. Therefore expected module yield loss due to delay defects can be projected before the modules are actually manufactured.
ID:208
CLASS:3
Title: Software trustability analysis
Abstract: A measure of software dependability called trustability is described. A program p has trustability T if we are at least T confident that p is free of faults. Trustability measurement depends on detectability. The detectability of a method is the probability that it will detect faults, when there are faults present. Detectability research can be used to characterize conditions under which one testing and analysis method is more effective than another. Several detectability results that were only previously described informally, and illustrated by example, are proved. Several new detectability results are also proved. The trustability model characterizes the kind of information that is needed to justify a given level of trustability. When the required information is available, the  trustability approach can be used to determine strategies in which methods are combined for maximum effectiveness. It can be used to determine the minimum amount of resources needed to guarantee a required degree of trustability, and the maximum trustability that is achievable with a given amount of resources. Theorems proving several optimization results are given. Applications of the trustability model are discussed. Methods for the derivation of detectability factors, the relationship between trustability and operational reliability, and the relationship between the software development process and trustability are described.
ID:209
CLASS:3
Title: Failure modelling in software architecture design for safety
Abstract: In mission-critical industries, early feedback on the safety properties of a software system is critical and cost effective. This paper presents a compositional method for failure analysis of a system based on the proposed software architecture. This method is based upon the use of CSP as the failure modelling language and its associated tools as failure analysis. Preliminary findings from the application of this approach are also presented.
ID:210
CLASS:3
Title: Towards integrated safety analysis and design
Abstract: There are currently many problems with the development and assessment of software intensive safety-critical systems. In this paper we describe the problems, and introduce a novel approach to their solution, based around goal-structuring concepts, which we believe will ameliorate some of the difficulties. We discuss the use of modified and new forms of safety assessment notations to provide evidence of safety, and the use of data derived from such notations as a means of providing quantified input into the design assessment process. We then show how the design assessment can be partially automated, and from this develop some ideas on how we might move from analytical to synthetic approaches, using safety criteria and evidence as a fitness function for comparing alternative automatically-generated designs.
ID:211
CLASS:3
Title: Mixture importance sampling and its application to the analysis of SRAM designs in the presence of rare failure events
Abstract: In this paper, we propose a novel methodology for statistical SRAM design and analysis. It relies on an efficient form of importance sampling, mixture importance sampling. The method is comprehensive, computationally efficient and the results are in excellent agreement with those obtained via standard Monte Carlo techniques. All this comes at significant gains in speed and accuracy, with speedup of more than 100X compared to regular Monte Carlo. To the best of our knowledge, this is the first time such a methodology is applied to the analysis of SRAM designs.
ID:212
CLASS:3
Title: Assessing the effect of failure severity, coincident failures and usage-profiles on the reliability of embedded control systems
Abstract: The increasingly ubiquitous use of embedded systems to manage and control our technologically (ever-increasing) complex lives makes us more vulnerable than ever before. Knowing how reliable such systems are is absolutely necessary especially for safety, mission and infrastructure critical applications. This paper presents a structured compositional modeling method for assessing reliability based on characteristic data and stochastic models. We illustrate this using a classic embedded control system (sensor-inputs | processing | actuator-outputs), Anti-lock Braking System (ABS) and empirical data. Special emphasis is laid on modeling extra-functional characteristics of severity of failures, coincident failures and usage-profiles with the goal of developing a modeling strategy that is realistic, generic and extensible. The validation approach compares the results from the two separate models. The results are comparable and indicate the effect of coincident failures, failure severity and usage-profiles is predictable.
ID:213
CLASS:3
Title: A design for failure analysis (DFFA) technique to ensure incorruptible signatures
Abstract: Fast failure analysis is a key enabler in shortening the time between design tape out and product introduction in the market. With faster detection of manufacturability issues, problems associated with parametric variations, model approximations or physical design rules can be fixed faster either at the process control level or at the mask level. Failure analysis can be accelerated with additional hardware support for design-for-testability (DFT) and design-for-failure-analysis (DFFA). In this paper, we will focus on one such DFFA technique deployed in the industry, identify its shortcomings and offer improvements to fix deficiencies.
ID:214
CLASS:3
Title: Safety verification in Murphy using fault tree analysis
Abstract: MURPHY is a language-independent, experimental methodology for building safety-critical, real time software, which will include an integrated tool set. Using Ada as an example, this paper presents a technique for verifying the safety of complex, real-time software using Software Fault Tree Analysis. The templates for Ada are presented along with an example of applying the technique to an Ada program. The tools in the MURPHY tool set to aid in this type of analysis are described.
ID:215
CLASS:3
Title: A model and sensitivity analysis of the quality economics of defect-detection techniques
Abstract: One of the main cost factors in software development is the detection and removal of defects. However, the relationships and influencing factors of the costs and revenues of defect-detection techniques are still not well understood. This paper proposes an analytical, stochastic model of the economics of defect detection and removal to improve this understanding. The model is able to incorporate dynamic as well as static techniques in contrast to most other models of that kind. We especially analyse the model with state-ofthe-art sensitivity analysis methods to (1) identify the most relevant factors for model simplification and (2) prioritise the factors to guide further research and measurements.
ID:216
CLASS:3
Title: A UML profile for dependability analysis of real-time embedded systems
Abstract: In this paper, we aim at giving a contribution toward the definition of a UML profile supporting the dependability analysis of real-time and embedded systems (RTES) that conforms to the upcoming profile named "Modeling and Analysis of Real-Time and Embedded Systems" (MARTE), for which a Request For Proposal has been issued by the Object Management Group (OMG).A set of basic dependability and fault-tolerance concepts need to be included in the profile to support the dependability analysis of RTES. We have exploited the best practices, proposed in the literature, on extending UML with dependability modeling capabilities in order to draw up a check list of requirements to be used as guideline for the definition of a dependability analysis profile. The proposed profile is then applied to the UML design of a case study: a gas turbine control system.
ID:217
CLASS:3
Title: Network survivability performance evaluation:: a quantitative approach with applications in wireless ad-hoc networks
Abstract: Network survivability reflects the ability of a network to continue to function during and after failures. Our purpose in this paper is to propose a quantitative approach to evaluate network survivability. We perceive the network survivability as a composite measure consisting of both network failure duration and failure impact on the network. A wireless ad-hoc network is analyzed as an example, and the excess packet loss due to failures (ELF) is taken as the survivability performance measure. To obtain ELF, we adopt a two phase approach consisting of the steady-state availability analysis and transient performance analysis. Assuming Markovian property for the system, this measure is obtained by solving a set of Markov models. By utilizing other analysis paradigms, our approach in this paper may also be applied to study the survivability performance of more complex systems.
ID:218
CLASS:3
Title: MEASURE+: a measurement-based dependability analysis package
Abstract: Most existing dependability modeling and evaluation tools are designed for building and solving commonly used models with emphasis on solution techniques, not for identifying realistic models from measurements. In this paper, a measurement-based dependability analysis package, MEASURE+, is introduced. Given measured data from real systems in a specified format MEASURE+ can generate appropriate dependability models and measures including Markov and semi-Markov models, k-out-of-n availability models, failure distribution and hazard functions, and correlation parameters. These models and measures obtained from data are valuable for understanding actual error/failure characteristics, identifying system bottlenecks, evaluating dependability for real systems, and verifying assumptions made in analytical models. The paper illustrates MEASURE+ by applying it to the data from a VAXcluster multicomputer system. Models of field failure behavior identified by MEASURE+ indicate that both traditional models assuming failure independence and those few taking correlation into account are not representative of the actual occurrence process of correlated failures.
ID:219
CLASS:3
Title: An empirical analysis and comparison of random testing techniques
Abstract: Testing with randomly generated test inputs, namely Random Testing, is a strategy that has been applied succefully in a lot of cases. Recently, some new adaptive approaches to the random generation of test cases have been proposed. Whereas there are many comparisons of Random Testing with Partition Testing, a systematic comparison of random testing techniques is still missing. This paper presents an empirical analysis and comparison of all random testing techniques from the field of Adaptive Random Testing (ART). The ART algorithms are compared for effectiveness using the mean F-measure, obtained through simulation and mutation analysis, and the P-measure. An interesting connection between the testing effectiveness measures F-measure and P-measure is described. The spatial distribution of test cases is determined to explain the behavior of the methods and identify possible shortcomings. Besides this, both the theoretical asymptotic runtime and the empirical runtime for each method are given.
ID:220
CLASS:3
Title: Pursuing failure: the distribution of program failures in a profile space
Abstract: Observation-based testing calls for analyzing profiles of executions induced by potential test cases, in order to select a subset of executions to be checked for conformance to requirements. A family of techniques for selecting such a subset is evaluated experimentally. These techniques employ automatic cluster analysis to partition executions, and they use various sampling techniques to select executions from clusters. The experimental results support the hypothesis that with appropriate profiling, failures often have unusual profiles that are revealed by cluster analysis. The results also suggest that failures often form small clusters or chains in sparsely-populated areas of the profile space. A form of adaptive sampling called failure-pursuit sampling is proposed for revealing failures in such regions, and this sampling method is evaluated experimentally. The results suggest that failure-pursuit sampling is effective.
ID:221
CLASS:3
Title: An impact analysis method for safety-critical user interface design
Abstract: We describe a method of assessing the implications for human error on user interface design of safety-critical systems. In previous work we have proposed a taxonomy of influencing factors that contribute to error. In this article, components of the taxonomy are combined into a mathematical and causal model for error, represented as a Bayesian Belief Net (BBN). TheBBN quantifies error influences arising from user knowledge, ability, and the task environ-ment, combined with factors describing the complexity of user action and user interface quality. The BBN model predicts probabilities of different types of errorslips and mistakes for each component action of a task involving user-system interaction. We propose an Impact Analysis Method that involves running test scenarios against this causal model of error in order to determine user interactions that are prone to different types of error. Applying the proposed method will enable the designer to determine the combinations of influencing factors and their interactions that are most likely to influence human error. Finally we show how such scenario-based causal analysis can be useful as a means of focusing on relevant guidelines for safe user interface design. The proposed method is demonstrated through a case study of an operator performing a task using the control system for a laser spectrophotometer.We describe a method of assessing the implications for human error on user interface design of safety-critical systems. In previous work we have proposed a taxonomy of influencing factors that contribute to error. In this article, components of the taxonomy are combined into a mathematical and causal model for error, represented as a Bayesian Belief Net (BBN). TheBBN quantifies error influences arising from user knowledge, ability, and the task environ-ment, combined with factors describing the complexity of user action and user interface quality. The BBN model predicts probabilities of different types of errorslip for each component action of a task involving user-system interaction. We propose an Impact Analysis Method that involves running test scenarios against this causal model of error in order to determine user interactions that are prone to different types of error. Applying the proposed method will enable the designer to determine the combinations of influencing factors and their interactions that are most likely to influence human error. Finally we show how such scenario-based causal analysis can be useful as a means of focusing on relevant guidelines for safe user interface design. The proposed method is demonstrated through a casestudy of an operator performing a task using the control system for a laser spectrophotometer.
ID:222
CLASS:3
Title: Automated support for classifying software failure reports
Abstract: This paper proposes automated support for classifying reported software failures in order to facilitate prioritizing them and diagnosing their causes. A classification strategy is presented that involves the use of supervised and unsupervised pattern classification and multivariate visualization. These techniques are applied to profiles of failed executions in order to group together failures with the same or similar causes. The resulting classification is then used to assess the frequency and severity of failures caused by particular defects and to help diagnose those defects. The results of applying the proposed classification strategy to failures of three large subject programs are reported. These results indicate that the strategy can be effective.
ID:223
CLASS:3
Title: Information systems outsourcing: a survey and analysis of the literature
Abstract: In the last fifteen years, academic research on information systems (IS) outsourcing has evolved rapidly. Indeed the field of outsourcing research has grown so fast that there has been scant opportunity for the research community to take a collective breath, and complete a global assessment of research activities to date. This paper seeks to address this need by exploring and synthesizing the academic literature on IS outsourcing. It offers a roadmap of the IS outsourcing literature, highlighting what has been done so far, how the work fits together under a common umbrella, and what the future directions might be.
ID:224
CLASS:3
Title: Sound methods and effective tools for engineering modeling and analysis
Abstract: Modeling and analysis is indispensable in engineering. To be safe and effective, a modeling method requires a language with a validated semantics; feature-rich, easy-to-use, dependable tools; and low engineering costs. Today we lack adequate means to develop such methods. We present a partial solution combining two techniques: formal methods for language design, and package-oriented programming for function and usability at low cost. We have evaluated the approach in an end-to-end experiment. We deployed an existing reliability method to NASA in a package-oriented tool and surveyed engineers to assess its usability. We formally specified, improved, and validated the language. To assess cost, we built a package-based tool for the new language. Our data show that the approach can enable costeffective deployment of sound methods by effective tools.
ID:225
CLASS:3
Title: Finding failure-inducing changes in java programs using change classification
Abstract: Testing and code editing are interleaved activities during program development. When tests fail unexpectedly, the changes that caused the failure(s) are not always easy to find. We explore how change classification can focus programmer attention on failure-inducing changes by automatically labeling changes Red, Yellow, or Green, indicating the likelihood that they have contributed to a test failure. We implemented our change classification tool JUnit/CIA as an ex-tension to the JUnit component within Eclipse, and evaluated its effectiveness in two case studies. Our results indicate that change classification is an effective technique for finding failure-inducing changes.
ID:226
CLASS:3
Title: A diagnostic expert system for analyzing multiple-failure transients in nuclear power plants
Abstract: CATALISP (Computer Aided Transient Analysis coded in Lisp) is a prototype expert system which is the result of a project investigating and implementing event confidence-levels (used by reactor safety experts in reactor transient analysis) in the form of an expert system. Currently, CATALISP is designed to diagnose reactor transients by analyzing simulated sensor and plant thermal hydraulic information from a system simulation. CATALISP uses a knowledge base of existing emergency nuclear plant operating guidelines and detailed thermal-hydraulic calculation results correlated to confidence-levels. CATALISP can diagnose a number of reactor transients (for example, loss-of-coolant accidents, steam-generator-tube ruptures, loss-of-offsite power, etc.). Future work includes the expansion of the knowledge base and improvement of the &ldquo;deep-knowledge&rdquo; qualitative models.
ID:227
CLASS:3
Title: Role allocation and reallocation in multiagent teams: towards a practical analysis
Abstract: Despite the success of the BDI approach to agent teamwork, initial role allocation (i.e. deciding which agents to allocate to key roles in the team) and role reallocation upon failure remain open challenges. What remain missing are analysis techniques to aid human developers in quantitatively comparing different initial role allocations and competing role reallocation algorithms. To remedy this problem, this paper makes three key contributions. First, the paper introduces RMTDP (Role-based Multiagent Team Decision Problem), an extension to MTDP [9], for quantitative evaluations of role allocation and reallocation approaches. Second, the paper illustrates an RMTDP-based methodology for not only comparing two competing algorithms for role reallocation, but also for identifying the types of domains where each algorithm is suboptimal, how much each algorithm can be improved and at what computational cost (complexity). Such algorithmic improvements are identified via a new automated procedure that generates a family of locally optimal policies for comparative evaluations. Third, since there are combinatorially many initial role allocations, evaluating each in RMTDP to identify the best is extremely difficult. Therefore, we introduce methods to exploit task decompositions among subteams to significantly prune the search space of initial role allocations. We present experimental results from two distinct domains.
ID:228
CLASS:3
Title: Analysis of actual fault mechanisms in CMOS logic gates
Abstract: An analysis of failure modes in CMOS logic gates is presented. An example 3-input NAND gate is analyzed in detail and the ramifications of its failure modes are discussed.
ID:229
CLASS:3
Title: Sensitivity analysis of reliability and performability measures for multiprocessor systems
Abstract: Traditional evaluation techniques for multiprocessor systems use Markov chains and Markov reward models to compute measures such as mean time to failure, reliability, performance, and performability. In this paper, we discuss the extension of Markov models to include parametric sensitivity analysis. Using such analysis, we can guide system optimization, identify parts of a system model sensitive to error, and find system reliability and performability bottlenecks.As an example we consider three models of a 16 processor. 16 memory system. A network provides communication between the processors and the memories. Two crossbar-network models and the Omega network are considered. For these models, we examine the sensitivity of the mean time to failure, unreliability, and performability to changes in component failure rates. We use the sensitivities to identify bottlenecks in the three system models.
ID:230
CLASS:3
Title: Phased-mission system analysis using Boolean algebraic methods
Abstract: Most reliability analysis techniques and tools assume that a system is used for a mission consisting of a single phase. However, multiple phases are natural in many missions. The failure rates of components, system configuration, and success criteria may vary from phase to phase. In addition, the duration of a phase may be deterministic or random. Recently, several researchers have addressed the problem of reliability analysis of such systems using a variety of methods. We describe a new technique for phased-mission system reliability analysis based on Boolean algebraic methods. Our technique is computationally efficient and is applicable to a large class of systems for which the failure criterion in each phase can be expressed as a fault tree (or an equivalent representation). Our technique avoids state space explosion that commonly plague Markov chain-based analysis. We develop a phase algebra to account for the effects of variable configurations and success criteria from phase to phase. Our technique yields exact (as opposed to approximate) results. We demonstrate the use of our technique by means of an example and present numerical results to show the effects of mission phases on the system reliability.
ID:231
CLASS:3
Title: System-based risk analysis in healthcare
Abstract: In this paper, we describe the use of Root Cause Analysis and Failure Mode and Effect Analysis in a large University Medical Center in the Netherlands. Both methods were successfully piloted and then implemented into the hospital-wide patient safety program. Systematic risk and hazard analysis can be used in healthcare. The effect on patient safety still has to be proven.
ID:232
CLASS:3
Title: TAOS: Testing with Analysis and Oracle Support
Abstract: Few would question that software testing is a necessary activity for assuring software quality, yet the typical testing process is a human intensive activity and as such, it is unproductive, error-prone, and often inadequately done. Moreover, testing is seldom given a prominent place in software development or maintenance processes, nor is it an integral part of them. Major productivity and quality enhancements can be achieved by automating the testing process through tool development and use and effectively incorporating it with development and maintenance processes.The TAOS toolkit, Testing with Analysis and Oracle Support, provides support for the testing process. It includes tools that automate many tasks in the testing process, including management and persistence of test artifacts and the relationships between those artifacts, test development, test execution, and test measurement. A unique aspect of TAOS is its support for test oracles and their use to verify behavioral correctness of test executions. TAOS also supports structural/dependence coverage, by measuring the adequacy of test criteria coverage, and regression testing, by identifying tests associated or dependent upon modified software artifacts. This is accomplished by integrating the ProDAG toolset, Program Dependence Analysis Graph, with TAOS, which supports the use of program dependence analysis in testing, debugging, and maintenance.This paper describes the TAOS toolkit and its capabilities as well as testing, debugging and maintenance processes based on program dependence analysis. We also describe our experience with the toolkit and discuss our future plans.
ID:233
CLASS:3
Title: Learning from information systems failures by using narrative and ante-narrative methods
Abstract: We see, know and experience information systems development failures in many domains and in many countries. This paper will explore some of the issues related to the study of these failures. Every year, billions of dollars are wasted on failed projects in developed countries. Developing countries can learn from these grim experiences so as not to waste precious resources in repeating similar failures. The paper will emphasise the fact that the study of failures can only take place post-hoc, once a failure has been identified. Preparation is therefore different to normal scientific study where a situation is pre-selected in advance, the precise parameters are identified and decisions are made about the best methods for measuring them accurately and objectively. The literature reveals that researchers and practitioners have been experiencing projects failures for many years. Indeed, acknowledgements of failures go back at least thirty-five years. However, failures are still a prevalent problem. A significant obstacle related to the study of failures is the lack of acknowledged research methods for understanding such complex phenomena. The evidence collected during failure investigations emerges from a variety of sources, perspectives and contexts. Not surprisingly, it often appears to be ambiguous, incoherent and confused. The information collected tends to be rich, messy, contradictory and subjective. Such situations call for a new repertoire of methods to address the unique features of failures. This paper will introduce possible alternative ways of looking at and constructing failure stories. The techniques described below come under the umbrella term &#60;i&#62;forensic analysis&#60;/i&#62;. The insights obtained from forensic analysis can be used for internal learning within organisations as well as externally within the discipline, thereby enabling practitioners and developing countries to benefit from the mistakes of others.
ID:234
CLASS:3
Title: Collecting data for software reliability analysis and modeling
Abstract: This paper studies the collection of appropriate data for software reliability analysis and modeling. Our approach satisfies the data requirements for various reliability models under the constraints imposed by our project environment. These data-model relations and data - environment constraints are characterized to provide a tentative roadmap for data collection. In the process of collecting data for a group of projects in the system testing stage, we encountered various problems and devised solutions and improvement initiatives to deal with them. We summarize our experience in this paper so that similar initiatives in quality improvement under similar environments can be implemented more effectively.
ID:235
CLASS:3
Title: A comparative analysis of groupware application protocols
Abstract: Two of the most difficult problems faced by developers of synchronous groupware applications are the handling of multiple session connections and the maintenance of replicated data. Protocols and algorithms to solve these problems have evolved over the years as developers gained experience and network standards were developed and enriched. This paper analyzes the efficiency of three common application level protocols used for the development of groupware systems. These include central sequencing, distributed operations, and independent-immutable objects. In order to perform the analysis, nine measures of network and processor efficiency were developed. The result of the analysis showed the independent object method to be superior in overall efficiency. Central sequencing is recommended for applications that are not conducive to the use of independent objects.
ID:236
CLASS:3
Title: Graph-theoretic analysis of structured peer-to-peer systems: routing distances and fault resilience
Abstract: This paper examines graph-theoretic properties of existing peer-to-peer networks and proposes a new infrastructure based on optimal-diameter de Bruijn graphs. Since generalized de Bruijn graphs exhibit very short average distances and high resilience to node failure, they are well suited for distributed hash tables (DHTs). Using the example of Chord, CAN, and de Bruijn, we study the routing performance, graph expansion, clustering properties, and bisection width of each graph. Having confirmed that de Bruijn graphs offer the best diameter and highest connectivity among the existing peer-to-peer structures, we offer a very simple incremental building process that preserves optimal properties of de Bruijn graphs under uniform user joins/departures. We call the combined peer-to-peer architecture optimal diameter routing infrastructure.
ID:237
CLASS:3
Title: Analysis and implementation of software rejuvenation in cluster systems
Abstract: Several recent studies have reported the phenomenon of "software aging", one in which the state of a software system degrades with time. This may eventually lead to performance degradation of the software or crash/hang failure or both. "Software rejuvenation" is a pro-active technique aimed to prevent unexpected or unplanned outages due to aging. The basic idea is to stop the running software, clean its internal state and restart it. In this paper, we discuss software rejuvenation as applied to cluster systems. This is both an innovative and an efficient way to improve cluster system availability and productivity. Using Stochastic Reward Nets (SRNs), we model and analyze cluster systems which employ software rejuvenation. For our proposed time-based rejuvenation policy, we determine the optimal rejuvenation interval based on system availability and cost. We also introduce a new rejuvenation policy based on prediction and show that it can dramatically increase system availability and reduce downtime cost. These models are very general and can capture a multitude of cluster system characteristics, failure behavior and performability measures, which we are just beginning to explore. We then briefly describe an implementation of a software rejuvenation system that performs periodic and predictive rejuvenation, and show some empirical data from systems that exhibit aging
ID:238
CLASS:3
Title: Graph-theoretic analysis of structured peer-to-peer systems: routing distances and fault resilience
Abstract: This paper examines graph-theoretic properties of existing peer-to-peer architectures and proposes a new infrastructure based on optimal diameter de Bruijn graphs. Since generalized de Bruijn graphs possess very short average routing distances and high resilience to node failure, they are well suited for structured peer-to-peer networks. Using the example of Chord, CAN, and de Bruijn, we first study routing performance, graph expansion, and clustering properties of each graph. We then examine bisection width, path overlap, and several other properties that affect routing and resilience of peer-to-peer networks. Having confirmed that de Bruijn graphs offer the best diameter and highest connectivity among the existing peer-to-peer structures, we offer a very simple incremental building process that preserves optimal properties of de Bruijn graphs under uniform user joins/departures. We call the combined peer-to-peer architecture ODRI -- Optimal Diameter Routing Infrastructure.
ID:239
CLASS:3
Title: VeriCDF: a new verification methodology for charged device failures
Abstract: A novel tool for full-chip verification is reported for CDM-ESD protection. Until recently, ESD protection has been simulated in device level, leading to the well known limitations on capturing global features such as the power protection circuits and package parasitics. In practice, fatal failures occur due to unexpected discharged paths in multi-power supply chips, which can only be verified by chip-level simulation. Associated with the new concept of macromodelling, hierarchical approach provides effective analysis methodology for mixed-signal chips. The hierarchical approach provides the analysis of chip-level discharging paths and reliability of gate oxide. Simulation results on a CMOS ASIC chip processed in a 0.25-&mgr; technology are in accordance with the measurement data. Scanning electron microscope locates a gate oxide fault as our analysis predicted.
ID:240
CLASS:3
Title: False-noise analysis using logic implications
Abstract: Cross-coupled noise analysis has become a critical concern in today's VLSI designs. Typically, noise analysis makes an assumption that all aggressing nets can simultaneously switch in the same direction. This creates a worst-case noise pulse on the victim net that often leads to false noise violations. In this paper, we present a new approach that uses logic implications to identify the maximum set of aggressor nets that can inject noise simultaneously under the logic constraints of the circuit. We propose an approach to efficiently generate logic implications from a transistor-level description and propagate them in the circuit using ROBDD representations and a newly proposed laterial propagation method. We then show that the problem of finding the worst case logically feasible noise can be represented as a maximum weighted independent set problem and show how to efficiently solve it. Initially, we restrict our discussion to zero-delay implications, which are valid for glitch-free circuits and then extend our approach to timed implications. The proposed approaches were implemented in an industrial noise analysis tool and results are shown for a number of industrial test cases. We demonstrate that a significant reduction in the number of noise failures can be obtained from considering the logic implications as proposed in this paper, underscoring the need for false-noise analysis.
ID:241
CLASS:3
Title: Knowledge and common knowledge in a byzantine environment I: crash failures
Abstract: By analyzing the states of knowledge that the processors attain in an unreliable system of a simple type, we capture some of the basic underlying structure of such systems. The analysis provides us with a better understanding of existing protocols for problems such as Byzantine agreement, generalizes them considerably, and facilitates the design of improved protocols for many related problems.
ID:242
CLASS:3
Title: Forensic analysis of database tampering
Abstract: Mechanisms now exist that detect tampering of a database, through the use of cryptographically-strong hash functions. This paper addresses the next problem, that of determining who, when, and what, by providing a systematic means of performing forensic analysis after such tampering has been uncovered. We introduce a schematic representation termed a "corruption diagram" that aids in intrusion investigation. We use these diagrams to fully analyze the original proposal, that of a linked sequence of hash values. We examine the various kinds of intrusions that are possible, including retroactive, introactive, backdating, and postdating intrusions. We then introduce successively more sophisticated forensic analysis algorithms: the monochromatic, RGB, and polychromatic algorithms, and characterize the "forensic strength" of these algorithms. We show how forensic analysis can efficiently extract a good deal of information concerning a corruption event.
ID:243
CLASS:3
Title: A dynamic Bayesian analysis of the drivers of Internet firm survival
Abstract: We study the impact of a set of industry, firm- and e-commerce-related factors on Internet firm survival. Through the use of one age-based and another calendar time-based Bayesian dynamic model, we are able to examine how the impact of these factors changes over time. Our results are based on data from 115 publicly-traded Internet firms and suggest that Internet firm survival depends on different factors at the different stages in their lifetimes. Early on, an Internet firm's likelihood of survival will be higher when the initial public offerings rate of Internet stocks in the market is high and when the firm has abundant financial capital. As firms grow, their survival is increasingly contingent on their financial capital and size. Our empirical results show that from the beginning of 2001 to the end of 2002, Internet firms experienced increasing pressure on survival from their debts and from labor expenses to pay its senior executives. In addition, Internet firms start to compete based on size in late 2002 as the sector grew. We obtained these findings using new econometric methods from Bayesian statistics.
ID:244
CLASS:3
Title: Understanding consistency maintenance in service discovery architectures during communication failure
Abstract: Current trends suggest future software systems will comprise collections of components that combine and recombine dynamically in reaction to changing conditions. Service-discovery protocols, which enable software components to locate available software services and to adapt to changing system topology, provide one foundation for such dynamic behavior. Emerging discovery protocols specify alternative architectures and behaviors, which motivate a rigorous investigation of the properties underlying their designs. Here, we assess the ability of selected designs for service-discovery protocols to maintain consistency in a distributed system during catastrophic communication failure. We use an architecture description language, called Rapide, to model two different architectures (two-party and three-party) and two different consistency-maintenance mechanisms (polling and notification). We use our models to investigate performance differences among combinations of architecture and consistency-maintenance mechanism as interface-failure rate increases. We measure system performance along three dimensions: (1) update responsiveness (How much latency is required to propagate changes?), (2) update effectiveness (What is the probability that a node receives a change?), and (3) update efficiency (How many messages must be sent to propagate a change throughout the topology?). We use Rapide to understand how failure-recovery strategies contribute to differences in performance. We also recommend improvements to architecture description languages.
ID:245
CLASS:3
Title: Isolating failure-inducing thread schedules
Abstract: Consider a multi-threaded application that occasionally fails due to non-determinism. Using the DEJAVU capture/replay tool, it is possible to record the thread schedule and replay the application in a deterministic way. By systematically narrowing down the difference between a thread schedule that makes the program pass and another schedule that makes the program fail, the Delta Debugging approach can pinpoint the error location automatically---namely, the location(s) where a thread switch causes the program to fail. In a case study, Delta Debugging isolated the failure-inducing schedule difference from 3.8 billion differences in only 50 tests.
ID:246
CLASS:3
Title: Type-based analysis of uncaught exceptions
Abstract: This article presents a program analysis to estimate uncaught exceptions in ML programs. This analysis relies on unification-based type inference in a nonstandard type system, using rows to approximate both the flow of escaping exceptions (a la effect systems) and the flow of result values (a la control-flow analyses). The resulting analysis is efficient and precise; in particular, arguments carried by exceptions are accurately handled.
ID:247
CLASS:3
Title: A syntactic analysis method of long Japanese sentences based on the detection of conjunctive structures
Abstract: This paper presents a syntactic analysis method that first detects conjunctive structures in a sentence by checking parallelism of two series of words and then analyzes the dependency structure of the sentence with the help of the information about the conjunctive structures. Analysis of long sentences is one of the most difficult problems in natural language processing. The main reason for this difficulty is the structural ambiguity that is common for conjunctive structures that appear in long sentences. Human beings can recognize conjunctive structures because of a certain, but sometimes subtle, similarity that exists between conjuncts. Therefore, we have developed an algorithm for calculating a similarity measure between two arbitrary series of words from the left and the right of a conjunction and selecting the two most similar series of words that can reasonably be considered as composing a conjunctive structure. This is realized using a dynamic programming technique. A long sentence can be reduced into a shorter form by recognizing conjunctive structures. Consequently, the total dependency structure of a sentence can be obtained by relatively simple head-dependent rules. A serious problem concerning conjunctive structures, besides the ambiguity of their scopes, is the ellipsis of some of their components. Through our dependency analysis process, we can find the ellipses and recover the omitted components. We report the results of analyzing 150 Japanese sentences to illustrate the effectiveness of this method.
ID:248
CLASS:3
Title: Using SoDIS\&trade; as a risk analysis process: a teaching perspective
Abstract: There are several difficulties we face when showing our students key processes and techniques for software development. In this paper, issues related to teaching students how to manage risks in software projects are profiled. The concepts and process for implementing Software Development Impact Statements (SoDIS) are outlined; with its supporting CASE tool the "SoDIS Project Auditor" being described. Different ways of applying the SoDIS process and the CASE tool are demonstrated, through some brief illustrative case studies. The paper suggests ways of using the process and the tool to enhance teaching in computing courses including software development projects, software engineering, project management, ethics and professionalism. This work occurs under the umbrella of the SoDIS SEPIA collaborative research programme which aims to promulgate use of the SoDIS process, in both industrial and educational computing spheres.
ID:249
CLASS:3
Title: Simplifying failure-inducing input
Abstract: Given some test case, a program fails.  Which part of the test case  is responsible for the particular failure?  We show how our delta debugging algorithm generalizes and simplifies some  failing input to a minimal test case that produces the  failure.In a case study, the Mozilla web browser crashed after 95 user  actions.  Our prototype implementation automatically simplified the  input to 3 relevant user actions.  Likewise, it simplified 896~lines  of HTML to the single line that caused the failure.  The case study required 139 automated test runs, or 35 minutes on a 500 MHz PC.
ID:250
CLASS:3
Title: Sentiment analysis: capturing favorability using natural language processing
Abstract: This paper illustrates a sentiment analysis approach to extract sentiments associated with polarities of positive or negative for specific subjects from a document, instead of classifying the whole document into positive or negative.The essential issues in sentiment analysis are to identify how sentiments are expressed in texts and whether the expressions indicate positive (favorable) or negative (unfavorable) opinions toward the subject. In order to improve the accuracy of the sentiment analysis, it is important to properly identify the semantic relationships between the sentiment expressions and the subject. By applying semantic analysis with a syntactic parser and sentiment lexicon, our prototype system achieved high precision (75-95%, depending on the data) in finding sentiments within Web pages and news articles.
ID:251
CLASS:3
Title: An Accurate Analysis of the Effects of Soft Errors in the Instruction and Data Caches of a Pipelined Microprocessor
Abstract: Instruction and data caches are well known architectural solutions that allow significantly improving the performance of high-end processors. Due to their sensitivity to soft errors they are often disabled in safety critical applications, thus sacrificing performance for improved dependability. In this paper we report an accurate analysis of the effects of soft errors in the instruction and data caches of a soft core implementing the SPARC architecture. Thanks to an efficient simulation-based fault injection environment we developed, we are able to present in this paper an extensive analysis of the effects of soft errors on a processor running several applications under different memory configurations. The procedure we followed allows the precise computation of the processor failure rate when the cache is enabled even without resorting to expensive radiation experiments.
ID:252
CLASS:3
Title: Static noise analysis with noise windows
Abstract: As processing technology scales down to the nanometer regime, capacitive crosstalk is having an increasingly adverse effect on circuit functionality, leading to increasing number of chip failures. In this paper, we propose mapping the static crosstalk functional noise problem into the well understood static timing problem. The key differences between static noise and static timing analyses, namely the injection of noise, accurate noise window propagation and register sensitive window computation are the contributions of this work. We demonstrate the effectiveness of this approach in two industrial designs by achieving 5X reduction in functional noise failures over noise propagation without considering timing of the composite noise pulse envelope, and 30X reduction in functional noise failures over net based noise failure metrics.
ID:253
CLASS:3
Title: Topological analysis of local-area internetworks
Abstract: It has become common to connect local-area networks together to form high-bandwidth internetworks. The topology of such an internetwork &mdash; how the component networks and gateways are interconnected &mdash; is an important factor in determining the reliability of the internet. We present several techniques for analyzing an internetwork's topology. These techniques are based on a novel mapping of network components onto a bipartite graph.We use these techniques to analyze the topology of the Stanford University Network and the impact of topological changes as it has grown to its current size of 60 networks.
ID:254
CLASS:3
Title: Partition testing, stratified sampling, and cluster analysis
Abstract: We present a new approach to reducing the manual labor required to estimate software reliability. It combines the ideas of partition testing methods with those of stratified sampling to reduce the sample size necessary to estimate reliability with a given degree of precision. Program executions are stratified by using automatic cluster analysis to group those with similar features. We describe the conditions under which stratification is effective for estimating software reliability, and we present preliminary experimental results suggesting that our approach may work well in practice.
ID:255
CLASS:3
Title: Analysis of fault detection coverage of a self-test program
Abstract: A method for determining the percent fault detection coverage of a self-test software program and a method to improve the coverage are presented. These methods are applied to analyze a self-test software program used to check the functional operation of the BDX-910 minicomputer Central Processing Unit (CPU). The results of each analysis are presented. The concepts, tools, assumptions, requirements and constructs of each method are discussed.
ID:256
CLASS:3
Title: Pressurized water reactor [PWR] system simulation and disturbance analysis for anomalous transients and degraded system conditions
Abstract: In this paper potential applications of disturbance analysis to improve availability and safety of light water reactors (LWR's) are discussed. Needs for developing on-line computer aided guidance to the reactor operator during anomalous transients are pointed out. Currently available methods to simulate primary and secondary systems of a pressurized water reactor (PWR) during anomalous transients and other conditions severely degraded from normal operation are reviewed. Limitations of these methods for simulation of operational transients are mentioned. Finally, using one of the existing codes, steam line rupture in a PWR is analyzed.
ID:257
CLASS:3
Title: Statistical analysis of SRAM cell stability
Abstract: The impact of process variation on SRAM yield has become a serious concern in scaled technologies. In this paper, we propose a methodology to analyze the stability of an SRAM cell in the presence of random fluctuations in the device parameters. We provide a theoretical framework for characterizing the DC noise margin of a memory cell and develop models for estimating the cell failure probabilities during read and write operations. The proposed models are verified against extensive Monte-Carlo simulations and are shown to match well over the entire range of the distributions well beyond the 3-sigma extreme.
ID:258
CLASS:3
Title: Hybrid performability analysis of logistic networks
Abstract: Resources in large logistic networks are occasionally unavailable or malfunctioning. This implies that perfomability becomes an issue for quantitative analysis of logistic networks. Different time scales between failures and normal operation often justify the decomposition of a performability model into a single availability model that considers failures and recovery of resources and a family of performance models whose individual instances depend on the state of resources. In this paper, we present an approach that simulates a set of performance models independently and in a distributed manner on a network of workstations. We propose to optimize the achievable quality of results for a given total amount of CPU time by minimizing the confidence intervals for performability measures. This is possible by an adaptive assignment of CPU time to simulate those models whose results have the largest impact on the width of confidence intervals.
ID:259
CLASS:3
Title: A characterization of the simple failure-biasing method for simulations of highly reliable Markovian Systems
Abstract: Simple failure biasing is an importance-sampling technique used to reduce the variance of estimates of performance measures and their gradients in simulations of highly reliable Markovian systems. Although simple failure biasing yields bounded relative error for the performance measure estimate when the system is balanced, it may not provide bounded relative error when the system is unbalanced.In this article, we provide a characterization of when the simple failure-biasing method produces estimators of a performance measure and its derivatives with bounded relative error. We derive a necessary and sufficient condition on the structure of the system for when the performance measure can be estimated with bounded relative error when using simple failure biasing.   Furthermore, a similar condition for the derivative estimators is established. One interesting aspect of the conditions is that it shows that to obtain bounded relative error, not only the most likely paths to system failure must be examined but also some secondary paths leading to failure as well. We also show by example that the necessary and sufficient conditions for a derivative estimator do not imply those for the performance measure estimator; i.e., it is possible to estimate a derivative more efficiently than the performance measure when using simple failure biasing.
ID:260
CLASS:3
Title: Simulation analysis: applications of discrete event simulation modeling to military problems
Abstract: The military is a big user of discrete event simulation models. The use of these models range from training and wargaming their constructive use in important military analyses. In this paper we discuss the uses of military simulation, the issues associated with military simulation to include categorizations of various types of military simulation. We then discuss three particular simulation studies undertaken with the Air Force Institute of Technology's Department of Operational Science focused on important Air Force and Army issues.
ID:261
CLASS:3
Title: The use of human factors and risk analysis in anesthesia in the Netherlands
Abstract: Health care is not as safe as it should be. Near-miss analysis may offer insight into the multiple causes of incidents and the factors that stopped progression towards adverse events. The applicability of the PRISMA (Prevention and Recovery Information System for Monitoring and Analysis) root-cause taxonomy of failure- and recovery factors on near misses in anesthesia was explored. Barriers for reporting were explored subsequently with a questionaire because of the less than expected number of reports. Members of the workinggroup on complication registration of the NVA (Dutch Soci&euml;ty of Anesthesiology) and their associates reported near miss events in anesthesia. PRISMA was applicable to near-miss incidents in anesthesia. Most incidents had a multifactorial origin. Human factors accounted for three-quarters of failure factors and for half of recovery factors. In half the number of incidents only one recovery factor could be identified. In most cases planned human interference stopped progression towards adverse events. In one-quarter of reports the patients themselves absorbed an error unexpectedly. In a subsequent study, a questionaire was given to all participants of the yearly Dutch Anesthesiology Meeting to explore barriers to reporting. There was only a small number of respondents, but these were willing to report to improve quality of care and to learn from errors. They don't see an advantage in reporting near misses. Efforts to improve safety in anesthesia should be directed at removing barriers for reporting, application of human factors knowledge and introducing a near-miss reporting system.
ID:262
CLASS:3
Title: Simple and effective analysis of statically-typed object-oriented programs
Abstract: To use modern hardware effectively, compilers need extensive control-flow information. Unfortunately, the frequent method invocations in object-oriented languages obscure control flow. In this paper, we describe and evaluate a range of analysis techniques to convert method invocations into direct calls for statically-typed object-oriented languages and thus improve control-flow information in object-oriented languages. We present simple algorithms for type hierarchy analysis, aggregate analysis, and interprocedural and intraprocedural type propagation. These algorithms are also fast, O(|procedures| * &amp;sum;pprocedure np * vp) worst case time (linear in practice) for our slowest analysis, where np is the size of procedure p and vp is the number of variables in procedure p, and are thus practical for use in a compiler. When they fail, we introduce cause analysis to reveal the source of imprecision and suggest where more powerful algorithms may be warranted. We show that our simple analyses perform almost as well as an oracle that resolves all method invocations that invoke only a single procedure.
ID:263
CLASS:3
Title: On the complexity of dataflow analysis of logic programs
Abstract: It is widely held that there is a correlation between complexity and precision in dataflow analysis, in the sense that the more precise an analysis algorithm, the more computationally expensive it must be. The details of this relationship, however, appear to not have been explored extensively. This article reports some results on this correlation in the context of logic programs. A formal notion of the &ldquo;precision&rdquo; of an analysis algorithm is proposed, and this is used to characterize the worst-case computational complexity of a number of dataflow analyses with different degrees of precision. While this article considers the analysis of logic programs, the technique proposed, namely the use of &ldquo;exactness sets&rdquo; to study relationships between complexity and precision of analyses, is not specific to logic programming in any way, and is equally applicable to flow analyses of other language families.
ID:264
CLASS:3
Title: Hold time validation on silicon and the relevance of hazards in timing analysis
Abstract: In this paper we motivate the explicit validation of hold-time violations in silicon and propose a method for doing so. New hold-time failure model and test pattern generation methodologies are defined.We outline conditions under which these tests can be applied reliably. We present results of applying these test patterns on a microprocessor and discuss the implications of intermittent failures on the relevance of hazards during timing analysis.
ID:265
CLASS:3
Title: An analysis of software project failure
Abstract: The main aim of this paper is to indicate how various losses may be reduced or avoided when the development of software does not proceed according to its schedule; i.e., if what we call &ldquo;bankruptcy&rdquo; occurs. Data were collected from twenty three projects in various types of applications, the projects together containing a million lines of code. The causes of failure in developing software were obtained by interviewing the managers of the projects under observation. Having analysed these two aspects, this paper points out under what circumstances managers are likely to fail and proposes a method of detecting failures in the software development.
ID:266
CLASS:3
Title: Performance analysis of IEEE 802.15.4 and ZigBee for large-scale wireless sensor network applications
Abstract: This paper analyses the performance of IEEE 802.15.4 Low-Rate Wireless Personal Area Network (LR-WPAN) in a large-scale Wireless Sensor Network (WSN) application. To minimize the energy consumption of the entire network and to allow adequate network coverage, IEEE 802.15.4 peer-to-peer topology is selected, and configured to a beacon-enabled cluster-tree structure. The analysis consists of models for CSMA-CA mechanism and MAC operations specified by IEEE 802.15.4. Network layer operations in a cluster-tree network specified by ZigBee are included in the analysis. For realistic results, power consumption measurements on an IEEE 802.15.4 evaluation board are also included. The performances of a device and a coordinator are analyzed in terms of power consumption and goodput. The results are verified with simulations using WIreless SEnsor NEtwork Simulator (WISENES). The results depict that the minimum device power consumption is as low as 73 &#956;W, when beacon interval is 3.93 s, and data are transmitted at 4 min intervals. Coordinator power consumption and goodput with 15.36 ms CAP duration and 3.93 s beacon interval are around 370 &#956;W and 34 bits/s
ID:267
CLASS:3
Title: Architecture based analysis of performance, reliability and security of software systems
Abstract: With software systems becoming more complex, and handling diverse and critical applications, the need for their thorough evaluation has become ever more important at each phase of software development. With the prevalent use of component-based design, the software architecture as well as the behavior of the individual components of the system needs to be taken into account when evaluating it. In recent past a number of studies have focused on architecture based reliability estimation. But areas such as security and cache behavior still lack such an approach. In this paper we propose an architecture based unified hierarchical model for software reliability, performance, security and cache behavior prediction. We define a metric called the vulnerability index of a software component for quantifying its (in)security. We provide expressions for predicting the overall behavior of the system based on the characteristics of individual components, which also takes into account second order architectural effects for providing an accurate prediction. This approach also facilitates the identification of reliability, performance, security and cache performance bottlenecks. In addition we illustrate how the approach could be applied to software systems by case studies and also provide expressions to perform sensitivity analysis.
ID:268
CLASS:3
Title: A simulation analysis of the vari-metric repairable inventory optimization procedure for the U.S. Coast Guard
Abstract: This paper documents a simulation study undertaken to gain insights into the Vari-Metric multi-echelon repairable inventory optimization method. The method was analyzed in the context of the Coast Guard's fleet of fixed and rotary wing aircraft, for which operational availability is the key performance metric. Failure rates of parts in this system exhibit variance-to-mean ratios higher than one, and we outline a procedure for generating a failure arrival process described by a negative binomial distribution. Previous studies of the Vari-Metric model examined a single repairable part; in this study we analyzed a system comprised of three repairable parts. This allowed us to gain insights into how the Vari-Metric procedure selects part stock levels to attain a desired level of system availability. Further analysis of the simulation model allowed us to examine the efficient frontier for this multi-criteria problem (maximizing spare part availability rates while minimizing the cost of part inventories).
ID:269
CLASS:3
Title: A hazard analysis of human factors in safety-critical systems engineering
Abstract: Safety incident studies often cite human factors as a major cause of accidents. At Bhopal in 1984 human error - the failure to follow safe operating procedures - instigated the deaths of thousands of people from cyanide poisoning. In this case, human factors introduced a common cause fault that disabled four separate safety measures designed to prevent cyanide gas from venting to the atmosphere.From this and other case studies I have taken the view that the competence and motivation of people responsible for the design and operation of safety-critical systems is our first and last line of defence against loss of life and property. The circumstances and influences that cause people to embrace or ignore best practice in safety-critical systems engineering invites detailed analysis.In this paper I assert that lack of competence and safety awareness in developers and operators is a hazard that can have catastrophic consequences. However, by taking a risk management approach we can reduce the severity and frequency of accidents by developing insights into why individuals and organisations might choose to adopt international standards for best practice in safety-related systems engineering, and why they might not.
ID:270
CLASS:3
Title: Dynamic syslog mining for network failure monitoring
Abstract: Syslog monitoring technologies have recently received vast attentions in the areas of network management and network monitoring. They are used to address a wide range of important issues including network failure symptom detection and event correlation discovery. Syslogs are intrinsically dynamic in the sense that they form a time series and that their behavior may change over time. This paper proposes a new methodology of dynamic syslog mining in order to detect failure symptoms with higher confidence and to discover sequential alarm patterns among computer devices. The key ideas of dynamic syslog mining are 1) to represent syslog behavior using a mixture of Hidden Markov Models, 2) to adaptively learn the model using an on-line discounting learning algorithm in combination with dynamic selection of the optimal number of mixture components, and 3) to give anomaly scores using universal test statistics with a dynamically optimized threshold. Using real syslog data we demonstrate the validity of our methodology in the scenarios of failure symptom detection, emerging pattern identification, and correlation discovery.
ID:271
CLASS:3
Title: Survivability analysis of networked systems
Abstract: Survivability is the ability of a system to continue operating despite the presence of abnormal events such as failures and intrusions. Ensuring system survivability has increased in importance as critical infrastructures have become heavily dependent on computers. In this paper we present a systematic method for performing survivability analysis of networked systems. An architect injects failure and intrusion events into a system model and then visualizes the effects of the injected events in the form of scenario graphs. Our method enables further global analyses, such as reliability, latency, and cost-benefit analyses, where mathematical techniques used in different domains are combined in a systematic manner. We illustrate our ideas on an abstract model of the United States Payment System.
ID:272
CLASS:3
Title: Analysis of TCP performance over mobile ad hoc networks
Abstract: Mobile ad hoc networks have attracted attention lately as a means of providing continuous network connectivity to mobile computing devices regardless of physical location. Recent research has focused primarily on the routing protocols needed in such an environment. In this paper, we investigate the effects that link breakage due to mobility has on TCP performance. Through simulation, we show that TCP throughput drops significantly when nodes move, due to TCP's inability to recognize the difference between link failure and congestion. We also analyze specific examples, such as a situation where throughput is zero for a particular connection. We introduce a new metric, expected throughput, for the comparison of throughput in multi-hop networks, and then use this metric to show how the use of explicit link failure notification (ELFN) techniques can significantly improve TCP performance.
ID:273
CLASS:3
Title: From design to analysis models: a kernel language for performance and reliability analysis of component-based systems
Abstract: To facilitate the use of non-functional analysis results in the selection and assembly of components for component-based systems, automatic prediction tools should be devised, to predict some overall quality attribute of the application without requiring extensive knowledge of analysis methodologies to the application designer. To achieve this goal, a key idea is to define a model transformation that takes as input some "design-oriented" model of the component assembly and produces as a result an "analysis-oriented" model that lends itself to the application of some analysis methodology. However, to actually devise such a transformation, we must face both the heterogeneous design level notations for component-based systems, and the variety of non-functional attributes and related analysis methodologies we could be interested in. In this perspective, we define a kernel language whose aim is to capture the relevant information for the analysis of non-functional attributes of component-based systems, with a focus on performance and reliability. Using this kernel language as a bridge between design-oriented and analysis-oriented notations we reduce the burden of defining a variety of direct transformations from the former to the latter to the less complex problem of defining transformations to/from the kernel language. The proposed kernel language is defined within the MOF (Meta-Object Facility) framework, to allow the exploitation of MOF-based model transformation facilities.
ID:274
CLASS:3
Title: Thorough static analysis of device drivers
Abstract: Bugs in kernel-level device drivers cause 85% of the system crashes in the Windows XP operating system [44]. One of the sources of these errors is the complexity of the Windows driver API itself: programmers must master a complex set of rules about how to use the driver API in order to create drivers that are good clients of the kernel. We have built a static analysis engine that finds API usage errors in C programs. The Static Driver Verifier tool (SDV) uses this engine to find kernel API usage errors in a driver. SDV includes models of the OS and the environment of the device driver, and over sixty API usage rules. SDV is intended to be used by driver developers "out of the box." Thus, it has stringent requirements: (1) complete automation with no input from the user; (2) a low rate of false errors. We discuss the techniques used in SDV to meet these requirements, and empirical results from running SDV on over one hundred Windows device drivers.
ID:275
CLASS:3
Title: Rx: treating bugs as allergies---a safe method to survive software failures
Abstract: Many applications demand availability. Unfortunately, software failures greatly reduce system availability. Prior work on surviving software failures suffers from one or more of the following limitations: Required application restructuring, inability to address deterministic software bugs, unsafe speculation on program execution, and long recovery time.This paper proposes an innovative safe technique, called Rx, which can quickly recover programs from many types of software bugs, both deterministic and non-deterministic. Our idea, inspired from allergy treatment in real life, is to rollback the program to a recent checkpoint upon a software failure, and then to re-execute the program in a modified environment. We base this idea on the observation that many bugs are correlated with the execution environment, and therefore can be avoided by removing the "allergen" from the environment. Rx requires few to no modifications to applications and provides programmers with additional feedback for bug diagnosis.We have implemented RX on Linux. Our experiments with four server applications that contain six bugs of various types show that RX can survive all the six software failures and provide transparent fast recovery within 0.017-0.16 seconds, 21-53 times faster than the whole program restart approach for all but one case (CVS). In contrast, the two tested alternatives, a whole program restart approach and a simple rollback and re-execution without environmental changes, cannot successfully recover the three servers (Squid, Apache, and CVS) that contain deterministic bugs, and have only a 40% recovery rate for the server (MySQL) that contains a non-deterministic concurrency bug. Additionally, RX's checkpointing system is lightweight, imposing small time and space overheads.
ID:276
CLASS:3
Title: Queueing analysis of fault-tolerant computer systems (extended abstract)
Abstract: Queueing models provide a useful tool for predicting the performance of many service systems including computer systems, telecommunication systems, computer/communication networks and flexible manufacturing systems. Traditional queueing models predict system performance under the assumption that all service facilities provide failure-free service. It must, however, be acknowledged that service facilities do experience failures and that they get repaired. In recent years, it has been increasingly recognized that this separation of performance and reliability/availability models is no longer adequate.An exact steady-state queueing analysis of such systems is considered by several authors and is carried out by means of generating functions, supplementary variables, imbedded Markov process and renewal theory, or probabilistic techniques [1,2,7,8]. Another approach is approximate, in which it is assumed that the time to reach the steady-state is much smaller than the times to failures/repairs. Therefore, it is reasonable to associate a performance measure (reward) with each state of the underlying Markov (or semi-Markov) model describing the failure/repair behavior of the system. Each of these performance measures is obtained from the steady-state queueing analysis of the system in the corresponding state [3,5].Earlier we have developed models to derive the distribution of job completion time in a failure-prone environment [3,4]. In these models, we need to consider a possible loss of work due to the occurrence of a failure, i.e., the interrupted job may be resumed or restarted upon service resumption. Note that the job completion time analysis includes the delays due to failures and repairs. The purpose of this paper [9] is to extend our earlier analysis so as to account for the queueing delays. In effect, we consider an exact queueing analysis of fault-tolerant systems in order to obtain the steady-state distribution and the mean of the number of jobs in the system. In particular, we study a system in which jobs arrive in a Poisson fashion and are serviced according to FCFS discipline. The service requirements of the incoming jobs form a sequence of independent and identically distributed random variables. The failure/repair behaviour of the system is modelled by an irreducible continuous-time Markov chain, which is independent of the number of jobs in the system. Let the state-space be {1,2, &hellip;,n}. When the computer system is in state i it delivers service at rate ri &ge; 0. Furthermore, depending on the type of the state, the work done on the job is preserved or lost upon entering that state. The actual time required to complete a job depends in a complex way upon the service requirement of the job and the evolution of the state of the system. Note that even though the service requirements of jobs are independent and identically distributed, the actual times required to complete these jobs are neither independent nor identically distributed, and hence the model cannot be reduced to a standard M/G/1 queue [8]. As loss of work due to failures and interruptions is quite a common phenomenon in fault-tolerant computer systems, the model proposed here is of obvious interest.Using our earlier results on the distribution of job completion time we set up a queueing model and show that it has the block M/G/1 structure. Queueing models with such a structure have been studied by Neuts, Lucantoni and others [6]. We demonstrate the usefulness of our approach by performing the numerical analysis for a system with two processors subject to failures and repairs.
ID:277
CLASS:3
Title: Software safety: why, what, and how
Abstract: Software safety issues become important when computers are used to control real-time, safety-critical processes. This survey attempts to explain why there is a problem, what the problem is, and what is known about how to solve it. Since this is a relatively new software research area, emphasis is placed on delineating the outstanding issues and research topics.
ID:278
CLASS:3
Title: Case analysis on the MIS/USER interface
Abstract: The ultimate test of the MIS/USER interface effectiveness is the company's systems implementation history. The analysis of a major system implementation failure is presented. Two well-known implementation process management models are used as a framework for analysis and to a great extent they seem to explain the reason for the failure. From the analysis, with hindsight, suggestions are made on what could have been done to improve the chances of the project.
ID:279
CLASS:3
Title: Applications of qualitative modeling to knowledge-based risk assessment studies
Abstract: Risk assessment of technological processes (chemical and power plants, electro-mechanical systems) is a complex process that requires enumeration of all possible failure modes, their probability of occurrence, and their consequences. Traditionally such studies have been performed by a committee of expert engineers with diverse backgrounds. This paper discusses the use of qualitative modeling techniques based on deriving behavior from structural descriptions and causal reasoning to aid automating and enhancing the risk analysis process. Hierarchical schemes are used for describing component structure, and system functionality is derived from a set of primitive functions and parameters defined for the domain. The system uses these models to automatically generate fault and event networks for hypothesized fault situations specified by users.
ID:280
CLASS:3
Title: A bandwidth analysis of reliable multicast transport protocols
Abstract: Multicast is an efficient communication technique to save bandwidth for group communication purposes. A number of protocols have been proposed in the past to provide a reliable multicast service. Briefly classified, they can be distinguished into sender-initiated, receiver-initiated and tree-based approaches.
ID:281
CLASS:3
Title: Failure proximity: a fault localization-based approach
Abstract: Recent software systems usually feature an automated failure reporting system, with which a huge number of failing traces are collected every day. In order to prioritize fault diagnosis, failing traces due to the same fault are expected to be grouped together. Previous methods, by hypothesizing that similar failing traces imply the same fault, cluster failing traces based on the literal trace similarity, which we call trace proximity. However, since a fault can be triggered in many ways, failing traces due to the same fault can be quite different. Therefore, previous methods actually group together traces exhibiting similar behaviors, like similar branch coverage, rather than traces due to the same fault. In this paper, we propose a new type of  failure proximity, called R-Proximity, which regards two failing traces as similar if they suggest roughly the same fault location. The fault location each failing case suggests is automatically obtained with Sober, an existing statistical debugging tool. We show that with R-Proximity, failing traces due to the same fault can be grouped together. In addition, we find that R-Proximity is helpful for statistical debugging: It can help developers interpret and utilize the statistical debugging result. We illustrate the usage of R-Proximity with a case study on the grep program and some experiments on the Siemens suite, and the result clearly demonstrates the advantage of R-Proximity over trace proximity.
ID:282
CLASS:3
Title: The notification based approach to implementing failure detectors in distributed systems
Abstract: Failure Detector (FD) is the fundamental component of fault tolerant computer systems. In recent years, many research works have been done on the study of QoS and implementation of FDs for distributed computing environments. Almost all of these works are based on the heartbeat approach (HBFD). In this paper, we propose a general model for implementing FDs which separates the processes to be monitored from the underlying running environment. We identify the potential problems of HBFD approach and propose an alternative approach to implementing FDs, called notification based FD (NTFD). Instead of letting the process periodically send heartbeat messages to show it is still alive, in NTFD, the underlying watchdog mechanism sends failure notification messages only when the failure of a monitored process is detected locally. Compared with HBFD implementation under our model, NTFD is more efficient and scalable, and can guarantee the strong accuracy property. Trade-off of achieving QoS of FD is analyzed and the results show that NTFD has much higher probability to achieve a better balance between completeness and accuracy, yet provides a much lower probability of false report and lower system cost. Based on the analysis, we propose the design of a hybrid FD which combines the advantages of HBFD and NTFD.
ID:283
CLASS:3
Title: Measurement and modeling of computer reliability as affected by system activity
Abstract: This paper demonstrates a practical approach to the study of the failure behavior of computer systems. Particular attention is devoted to the analysis of permanent failures. A number of important techniques, which may have general applicability in both failure and workload analysis, are brought together in this presentation. These include: smeared averaging of the workload data, clustering of like failures, and joint analysis of workload and failures. Approximately 17 percent of all failures affecting the CPU were estimated to be permanent. The manifestation of a permanent failure was found to be strongly correlated with the level and type of workload prior to the failure. Although, in strict terms, the results only relate to the manifestation of permanent failures and not to their occurrence, there are strong indications that permanent failures are both caused and discovered by increased activity. More measurements and experiments are necessary to determine their respective contributions to the measured workload/failure relationship.
ID:284
CLASS:3
Title: Reference identification and reference identification failures
Abstract: The goal of this work is the enrichment of human-machine interactions in a natural language environment. Because a speaker and listener cannot be assured to have the same beliefs, contexts, perceptions, backgrounds, or goals at each point in a conversation, difficulties and mistakes arise when a listener interprets a speaker's utterance. These mistakes can lead to various kinds of misunderstandings between speaker and listener, including reference failures or failure to understand the speaker's intention. We call these misunderstandings miscommunication. Such mistakes can slow, and possibly break down, communication. Our goal is to recognize and isolate such miscommunications and circumvent them. This paper highlights a particular class of miscommunication - reference problems - by describing a case study and techniques for avoiding failures of reference. We want to illustrate a framework less restrictive than earlier ones by allowing a speaker leeway in forming an utterance about a task and in determining the conversational vehicle to deliver it. The paper also promotes a new view for extensional reference.
ID:285
CLASS:3
Title: Framework for Fault Analysis and Test Generation in DRAMs
Abstract: With the increasing complexity of memory behavior, attempts are being made to come up with a methodical approach that employs electrical simulation to tackle the memory test problem. This paper describes a framework of algorithms and tools developed jointly by the Delft University of Technology and Infineon Technologies to systematically generate DRAM tests using Spice simulation. The proposed Spice-based test approach enjoys the advantage of being relatively inexpensive, yet highly accurate in describing the desired memory faulty behavior.
ID:286
CLASS:3
Title: On lifetime-based node failure and stochastic resilience of decentralized peer-to-peer networks
Abstract: To understand how high rates of churn and random departure decisions of end-users affect connectivity of P2P networks, this paper investigates resilience of random graphs to lifetime-based node failure and derives the expected delay before a user is forcefully isolated from the graph and the probability that this occurs within his/her lifetime. Our results indicate that systems with heavy-tailed lifetime distributions are more resilient than those with light-tailed (e.g., exponential) distributions and that for a given average degree, k-regular graphs exhibit the highest resilience. As a practical illustration of our results, each user in a system with n = 100 billion peers, 30-minute average lifetime, and 1-minute node-replacement delay can stay connected to the graph with probability 1 - 1 n using only 9 neighbors. This is in contrast to 37 neighbors required under previous modeling efforts. We finish the paper by showing that many P2P networks are almost surely (i.e., with probability 1-o(1)) connected if they have no isolated nodes and derive a simple model for the probability that a P2P system partitions under churn.
ID:287
CLASS:3
Title: Isolating cause-effect chains from computer programs
Abstract: Consider the execution of a failing program as a sequence of program states. Each state induces the following state, up to the failure. Which variables and values of a program state are relevant for the failure? We show how the Delta Debugging algorithm isolates the relevant variables and values by systematically narrowing the state difference between a passing run and a failing run---by assessing the outcome of altered executions to determine wether a change in the program state makes a difference in the test outcome. Applying Delta Debugging to multiple states of the program automatically reveals the cause-effect chain of the failure---that is, the variables and values that caused the failure.In a case study, our prototype implementation successfully isolated the cause-effect chain for a failure of the GNU C compiler: "Initially, the C program to be compiled contained an addition of 1.0; this caused an addition operator in the intermediate RTL representation; this caused a cycle in the RTL tree---and this caused the compiler to crash."
ID:288
CLASS:3
Title: On optimizing the location update costs in the presence of database failures
Abstract: This paper studies the database failure recovery procedure for cellular phone networks as part of the Electronic Industries Association/Telecommunications Industry Association Interim Standard 41 (EIA/TIA IS-41). Before the location information of the database is recovered, phone calls may be lost. The restoration process can be sped up by having the mobile phones to periodically confirm their existence by radio contact with the cellular network. We show that, under some cost assumptions, periodic update interval should be chosen to be approximately equal to the call interarrival time, with more frequent updates for more unreliable system. We also show that the cost of an optimized system is relatively small and stable, if the system is even moderately reliable. Finally, if the system is at least moderately reliable, the effects of call origination rate and the rate at which Location Areas are crossed, are rather small, assuming that the periodic update interval was chosen as stated above. Thus, in such cases, optimization of the size of the Location Area can be made independent of the optimization of the periodic update process.
ID:289
CLASS:3
Title: Analysis of routing table update activity after resource failure in a distributed computer network
Abstract: In a distributed computer network each node participates in the routing process, making routing decisions based on information about network topology which is stored in tables in the node. To maintain the accuracy of these tables without interrupting the functionality of the network, they must be dynamically updated whenever there is a topology change in the network. The purpose of this paper is to analyze update activity required by a topology information maintenance scheme which has been implemented on the three-node MERIT Computer Network in Michigan. The main theorem gives upper and lower bounds on the number of update messages required after failure of a single network resource, either a node or a link.
ID:290
CLASS:3
Title: SEM: enterprise modeling of JSF global sustainment
Abstract: The Joint Strike Fighter (JSF) Program is implementing a paradigm shift to a performance-based logisties environment for force sustainment. This approach produces the necessary levels of performance at a significantly reduced cost of ownership. The resulting logistics environment is multi-national, multi-echelon, and multi-service. The magnitude of the change in the support concept requires an enterprise-level model that can instill customer confidence in unproven alternatives to legacy approaches and capture investment/commitment to enable a profitable execution. The Support Enterprise Model (SEM) was developed by Lockheed Martin to provide a consistent/accurate global view for support of strategic decisions during design/implementation of a JSF global sustainment solution. SEM is a discrete event simulation that allows analysts to define operational/support environment, ascertain measures of effectiveness for performance/cost metrics, and characterize sensitivity to changes in Support System architecture, processes, and business approach as well as air vehicle reliability and maintainability characteristics.
ID:291
CLASS:3
Title: Toward an effective software reliability evaluation
Abstract: Effective software reliability evaluation requires theories of software reliability which define and deal with software reliability quantitatively, technologies for reliability data measurement and data analysis, techniques to estimate or predict software reliability, and practical reliability evaluation methodologies which effectively reflect the characteristics of software. This paper assesses the extents to which these requirements are currently met, and introduces improved approaches for an effective software reliability evaluation. Introduced are the methodologies for software reliability evaluation and the software reliability evaluation-aid tools.
ID:292
CLASS:3
Title: Bimodal multicast
Abstract: There are many methods for making a multicast protocol &ldquo;reliable.&rdquo; At one end of the spectrum, a reliable multicast protocol might offer tomicity guarantees, such as all-or-nothing delivery, delivery ordering, and perhaps additional properties such as virtually synchronous addressing. At the other are protocols that use local repair to overcome transient packet loss in the network, offering &ldquo;best effort&rdquo; reliability. Yet none of this prior work has treated stability of multicast delivery as a basic reliability property, such as might be needed in an internet radio, television, or conferencing application. This article looks at reliability with a new goal: development of a multicast protocol which is reliable in a sense that can be rigorously quantified and  includes throughput stability guarantees. We characterize this new protocol as a &ldquo;bimodal multicast&rdquo; in reference to its reliability model, which corresponds to a family of bimodal probability distributions. Here, we introduce the protocol, provide a theoretical analysis of its behavior, review experimental results, and discuss some candidate applications. These confirm that bimodal multicast is reliable, scalable, and that the protocol provides remarkably stable delivery throughput.
ID:293
CLASS:3
Title: Accelerated reliability analysis for self-healing SONET networks
Abstract: Recently, a parametric State Reward Markov Model (SRMM/p) has been developed for the reliability and availability analysis of self-healing SONET mesh networks [2]. In this paper, we investigate the factors that affect the run-time complexity of the model presented in [2]. In order to accelerate the reliability and availability analysis, we present an approach that aggregates a set of states in the model based on 2-phase hypoexponential distribution. A comparison of the original and the reduced model, with respect to runtime complexity and accuracy, is carried out by applying the models for the analysis of few complex networks.
ID:294
CLASS:3
Title: Static analysis to support the evolution of exception structure in object-oriented systems
Abstract: Exception-handling mechanisms in modern programming languages provide a means to help software developers build robust applications by separating the normal control flow of a program from the control flow of the program under exceptional situations. Separating the exceptional structure from the code associated with normal operations bears some consequences. One consequence is that developers wishing to improve the robustness of a program must figure out which exceptions, if any, can flow to a point in the program. Unfortunately, in large programs, this exceptional control flow can be difficult, if not impossible, to determine.In this article, we present a model that encapsulates the minimal concepts necessary for a developer to determine exception flow for object-oriented languages that define exceptions as objects. Using these concepts, we describe why exception-flow information is needed to build and evolve robust programs. We then describe Jex, a static analysis tool we have developed to provide exception-flow information for Java systems based on this model. The Jex tool provides a view of the actual exception types that might arise at different program points and of the handlers that are present. Use of this tool on a collection of Java library and application source code demonstrates that the approach can be helpful to support both local and global improvements to the exception-handling structure of a system.
ID:295
CLASS:3
Title: Analysis of a composite performance reliability measure for fault-tolerant systems
Abstract: Today's concomitant needs for higher computing power and reliability has increased the relevance of multiple-processor fault-tolerant systems. Multiple functional units improve the raw performance (throughput, response time, etc.) of the system, and, as units fail, the system may continue to function albeit with degraded performance. Such systems and other fault-tolerant systems are not adequately characterized by separate performance and reliability measures. A composite measure for the performance and reliability of a fault-tolerant system observed over a finite mission time is analyzed. A Markov chain model is used for system state-space representation, and transient analysis is performed to obtain closed-form solutions for the density and moments of the composite measure. Only failures that cannot be repaired until the end of the mission are modeled. The time spent in a specific system configuration is assumed to be large enough to permit the use of a hierarchical model and static measures to quantify the performance of the system in individual configurations. For a multiple-processor system, where performance measures are usually associated with and aggregated over many jobs, this is tantamount to assuming that the time to process a job is much smaller than the time between failures. An extension of the results to general acyclic Markov chain models is included.
ID:296
CLASS:3
Title: Player action recognition in broadcast tennis video with applications to semantic analysis of sports game
Abstract: Recognition of player actions in broadcast sports video is a challenging task due to low resolution of the players in video frames. In this paper, we present a novel method to recognize the basic player actions in broadcast tennis video. Different from the existing appearance-based approaches, our method is based on motion analysis and considers the relationship between the movements of different body parts and the regions in the image plane. A novel motion descriptor is proposed and supervised learning is employed to train the action classifier. We also propose a novel framework by combining the player action recognition with other multimodal features for semantic and tactic analysis of the broadcast tennis video. Incorporating action recognition into the framework not only improves the semantic indexing and retrieval performance of the video content, but also conducts highlights ranking and tactics analysis in tennis matches, which is the first solution to our knowledge for tennis game. The experimental results demonstrate that our player action recognition method outperforms existing appearance-based approaches and the multimodal framework is effective for broadcast tennis video analysis.
ID:297
CLASS:3
Title: An evaluation of software test environment architectures
Abstract: Software test environments (STEs) provide a means of automating the test process and integrating testing tools to support required testing capabilities across the test process. Specifically, STEs may support test planning, test management, test measurement, test failure analysis, test development and test execution. The software architecture of an STE describes the allocation of the environment's functions to specific implementation structures. An STE's architecture can facilitate or impede modifications such as changes to processing algorithms, data representation or functionality. Performance and reusability are also subject to architecturally imposed constraints. Evaluation of an STE's architecture can provide insight into modifiability, extensibility, portability and reusability of the STE. This paper proposes a reference architecture for STEs. Its analytical value is demonstrated by using SAAM (Software Architectural Analysis Method) to compare three software test environments: PROTest II (PROLOG Test Environment, Version II), TAOS (Testing with Analysis and Oracle Support), and CITE (CONVEX Integrated Test Environment).
ID:298
CLASS:3
Title: Predicting component failures at design time
Abstract: How do design decisions impact the quality of the resulting software? In an empirical study of 52 ECLIPSE plug-ins, we found that the software design as well as past failure history, can be used to build models which accurately predict failure-prone components in new programs. Our prediction only requires usage relationships between components, which are typically defined in the design phase; thus, designers can easily explore and assess design alternatives in terms of predicted quality. In the ECLIPSE study, 90% of the 5% most failure-prone components, as predicted by our model from design data, turned out to actually produce failures later; a random guess would have predicted only 33%.
ID:299
CLASS:3
Title: A perturbation-aware noise convergence methodology for high frequency microprocessors
Abstract: We present a practical flow that automates the process of analyzing noise failures and determining and implementing the most appropriate design fixes in high performance designs. For each noise problem, the flow implicitly identifies the most sensitive relevant electrical parameter(s) which it then maps to a physical solution that minimizes design perturbation. Integrated with standard physical synthesis, it was used extensively in a high volume 90 nm multi-GHz microprocessor project.
ID:300
CLASS:3
Title: Augmenting descriptive scenario analysis for improvements in human reliability design
Abstract: It is typical for cycles of iteration to be used to refine the current state of the design of a system so that it more closely meets its requirements. Such refinements are in terms of the original requirements specification and any new requirements that have been identified during this process. However, not all defined requirements are equally essential, particularly in high consequence systems where there are issues of dependability. Although descriptive methods for scenario analysis can be used to highlight new requirements, it can be difficult to evaluate the impact of these new requirements.In this paper, we exemplify this problem and investigate how numeric methods can be used to highlight the impact of consequences identified by descriptive scenario analysis. An example from the context of human reliability analysis is presented and dependability issues for system design are considered.
ID:301
CLASS:4
Title: Image-based skin color and texture analysis/synthesis by extracting hemoglobin and melanin information in the skin
Abstract: This paper proposes an E-cosmetic function for digital images based on physics and physiologically-based image processing. A practical skin color and texture analysis/synthesis technique is introduced for this E-cosmetic function. Shading on the face is removed by a simple color vector analysis in the optical density domain as an inverse lighting technique. The image without shading is analyzed by a previously introduced technique that extracts hemoglobin and melanin components by independent component analysis. Experimental results using UV-B irradiation and the application of methyl nicotinate on the arms support the physiological validity of the analysis and the effectiveness of the proposed shading removal. We synthesized the way facial images changed due to tanning or alcohol consumption, and compared the synthesized images with images of actual changes in skin color. The comparison shows an excellent match between the synthesized and actual images of changes due to tanning and alcohol consumption. We also proposed a technique to synthesize the change of texture in pigment due to aging or the application of cosmetics. The pyramid-based texture analysis/synthesis technique was used for the spatial processing of texture. Using the proposed technique, we could realistically change the skin color and texture of a 50 year-old woman to that of a 20 year-old woman.
ID:302
CLASS:4
Title: Contrast-based image attention analysis by using fuzzy growing
Abstract: Visual attention analysis provides an alternative methodology to semantic image understanding in many applications such as adaptive content delivery and region-based image retrieval. In this paper, we propose a feasible and fast approach to attention area detection in images based on contrast analysis. The main contributions are threefold: 1) a new saliency map generation method based on local contrast analysis is proposed; 2) by simulating human perception, a fuzzy growing method is used to extract attended areas or objects from the saliency map; and 3) a practicable framework for image attention analysis is presented, which provides three-level attention analysis, i.e., attended view, attended areas and attended points. This framework facilitates visual analysis tools or vision systems to automatically extract attentions from images in a manner like human perception. User study results indicate that the proposed approach is effective and practicable.
ID:303
CLASS:4
Title: A Parallel Implementation of 4-Dimensional Haralick Texture Analysis for Disk-Resident Image Datasets
Abstract: Texture analysis is one possible method to detect features in biomedical images. During texture analysis, texture related information is found by examining local variations in image brightness. 4-dimensional (4D) Haralick texture analysis is a method that extracts local variations along space and time dimensions and represents them as a collection of fourteen statistical parameters. However, the application of the 4D Haralick method on large time-dependent 2D and 3D image datasets is hindered by computation and memory requirements. This paper presents a parallel implementation of 4D Haralick texture analysis on PC clusters. We present a performance evaluation of our implementation on a cluster of PCs. Our results show that good performance can be achieved for this application via combined use of task- and data-parallelism.
ID:304
CLASS:4
Title: A parallel implementation of collective learning systems theory: Adaptive Learning Image Analysis System (ALIAS)
Abstract: An alternative to preprogrammed rule-based Artificial Intelligence is a hierarchical network of cellular automata which acquire their knowledge through learning based on a series of trial-and-error interactions with an evaluating Environment, much as humans do. The input to the hierarchical network is provided by a set of sensors which perceive the external world. Based upon this perceived information and past experience (memory), the learning automata synthesize collections of trial responses. Periodically the automata estimate the effectiveness of these collections using either internal evaluations (unsupervised learning) or external evaluations from the Environment (supervised learning), modifying their memories accordingly. Known as Collective Learning Systems Theory, this paradigm has been applied to many sophisticated gaming problems, demonstrating robust learning and dynamic adaptivity.Based on a versatile architecture for massively parallel networks of processors for Collective Learning Systems, a Transputer-based parallel-processing image processing engine comprising 32 learning cells and 32 non-learning cells has been applied to a sophisticated image processing task: the scale-invariant and translation-invariant detection of anomalous features in otherwise &ldquo;normal&rdquo; images. In cooperation with Robert Bosch GmbH, this engine is currently being constructed and tested under the direction of the author at the Research Institute for Applied Knowledge Processing (FAW-Ulm) as Project ALIAS: Adaptive Learning Image Analysis System. Initial results indicate excellent detection, discrimination, and localization of anomalies.
ID:305
CLASS:4
Title: Volumetric heart modeling and analysis
Abstract: Heart disease is the leading cause of death in the Western world and consequently the study of normal and pathological heart behavior is an active research area. In particular, the study of the shape and motion of the heart is important because many heart diseases are strongly correlated to these two factors. The human heart is composed of two separate pumps: a right heart that pumps the blood through the lungs and a left heart that pumps the blood through the peripheral organs. In turn, each of these "hearts" is a two-chamber pump composed of an atrium and a ventricle. Special mechanisms in the heart provide cardiac rhythm and transmit action potentials throughout the heart muscle to cause the heart's rhythmic relaxation (diastole) and contraction (systole).
ID:306
CLASS:4
Title: User-directed analysis of scanned images
Abstract: Digital capture (scanning in all its forms, and digital photography/video recording), in providing virtually free temporary memory of captured information, allows users to "over-gather" information during capture, and then to discard unwanted material later. For cameras and video recorders, such editing largely consists of discarding images or frames in their entirety. For scanners (and high-resolution camera/video), such editing benefits from a preview capability that provides quick and reliable user-interface tools for selecting, filtering and saving specific portions of the input. Appropriate preview user interface (UI) tools ease the accessing, editing and dispatch to desired destination (archive, application, webpage, etc.) of captured information (text, tables, drawings, photos, etc.). In this paper, we present several different means for the user-directed "rapid capture" of portions of a scanned image. Specifically, we review past, present and future preview-based UI tools that allow efficient and accurate means of capture to the user. The bases of these tools, as described herein, are user-directed zoning analysis, known as "click and select", which incorporates a bottom-up zoning analysis engine; and statistics-based region classification, which allows rapid reconfiguration of region identification and clustering. We conclude with our view of the future of UI-directed capture.
ID:307
CLASS:4
Title: Robust subspace analysis for detecting visual attention regions in images
Abstract: Detecting visually attentive regions of an image is a challenging but useful issue in many multimedia applications. In this paper, we describe a method to extract visual attentive regions in images using subspace estimation and analysis techniques. The image is represented in a 2D space using polar transformation of its features so that each region in the image lies in a 1D linear subspace. A new subspace estimation algorithm based on Generalized Principal Component Analysis (GPCA) is proposed. The robustness of subspace estimation is improved by using weighted least square approximation where weights are calculated from the distribution of K nearest neighbors to reduce the sensitivity of outliers. Then a new region attention measure is defined to calculate the visual attention of each region by considering both feature contrast and geometric properties of the regions. The method has been shown to be effective through experiments to be able to overcome the scale dependency of other methods. Compared with existing visual attention detection methods, it directly measures the global visual contrast at the region level as opposed to pixel level contrast and can correctly extract the attentive region.
ID:308
CLASS:4
Title: Extraction of text areas in printed document images
Abstract: In this paper, we present a document analysis system which is expected to extract regions of interest in greyscale document images. Collected areas are then clustered in text zones and non-text areas using geometric and texture features. The system works in two steps. Regions of interest are retrieved via cumulative gradient considerations. In classification module, we introduced some entropic heuristic. Experiments are done on the MediaTeam Document Database to show the relevance of this criteria.
ID:309
CLASS:4
Title: MACE: lossless compression and analysis of microarray images
Abstract: The ubiquity of microarray expression data in state-of-the-art biology has been well established. The widespread adoption of this technology coupled with the significant volume of image-based experimental data generated per experiment (averaging 40 MB), have led to significant challenges in storage and query-retrieval of primary data from microarray experiments. Research in the yet nascent area of microarray data-compression seeks to address this problem. In this paper, we propose a conceptually novel approach that achieves significantly better lossless compression. Unlike lossy compression, our algorithm is guaranteed against loss of information that may have potential biological relevance. The proposed method supports key operations such as automated grid and spot finding, histogram-based automatic thresholding for spot segmentation, and subsequent foreground and background separation. Based on the proposed approach, we have developed a standardized format for storing microarray data that encapsulates all the relevant information, including both the Cy3 and Cy5 expression images significantly compressed. We have also developed a software application called MACE (Microarray Compression and Extraction application) to compress-decompress microarray data and generate the aforementioned format. Compression-decompression results on a wide class of microarray experiments involving different spot layouts validate the effectiveness of our approach and its potential to significantly address the aforementioned challenges in storage and management of microarray data.
ID:310
CLASS:4
Title: Road extraction from motion cues in aerial video
Abstract: Aerial video provides strong cues for automatic road extraction that are not available in static aerial images. Using stabilized (or geo-referenced) video data, capturing the distribution of spatio-temporal image derivatives gives a powerful, local representation of the scene variation and motion typical at each pixel. This allows a functional attribution of the scene; a "road" is defined as paths of consistent motion --- a definition which is valid in a large and diverse set of environments. Using a classical relationship between image motion and spatio-temporal image derivatives, road features can be extracted as image regions that have significant image variation and a motion consistent with its neighbors. The video pre-processing to generate image derivative distributions over arbitrarily long sequences is implemented in real time on standard laptops, and the flow field computation and interpretation involves a small number of 3 by 3 matrix operations at each pixel location. Example results are shown for an urban scene with both well-traveled and infrequently traveled roads, indicating that both can be discovered simultaneously. This method works robustly in scenes with significant traffic motion and is thus ideal for urban traffic scenes, which often are difficult to analyze using static imagery.
ID:311
CLASS:4
Title: Data grid for large-scale medical image archive and analysis
Abstract: Storage and retrieval technology for large-scale medical image systems has matured significantly during the past ten years but many implementations still lack cost-effective backup and recovery solutions. As an example, a PACS (Picture Archiving and Communication system) in a general medical center requires about 40 Terabytes of storage capacity for seven years. Despite many healthcare centers are relying on PACS for 24/7 clinical operation, current PACS lacks affordable fault-tolerance storage strategies for archive, backup, and disaster recovery. Existing solutions are difficult to administer, and often time consuming for effective recovery after a disaster. For this reason, PACS still encounters unexpected downtime for hours or days, which could cripple daily clinical service and research operations. Grid Computing represents the latest and most exciting technology to evolve from the familiar realm of parallel, peer-to-peer, and client-server models that can address the problem of fault-tolerant storage for backup and recovery of medical images. We have researched and developed a novel Data Grid testbed involving several federated PAC systems based on grid computing architecture. By integrating grid architecture to the PACS DICOM (Digital Imaging and Communication in Medicine) environment, in addition to use its own storage device, a PACS also uses a federated Data Grid composing of several PAC systems for off-site backup archive. In case its own storage fails, the PACS can retrieve its image data from the Data Grid timely and seamlessly. The design reflects the Globus Toolkit 3.0 five-layer architecture of the grid computing: Fabric, Resource, Connectivity, Collective, and Application Layers. The testbed consists of three federated PAC systems, the Fault-Tolerant PACS archive server at the Image Processing and Informatics Laboratory, the clinical PACS at Saint John's Health Center, and the clinical PACS at the Healthcare Consultation Center II, USC Health Science Campus.In the testbed, we also implement computational services in the Data Grid for image analysis and data mining. The federated PAC systems can use this resource by sharing image data and computational services available in the Data Grid for image analysis and data mining application.In the paper, we first review PACS and its clinical operation, followed by the description of the Data Grid architecture in the testbed. Different scenarios of using the DICOM store and query/retrieve functions of the laboratory model to demonstrate the fault-tolerance features of the Data Grid are illustrated. The status of current clinical implementation of the Data Grid is reported. An example of using the digital hand atlas for bone age assessment of children is presented to describe the concept of computational services in the Data Grid.
ID:312
CLASS:4
Title: Image retrieval using color component analysis
Abstract: This paper intends to propose a way of utilizing color components for image retrieval. Like CLCM, GLCM (Color, Gray Level Co-occurrence Matrix)[11] and Invariant Moment [14] [15] use 2D distribution chart, which use basic statistical techniques in order to interpret 2D data. In order to interpret the spatial relationship and weight of data, this study has used Principal Component Analysis [1][7] that is used in multivariate statistics. In order to increase accuracy of data, it has proposed a way to project color components on 3D space, to rotate it and then, to extract features of data from all angles.
ID:313
CLASS:4
Title: Digital geometry image analysis for medical diagnosis
Abstract: This paper describes a new medical image analysis technique for polygon mesh surfaces of human faces for a medical diagnosis application. The goal is to explore the natural patterns and 3D facial features to provide diagnostic information for Fetal Alcohol Syndrome (FAS). Our approach is based on a digital geometry analysis framework that applies pattern recognition techniques to digital geometry (polygon mesh) data from 3D laser scanners and other sources. Novel 3D geometric features are extracted and analyzed to determine the most discriminatory features that best represent FAS characteristics. As part of the NIH Consortium for FASD, the techniques developed here are being applied and tested on real patient datasets collected by the NIH Consortium both within and outside the US.
ID:314
CLASS:4
Title: Microarray image processing based on clustering and morphological analysis
Abstract: Microarrays allow the monitoring of expressions for tens of thousands of genes simultaneously. Image analysis is an important aspect for microarray experiments that can affect subsequent analysis such as identification of differentially expressed genes. Image processing for microarray images includes three tasks: spot gridding, segmentation and information extraction. In this paper, we address the segmentation and information extraction problems, and proposed a new segmentation method based on K-means clustering and a new background and foreground correction algorithm based on mathematical morphological and histogram analysis for information extraction. The advantage of our method is that it does not have any restrictions for the shape of spots. We compare our experimental results with those obtained from the popular software GenePix.
ID:315
CLASS:4
Title: Learning image similarities and categories from content analysis and relevance feedback
Abstract: In this work, a scheme that learns image similarities and categories from relevance feedback is presented. First, we choose the most suitable features to describe images by content analysis and categorize each image by predicting its semantic meanings. During the retrieval process, users are allowed to confirm semantic classification of the query example and evaluate retrieval results with relevance feedback. By analyzing the feedback information, the system learns both image similarities and semantic meanings. In similarity learning, the retrieving results are refined by modifying the similarity metric. Semantic learning is performed by using the decision tree training algorithm.
ID:316
CLASS:4
Title: The image analysis system with Adaptive Component
Abstract: This paper proposes ACA (Adaptive Component Analysis) as a method for feature extraction and analysis of the content-based image retrieval system. Retrieval measurement is a standard indicating how important the value of a relevant feature is to image retrieval. This process of analysis is called ACA (Adaptive Component Analysis) and the result extracted through ACA becomes an adaptive component for retrieval or statistical/mechanical learning. From the viewpoints of algorithm and system, ACA is a middle stage for content-based image retrieval and it purposes to improve the retrieval speed and performance.
ID:317
CLASS:4
Title: Decomposing image computation for symbolic reachability analysis using control flow information
Abstract: The main challenge in BDD-based symbolic reachability analysis is represented by the sizes of the intermediate decision diagrams obtained during image computations. Methods proposed to mitigate this problem fall broadly into two categories: Search strategies that depart from breadth-first search, and efficient techniques for image computation. In this paper we present an algorithm that belongs to the latter category. It exploits define-use information along executable paths extracted from the control-flow graph of the model being analyzed; this information enables an effective constraining of the transition relation and a decomposition of the image computation process that often leads to much smaller intermediate BDDs. Our experiments confirm that this reduction in the size of the representation of state sets translates in significant decreases in CPU and memory requirements.
ID:318
CLASS:4
Title: A general system for computer based acquisition, analysis and display of medical image data
Abstract: A general computer-based system has been developed and implemented for acquiring and viewing medical image data. Originally developed for neuroanatomic studies (1), including investigations of cell topography and connectivity in brainstem nuclei, the system has become a versatile and powerful tool for three-dimensional analysis and display of a variety of types of image data, including studies of cardiac morphometry and 2-d gel electrophoresis. The system includes components for data input via video frame digitizer or digitizing tablet; graphical output through a high-resolution color graphics display or hardcopy plotter. Keys to the system's flexibility and power are a tree-structured data file system, in which line segments and shaded strips may be combined to form complex three-dimensional structures, and a disk-based virtual memory system which permits greater numerical accuracy and use of larger structures than would be otherwise possible with a 16-bit minicomputer.
ID:319
CLASS:4
Title: A fast orientation and skew detection algorithm for monochromatic document images
Abstract: Very often in the digitization process, documents are either not placed with the correct orientation or are rotated of small angles in relation to the original image axis. These factors make more difficult the visualization of images by human users, increase the complexity of any sort of automatic image recognition, degrade the performance of OCR tools, increase the space needed for image storage, etc. This paper presents a fast algorithm for orientation and skew detection for complex monochromatic document images, which is capable of detecting any document rotation at a high precision.
ID:320
CLASS:4
Title: Visual signature based identification of Low-resolution document images
Abstract: In this paper, we present (a) a method for identifying documents captured from low-resolution devices such as web-cams, digital cameras or mobile phones and (b) a technique for extracting their textual content without performing OCR. The first method associates a hierarchically structured visual signature to the low-resolution document image and further matches it with the visual signatures of the original high-resolution document images, stored in PDF form in a repository. The matching algorithm follows the signature hierarchy, which speeds-up the search by guiding it towards fruitful solution spaces. In a second step, the content of the original PDF document is extracted, structured, and matched with its corresponding high-resolution visual signature. Finally, the matched content is attached to the low-resolution document image's visual signature, which greatly enriches the document's content and indexing. We present in this article both these identification and extraction methods and evaluate them on various documents, resolutions and lighting conditions, using different capture devices.
ID:321
CLASS:4
Title: Effective text extraction and recognition for WWW images
Abstract: Images play a very important role in web content delivery. Many WWW images contain text information that can be used for web indexing and searching. A new text extraction and recognition algorithm is proposed in this paper. The character strokes in the image are first extracted by color clustering and connected component analysis. A novel stroke verification algorithm is used to effectively remove non-character strokes. The verified strokes are then used to build the binary text line image, which is segmented and recognized by dynamic programming. Since text in WWW image usually has close relationship with webpage content, approximate string matching is used to revise the recognition result by matching the content in the webpage with the content in the image. This effective post-processing not only improves the recognition performance, but also can be used in other applications such like image - webpage paragraph corresponding.
ID:322
CLASS:4
Title: Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography
Abstract: A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing
ID:323
CLASS:4
Title: A Bayesian network classifier with inverse tree structure for voxelwise magnetic resonance image analysis
Abstract: We propose a Bayesian-network classifier with inverse-tree structure (BNCIT) for joint classification and variable selection. The problem domain of voxelwise magnetic-resonance image analysis often involves millions of variables but only dozens of samples. Judicious variable selection may render classification tractable, avoid over-fitting, and improve classifier performance. BNCIT embeds the variable-selection process within the classifier-training process, which makes this algorithm scalable. BNCIT is based on a Bayesian-network model with inverse-tree structure, i.e., the class variable C is a leaf node, and predictive variables are parents of C; thus, the classifier-training process returns a parent set for C, which is a subset of the Markov blanket of C. BNCIT uses voxels in the parent set, and voxels that are probabilistically equivalent to them, as variables for classification of new image data. Since the data set has a limited number of samples, we use the jackknife method to determine whether the classifier generated by BNCIT is a statistical artifact. In order to enhance stability and improve classification accuracy, we model the state of the probabilistically equivalent voxels with a latent variable. We employ an efficient method for determining states of hidden variables, thus reducing dramatically the computational cost of model generation. Experimental results confirm the accuracy and efficiency of BNCIT.
ID:324
CLASS:4
Title: Projection defocus analysis for scene capture and image display
Abstract: In order to produce bright images, projectors have large apertures and hence narrow depths of field. In this paper, we present methods for robust scene capture and enhanced image display based on projection defocus analysis. We model a projector's defocus using a linear system. This model is used to develop a novel temporal defocus analysis method to recover depth at each camera pixel by estimating the parameters of its projection defocus kemel in frequency domain. Compared to most depth recovery methods, our approach is more accurate near depth discontinuities. Furthermore, by using a coaxial projector-camera system, we ensure that depth is computed at all camera pixels, without any missing parts. We show that the recovered scene geometry can be used for refocus synthesis and for depth-based image composition. Using the same projector defocus model and estimation technique, we also propose a defocus compensation method that filters a projection image in a spatially-varying, depth-dependent manner to minimize its defocus blur after it is projected onto the scene. This method effectively increases the depth of field of a projector without modifying its optics. Finally, we present an algorithm that exploits projector defocus to reduce the strong pixelation artifacts produced by digital projectors, while preserving the quality of the projected image. We have experimentally verified each of our methods using real scenes.
ID:325
CLASS:4
Title: Correlation Statistics for cDNA Microarray Image Analysis
Abstract: In this paper, correlation of the pixels comprising a microarray spot is investigated. Subsequently, correlation statistics, namely, Pearson correlation and Spearman rank correlation, are used to segment the foreground and background intensity of microarray spots. The performance of correlation-based segmentation is compared to clustering-based (PAM, k-means) and seeded-region growing techniques (SPOT). It is shown that correlation-based segmentation is useful in flagging poorly hybridized spots, thus minimizing false-positives. The present study also raises the intriguing question of whether a change in correlation can be an indicator of differential gene expression.
ID:326
CLASS:4
Title: Coevolutionary feature synthesized EM algorithm for image retrieval
Abstract: As a commonly used unsupervised learning algorithm in Content-Based Image Retrieval (CBIR), Expectation-Maximization (EM) algorithm has several limitations, especially in high dimensional feature spaces where the data are limited and the computational cost varies exponentially with the number of feature dimensions. Moreover, the convergence is guaranteed only at a local maximum. In this paper, we propose a unified framework of a novel learning approach, namely Coevolutionary Feature Synthesized Expectation-Maximization (CFS-EM), to achieve satisfactory learning in spite of these difficulties. The CFS-EM is a hybrid of coevolutionary genetic programming (CGP) and EM algorithm. The advantages of CFS-EM are: 1) it synthesizes low-dimensional features based on CGP algorithm, which yields near optimal nonlinear transformation and classification precision comparable to kernel methods such as the support vector machine (SVM); 2) the explicitness of feature transformation is especially suitable for image retrieval because the images can be searched in the synthesized low-dimensional space, while kernel-based methods have to make classification computation in the original high-dimensional space; 3) the unlabeled data can be boosted with the help of the class distribution learning using CGP feature synthesis approach. Experimental results show that CFS-EM outperforms pure EM and CGP alone, and is comparable to SVM in the sense of classification. It is computationally more efficient than SVM in query phase. Moreover, it has a high likelihood that it will jump out of a local maximum to provide near optimal results and a better estimation of parameters.
ID:327
CLASS:4
Title: INFTY: an integrated OCR system for mathematical documents
Abstract: An integrated OCR system for mathematical documents, called INFTY, is presented. INFTY consists of four procedures, i.e., layout analysis, character recognition, structure analysis of mathematical expressions, and manual error correction. In those procedures, several novel techniques are utilized for better recognition performance. Experimental results on about 500 pages of mathematical documents showed high character recognition rates on both mathematical expressions and ordinary texts, and sufficient performance on the structure analysis of the mathematical expressions.
ID:328
CLASS:4
Title: Facial emotion recognition by adaptive processing of tree structures
Abstract: We present an emotion recognition system based on a probabilistic approach to adaptive processing of Facial Emotion Tree Structures (FETS). FETS are made up of localized Gabor features related to the facial components according to the Facial Action Coding System. The proposed model is an extension of the probabilistic based recursive neural network model applying in face recognition by Cho and Wong [1]. The robustness of the model in an emotion recognition system is evaluated by testing with known and unknown subjects with different emotions. The experiment results shows that the proposed model significantly improved the recognition rate in terms of generalization.
ID:329
CLASS:4
Title: Image analysis by analogy with Taylor expansion
Abstract: The Taylor expansion has shown - in many fields - to be an extremely powerful tool. In this paper, we investigated image features and their relationships by analogy with Taylor expansion. The kind of expansion could be used to investigate positions of image feature analysis and engraftment, such as transferring color between images. By analogy with Taylor expansion, we designed the image-rendering algorithm to find a best match in the source image by first and second-order information. The luminance histogram represents the first-order information of image, and the co-occurrence matrix represents the second-order information of image. Some results of our processing showed the algorithm worked very well. In our study, each polynomial in our analogy Taylor expansion of images was considered as one of image features, which makes us re-understand images and its features. It provided us a cue that the features of image, such as color, texture, dimension, time series, would be not isolated but mutual relational based on image expansion.
ID:330
CLASS:4
Title: Person identification from heavily occluded face images
Abstract: In numerous occasions there is need to identify subjects shown in heavily occluded face images. Typical examples include the recognition of criminals whose facial images are captured by surveillance cameras. In such cases a significant part of the subjects face is occluded making the process of identification extremely difficult, both for automatic face recognition systems and human observers. In this paper we propose a face recognition algorithm, which can be used for identifying individuals with hidden facial parts. During the face recognition procedure, occluded facial regions are detected so that the model-based face recognition algorithm implemented makes use of information only from the non-occluded facial regions. With our approach information from occluded facial regions is not utilized during the process of face recognition hence the occlusions do not destruct the recognition process and as a result the probability of achieving correct identification is improved.
ID:331
CLASS:4
Title: APL Based medical image analysis
Abstract: Ophthalmology (eye care) is an important medical discipline. Since 1961 the representation of the ocular vascular system using fluorescence dyes has been one of the most important methods used in the diagnosis of diseases of the human eye. The recently introduced laser scanning systems allows fast and continuous imaging of the complete inflow of dye into the vascular system. With the aid of APL as a well-suited image analysis tool it is now possible to analyze these image sequences to extract diagnostic relevant blood flow parameters. During the image acquisition there are unavoidable eye movements which render the automatic follow up of a specific area over time more difficult. Therefore a method has been developed to robustly analyze the eye movements. Beside the determination of the filling delay, i.e. the time of dye appearance for every point, the filling time has been measured. Result images will be presented for two different cases. A slightly modified algorithm is able to automatically compose a panoramic wide-angle retinal image out of partially overlapping single images. The APL system itself (called APL2C) and the algorithms for the image processing have been developed by the author and are not commercially available.
ID:332
CLASS:4
Title: Information-theoretic analysis of steganalysis in real images
Abstract: In this paper we consider the problem of performance improvement of non-blind statistical steganalysis of additive steganography in real images. The proposed approach differs from the existing solutions in two main aspects:(a) a locally non-stationary Gaussian model is introduced via source splitting to represent the statistics of the cover image and (b)the detection of the hidden information is performed not from all but from those channels that allow to perform it with the required accuracy. We analyze the theoretically attainable bounds in such a framework and compare them to the corresponding limits of the existing state-of-the-art frameworks. The performed analysis demonstrates the superiority of the proposed approach.
ID:333
CLASS:4
Title: Estimating manifold dimension by inversion error
Abstract: Video and image datasets can often be described by a small number of parameters, even though each image usually consists of hundreds or thousands of pixels. This observation is often exploited in computer vision and pattern recognition by the application of dimensionality reduction techniques. In particular, there has been recent interest in the application of a class of nonlinear dimensionality reduction algorithms which assume that an image dataset has been sampled from a manifold.From this assumption, it follows that estimating the dimension of the manifold is the first step in analyzing an image dataset. Typically, this estimate is obtained either by using a priori knowledge, or by applying one of the various statistical and geometrical methods available. Once an estimate is obtained, it is used as a parameter for the nonlinear dimensionality reduction algorithm.In this paper, we consider reversing this approach. Instead of estimating the dimension of the manifold in order to obtain a low dimensional representation, we consider producing low dimensional representations in order to estimate of the dimensionality of the manifold. By varying the dimensionality parameter, we obtain different low dimensional representations of the original dataset. The dimension of the best representation should then correspond to the actual dimension of the manifold.In order to determine the best representation, we propose a metric based on inversion. In particular, we propose that a good representation should be invertible, in that we should be able to reverse the reduction algorithm's transformation to obtain the original dataset. By coupling this metric with any reduction algorithm, we can estimate the dimensionality of an image manifold. We apply our method in the context of locally linear embedding (LLE) and Isomap to six frequently used examples and two image datasets.
ID:334
CLASS:4
Title: Measurement of variations in free-hand renderings using image analysis of geometric tasks
Abstract: The ability to analyze pixel image data for variations from the expected or ideal pixel locations is useful in such diverse disciplines as machine vision, industrial quality control, military target selection, child psychomotor development, geometric instruction, and computer games. This tutorial is a gentle, but interesting, exercise introducing students to the elementary concepts of bit-mapped formats, using pixel level data, and establishing local standards for valid statistical measures. Beyond presenting one of the simplest black and white bit-mapped formats, the tutorial emphasizes development of a locally standardized way to capture each image, use of a mathematical model to describe the ideal pixel locations, a sampling scheme, a nested loop scanning scheme to locate pixels of interest, and an operational definition of variation. The looping scans and measurement algorithms are implemented in C++, but could be implemented in other languages. The example images, on 5"X 8" index cards, are responses from such free-hand tasks as drawing a line between two given points, or drawing a line parallel to a given line through a given point. Average errors smaller than .002 inches and standard deviations near zero have been achieved. Once students understand the basic concepts, they can easily progress to more advanced statistical and practical image analysis applications.
ID:335
CLASS:4
Title: A generative model for separating illumination and reflectance from images
Abstract: It is well known that even slight changes in nonuniform illumination lead to a large image variability and are crucial for many visual tasks. This paper presents a new ICA related probabilistic model where the number of sources exceeds the number of sensors to perform an image segmentation and illumination removal, simultaneously. We model illumination and reflectance in log space by a generalized autoregressive process and Hidden Gaussian Markov random field, respectively.The model ability to deal with segmentation of illuminated images is compared with a Canny edge detector and homomorphic filtering. We apply the model to two problems: synthetic image segmentation and sea surface pollution detection from intensity images.
ID:336
CLASS:4
Title: Semantic image retrieval based on probabilistic latent semantic analysis
Abstract: Content-based image retrieval (CBIR) systems combine computer vision techniques and learning methodologies to find images in the database similar to the query images. Relevance feedback methods are introduced to the CBIR area as a tool to help the user to guide the retrieval system during the search process. Search history of the retrieval system, which is the accumulated feedbacks from past retrievals, has been recently used as a prior knowledge to improve the image retrieval performance. In this paper, we introduce an image retrieval model based on probabilistic latent semantic analysis (PLSA) that utilizes the system's search history to find hidden image semantics of the database. Image features are integrated to the model as well. The model is capable of detecting images and image features that efficiently represent semantic classes in the database. We demonstrate the effectiveness of our approach by comparing to previous work in this area.
ID:337
CLASS:4
Title: Mixed-integer optimization of coronary vessel image analysis using evolution strategies
Abstract: In this paper we compare Mixed-Integer Evolution Strategies (MI-ES)and standard Evolution Strategies (ES)when applied to find optimal solutions for artificial test problems and medical image processing problems. MI-ES are special instantiations of standard ES that can solve optimization problems with different objective variable types (continuous, integer, and nominal discrete). Artificial test problems are generated with a mixed-integer test generator.The practical image processing problem iss the detection of the lumen boundary in IntraVascular UltraSound (IVUS)images. Based on the experimental results, it is shown that MI-ES generally perform better than standard ES on both artifical and practical image processing problems. Moreover it is shown that MI-ES can effectively improve the parameters settings for the IVUS lumen detection algorithm.
ID:338
CLASS:4
Title: Latent semantic analysis of facial action codes for automatic facial expression recognition
Abstract: For supervised training of automatic facial expression recognition systems, adequate ground truth labels that describe relevant facial expression categories are necessary. One possibility is to label facial expressions into emotion categories. Another approach is to label facial expressions independently from any interpretation attempts. This can be achieved via the facial action coding system (FACS). In this paper we present a novel approach that allows to automatically cluster FACScodes into meaningful categories. Our approach exploits the fact that FACScodes can be seen as documents containing terms -the action units (AUs) present in the codes-and so text modeling methods that capture co-occurrence information in low-dimensional spaces can be used. The FACScode derived descriptions are computed by Latent Semantic Analysis (LSA) and Probabilistic Latent Semantic Analysis (PLSA). We show that, as a high-level description of facial actions, the newly derived codes constitute a competitive alternative to both basic emotion and FACScodes. We have used them to train different types of artificial neural networks
ID:339
CLASS:4
Title: Morphing of image represented objects using a physical methodology
Abstract: This paper presents a methodology to do morphing between image represented objects, attending to their physical properties. It can be used amongst images of different objects, or otherwise, between different images of the same object.According to the used methodology the given objects are modelled by the Finite Element Method, and some nodes are matched by Modal Analysis. Then, by solving the Dynamic Equilibrium Equation the displacement field is determined, which allows the simulation of the objects' deformation.This physical approach also allows the computation of the involved strain energy, therefore the estimated morphing can be represented by the local or global strain energy values.This paper also describes the solution used to simulate only the non-rigid components of the involved deformation.
ID:340
CLASS:4
Title: Multimodal fusion using learned text concepts for image categorization
Abstract: Conventional image categorization techniques primarily rely on low-level visual cues. In this paper, we describe a multimodal fusion scheme which improves the image classification accuracy by incorporating the information derived from the embedded texts detected in the image under classification. Specific to each image category, a text concept is first learned from a set of labeled texts in images of the target category using Multiple Instance Learning [1]. For an image under classification which contains multiple detected text lines, we calculate a weighted Euclidian distance between each text line and the learned text concept of the target category. Subsequently, the minimum distance, along with low-level visual cues, are jointly used as the features for SVM-based classification. Experiments on a challenging image database demonstrate that the proposed fusion framework achieves a higher accuracy than the state-of-art methods for image classification.
ID:341
CLASS:4
Title: Imaging and visual analysis---Large image correction and warping in a cluster environment
Abstract: This paper is concerned with efficient execution of a pipeline of data processing operations on very large images obtained from confocal microscopy instruments. We describe parallel, out-of-core algorithms for each operation in this pipeline. One of the challenging steps in the pipeline is the warping operation using inverse mapping based methods. We propose and investigate a set of algorithms to handle the warping computations on storage clusters. Our experimental results show that the proposed approaches are scalable both in terms of number of processors and the size of images.
ID:342
CLASS:4
Title: Looking for a picture: an analysis of everyday image information searching
Abstract: There is at present a dearth of information on the everyday image information behavior of ordinary people. Analysis of a set of 64 image-related searches provides insight into potentially useful facilities for an image digital library.
ID:343
CLASS:4
Title: A first-order analysis of lighting, shading, and shadows
Abstract: The shading in a scene depends on a combination of many factors---how the lighting varies spatially across a surface, how it varies along different directions, the geometric curvature and reflectance properties of objects, and the locations of soft shadows. In this article, we conduct a complete first-order or gradient analysis of lighting, shading, and shadows, showing how each factor separately contributes to scene appearance, and when it is important. Gradients are well-suited to analyzing the intricate combination of appearance effects, since each gradient term corresponds directly to variation in a specific factor. First, we show how the spatial and directional gradients of the light field change as light interacts with curved objects. This extends the recent frequency analysis of Durand et al. [2005] to gradients, and has many advantages for operations, like bump mapping, that are difficult to analyze in the Fourier domain. Second, we consider the individual terms responsible for shading gradients, such as lighting variation, convolution with the surface BRDF, and the object's curvature. This analysis indicates the relative importance of various terms, and shows precisely how they combine in shading. Third, we understand the effects of soft shadows, computing accurate visibility gradients, and generalizing previous work to arbitrary curved occluders. As one practical application, our visibility gradients can be directly used with conventional ray-tracing methods in practical gradient interpolation methods for efficient rendering. Moreover, our theoretical framework can be used to adaptively sample images in high-gradient regions for efficient rendering.
ID:344
CLASS:4
Title: A web-based collaborative system for medical image analysis and diagnosis
Abstract: The overall objective of this paper is to show the development of a web-based collaborative system for medical image analysis and diagnosis that is affordable, usable, reliable and efficient for medical area. The system consists of four components that are chat system, online image manipulation system, message board and server system. To carry out the objective of the system, various method are applied such as JAVA applet which is system independent using a virtual machine technology, server and client network model, distributed data structure to manage data for multi-users and image manipulation using Java Awt Graphics library. This system was developed and tested to provide better interface for CSCW (Computer Supported Collaborative Work) in medical area. The collaborative system for medical image analysis and diagnosis being able to be used via Internet provides system independence, convenience and efficiencies.
ID:345
CLASS:4
Title: Spatio-Temporal Analysis of Constitutive Exocytosis in Epithelial Cells
Abstract: Exocytosis is an essential cellular trafficking process integral to the proper distribution and function of a plethora of molecules, including transporters, receptors, and enzymes. Moreover, incorrect protein targeting can lead to pathological conditions. Recently, the application of evanescent wave microscopy has allowed us to image the final steps of exocytosis. However, spatio-temporal analysis of fusion of constitutive vesicular traffic with the plasma membrane has not been systematically performed. Also, the spatial sites and times of vesicle fusion have not yet been analyzed together. In addition, more formal tests are required in testing biological hypotheses, rather than visual inspection combined with statistical descriptives. Ripley {\cal K}{\hbox{-}}{\rm functions} are used to examine the joint and marginal behavior of locations and fusion times. Semiautomatic detection and mapping of constitutive fusion sites reveals spatial and temporal clustering, but no dependency between the locations and times of fusion events. Our novel approach could be translated to other studies of membrane trafficking in health and diseases such as diabetes.
ID:346
CLASS:4
Title: Soccer video analysis by ball, player and referee tracking
Abstract: Soccer ball detection and tracking plays a pivotal role in soccer event detection. In fact, all events in the game take place around the ball, and it is crucial to track the movement of the ball, players of both teams as well as the referee in order to perform analysis of the match both real-time and post match. A ball, player and referee detection, classification and tracking; team identification, and a field extraction approach is proposed in this paper. A higher level automatic offside event (OE) detection method is also proposed. The principle constraint considered during the development of the system was that the pattern recognition techniques had to be computationally efficient in terms of processing time. The system developed is a non-invasive vision-based decision support tool, capable of providing real-time analysis of a soccer match, including assisting the referee in making a decision concerning the offside rule.
ID:347
CLASS:4
Title: Gazetracker: software designed to facilitate eye movement analysis
Abstract: The Eye-gaze Response Interface Computer Aid (ERICA) is a computer system developed at the University of Virginia that tracks eye movement. Originally developed as a means to allow individuals with disabilities to communicate, ERICA was then expanded to provide methods for experimenters to analyze eye movements. This paper describes an application called Gaze Tracker&trade; that facilitates the analysis of a test subject's eye movements and pupil response to visual stimuli, such as still images or dynamic software applications that the test subject interacts with (for example, Internet Explorer).
ID:348
CLASS:4
Title: Considerations in processing satellite images
Abstract: Legislated demands for better control of natural resources have motivated many state and local resource management agencies to investigate application of satellite image data to their management activities. The University of Santa Clara and NASA-Ames Research Center have been jointly considering the problems faced by such agencies. Advantages and disadvantages of using relatively small systems for the required computer processing activities are briefly discussed in the paper. In addition, summaries are provided of current status on investigations of user interface considerations, steps involved in developing local processing capability, and improvement of software transportability.
ID:349
CLASS:4
Title: Information encoding into and decoding from dot texture for active forms
Abstract: We describe here information encoding and decoding methods applied to dot texture for active forms. We employ dot texture made of tiny dots and looking like gray color to print various forms. This facilitates the separation of handwriting from its input frame even under monochrome printing/reading environments. It also makes the forms determine how to process filled-in handwriting according to the information embedded in the dot texture. The embedded information results in an improved recognition rate of handwriting, and allows the form processing to be directed by the form itself rather than by the form reading machine. Thus, the form-reading machine becomes a general-purpose machine allowing different forms inputted into it to be processed differently as specified by each form. We compare various dot shapes and information encoding/decoding methods for those shapes. Then, we present how to locate input frames, separate handwriting from input frames and segment handwriting into characters. We also present preliminary evaluation of the described methods.
ID:350
CLASS:4
Title: The effects of invisible watermarking on satellite image classification
Abstract: Remotely sensed satellite images are an important source of geographical data commonly used as input for various types of classification algorithms. For example, these algorithms are commonly used to classify earth land cover, analyze crop conditions, assess mineral and petroleum deposits, and quantify urban growth. Many vendors of digital images are using or are considering the use of invisible watermarking as a means of protecting their images from theft or unauthorized usage. Indeed, the use of invisible watermarking is routinely considered for use in emerging digital rights management~(DRM) systems that may be deployed to manage and protect the rights associated with satellite imagery, or types of "scientific" imagery~(e.g., in the medical field) that routinely have mathematical analyses applied to them. The concern then is how this watermarking impacts subsequent analyses. Specifically, the invisible watermarking process involves making imperceptible modifications to the pixel values of an image. However, even though these changes may be imperceptible to the human observer, they must be of sufficient magnitude to allow for watermark detection. Because of this, the use of invisible watermarking can also impact the performance of image classification algorithms. This paper is concerned with quantifying the impact that invisible watermarks have on satellite image classification. In particular, Landsat satellite images were watermarked using a number of well-known techniques, and the misclassification that resulted from this watermarking was measured. Experimental results show that even weak watermarking can lead to significant misclassification when common image classification algorithms are applied. Thus, the use of watermarking within DRM systems needs to be carefully considered, with particular attention given to the type of content that the watermarking will be applied to.
ID:351
CLASS:4
Title: Towards a laboratory instrument for motion analysis
Abstract: Motion analysis is the systematic and usually quantitative study of the movements of humans, animals, organisms, cells, or other entities as recorded on movie film or video tape. Despite the utility of computer-aided motion analysis to many biological, social, and physical sciences, its role has been limited because it is so time-consuming and so expensive. Automated techniques can only be used on real images in very special cases; interactive techniques have involved laborious frame by frame operations. In recent years, Futrelle and Potel have revolutionized the process of interactive motion analysis by demonstrating how to digitize entire motions with a single sketch rather than with a sequence of operations on each constituent frame. However, their GALA-TEA system is applicable only to film and not to video tape records, and consists of equipment which cannot easily be engineered into a reliable and mass-producible laboratory instrument. The paper describes a prototype laboratory instrument for the interactive motion analysis of video tape records. The prototype includes a host PDP 11/45 computer, an experimental display processor called SPIWRIT, and a microprocessor-controlled video disk. SPIWRIT generates a computer-animated video representation of the phenomenon being analysed, and superimposes it on the actual video record being streamed from the video disk. SPIWRIT's bit-slice microprogrammable display processor produces true animation on a colour raster display by decoding segmented display file images into alternate halves of a double frame buffer.
ID:352
CLASS:4
Title: Interactive reconstruction of virtual environments from photographs, with application to scene-of-crime analysis
Abstract: There are many real-world applications of Virtual Reality that require the construction of complex and accurate three-dimensional models, suitably structured for interactive manipulation. In this paper, we present semi-automatic methods that allow such environments to be quickly and easily built from photographs taken with uncalibrated cameras, and illustrate the techniques by application to the real-world problem of scene-of-crime reconstruction.
ID:353
CLASS:4
Title: Evaluating strategies and systems for content based indexing of person images on the Web
Abstract: Content based indexing of multimedia has always been a challenging task. The enormity and the diversity of the multimedia content on the web adds another dimension to this challenge. In this paper, we examine ways of combining visual and textual information for content based indexing of multimedia on the web. In particular, we examine different methods of combining evidences due to face detection, Text/HTML analysis and face recognition for identifying person images. We provide experimental evaluation of the following strategies: i) Face detection on the image followed by Text/HTML analysis of the containing page; ii) face detection followed by face recognition; iii) face detection followed by a linear combination of evidences due to text/HTML analysis and face recognition; and iv) face detection followed by a Dempster-Shafer combination of evidences due to text/HTML analysis and face recognition. These strategies were implemented in an automatic web search agent named Diogenes1 and compared against some well known web image search engines. The latter includes commercial systems such as Alta Vista, Lycos and Ditto, and a research prototype, WebSEEk. We report the results of our experimental retrievals where Diogenes outperformed these search engines for celebrity image queries in terms of average precision.
ID:354
CLASS:4
Title: Analysis and visualization of DNA spectrograms: open possibilities for the genome research
Abstract: The demand for technology that can process biological information is becoming more and more obvious and urgent. Existing research in bioinformatics has been focusing on various types of analysis of DNA sequences and various measurements taken at the protein, RNA transcript and DNA level. In this paper we will show the application of spectral analysis and image processing in analyzing DNA sequences of specific structure. In addition, we extend the framework to visualize long DNA sequences and help in identifying patterns that are visible at high resolution of DNA spectral images.
ID:355
CLASS:4
Title: Region-of-interest based image resolution adaptation for MPEG-21 digital item
Abstract: The upcoming MPEG-21 standard proposes a general framework for augmented use of multimedia services in different network environments, for various users with various terminal devices. In the context of image adaptation, terminals with different screen size limitation require the multimedia adaptation engine to adapt image resources intelligently. Saliency map based visual attention analysis provides some intelligence for finding the attention area within the image. In this paper, we improved the standard MPEG-21 metadata driven adaptation engine by using enhanced saliency map based visual attention model which provides a mean to intelligently adapt JPEG2000 image resolution for different terminal devices with varying screen size according to human visual attention.
ID:356
CLASS:4
Title: A compact and efficient image retrieval approach based on border/interior pixel classification
Abstract: This paper presents \bic (Border/Interior pixel Classification), a compact and efficient CBIR approach suitable for broad image domains. It has three main components: (1) a simple and powerful image analysis algorithm that classifies image pixels as either border or interior, (2) a new logarithmic distance (dLog) for comparing histograms, and (3) a compact representation for the visual features extracted from images. Experimental results show that the BIC approach is consistently more compact, more efficient and more effective than state-of-the-art CBIR approaches based on sophisticated image analysis algorithms and complex distance functions. It was also observed that the dLog distance function has two main advantages over vectorial distances (e.g., L1): (1) it is able to increase substantially the effectiveness of (several) histogram-based CBIR approaches and, at the same time, (2) it reduces by 50% the space requirement to represent a histogram.
ID:357
CLASS:4
Title: An approach to image retrieval from large image databases
Abstract: In this paper we address the problem of retrieving images from large image databases, giving a partial description of the image content. This approach allows a limited automatic analysis for image belonging to a domain described in advance to the system using a formalism based on fuzzy sets. The image query processing is based on special access structures generated from the image analysis process.
ID:358
CLASS:4
Title: Imaging and visual analysis---Detecting distributed scans using high-performance query-driven visualization
Abstract: Modern forensic analytics applications, like network traffic analysis, perform high-performance hypothesis testing, knowledge discovery and data mining on very large datasets. One essential strategy to reduce the time required for these operations is to select only the most relevant data records for a given computation. In this paper, we present a set of parallel algorithms that demonstrate how an efficient selection mechanism -- bitmap indexing -- significantly speeds up a common analysis task, namely, computing conditional histogram on very large datasets. We present a thorough study of the performance characteristics of the parallel conditional histogram algorithms. As a case study, we compute conditional histograms for detecting distributed scans hidden in a dataset consisting of approximately 2.5 billion network connection records. We show that these conditional histograms can be computed on interactive time scale (i.e., in seconds). We also show how to progressively modify the selection criteria to narrow the analysis and find the sources of the distributed scans.
ID:359
CLASS:4
Title: Syntactic approach to image analysis (abstract only)
Abstract: During the past several years, syntactic approach [1,2] has attracted growing attention as promising avenues of approach in image analysis. The object of image analysis is to extract as much information as possible from a given image or a set of images. In this abstract, we will focus our attention on the use of semantic information and grammatical inference.In an attributed grammar, there are still a set of nonterminals, a set of terminals and a start symbol just as in conventional grammars. The productions are different. Each semantic rule. Two kinds of attributes are included in the semantic rules: inherited attributes and synthesized attributes. One example of the attributes is the length of a specific line segment used as a primitive. All the attributes identified for a pattern are expressed in a &ldquo;total attribute vector&rdquo;.Instead of using attributes, stochastic grammars associate with each production a probability. That means, one sub-pattern may generate one subpattern with some probability, and another with a different probability. A string may have two or more possible parses. In this case of ambiguity, the probabilities associated with the several possible productions are compared to determine the best fit one. Probabilities are multiplied in multiple steps of stochastic derivations.Besides these, fuzzy languages[3-6] have also been introduced into pattern recognition. By using similarity measures as membership functions, this approach describes patterns in a more understandable way than stochastic grammars. Moreover, fuzzy languages make use of individual characteristics of a class of patterns rather than collective characteristics as in stochastic languages, and therefore it is probably easier to develop grammars than stochastic languages. Yet a lot of work still need to be done in order to develop sufficient theories in this field for practical uses.An appropriate grammar is the core of any type of syntactic pattern recognition process. Grammars may be established by inferring from a priori knowledge about the objects or scenes to be recognized. Another way to establish a pattern grammar is by direct inference from some sample input patterns.Once a grammar is derived from some sample input patterns, other patterns similar to them or belonging to the same class can be parsed according to the grammar. Therefore grammatical inference enables a system to learn most information from an input pattern, and, furthermore, to apply the obtained knowledge to future recognition processes. It seems to be the ultimate aim of image analysis.Inference can be supervised or unsupervised. In supervised inference, a &ldquo;teacher&rdquo; who is able to discriminate valid and invalid strings helps in reducing the length of sentences or inserting substrings until some iterative regularity is detected. In unsupervised inference, no prior knowledge about the grammar is assumed.The difficulty of inference is proportional to the complexity of the grammar, and the inference problem does not have a unique solution unless some additional constraints are placed upon the grammars. Some theoretical algorithms have been developed for inferencing regular (finite-state) grammars, but they still have severe limitations for practical use because of large amount computation due to the combinatorial effect.Context-free grammars are even harder to deal with since many decidable properties about regular grammars are undecidable for context-free grammars, such as the equivalency of two contex-free grammars. Therefore, inference algorithms have been developed only for some specific types of context-free grammars and most of them rely on heuristic methods.Syntactic approach to image analysis may be applied to many areas including space object surveillance and identification [7].
ID:360
CLASS:4
Title: Digital capture for automated scanner workflows
Abstract: The use of scanners and other capture devices to incorporate film- and paper-based materials into digital workflows is an important part of "digital convergence", or the bringing of paper-based and electronic documents together into the same electronic workflows. The diversity of captured information-from text and mixed-type documents to photos, negatives, slides and transparencies-requires a combination of document analysis techniques to perform, automatically, the segmentation, classification and workflow assignment of the scanned images. We herein present technologies that provide fast (&#60; 1.0 sec) and reliable (&#62; 95% job accuracy) capture solutions for all of these input content types. These solutions offer near real-time capture that provides automated workflow capabilities to a repertoire of scanning hardware: scanners, all-in-one devices, copiers and multifunctional printers. The techniques used to categorize the documents, perform zoning analysis on the documents, and then perform closed loop quality assurance on the documents are presented.
ID:361
CLASS:4
Title: ICA for watermarking digital images
Abstract: We present a domain-independent ICA-based approach to watermarking. This approach can be used on images, music or video to embed either a robust or fragile watermark.In the case of robust watermarking, the method shows high information rate and robustness against malicious and non-malicious attacks, while keeping a low induced distortion. The fragile watermarking scheme, on the other hand, shows high sensitivity to tampering attempts while keeping the requirement for high information rate and low distortion. The improved performance is achieved by employing a set of statistically independent sources (the independent components) as the feature space and principled statistical decoding methods. The performance of the suggested method is compared to other state of the art approaches. The paper focuses on applying the method to digitized images although the same approach can be used for other media, such as music or video.
ID:362
CLASS:4
Title: Effective image and video mining: an overview of model-based approaches
Abstract: This paper is dedicated to revisiting image and video mining techniques from the viewpoint of image modeling approaches, which constitute the theoretical basis for these techniques. The most important areas belonging to image or video mining are: image knowledge extraction, content-based image retrieval, video retrieval, video sequence analysis, change detection, model learning, as well as object recognition. Traditionally, these areas have been developed independently, and hence have not benefited from some common sense approaches which provide potentially optimal and time-efficient solutions. Two different types of input data for knowledge extraction from an image collection or video sequences are considered: original image or symbolic (model) description of the image. Several basic models are described briefly and compared with each other in order to find effective solutions for the image and video mining problems. They include feature-based models and object-related structural models for the representation of spatial and temporal entities (objects, scenes or events).
ID:363
CLASS:4
Title: Image-based change detection of areal objects using differential snakes
Abstract: Change detection is an important issue for modern geospatial information systems. In this paper we address change detection of areal objects (i.e. objects with closed-curve outlines). We specifically focus on the detection of movement (translation and rotation) and/or deformation of such objects using aerial imagery. The innovative approach we present in this paper combines geometric analysis with our model of differential snakes to support change detection. Geometric analysis proceeds by comparing the first moments of the two outlines describing the same object in different instances, to estimate translation. Moment information allows us to determine the principal axes and eigenvectors of these outlines, and this we can determine object rotation as the angle between these principal axes. Next, we apply polygon-clipping techniques to calculate the intersection and difference of these two outlines. We use this result to estimate the radial deformation of the object (expansion and contraction). The results are further refined through the use of our differential snakes model, to distinguish true change from the effects of inaccuracy in object determination. The aggregation of these tools defines a powerful approach for change detection. In the paper we present the theoretical background behind these components, and experimental results that demonstrate the performance of our approach.
ID:364
CLASS:4
Title: PLSA-based image auto-annotation: constraining the latent space
Abstract: We address the problem of unsupervised image auto-annotation with probabilistic latent space models. Unlike most previous works, which build latent space representations assuming equal relevance for the text and visual modalities, we propose a new way of modeling multi-modal co-occurrences, constraining the definition of the latent space to ensure its consistency in semantic terms (words), while retaining the ability to jointly model visual information. The concept is implemented by a linked pair of Probabilistic Latent Semantic Analysis (PLSA) models. On a 16000-image collection, we show with extensive experiments that our approach significantly outperforms previous joint models.
ID:365
CLASS:4
Title: A Hill-Climbing Approach for Automatic Gridding of cDNA Microarray Images
Abstract: Image and statistical analysis are two important stages of cDNA microarrays. Of these, gridding is necessary to accurately identify the location of each spot while extracting spot intensities from the microarray images and automating this procedure permits high-throughput analysis. Due to the deficiencies of the equipment used to print the arrays, rotations, misalignments, high contamination with noise and artifacts, and the enormous amount of data generated, solving the gridding problem by means of an automatic system is not trivial. Existing techniques to solve the automatic grid segmentation problem cover only limited aspects of this challenging problem and require the user to specify the size of the spots, the number of rows and columns in the grid, and boundary conditions. In this paper, a hill-climbing automatic gridding and spot quantification technique is proposed which takes a microarray image (or a subgrid) as input and makes no assumptions about the size of the spots, rows, and columns in the grid. The proposed method is based on a hill-climbing approach that utilizes different objective functions. The method has been found to effectively detect the grids on microarray images drawn from databases from GEO and the Stanford genomic laboratories.
ID:366
CLASS:4
Title: Human-centered multimedia: representations and challenges
Abstract: Human has always been a part of the computational loop. So, what do we mean by human-centered computing (HCC)? aren't humans always the focus of computations some how? The goal of this paper is to help answer this question within the context of multimedia applilations. So, what do we mean by human-centered multimedia systems. We discuss some issues and challenges facing developing real humancentered multimedia applications.
ID:367
CLASS:4
Title: Concatenate feature extraction for robust 3D elliptic object localization
Abstract: Developing an efficient object localization system for complicated industrial objects is an important, yet difficult robotic task. To tackle this problem, we have developed a system consisting first of a vision model acquisition editor, where the object salient features are acquired through a human-in-the-loop approach. Subsequently, two feature extraction algorithms, region-growing and edge-grouping, are applied to the object scene. Finally, by Kalman filter estimation of a proper ellipse representation, our object localization system successfully generates ellipse hypotheses by grouping edge fragments in the scene. The proposed system is validated by experiments using actual industrial objects.
ID:368
CLASS:4
Title: Spontaneous vs. posed facial behavior: automatic analysis of brow actions
Abstract: Past research on automatic facial expression analysis has focused mostly on the recognition of prototypic expressions of discrete emotions rather than on the analysis of dynamic changes over time, although the importance of temporal dynamics of facial expressions for interpretation of the observed facial behavior has been acknowledged for over 20 years. For instance, it has been shown that the temporal dynamics of spontaneous and volitional smiles are fundamentally different from each other. In this work, we argue that the same holds for the temporal dynamics of brow actions and show that velocity, duration, and order of occurrence of brow actions are highly relevant parameters for distinguishing posed from spontaneous brow actions. The proposed system for discrimination between volitional and spontaneous brow actions is based on automatic detection of Action Units (AUs) and their temporal segments (onset, apex, offset) produced by movements of the eyebrows. For each temporal segment of an activated AU, we compute a number of mid-level feature parameters including the maximal intensity, duration, and order of occurrence. We use Gentle Boost to select the most important of these parameters. The selected parameters are used further to train Relevance Vector Machines to determine per temporal segment of an activated AU whether the action was displayed spontaneously or volitionally. Finally, a probabilistic decision function determines the class (spontaneous or posed) for the entire brow action. When tested on 189 samples taken from three different sets of spontaneous and volitional facial data, we attain a 90.7% correct recognition rate.
ID:369
CLASS:4
Title: Challenges in the analysis of multimodal messaging
Abstract: New forms of computer-mediated communication are increasingly multimodal, providing capabilities for communicating with some combination of text, image, audio, and video. In this paper, we point to the need to develop better methods for studying multimodal communication -- more specifically, for studying thencommunicative role of and relationships among differentnmodalities within their increasingly complex, multimodal semioticnlandscapes. We present two challenges in the analysis of multimodal communication, point of view and unit of analysis, both encountered in the context of our study of the use of photo-enhanced instant messaging.
ID:370
CLASS:4
Title: Interactive training of advanced classifiers for mining remote sensing image archives
Abstract: Advances in satellite technology and availability of downloaded images constantly increase the sizes of remote sensing image archives. Automatic content extraction, classification and content-based retrieval have become highly desired goals for the development of intelligent remote sensing databases. The common approach for mining these databases uses rules created by analysts. However, incorporating GIS information and human expert knowledge with digital image processing improves remote sensing image analysis. We developed a system that uses decision tree classifiers for interactive learning of land cover models and mining of image archives. Decision trees provide a promising solution for this problem because they can operate on both numerical (continuous) and categorical (discrete) data sources, and they do not require any assumptions about neither the distributions nor the independence of attribute values. This is especially important for the fusion of measurements from different sources like spectral data, DEM data and other ancillary GIS data. Furthermore, using surrogate splits provides the capability of dealing with missing data during both training and classification, and enables handling instrument malfunctions or the cases where one or more measurements do not exist for some locations. Quantitative and qualitative performance evaluation showed that decision trees provide powerful tools for modeling both pixel and region contents of images and mining of remote sensing image archives.
ID:371
CLASS:4
Title: Content-based image retrieval: approaches and trends of the new age
Abstract: The last decade has witnessed great interest in research on content-based image retrieval. This has paved the way for a large number of new techniques and systems, and a growing interest in associated fields to support such systems. Likewise, digital imagery has expanded its horizon in many directions, resulting in an explosion in the volume of image data required to be organized. In this paper, we discuss some of the key contributions in the current decade related to image retrieval and automated image annotation, spanning 120 references. We also discuss some of the key challenges involved in the adaptation of existing image retrieval techniques to build useful systems that can handle real-world data. We conclude with a study on the trends in volume and impact of publications in the field with respect to venues/journals and sub-topics.
ID:372
CLASS:4
Title: A content-based image retrieval system for fish taxonomy
Abstract: It is estimated that less than ten percent of the world's species have been discovered and described. The main reason for the slow pace of new species description is that the science of taxonomy, as traditionally practiced, can be very laborious: taxonomists have to manually gather and analyze data from large numbers of specimens, often from broad geographic areas, and identify the smallest subset of external body characters that uniquely diagnoses the new species as distinct from all its known relatives. The pace of data gathering and analysis in taxonomy can be greatly increased by the development of information technology. The Internet is being used to link taxonomists,taxonomic literature and specimen databases in different parts of the globe, and hence enables the development of tools for remote study of specimens archived as digital images. In this paper, we propose a content-based image retrieval system for taxonomic research. The system has a learning component that can identify representative body shape characters of known species based on digitized landmarks. The system can also provide statistical clues for assisting taxonomists to identify new species or subspecies. The experiments on a taxonomic problem involving species of suckers in the genera Carpiodes demonstrate promising results.
ID:373
CLASS:4
Title: Hierarchical clustering of WWW image search results using visual, textual and link information
Abstract: We consider the problem of clustering Web image search results. Generally, the image search results returned by an image search engine contain multiple topics. Organizing the results into different semantic clusters facilitates users' browsing. In this paper, we propose a hierarchical clustering method using visual, textual and link analysis. By using a vision-based page segmentation algorithm, a web page is partitioned into blocks, and the textual and link information of an image can be accurately extracted from the block containing that image. By using block-level link analysis techniques, an image graph can be constructed. We then apply spectral techniques to find a Euclidean embedding of the images which respects the graph structure. Thus for each image, we have three kinds of representations, i.e. visual feature based representation, textual feature based representation and graph based representation. Using spectral clustering techniques, we can cluster the search results into different semantic clusters. An image search example illustrates the potential of these techniques.
ID:374
CLASS:4
Title: A computer vision system for automated corn seed purity analysis
Abstract: Electrophoresis gel analysis is a viable technique for determining the purity of hybrid corn seeds. Visually analyzing the electrophoretic gel images is a very tedious and time-consuming task. In this paper, a computer vision system integrating image processing and pattern recognition techniques with domain-specific structural information to automate the electrophoresis gel scoring procedure is presented. A set of image processing algorithms are developed to perform extraction of the region of interest, segmentation of samples, identification of bands within samples, and final classification of different types of seeds.The image processing algorithms utilize the structural information and operator expertise to achieve high classification rate with fuzzy and incomplete information contained on the electrophoresis gels. The developed technique clearly demonstrates the potential of using computer vision in automating the gel scoring procedure. The developed technology may also be extended to other areas such as general one-dimensional electrophoresis and high performance thin layer chromatography.
ID:375
CLASS:4
Title: Analysis of human faces using a measurement-based skin reflectance model
Abstract: We have measured 3D face geometry, skin reflectance, and subsurface scattering using custom-built devices for 149 subjects of varying age, gender, and race. We developed a novel skin reflectance model whose parameters can be estimated from measurements. The model decomposes the large amount of measured skin data into a spatially-varying analytic BRDF, a diffuse albedo map, and diffuse subsurface scattering. Our model is intuitive, physically plausible, and -- since we do not use the original measured data -- easy to edit as well. High-quality renderings come close to reproducing real photographs. The analysis of the model parameters for our sample population reveals variations according to subject age, gender, skin type, and external factors (e.g., sweat, cold, or makeup). Using our statistics, a user can edit the overall appearance of a face (e.g., changing skin type and age) or change small-scale features using texture synthesis (e.g., adding moles and freckles). We are making the collected statistics publicly available to the research community for applications in face synthesis and analysis.
ID:376
CLASS:4
Title: Optimal multimodal fusion for multimedia data analysis
Abstract: Considerable research has been devoted to utilizing multimodal features for better understanding multimedia data. However, two core research issues have not yet been adequately addressed. First, given a set of features extracted from multiple media sources (e.g., extracted from the visual, audio, and caption track of videos), how do we determine the best modalities? Second, once a set of modalities has been identified, how do we best fuse them to map to semantics? In this paper, we propose a two-step approach. The first step finds &#60;i>statistically independent modalities&#60;/i> from raw features. In the second step, we use &#60;i>super-kernel fusion&#60;/i> to determine the optimal combination of individual modalities. We carefully analyze the tradeoffs between three design factors that affect fusion performance: &#60;i>modality independence&#60;/i>, &#60;i>curse of dimensionality&#60;/i>, and &#60;i>fusion-model complexity&#60;/i>. Through analytical and empirical studies, we demonstrate that our two-step approach, which achieves a careful balance of the three design factors, can improve class-prediction accuracy over traditional techniques.
ID:377
CLASS:4
Title: Visual search: structure from noise
Abstract: In this paper, we present two techniques to reveal image features that attract the eye during visual search: the discrimination image paradigm and principal component analysis. In preliminary experiments, we employed these techniques to identify image features used to identify simple targets embedded in 1/&fnof; noise. Two main findings emerged. First, the loci of fixations were not random but were driven by local image features, even in very noisy displays. Second, subjects often searched for a component feature of a target rather that the target itself, even if the target was a simple geometric form. Moreover, the particular relevant component varied from individual to individual. Also, principal component analysis of the noise patches at the point of fixation reveals global image features used by the subject in the search task. In addition to providing insight into the human visual system, these techniques have relevance for machine vision as well. The efficacy of a foveated machine vision system largely depends on its ability to actively select 'visually interesting' regions in its environment. The techniques presented in this paper provide valuable low-level criteria for executing human-like scanpaths in such machine vision systems.
ID:378
CLASS:4
Title: Hybrid stereo camera: an IBR approach for synthesis of very high resolution stereoscopic image sequences
Abstract: This paper introduces a novel application of IBR technology for efficient rendering of high quality CG and live action stereoscopic sequences. Traditionally, IBR has been applied to render novel views using image and depth based representations of the plenoptic functions. In this work, we present a restricted form of IBR in which lower resolution images for the views to be generated at a very high resolution are assumed to be available. Specifically, the paper addresses the problem of synthesizing stereo IMAX(R)1 3D motion picture images at a standard resolution of 4-6K. At such high resolutions, producing CG content is extremely time consuming and capturing live action requires bulky cameras. We propose a Hybrid Stereo Camera concept in which one view is rendered at the target high resolution but the other is rendered at a much lower resolution. Methods for synthesizing the second view sequence at the target resolution using image analysis and IBR techniques are the focus of this work. The high quality results from the techniques presented in this paper have been visually evaluated in the IMAX 3D large screen projection environment. The paper also highlights generalizations and extensions of the hybrid stereo camera concept.
ID:379
CLASS:4
Title: Automatic mining of fruit fly embryo images
Abstract: We present FEMine, an automatic system for image-based gene expression analysis. We perform experiments on the largest publicly available collection of Drosophila ISH (in situ hybridization) images, showing that our FEMine system achieves excellent performance in classification, clustering, and content-based image retrieval. The major innovation of FEMine is the use of automatically discovered latent spatial "themes" of gene expressions, LGEs, in the whole-embryo context, as opposed to patterns in nearly disjoint portions of an embryo proposed in previous methods.
ID:380
CLASS:4
Title: Imaging and visual analysis---Toward real-time image guided neurosurgery using distributed and grid computing
Abstract: Neurosurgical resection is a therapeutic intervention in the treatment of brain tumors. Precision of the resection can be improved by utilizing Magnetic Resonance Imaging (MRI) as an aid in decision making during Image Guided Neurosurgery (IGNS). Image registration adjusts pre-operative data according to intra-operative tissue deformation. Some of the approaches increase the registration accuracy by tracking image landmarks through the whole brain volume. High computational cost used to render these techniques inappropriate for clinical applications.In this paper we present a parallel implementation of a state of the art registration method, and a number of needed incremental improvements. Overall, we reduced the response time for registration of an average dataset from about an hour and for some cases more than an hour to less than seven minutes, which is within the time constraints imposed by neurosurgeons. For the first time in clinical practice we demonstrated, that with the help of distributed computing non-rigid MRI registration based on volume tracking can be computed intra-operatively.
ID:381
CLASS:4
Title: On the impact of outliers on high-dimensional data analysis methods for face recognition
Abstract: In this paper, the impact of outliers on the performance of high-dimensional data analysis methods is studied in the context of face recognition. Most of the existing face recognition methods are based on PCA-like methods: Faces are projected into a lower dimensional space in which similarity between faces is more easily evaluated. These methods are, however, very sensitive to the quality of face images used in the training and the recognition phases. Their performance significantly degrades when faces are not well centered or taken under variable illumination conditions. In this paper, we study this phenomenon for two face recognition methods (PCA and LDA2D) and we propose a filtering process that allows an automatic isolation of noisy faces which are responsible for the performance degradation. This process is performed during the training phase as well as the recognition phase. It is based-on the recently proposed robust high-dimensional data analysis method RobPCA. Experiments show that this filtering process improves the recognition rate by 10 to 20%.
ID:382
CLASS:4
Title: Content-based image retrieval
Abstract: In order to retrieve images it is much more sophisticated and usual for human beings to use natural language concepts, e.g. mountainlake, than syntactical features, e.g. red region left up. This leads to a content-based image retrieval. Furthermore, it is unreasonable for any human being to make the content description for 1000 of images manually.From this point of view, the project IRIS1 (Image Retrieval for Information Systems) combines well-known methods and techniques in computer vision and AI in a new way to generate content descriptions of images in a textual form automatically. The text retrieval is done by IBM SearchManager for AIX.The system is implemented on IBM2 RISC Sytem/60003 using AIX4. It has already been tested with 1200 images.
ID:383
CLASS:4
Title: Collaborative multi-strategy classification: application to per-pixel analysis of images
Abstract: This paper presents a new process of collaborative multi-step multi-strategy classification of complex data. Our goal is to be able to handle in the same system several instances of classifiers in order to make them collaborate. In this paper, we highlight how the classifiers collaborate. We present the implementation of our method dedicated to remote sensing images. Finally, we validate it with a pixel based classification application.
ID:384
CLASS:4
Title: Analysis and reconstruction of the tiling of Alcazar in Seville using computer vision tools
Abstract: This paper describes different tools, developed by the authors, for the comprehensive analysis and cataloguing of Islamic design patterns from digital images, in the context of the Plane Symmetry Groups theory. For validation of the results, was chosen the ceramic tiles of the Real Alcazar of Seville (Spain), a palace built in the 14th century, highly decorated with ceramic mosaics whose patterns were designed using a specific technique called "lacer&iacute;as" (interweaving manufacturing technique in tiling, where the pieces seem to be interweaved). This fact together with its handcraft nature has required the development of specific stages for design pattern analysis.
ID:385
CLASS:4
Title: Fast online pointer analysis
Abstract: Pointer analysis benefits many useful clients, such as compiler optimizations and bug finding tools. Unfortunately, common programming language features such as dynamic loading, reflection, and foreign language interfaces, make pointer analysis difficult. This article describes how to deal with these features by performing pointer analysis online during program execution. For example, dynamic loading may load code that is not available for analysis before the program starts. Only an online analysis can analyze such code, and thus support clients that optimize or find bugs in it. This article identifies all problems in performing Andersen's pointer analysis for the full Java language, presents solutions to these problems, and uses a full implementation of the solutions in a Java virtual machine for validation and performance evaluation. Our analysis is fast: On average over our benchmark suite, if the analysis recomputes points-to results upon each program change, most analysis pauses take under 0.1 seconds, and add up to 64.5 seconds.
ID:386
CLASS:4
Title: Domain decomposition for multiresolution analysis
Abstract: This paper describes a method for converting an arbitrary mesh with irregular connectivity to a semi-regular multiresolution representation. A shape image encoding geometric and differential properties of the input model is computed. Standard image processing operations lead to an initial decomposition of the model that conforms to its salient features. A triangulation step performed on the resulting partition in image space, followed by resampling and multiresolution analysis in object space, complete the procedure. The conversion technique is automatic, takes into account surface properties for deriving a base domain, and is computationally efficient as the bulk of the processing is carried out in image space. Besides domain decomposition, our image-based approach to handling geometry may be used in the context of related applications, including model simplification, remeshing, and wireframe generation.
ID:387
CLASS:4
Title: A statistical method for binary classification of images
Abstract: The classification of documents with sparse text, and video analysis, relies on accurate image classification. We herein present a method for binary classification that accommodates any number of individual classifiers. Each individual classifier is defined by the critical point between its two means, and its relative weighting is inversely proportional to its expected error rate. Using 10 simple image analysis metrics, we distinguish a set of "natural" and "city" scenes, providing a "semantically meaningful" classification. The optimal combination of 5 of these 10 classifiers provides 85.8% accuracy on a small (120 image) feasibility corpus. When this feasibility corpus is then split into half training and half testing images, the mean accuracy of the optimum set of classifiers was 81.7%. Accuracy as high as 90% was obtained for the test set when training percentage was increased. These results demonstrate that an accurate classifier can be constructed from a large pool of simple classifiers through the use of the statistical ("Normal") classification method described herein.
ID:388
CLASS:4
Title: Integrated text and image understanding for document understanding
Abstract: Because of the complexity of documents and the variety of applications which must be supported, document understanding requires the integration of image understanding with text understanding. Our document understanding technology is implemented in a system called IDUS (Intelligent Document Understanding System), which creates the data for a text retrieval application and the automatic generation of hypertext links. This paper summarizes the areas of research during IDUS development where we have found the most benefit from the integration of image and text understanding.
ID:389
CLASS:4
Title: Image classification for mobile web browsing
Abstract: It is difficult for users of mobile devices such as cellular phones equipped with a small screen and a poor input interface to browse Web pages designed for desktop PCs with large displays. Many studies and commercial products have tried to solve this problem. Web pages include images that have various roles such as site menus, line headers for itemization, and page titles. However, most studies of mobile Web browsing haven't paid much attention to the roles of Web images. In this paper, we define eleven Web image categories according to their roles and use these categories for proper Web image handling. We manually categorized 3,901 Web images collected from forty Web sites and extracted image features of each category according to the classification. By making use of the extracted features, we devised an automatic Web image classification method. Furthermore, we evaluated the automatic classification of real Web pages and achieved up to 83.1% classification accuracy. We also implemented an automatic Web page scrolling system as an application of our automatic image classification method.
ID:390
CLASS:4
Title: Digital photo similarity analysis in frequency domain and photo album compression
Abstract: With the increasing popularity of digital camera, organizing and managing the large collection of digital photos effectively are therefore required. In this paper, we study the techniques of photo album sorting, clustering and compression in DCT frequency domain without having to decompress JPEG photos into spatial domain firstly. We utilize the first several non-zero DCT coefficients to build our feature set and calculate the energy histograms in frequency domain directly. We then calculate the similarity distances of every two photos, and perform photo album sorting and adaptive clustering algorithms to group the most similar photos together. We further compress those clustered photos by a MPEG-like algorithm with variable IBP frames and adaptive search windows. Our methods provide a compact and reasonable format for people to store and transmit their large number of digital photos. Experiments prove that our algorithm is efficient and effective for digital photo processing.
ID:391
CLASS:4
Title: Interfacing with C.H.A.A.T. (Cultural Heritage Assisted Analysis Tools)
Abstract: User-interface and functionality's of a system oriented to support the analysis of the conservation state of historical building monuments are presented. A main aspect of the system is its capability to simulate possible visual scenarios of evolution.
ID:392
CLASS:4
Title: A programming system for the on-line analysis of biomedical images
Abstract: A preliminary description of the software for a computer-display system is given with special emphasis on the man-machine interaction. This system is intended for a wide variety of biomedical applications. As an example, the methods are applied to the karyotyping of chromosomes. The system is separated into four programming tasks: picture transformations, file maintenance, picture structuring, and display management. Picture structuring is considered as the vehicle for man-machine communication. A prototype data format for pictures, called a picture-form, is developed. Structure operators are defined which manipulate picture-forms to produce new picture-forms. Many of the ideas are taken from the symbolic mathematical laboratory at MIT conceived by Marvin Minsky.
ID:393
CLASS:4
Title: The Compositional Far Side of Image Computation
Abstract: Symbolic image computation is the most fundamental computationin BDD-based sequential system optimization and formal verification.In this paper, we explore the use of over-approximationand BDD minimization with donýt cares during image computation.Our new method, based on the partitioned representation ofthe transition relation, consists of three phases: First, the model istreated as a set of loosely coupled components, and over-approximateimages are computed to minimize the transition relation ofeach component. A refined overall image is then computed usingthe simplified transition relation. Finally, the exact image isobtained by a clipping operation that recovers all previous over-approximations.Since BDD minimization employs constraints on thenext-state variables of the transition relation, instead of the customaryconstraints on the present-state variables, we call the resultingmethod far side image computation.The new method can be implemented on top of any image computationalgorithm that is based on the partitioned transition relation.(For example, IWLS95, MLP, and Fine-Grain.)We demonstrate the effectiveness of our approach by experimentson models ranging from easy to hard: The new method wins significantlyover the best known algorithms so far in both CPU timeand memory usage, especially on the hard models.
ID:394
CLASS:4
Title: A new hardware architecture for performing the gridding of DNA microarray images
Abstract: DNA microarray technologies are an essential part of modern biomedical research. The analysis of DNA microarray images allows the identification of gene expressions in order to drawn biologically meaningful conclusions for applications that ranges from the genetic profiling to the diagnosis of oncology diseases. Unfortunately, DNA microarray technology has a high variation of data quality. Therefore, in order to obtain reliable results, complex and extensive image analysis algorithms should be applied before actual DNA microarray information can be used for biomedical purpose. In this paper, we present a novel hardware acceleration architecture specifically designed to process DNA microarray images. The proposed architecture uses several units working in a single instruction-multiple data fashion managed by a microprocessor core. An FPGA-based prototypal implementation of the developed architecture is presented. Experimental results on several realistic DNA microarray images show a reduction of the computation time of one order of magnitude if compared with previously developed software-based approach.
ID:395
CLASS:4
Title: IRS: a hierarchical knowledge based system for aerial image interpretation
Abstract: A knowledge based architecture for the interpretation of aerial images is presented. The Image Recognition System (IRS) utilises a multiresolution perceptual clustering methodology as a robust alternative to the more traditional edge or region based approaches. Initially, data driven feature generation and primary perceptual clustering is performed independently for two or more reduced resolution versions of the image. A Rule Based Frame System (RBFS) is then used to instantiate more complex geometrical structures from symbolic multiresolution feature representations. Final interpretation is achieved by using knowledge of contextual relations between objects in the domain.
ID:396
CLASS:4
Title: A generic approach to the static analysis of concurrent programs with procedures
Abstract: We present a generic aproach to the static analysis of concurrent programs with procedures. We model programs as communicating pushdown systems. It is known that typical dataflow problems for this model are undecidable, because the emptiness problem for the intersection of context-free languages, which is undecidable, can be reduced to them. In this paper we propose an algebraic framework for defining abstractions (upper approximations) of context-free languages. We consider two classes of abstractions: finite-chain abstractions, which are abstractions whose domains do not contain any infinite chains, and commutative abstractions corresponding to classes of languages that contain a word if and only if they contain all its permutations. We show how to compute such approximations by combining automata theoretic techniques with algorithms for solving systems of polynomial inequations in Kleene algebras.
ID:397
CLASS:4
Title: A survey of image registration techniques
Abstract: Registration is a fundamental task in image processing used to match two or more pictures taken, for example, at different times, from different sensors, or from different viewpoints. Virtually all large systems which evaluate images require the registration of images, or a closely related operation, as an intermediate step. Specific examples of systems where image registration is a significant component include matching a target with a real-time image of a scene for target recognition, monitoring global land usage using satellite images, matching stereo images to recover shape for autonomous navigation, and aligning images from different medical modalities for diagnosis.Over the years, a broad range of techniques has been developed for various types of data and problems.   These techniques have been independently studied for several different applications, resulting in a large body of research. This paper organizes this material by establishing the relationship between the variations in the images and the type of registration techniques which can most appropriately be applied. Three major types of variations are distinguished. The first type are the variations due to the differences in acquisition which cause the images to be misaligned. To register images, a spatial transformation is found which will remove these variations. The class of transformations which must be searched to find the optimal transformation is determined by knowledge about the variations of this type. The transformation class in turn influences the general technique that should be taken. The second type of variations are those which are also due to differences in acquisition, but cannot be modeled easily such as lighting and atmospheric conditions. This type usually effects intensity values, but they may also be spatial, such as perspective distortions. The third type of variations are differences in the images that are of interest such as object movements, growths, or other scene changes. Variations of the second and third type are not directly removed by registration, but they make registration more difficult since an exact match is no longer possible. In particular, it is critical that variations of the third type are not removed. Knowledge about the characteristics of each type of variation effect the choice of feature space, similarity measure, search space,   and  search strategy which will make up the final technique. All registration techniques can be viewed as different combinations of these choices. This framework is useful for understanding the merits and relationships between the wide variety of existing techniques and for assisting in the selection of the most suitable technique for a specific problem.
ID:398
CLASS:4
Title: Image clustering with tensor representation
Abstract: We consider the problem of image representation and clustering. Traditionally, an n1 x n2 image is represented by a vector in the Euclidean space &Ropf; n1 x n2. Some learning algorithms are then applied to these vectors in such a high dimensional space for dimensionality reduction, classification, and clustering. However, an image is intrinsically a matrix, or the second order tensor. The vector representation of the images ignores the spatial relationships between the pixels in an image. In this paper, we introduce a tensor framework for image analysis. We represent the images as points in the tensor space Rn1 mathcal Rn2 which is a tensor product of two vector spaces. Based on the tensor representation, we propose a novel image representation and clustering algorithm which explicitly considers the manifold structure of the tensor space. By preserving the local structure of the data manifold, we can obtain a tensor subspace which is optimal for data representation in the sense of local isometry. We call it TensorImage approach. Traditional clustering algorithm such as k-means is then applied in the tensor subspace. Our algorithm shares many of the data representation and clustering properties of other techniques such as Locality Preserving Projections, Laplacian Eigenmaps, and spectral clustering, yet our algorithm is much more computationally efficient. Experimental results show the efficiency and effectiveness of our algorithm.
ID:399
CLASS:5
Title: Statistical query translation models for cross-language information retrieval
Abstract: Query translation is an important task in cross-language information retrieval (CLIR), which aims to determine the best translation words and weights for a query. This article presents three statistical query translation models that focus on the resolution of query translation ambiguities. All the models assume that the selection of the translation of a query term depends on the translations of other terms in the query. They differ in the way linguistic structures are detected and exploited. The co-occurrence model treats a query as a bag of words and uses all the other terms in the query as the context for translation disambiguation. The other two models exploit linguistic dependencies among terms. The noun phrase (NP) translation model detects NPs in a query, and translates each NP as a unit by assuming that the translation of a term only depends on other terms within the same NP. Similarly, the dependency translation model detects and translates dependency triples, such as verb-object, as units. The evaluations show that linguistic structures always lead to more precise translations. The experiments of CLIR on TREC Chinese collections show that all three models have a positive impact on query translation and lead to significant improvements of CLIR performance over the simple dictionary-based translation method. The best results are obtained by combining the three models.
ID:400
CLASS:5
Title: Inferential language models for information retrieval
Abstract: Language modeling (LM) has been widely used in IR in recent years. An important operation in LM is smoothing of the document language model. However, the current smoothing techniques merely redistribute a portion of term probability according to their frequency of occurrences only in the whole document collection. No relationships between terms are considered and no inference is involved. In this article, we propose several inferential language models capable of inference using term relationships. The inference operation is carried out through a semantic smoothing either on the document model or query model, resulting in document or query expansion. The proposed models implement some of the logical inference capabilities proposed in the previous studies on logical models, but with necessary simplifications in order to make them tractable. They are a good compromise between inference power and efficiency. The models have been tested on several TREC collections, both in English and Chinese. It is shown that the integration of term relationships into the language modeling framework can consistently improve the retrieval effectiveness compared with the traditional language models. This study shows that language modeling is a suitable framework to implement basic inference operations in IR effectively.
ID:401
CLASS:5
Title: Creating and exploiting a comparable corpus in cross-language information retrieval
Abstract: We present a method for creating a comparable text corpus from two document collections in different languages. The collections can be very different in origin. In this study, we build a comparable corpus from articles by a Swedish news agency and a U.S. newspaper. The keys with best resolution power were extracted from the documents of one collection, the source collection, by using the relative average term frequency (RATF) value. The keys were translated into the language of the other collection, the target collection, with a dictionary-based query translation program. The translated queries were run against the target collection and an alignment pair was made if the retrieved documents matched given date and similarity score criteria. The resulting comparable collection was used as a similarity thesaurus to translate queries along with a dictionary-based translator. The combined approaches outperformed translation schemes where dictionary-based translation or corpus translation was used alone.
ID:402
CLASS:5
Title: Precision recall with user modeling (PRUM): Application to structured information retrieval
Abstract: Standard Information Retrieval (IR) metrics are not well suited for new paradigms like XML or Web IR in which retrievable information units are document elements and/or sets of related documents. Part of the problem stems from the classical hypotheses on the user models: They do not take into account the structural or logical context of document elements or the possibility of navigation between units. This article proposes an explicit and formal user model that encompasses a large variety of user behaviors. Based on this model, we extend the probabilistic precision-recall metric to deal with the new IR paradigms.
ID:403
CLASS:5
Title: Context-Aware, adaptive information retrieval for investigative tasks
Abstract: We are building an intelligent information system to aid users in their investigative tasks, such as detecting fraud. In such a task, users must progressively search and analyze relevant information before drawing a conclusion. In this paper, we address how to help users find relevant informa-tion during an investigation. Specifically, we present a novel approach that can improve information retrieval by exploiting a user's investigative context. Compared to existing retrieval systems, which are either context insensitive or leverage only limited user context, our work offers two unique contributions. First, our system works with users cooperatively to build an investigative context, which is otherwise very difficult to capture by machine or human alone. Second, we develop a context-aware method that can adaptively retrieve and evaluate information relevant to an ongoing investigation. Experiments show that our approach can improve the relevance of retrieved information significantly. As a result, users can fulfill their investigative tasks more efficiently and effectively.
ID:404
CLASS:5
Title: Effect of relationships between words on Japanese information retrieval
Abstract: Two Japanese-language information retrieval (IR) methods that enhance retrieval effectiveness by utilizing the relationships between words are proposed. The first method uses dependency relationships between words in a sentence. The second method uses proximity relationships, particularly information about the ordered co-occurrence of words in a sentence, to approximate the dependency relationships between them. A Structured Index has been constructed for these two methods, which represents the dependency relationships between words in a sentence as a set of binary trees. The Structured Index is created by morphological analysis and dependency analysis based on simple template matching and compound noun analysis derived from word statistics. Through retrieval experiments using the Japanese test collection for information retrieval systems (NTCIR-1, the NACSIS Test Collection for IR systems), it is shown that these two methods offer superior retrieval effectiveness compared with the TF--IDF method, and are effective with different databases and diverse search topics sets. There is little difference in retrieval effectiveness between these two methods.
ID:405
CLASS:5
Title: Determining the functionality features of an intelligent interface to an information retrieval system
Abstract: In this paper, we propose a method for specifying the functionality of an intelligent interface to large-scale information retrieval systems, and for implementing those functions in an operational environment. The method is based on a progressive, three-stage model of intelligent information support; a high-level cognitive task analysis of the information retrieval problem; a low-level specification of the host system functionality; and, derivation of explicit relations between the system functions and the cognitive tasks. This method is applied, by example, in the context of the European Space Agency Information Retrieval Service, with some specific suggestions for implementation of a stage one intelligent interface to that system.
ID:406
CLASS:5
Title: XIRQL: An XML query language based on information retrieval concepts
Abstract: XIRQL ("circle") is an XML query language that incorporates imprecision and vagueness for both structural and content-oriented query conditions. The corresponding uncertainty is handled by a consistent probabilistic model. The core features of XIRQL are (1) document ranking based on index term weighting, (2) specificity-oriented search for retrieving the most relevant parts of documents, (3) datatypes with vague predicates for dealing with specific types of content and (4) structural vagueness for vague interpretation of structural query conditions. A XIRQL database may contain several classes of documents, where all documents in a class conform to the same DTD; links between documents also are supported. XIRQL queries are translated into a path algebra, which can be processed by our HyREX retrieval engine.
ID:407
CLASS:5
Title: A study of smoothing methods for language models applied to information retrieval
Abstract: Language modeling approaches to information retrieval are attractive and promising because they connect the problem of retrieval with that of language model estimation, which has been studied extensively in other application areas such as speech recognition. The basic idea of these approaches is to estimate a language model for each document, and to then rank documents by the likelihood of the query according to the estimated language model. A central issue in language model estimation is smoothing, the problem of adjusting the maximum likelihood estimator to compensate for data sparseness. In this article, we study the problem of language model smoothing and its influence on retrieval performance. We examine the sensitivity of retrieval performance to the smoothing parameters and compare several popular smoothing methods on different test collections. Experimental results show that not only is the retrieval performance generally sensitive to the smoothing parameters, but also the sensitivity pattern is affected by the query type, with performance being more sensitive to smoothing for verbose queries than for keyword queries. Verbose queries also generally require more aggressive smoothing to achieve optimal performance. This suggests that smoothing plays two different role---to make the estimated document language model more accurate and to "explain" the noninformative words in the query. In order to decouple these two distinct roles of smoothing, we propose a two-stage smoothing strategy, which yields better sensitivity patterns and facilitates the setting of smoothing parameters automatically. We further propose methods for estimating the smoothing parameters automatically. Evaluation on five different databases and four types of queries indicates that the two-stage smoothing method with the proposed parameter estimation methods consistently gives retrieval performance that is close to---or better than---the best results achieved using a single smoothing method and exhaustive parameter search on the test data.
ID:408
CLASS:5
Title: Online query refinement on information retrieval systems: a process model of searcher/system interactions
Abstract: This article reports findings of empirical research that investigated information searchers' online query refinement process. Prior studies have recognized the information specialists' role in helping searchers articulate and refine queries. Using a semantic network and a Problem Behavior Graph to represent the online search process, our study revealed that searchers also refined their own queries in an online task environment. The information retrieval system played a passive role in assisting online query refinement, which was, however, or that confirmed Taylor's four-level query formulation model. Based on our empirical findings, we proposed using a process model to facilitate and improve query refinement in an online environment. We believe incorporating this model into retrieval systems can result in the design of more &ldquo;intelligent&rdquo; and useful information retrieval systems.
ID:409
CLASS:5
Title: An adaptive information retrieval system based on associative networks
Abstract: In this paper we present a multilingual information retrieval system that provides access to Tourism information by exploiting the intuitiveness of natural language. In particular, we describe the knowledge representation model underlying the information retrieval system. This knowledge representation approach is based on associative networks and allows the definition of semantic relationships between domain-intrinsic information items. The network structure is used to define weighted associations between information items and augments the system with a fuzzy search strategy. This particular search strategy is performed by a constrained spreading activation algorithm that implements information retrieval on associative networks. Strictly speaking, we take the relatedness of terms into account and show, how this fuzzy search strategy yields beneficial results and, moreover, determines highly associated matches to users' queries. Thus, the combination of the associative network and the constrained spreading activation approach constitutes a search algorithm that evaluates the relatedness of terms and, therefore, provides a means for implicit query expansion.
ID:410
CLASS:5
Title: Embedding web-based statistical translation models in cross-language information retrieval
Abstract: Although more and more language pairs are covered by machine translation (MT) services, there are still many pairs that lack translation resources. Cross-language information retrieval (CLIR) is an application that needs translation functionality of a relatively low level of sophistication, since current models for information retrieval (IR) are still based on a bag of words. The Web provides a vast resource for the automatic construction of parallel corpora that can be used to train statistical translation models automatically. The resulting translation models can be embedded in several ways in a retrieval model. In this article, we will investigate the problem of automatically mining parallel texts from the Web and different ways of integrating the translation models within the retrieval process. Our experiments on standard test collections for CLIR show that the Web-based translation models can surpass commercial MT systems in CLIR tasks. These results open the perspective of constructing a fully automatic query translation device for CLIR at a very low cost.
ID:411
CLASS:5
Title: Retrieval effectiveness of an ontology-based model for information selection
Abstract: Technology in the field of digital media generates huge amounts of nontextual information, audio, video, and images, along with more familiar textual information. The potential for exchange and retrieval of information is vast and daunting. The key problem in achieving efficient and user-friendly retrieval is the development of a search mechanism to guarantee delivery of minimal irrelevant information (high precision) while insuring relevant information is not overlooked (high recall). The traditional solution employs keyword-based search. The only documents retrieved are those containing user-specified keywords. But many documents convey desired semantic information without containing these keywords. This limitation is frequently addressed through query expansion mechanisms based on the statistical co-occurrence of terms. Recall is increased, but at the expense of deteriorating precision. One can overcome this problem by indexing documents according to context and meaning rather than keywords, although this requires a method of converting words to meanings and the creation of a meaning-based index structure. We have solved the problem of an index structure through the design and implementation of a concept-based model using domain-dependent ontologies. An ontology is a collection of concepts and their interrelationships that provide an abstract view of an application domain. With regard to converting words to meaning, the key issue is to identify appropriate concepts that both describe and identify documents as well as language employed in user requests. This paper describes an automatic mechanism for selecting these concepts. An important novelty is a scalable disambiguation algorithm that prunes irrelevant concepts and allows relevant ones to associate with documents and participate in query generation. We also propose an automatic query expansion mechanism that deals with user requests expressed in natural language. This mechanism generates database queries with appropriate and relevant expansion through knowledge encoded in ontology form. Focusing on audio data, we have constructed a demonstration prototype. We have experimentally and analytically shown that our model, compared to keyword search, achieves a significantly higher degree of precision and recall. The techniques employed can be applied to the problem of information selection in all media types.
ID:412
CLASS:5
Title: Content-based retrieval in hybrid peer-to-peer networks
Abstract: Hybrid peer-to-peer architectures use special nodes to provide directory services for regions of the network ("regional directory services"). Hybrid peer-to-peer architectures are a potentially powerful model for developing large-scale networks of complex digital libraries, but peer-to-peer networks have so far tended to use very simple methods of resource selection and document retrieval. In this paper, we study the application of content-based resource selection and document retrieval to hybrid peer-to-peer networks. The directory nodes that provide regional directory services construct and use the content models of neighboring nodes to determine how to route query messages through the network. The leaf nodes that provide information use content-based retrieval to decide which documents to retrieve for queries. The experimental results demonstrate that using content-based retrieval in hybrid peer-to-peer networks is both more accurate and more efficient for some digital library environments than more common alternatives such as Gnutella 0.6.
ID:413
CLASS:5
Title: Bubbleworld: a new visual information retrieval technique
Abstract: Visualisation has significant advantages over traditional textual lists for improving cognition in information retrieval. To realise these advantages, we identify a set of cognitive principles and usage patterns for information retrieval. We apply these principles and patterns to the design of a prototype visual information retrieval system, Bubbleworld. In Bubbleworld, we apply a variety of visual techniques that successfully transform the internal mental representations of the information retrieval problem to an efficient external view and, through visual cues, provide cognitive amplification at key stages of the information retrieval process. We enhance the knowledge acquisition process by providing query refinement and interaction techniques that facilitate the specification of complex search schemas and integrate these with the mechanisms to incorporate predefined ontological models. We then attempt to validate the new visual techniques through empirical user trials to gain a better understanding of its benefits.
ID:414
CLASS:5
Title: A Chinese dictionary construction algorithm for information retrieval
Abstract: In this article we propose a method for constructing, from raw Chinese text, a statistics-based automatic dictionary. The method makes use of local statistical information (i.e., data within a document) to identify and discard repeated string patterns, which, at an earlier stage, were substrings of legitimate words. Global statistical information (which exists throughout the entire corpus) and contextual constraints are then used for further filtering. The method can be used to alleviate the out-of-vocabulary (OOV) problem, which is commonly found in dictionary-based natural language information-processing applications, e.g., word segmentation. It can handle text corpora dynamically and, further, it does not impose any strict requirements on the size and quality of the training corpora. Based on our method, we constructed Chinese dictionaries from different Chinese corpora. We then applied the words in the constructed dictionaries to indexing in information retrieval (IR). Retrieval performance using such indexes was compared to the same, but based on indexes produced by static dictionaries. Three Chinese corpora using various character-encoding schemes and language styles were used in the experiments. The results show that retrieval using indexes based on the constructed dictionary is effective. This implies that fully automatic Chinese dictionary construction based on dynamic data sources, e.g., from the Internet, for the purposes of IR is feasible. Drawing on the experiment, we were able to make some interesting observations: (1) using only a portion of a dictionary is enough to produce good retrieval performance, e.g., a dictionary consisting of only the 500 highest-frequency strings extracted from the NTCIR 2 Chinese corpus produced as good a retrieval result as using a more complete dictionary with over 100K entries; and (2) complete word segmentation is not a strict requirement for achieving practical information retrieval.
ID:415
CLASS:5
Title: Fast image retrieval using color-spatial information
Abstract: In this paper, we present an image retrieval system that employs both the color and spatial information of images to facilitate the retrieval process. The basic unit used in our technique is a single-colored cluster, which bounds a homogeneous region of that color in an image. Two clusters from two images are similar if they are of the same color and overlap in the image space. The number of clusters that can be extracted from an image can be very large, and it affects the accuracy of retrieval. We study the effect of the number of clusters on retrieval effectiveness to determine an appropriate value for &ldquo;optimal'' performance. To facilitate efficient retrieval, we also propose a multi-tier indexing mechanism called the Sequenced Multi-Attribute Tree (SMAT). We implemented a two-tier SMAT, where the first layer is used to prune away clusters that are of different colors, while the second layer discriminates clusters of different spatial locality. We conducted an experimental study on an image database consisting of 12,000 images. Our results show the effectiveness of the proposed color-spatial approach, and the efficiency of the proposed indexing mechanism.
ID:416
CLASS:5
Title: Integrated information retrieval in a knowledge worker support system
Abstract: This paper describes the design of the information retrieval facilities of an integrated information system called EUROMATH. EUROMATH is an example of a Knowledge Worker Support System: it has been designed specifically to support mathematicians in their research work. EUROMATH is required to provide uniform retrieval facilities for searching in a user's personal data, in a shared database of structured documents and in public, bibliographic databases. The design of information retrieval facilities that satisfy these and other requirements posed several interesting design issues regarding the integration of various retrieval techniques. As well as a uniform query language, designed to be highly usable by the target user group, the retrieval facilities provide expert intermediary functions, i.e. sophisticated support for the retrieval of bibliographic data. This support is achieved using a model of the user, a model of the user's information need and a set of search strategies based on those used by human intermediaries. The expert intermediary facilities include extensive help facilities, automatic query reformulation and browsing of a variety of sources of query terms.
ID:417
CLASS:5
Title: The constituent object parser: syntactic structure matching for information retrieval
Abstract: The Constituent Object Parser is a shallow syntactic parser designed to produce dependency tree representations of syntactic structure that can be used to specify the intended meanings of a sentence more precisely than can the key terms of the sentence alone. It is intended to improve the precision/recall performance of information retrieval and similar text processing applications by providing more powerful matching procedures. The dependency tree representation and the relationship between the intended use of this parser and its design is described, and several problems concerning the processing of ambiguous structures are discussed.
ID:418
CLASS:5
Title: Natural language techniques for intelligent information retrieval
Abstract: Neither natural language processing nor information retrieval is any longer a young field, but the two areas have yet to achieve a graceful interaction. Mainly, the reason for this incompatibility is that information retrieval technology depends upon relatively simple but robust methods, while natural language processing involves complex knowledge-based systems that have never approached robustness. We provide an analysis of areas in which natural language and information retrieval come together, and describe a system that joins the two fields by combining technology, choice of application area, and knowledge acquisition techniques.
ID:419
CLASS:5
Title: Context-aware Retrieval: Exploring a New Environment for Information Retrieval and Information Filtering
Abstract: The opportunities for context-aware computing are fast expanding. Computing systems can be made aware of their environment by monitoring attributes such as their current location, the current time, the weather, or nearby equipment and users. Context-aware computing often involves retrieval of information: it introduces a new aspect to technologies for information delivery; currently these technologies are based mainly on contemporary approaches to information retrieval and information filtering. In this paper, we consider how the closely related, but distinct, topics of information retrieval and information filtering relate to context-aware retrieval. Our thesis is that context-aware retrieval is as yet a sparsely researched and sparsely understood area, and we aim in this paper to make a start towards remedying this.
ID:420
CLASS:5
Title: Information retrieval on the semantic web
Abstract: We describe an approach to retrieval of documents that contain of both free text and semantically enriched markup. In particular, we present the design and implementation prototype of a framework in which both documents and queries can be marked up with statements in the DAML+OIL semantic web language. These statements provide both structured and semi-structured information about the documents and their content. We claim that indexing text and semantic markup together will significantly improve retrieval performance. Our approach allows inferencing to be done over this information at several points: when a document is indexed, when a query is processed and when query results are evaluated.
ID:421
CLASS:5
Title: A language modeling framework for resource selection and results merging
Abstract: Statistical language models have been proposed recently for several information retrieval tasks, including the resource selection task in distributed information retrieval. This paper extends the language modeling approach to integrate resource selection, ad-hoc searching, and merging of results from different text databases into a single probabilistic retrieval model. This new approach is designed primarily for Intranet environments, where it is reasonable to assume that resource providers are relatively homogeneous and can adopt the same kind of search engine. Experiments demonstrate that this new, integrated approach is at least as effective as the prior state-of-the-art in distributed IR.
ID:422
CLASS:5
Title: Probabilistic models of information retrieval based on measuring the divergence from randomness
Abstract: We introduce and create a framework for deriving probabilistic models of Information Retrieval. The models are nonparametric models of IR obtained in the language model approach. We derive term-weighting models by measuring the divergence of the actual term distribution from that obtained under a random process. Among the random processes we study the binomial distribution and Bose--Einstein statistics. We define two types of term frequency normalization for tuning term weights in the document--query matching process. The first normalization assumes that documents have the same length and measures the information gain with the observed term once it has been accepted as a good descriptor of the observed document. The second normalization is related to the document length and to other statistics. These two normalization methods are applied to the basic models in succession to obtain weighting formulae. Results show that our framework produces different nonparametric models forming baseline alternatives to the standard tf-idf model.
ID:423
CLASS:5
Title: The leadermart system and service
Abstract: The LEADERMART System and the service it provides are the product of a coordinated three-phased development project conducted at Lehigh University with National Science Foundation support. The service comprises a user-oriented, multi-optioned information retrieval system featuring innovative as well as standard retrieval techniques. Its available online data bases include nearly one half million documents derived from COMPENDEX, CAS Condensates, and various other document sets, oriented toward the transdisciplinary use of science and engineering information.The service also provides the user with special services which greatly expand his ability to control his information environment as desired or required. Built-in evaluation and feedback mechanisms also insure the most effective results relative to the user's needs over a given period of time.Finally, the possibility of expanding the LEADERMART System and Service into major/minor information nodes offers the user a maximum degree of flexibility and information coverage without modification of his information searching patterns.
ID:424
CLASS:5
Title: Term-specific smoothing for the language modeling approach to information retrieval: the importance of a query term
Abstract: This paper follows a formal approach to information retrieval based on statistical language models. By introducing some simple reformulations of the basic language modeling approach we introduce the notion of importance of a query term. The importance of a query term is an unknown parameter that explicitly models which of the query terms are generated from the relevant documents (the important terms), and which are not (the unimportant terms). The new language modeling approach is shown to explain a number of practical facts of today's information retrieval systems that are not very well explained by the current state of information retrieval theory, including stop words, mandatory terms, coordination level ranking and retrieval using phrases.
ID:425
CLASS:5
Title: Some research problems in automatic information retrieval
Abstract: Information retrieval components are currently incorporated in several types of information systems, including bibliographic retrieval systems, data base management systems and question-answering systems. Some of the problems arising in the real-time environment in which these systems operate are briefly discussed. Certain recent advances in information retrieval research are then mentioned, including the formulation of new probabilistic retrieval models, and the development of automatic document analysis and Boolean query processing techniques.
ID:426
CLASS:5
Title: A structured documents retrieval method supporting attribute-based structure information
Abstract: There are many studies on retrieval methods for structured documents but most of the studies are for those whose structure information is expressed by elements. But when elements are used to describe a document structure, the structure becomes static and difficult to expand. So describing a document structure using attributes is used in many standards. But most of the existing systems support mainly element-based structured documents and do not consider attribute-based ones. Hence they do not support attribute-based structured documents well. So, we propose a new indexing method that supports attribute-based structured documents. In our index scheme, element-based structure information and attribute-based structure information are seamlessly integrated to describe a general document structure. Also, we consider possible searching methods under the proposed index, and implement each method. And then, we experiment each method using the document actually being used in business, then analyze the results.
ID:427
CLASS:5
Title: Exploiting contextual change in context-aware retrieval
Abstract: Information retrieval systems are usually unaware of the context in which they are being used. We believe that exploiting context information to augment existing retrieval methods can lead to increased retrieval precision. This approach is particularly important with the development of wireless mobile information appliances, such as PDAs. Many of these devices are aware of the user's physical context, and this has led to the evolution of context-aware applications. Such applications can automatically utilise the user's current context, e.g. location or ambient temperature. Context-Aware Retrieval is related to traditional Information Retrieval and Information Filtering, but is potentially more challenging due to the often continuous changes in user context. To meet these challenges we suggest a potential advantage of Context-Aware Retrieval: this is that the current context is often changing gradually and semi-predictably. In this paper we suggest new methods based on a context-diary and caching aimed at improving both the precision of relevant retrieved information and the speed/availability of retrieval. The methods can be used, in principle, on top of existing retrieval systems.
ID:428
CLASS:5
Title: Application of aboutness to functional benchmarking in information retrieval
Abstract: Experimental approaches are widely employed to benchmark the performance of an information retrieval (IR) system. Measurements in terms of recall and precision are computed as performance indicators. Although they are good at assessing the retrieval effectiveness of an IR system, they fail to explore deeper aspects such as its underlying functionality and explain why the system shows such performance. Recently, inductive (i.e., theoretical) evaluation of IR systems has been proposed to circumvent the controversies of the experimental methods. Several studies have adopted the inductive approach, but they mostly focus on theoretical modeling of IR properties by using some metalogic. In this article, we propose to use inductive evaluation for functional benchmarking of IR models as a complement of the traditional experiment-based performance benchmarking. We define a functional benchmark suite in two stages: the evaluation criteria based on the notion of "aboutness," and the formal evaluation methodology using the criteria. The proposed benchmark has been successfully applied to evaluate various well-known classical and logic-based IR models. The functional benchmarking results allow us to compare and analyze the functionality of the different IR models.
ID:429
CLASS:5
Title: The effectiveness of query expansion for distributed information retrieval
Abstract: Query expansion has been shown effective for both single database retrieval and for distributed information retrieval where complete collection information is available. One might expect that query expansion would then work for distributed information retrieval when complete collection information is not available. However, this does not appear to be the case. When using local context analysis for query expansion in distributed retrieval with partial information, the most significant reason query expansion does not work is that merging scores of documents retrieved by expanded queries is very difficult. However, we have found that using sampled information for query expansion can give boosts in a single database environment, and that when more information is available, query expansion can work in distributed environments. We also show that most of the benefit of query expansion in distributed retrieval comes from finding good documents, and not from selecting good databases.
ID:430
CLASS:5
Title: A model of multimedia information retrieval
Abstract: Research on multimedia information retrieval (MIR) has recently witnessed a booming interest. A prominent feature of this research trend is its simultaneous but independent materialization within several fields of computer science. The resulting richness of paradigms, methods and systems may, on the long run, result in a fragmentation of efforts and slow down progress. The primary goal of this study is to promote an integration of methods and techniques for MIR by contributing a conceptual model that encompasses in a unified and coherent perspective the many efforts that are being produced under the label of MIR. The model offers a retrieval capability that spans two media, text and images, but also several dimensions: form, content and structure. In this way, it reconciles similarity-based methods with semantics-based ones, providing the guidelines for the design of systems that are able to provide a generalized multimedia retrieval service, in which the existing forms of retrieval not only coexist, but can be combined in any desired manner. The model is formulated in terms of a fuzzy description logic, which plays a twofold role: (1) it directly models semantics-based retrieval, and (2) it offers an ideal framework for the integration of the multimedia and multidimensional aspects of retrieval mentioned above. The model also accounts for relevance feedback in both text and image retrieval, integrating known techniques for taking into account user judgments. The implementation of the model is addressed by presenting a decomposition technique that reduces query evaluation to the processing of simpler requests, each of which can be solved by means of widely known methods for text and image retrieval, and semantic processing. A prototype for multidimensional image retrieval is presented that shows this decomposition technique at work in a significant case.
ID:431
CLASS:5
Title: A connectionist approach to conceptual information retrieval
Abstract: This report proposes that recent advances using low-level connectionist representations offer new possibilities to those interested in free text information retrieval (IR). The AIR system demonstrates that this representation suits the IR domain well, particularly the special problems attending the more sophisticated forms of conceptual retrieval required in legal applications. Also, the natural way in which connectionist representations allow learning means that AIR can avoid the high costs associated with manual indexing while providing comparable results. The paper begins by motivating the importance of legal information retrieval, from the perspectives of both the Law and artificial intelligence (AI). Our approach is then compared to traditional methods for IR, and to more recent work using higher-level symbolic representations from AL After a brief introduction to connectionist representations in general, the AIR system is presented. The paper closes with evidence that this system does, in fact, begin to support the use of those &ldquo;open textured&rdquo; concepts that make the Law both a very difficult and a very illuminating domain for AI research.
ID:432
CLASS:5
Title: Information retrieval and situation theory
Abstract: In 1986, Van Rijsbergen suggested a model of an information retrieval system based on logic. We have advocated in earlier work that a logical approach should be based on a theory of information, Situation Theory, which provides a powerful range of concepts, and is useful for modelling documents and queries for the purpose of information retrieval. We also showed that Situation Theory provides a framework to represent different types of information retrieval models, thus allowing speculation on their properties and their characterization language. This paper is an essay to convince the reader that Situation Theory presents many characteristics that are both adequate and appropriate for the modelling and the study of information retrieval.
ID:433
CLASS:5
Title: The use of mediation and ontology technologies for software component information retrieval
Abstract: Component Based Developed aims at constructing software through the inter-relationship between pre-existing components. However, these components should be bound to a specific application domain in order to be effectively reused. Reusable domain components and  Their related documentation are usually stored in a great variety of data sources. Thus, a possible solution for accessing this information is to use a software layer that integrates different component information sources. We present a component information integration data layer, based on mediators. Through mediators, domain ontology acts as a technique/formalism for specifying ontological commitments or agreements between component users and providers, enabling more accurate software component information search.
ID:434
CLASS:5
Title: Experimental evaluation of information retrieval through a teletypewriter
Abstract: Experiments designed to evaluate the capabilities of mechanized information retrieval systems, with emphasis on interactive (man-machine) language and on some of the mechanical and psychological limitations in their design, were conducted at the Moore School Information Systems Laboratory. The basic assumption of the research is that an information retrieval system that provides for man-machine dialogue at a remote inquiry terminal should provide a searcher with many of the tools which would be available to him were he actually performing his search at a library or repository of documents. Factors involved in evaluation of such a system include ease of use, learning time, and effectiveness of actual retrieval. Three experiments and the conclusions resulting from them are detailed.
ID:435
CLASS:5
Title: The use of normal multiplication tables for information storage and retrieval
Abstract: This paper describes a method for the organization and retrieval of attribute based information systems, using the normal multiplication table as a directory for the information system. Algorithms for the organization and retrieval of information are described. This method is particularly suitable for queries requesting a group of information items, all of which possess a particular set of attributes (and possibly some other attributes as well). Several examples are given; the results with respect to the number of disk accesses and disk space are compared to other common approaches. Algorithms evaluating the appropriateness of the above approach to a given information system are described. For a certain class of information systems, the normal multiplication table method yields far more rapid retrieval with a more economical space requirement than conventional systems. Moreover this method incorporates an improved modification of the inverted file technique.
ID:436
CLASS:5
Title: Improving the effectiveness of information retrieval with local context analysis
Abstract: Techniques for automatic query expansion have been extensively studied in information research as a means of addressing the word mismatch between queries and documents. These techniques can be categorized as either global or local. While global techniques rely on analysis of a whole collection to discover word relationships, local techniques emphasize analysis of the top-ranked documents retrieved for a query. While local techniques have shown to be more effective that global techniques in general, existing local techniques are not robust and can seriously hurt retrieved when few of the retrieval documents are relevant. We propose a new technique, called local context analysis, which selects expansion terms based on cooccurrence with the query terms within the  top-ranked documents. Experiments on a number of collections, both English and non-English, show that local context analysis offers more effective and consistent retrieval results. 
ID:437
CLASS:5
Title: Evaluating the performance of distributed architectures for information retrieval using a variety of workloads
Abstract: The information explosion across the Internet and elswhere offers access to an increasing number of document collections. In order for users to effectively access these collections, information retrieval (IR) systems must provide coordinated, concurrent, and distributed access. In this article, we explore how to achieve scalable performance in a distributed system for collection sizes ranging from 1GB to 128GB. We implement a fully functional distributed IR system based on a multithreaded version of the Inquery simulation model. We measure performance as a function of system parameters such as client command rate, number of document collections, ter ms per query, query term frequency, number of answers returned, and command mixture. Our results show that it is important to model both  query and document commands because the heterogeneity of commands significantly impacts performance. Based on our results, we recommend simple changes to the prototype and  evaluate the changes using the simulator. Because of the significant resource demands of information retrieval, it is not difficult to generate workloads that overwhelm system resources regardless of the architecture. However under some realistic workloads, we demonstrate system organizations for which response time gracefully degrades as the workload increases and performance scales with the number of processors. This scalable architecture includes a surprisingly small number of brokers through which a large number of clients and servers communicate.
ID:438
CLASS:5
Title: On Relevance, Probabilistic Indexing and Information Retrieval
Abstract: This paper reports on a novel technique for literature indexing and searching in a mechanized library system. The notion of relevance is taken as the key concept in the theory of information retrieval and a comparative concept of relevance is explicated in terms of the theory of probability. The resulting technique called &ldquo;Probabilistic Indexing,&rdquo; allows a computing machine, given a request for information, to make a statistical inference and derive a number (called the &ldquo;relevance number&rdquo;) for each document, which is a measure of the probability that the document will satisfy the given request. The result of a search is an ordered list of those documents which satisfy the request ranked according to their probable relevance.The paper goes on to show that whereas in a conventional library system the cross-referencing (&ldquo;see&rdquo; and &ldquo;see also&rdquo;) is based solely on the &ldquo;semantical closeness&rdquo; between index terms, statistical measures of closeness between index terms can be defined and computed. Thus, given an arbitrary request consisting of one (or many) index term(s), a machine can elaborate on it to increase the probability of selecting relevant documents that would not otherwise have been selected.Finally, the paper suggests an interpretation of the whole library problem as one where the request is considered as a clue on the basis of which the library system makes a concatenated statistical inference in order to provide as an output an ordered list of those documents which most probably satisfy the information needs of the user.
ID:439
CLASS:5
Title: Quality of service transferred to information retrieval: the adaptive information retrieval system
Abstract: Users often quit an information retrieval system (IR system) very frustrated, because they cannot find the information matching their information needs. We have identified the following two main reasons: too high expectations and wrong use of the system. Our approach which addresses both issues is based on the transfer of the concept of Quality of Service to IR systems: The user first negotiates the retrieval success rates with the IR system, so he knows what to expect from the system in advance. Second, by dynamic adaptation to the retrieval context, the IR system tries to improve the user's queries and thereby tries to exploit the underlying information source as best as possible.
ID:440
CLASS:5
Title: \&ldquo;Is this document relevant?\&hellip;probably\&rdquo;: a survey of probabilistic models in information retrieval
Abstract: This article surveys probablistic approaches to modeling information retrieval. The basic concepts of probabilistic approaches to information retrieval are outlined and the principles and assumptions upon which the approaches are based are presented. The various models proposed in the development of IR are described, classified, and compared using a common formalism. New approaches that constitute the basis of future research are described.
ID:441
CLASS:5
Title: Private information retrieval
Abstract: Publicly accessible databases are an indispensable resource for retrieving up-to-date information. But they also pose a significant risk to the privacy of the user, since a curious database operator can follow the user's queries and infer what the user is after. Indeed, in cases where the users' intentions are to be kept secret, users are often cautious about accessing the database. It can be shown that when accessing a single database, to completely guarantee the privacy of the user, the whole database should be down-loaded; namely n bits should be communicated (where n is the number of bits in the database).In this work, we investigate whether by replicating the database, more efficient solutions to the private retrieval problem can be  obtained. We describe schemes that enable a user to access k replicated copies of a database (k&ge;2) and privately retrieve information stored in the database. This means that each individual server (holding a replicated copy of the database) gets no information on the identity of the item retrieved by the user. Our schemes use the replication to gain substantial saving. In particular, we present a two-server scheme with communication complexity O(n1/3).
ID:442
CLASS:5
Title: A study of probability kinematics in information retrieval
Abstract: We analyze the kinematics of probabilistic term weights at retrieval time for different Information Retrieval models. We present four models based on different notions of probabilistic retrieval. Two of these models are based on classical probability theory and can be considered as prototypes of models long in use in Information Retrieval, like the Vector Space Model and the Probabilistic Model. The two other models are based on a logical technique of evaluating the probability of a conditional called imaging; one is a generalization of the other. We analyze the transfer of probabilities occurring in the term space at retrieval time for these four models, compare their retrieval performance using classical test collections, and discuss the results. We believe that our results provide  useful suggestions on how to improve existing probabilistic models of Information Retrieval by taking into consideration term-term similarity.
ID:443
CLASS:5
Title: Incorporating syntactic information into a document retrieval strategy: an investigation
Abstract: This paper deals with mechanisms for performing text retrieval which incorporate a degree of linguistic processing into the overall strategy. We have performed some experiments using parsing of text an a test collection of documents and queries to try and find out exactly if and how parsing could contribute to an overall improvement in retrieval effectiveness. Investigating this topic has led us to the definition of a retrieval strategy which incorporates parsing of query text and a more &ldquo;shallow&rdquo; parsing of document texts, whose retrieval effectiveness is investigated and described. Our results indicate that significant improvements in retrieval effectiveness can be obtained by incorporating such linguistic processing into an overall retrieval strategy.
ID:444
CLASS:5
Title: A probabilistic relational algebra for the integration of information retrieval and database systems
Abstract: We present a probabilistic relational algebra (PRA) which is a generalization of standard relational algebra. In PRA, tuples are assigned probabilistic weights giving the probability that a tuple belongs to a relation. Based on intensional semantics, the tuple weights of the result of a PRA expression always conform to the underlying probabilistic model. We also show for which expressions extensional semantics yields the same results. Furthermore, we discuss complexity issues and indicate possibilities for optimization. With regard to databases, the approach allows for representing imprecise attribute values, whereas for information retrieval, probabilistic document indexing and probabilistic search term weighting can be modeled. We introduce the concept of vague predicates which  yield probabilistic weights instead of Boolean values, thus allowing for queries with vague selection conditions. With these features, PRA implements uncertainty and vagueness in combination with the relational model.
ID:445
CLASS:5
Title: A visual retrieval environment for hypermedia information systems
Abstract: We present a graph-based object model that may be used as a uniform framework for direct manipulation of multimedia information. After an introduction motivating the need for abstraction and structuring mechanisms in hypermedia systems, we introduce the data model and the notion of perspective, a form of data abstraction that acts as a user interface to the system, providing control over the visibility of the objects and their properties. A perspective is defined to include an intension and an extension. The intension is defined in terms of a pattern, a subgraph of the schema graph, and the extension is the set of pattern-matching instances. Perspectives, as well as database schema and instances, are graph structures that can be manipulated in various ways. The resulting uniform approach is well suited to a visual interface. A visual interface for complex information systems provides high semantic power, thus exploiting the semantic expressibility of the underlying data model, while maintaining ease of interaction with the system. In this way, we reach the goal of decreasing cognitive load on the user, with the additional advantage of always maintaining the same interaction style. We present a visual retrieval environment that effectively combines filtering, browsing, and navigation to provide an integrated view of the retrieval problem. Design and implementation issues are outlined for MORE (Multimedia Object Retrieval Environment), a prototype system relying on the proposed model. The focus is on the main user interface functionalities, and actual interaction sessions are presented including schema creation, information loading, and information retrieval.
ID:446
CLASS:5
Title: A network approach to probabilistic information retrieval
Abstract: In this article we show how probabilistic information retrieval based on document components may be implemented as a feedforward (feedbackward) artificial neural network. The network supports adaptation of connection weights as well as the growing of new edges between queries and terms based on user relevance feedback data for training, and it reflects query modification and expansion in information retrieval. A learning rule is applied that can also be viewed as supporting sequential learning using a harmonic sequence learning rate. Experimental results with four standard small collections and a large Wall Street Journal collection (173,219 documents) show that performance of feedback improves substantially over no feedback, and further gains are obtained when queries are expanded  with terms from the feedback documents. The effect is much more pronounced in small collections than in the large collection. Query expansion may be considered as a tool for both precision and recall enhancement. In particular, small query expansion levels of about 30 terms can achieve most of the gains at the low-recall high-precision region, while larger expansion levels continue to provide gains at the high-recall low-precision region of a precision recall curve.
ID:447
CLASS:5
Title: On modeling information retrieval with probabilistic inference
Abstract: This article examines and extends the logical models of information retrieval in the context of probability theory. The fundamental notions of term weights and relevance are given probabilistic interpretations. A unified framework is developed for modeling the retrieval process with probabilistic inference. This new approach provides a common conceptual and mathematical basis for many retrieval models, such as the Boolean, fuzzy set, vector space, and conventional probabilistic models. Within this framework, the underlying assumptions employed by each model are identified, and the inherent relationships between these models are analyzed. Although this article is mainly a theoretical analysis of probabilistic inference for information retrieval, practical methods for estimating the required probabilities are provided by simple examples.
ID:448
CLASS:5
Title: A unified approach to indexing and retrieval of information
Abstract: This paper takes another look at information retrieval. It starts from the purposes of retrieval, looks at what people would like from a retrieval system, builds a conceptual model for how a retrieval system could work and from that determines what and how to do appropriate indexing to fit the model. The approach leads to the idea of the duality of indexing and retrieval. The ideas are illustrated by giving the design of a text based system and of a system to store pictures of faces. It is shown that the underlying mechanisms are the same for both systems and it suggests that other retrieval systems using this approach will have similar structures. Other implications of the approach are that retrieval and indexing can be monitored by the machine and the systems can learn to better respond to human needs. Ongoing research in this area is outlined.
ID:449
CLASS:5
Title: Structured document handling\&mdash;a case for integrating databases and information retrieval
Abstract: In this paper we discuss the structured multimedia documents that will be, or already are, to some degree the communication backbone of the so-called superhighways. It will be shown that storage and retrieval of such documents will best be handled by an integration of database and information retrieval technologies. We assume documents to be structured with the help of standards like SGML/HyTime and represented by the multitude of formats currently used for multimedia data.Starting with an approach based on object-oriented database technology we extend both their functionality on the cost models for query evaluation on one side with multimedia features and on the other with logic-based models of information retrieval to truly combine structure and content information about the documents in question.
ID:450
CLASS:5
Title: Probabilistic information retrieval as a combination of abstraction, inductive learning, and probabilistic assumptions
Abstract: We show that former approaches in probabilistic information retrieval are based on one or two of the three concepts abstraction, inductive learning, and probabilistic assumptions, and we propose a new approach which combines all three concepts. This approach is illustrated for the case of indexing with a controlled vocabulary. For this purpose, we describe a new probabilistic model first, which is then combined with logistic regression, thus yielding a generalization of the original model. Experimental results for the pure theoretical model as well as for heuristic variants are given. Furthermore, linear and logistic regression are compared.
ID:451
CLASS:5
Title: An application of a multimedia cognitive-based information retrieval system (AMCIRS) in mineralogy
Abstract: A Multimedia Cognitive-based Information Retrieval System called AMCIRS which integrates image and text information has been described in [11], [12].The AMCIRS query based mechanism is based on multimedia objects content search using the vector model. The content search process is deduced to the similarity estimation between query and index vectors.The main objective of this paper is to present an application of AMCIRS in Mineralogy. The experimental evaluation of AMCIRS retrieval effectiveness is also given. The retrieval effectiveness is expressed by recall and precision parameters which are the standard measures for the effectivity of the information retrieval systems. We confirmed our assumption that multiple media retrieval has advantages with respect to single media retrieval.
ID:452
CLASS:5
Title: Lexical ambiguity and information retrieval
Abstract: Lexical ambiguity is a pervasive problem in natural language processing. However, little quantitative information is available about the extent of the problem or about the impact that it has on information retrieval systems. We report on an analysis of lexical ambiguity in information retrieval test collections and on experiments to determine the utility of word meanings for separating relevant from nonrelevant documents. The experiments show that there is considerable ambiguity even in a specialized database. Word senses provide a significant separation between relevant and nonrelevant documents, but several factors contribute to determining whether disambiguation will make an improvement in performance. For example, resolving lexical ambiguity was found to have little impact on retrieval effectiveness for documents that have many words in common with the query. Other uses of word sense disambiguation in an information retrieval context are discussed.
ID:453
CLASS:5
Title: Probabilistic information retrieval approach for ranking of database query results
Abstract: We investigate the problem of ranking the answers to a database query when many tuples are returned. In particular, we present methodologies to tackle the problem for conjunctive and range queries, by adapting and applying principles of probabilistic models from information retrieval for structured data. Our solution is domain independent and leverages data and workload statistics and correlations. We evaluate the quality of our approach with a user survey on a real database. Furthermore, we present and experimentally evaluate algorithms to efficiently retrieve the top ranked results, which demonstrate the feasibility of our ranking system.
ID:454
CLASS:5
Title: Semantic term matching in axiomatic approaches to information retrieval
Abstract: A common limitation of many retrieval models, including the recently proposed axiomatic approaches, is that retrieval scores are solely based on exact (i.e., syntactic) matching of terms in the queries and documents, without allowing distinct but semantically related terms to match each other and contribute to the retrieval score. In this paper, we show that semantic term matching can be naturally incorporated into the axiomatic retrieval model through defining the primitive weighting function based on a semantic similarity function of terms. We define several desirable retrieval constraints for semantic term matching and use such constraints to extend the axiomatic model to directly support semantic term matching based on the mutual information of terms computed on some document set. We show that such extension can be efficiently implemented as query expansion. Experiment results on several representative data sets show that, with mutual information computed over the documents in either the target collection for retrieval or an external collection such as the Web, our semantic expansion consistently and substantially improves retrieval accuracy over the baseline axiomatic retrieval model. As a pseudo feedback method, our method also outperforms a state-of-the-art language modeling feedback method.
ID:455
CLASS:5
Title: Content-based multimedia information retrieval: State of the art and challenges
Abstract: Extending beyond the boundaries of science, art, and culture, content-based multimedia information retrieval provides new paradigms and methods for searching through the myriad variety of media all over the world. This survey reviews 100&plus; recent articles on content-based multimedia information retrieval and discusses their role in current research directions which include browsing and search paradigms, user studies, affective computing, learning, semantic queries, new features and media types, high performance indexing, and evaluation techniques. Based on the current state of the art, we discuss the major challenges for the future.
ID:456
CLASS:5
Title: Chinese-Japanese cross language information retrieval: a Han character based approach
Abstract: In this paper, we investigate cross language information retrieval (CLIR) for Chinese and Japanese texts utilizing the Han characters - common ideographs used in writing Chinese, Japanese and Korean (CJK) languages. The Unicode encoding scheme, which encodes the superset of Han characters, is used as a common encoding platform to deal with the multilingual collection in a uniform manner. We discuss the importance of Han character semantics in document indexing and retrieval of the ideographic languages. We also analyse the baseline results of the cross language information retrieval using the common Han characters appeared in both Chinese and Japanese texts.
ID:457
CLASS:5
Title: Topic-structure-based complementary information retrieval and its application
Abstract: A great deal of technology has been developed to help people access the information they require. With advances in the availability of information, information-seeking activities are becoming more sophisticated. This means that information technology must move to the next stage, i.e., enable users to acquire information from multiple perspectives to satisfy diverse needs. For instance, with the spread of digital broadcasting and broadband Internet connection services, infrastructure for the integration of TV programs and the Internet has been developed that enables users to acquire information from different media at the same time to improve information quality and the level of detail. In this paper, we propose a novel content-based join model for data streams (closed captions of videos or TV programs) and Web pages based on the concept of topic structures. We then propose a mechanism based on this model for retrieving complementary Web pages to augment the content of video or television programs. One of the most notable features of this complementary retrieval mechanism is that the retrieved information is not just similar to the video or TV program, but also provides additional information. In addition, we introduce an application system called WebTelop, which augments the content of TV programs in real time by using complementary Web pages. We also describe some experimental results.
ID:458
CLASS:5
Title: Chinese information retrieval based on terms and relevant terms
Abstract: In this article we describe our approach to Chinese information retrieval, where a query is a short natural language description. First, we use automatically extracted short terms from document sets to build indexes and use the short terms in both the query and documents to do initial retrieval. Next, we use long terms extracted from the document collection to reorder the top N retrieved documents to improve precision. Finally, we acquire the relevant terms of the short terms from the Internet and the top retrieved documents and use them to do query expansion. Experiments on the NTCIR-4 CLIR Chinese SLIR sub-collection show that document reranking can both improve the retrieval performance on its own and make a significant contribution to query expansion. The experiments also show that the extended query expansion proposed in this article is more effective than the standard Rocchio query expansion.
ID:459
CLASS:5
Title: Towards effective strategies for monolingual and bilingual information retrieval: Lessons learned from NTCIR-4
Abstract: At the NTCIR-4 workshop, Justsystem Corporation (JSC) and Clairvoyance Corporation (CC) collaborated in the cross-language retrieval task (CLIR). Our goal was to evaluate the performance and robustness of our recently developed commercial-grade CLIR systems for English and Asian languages. The main contribution of this article is the investigation of different strategies, their interactions in both monolingual and bilingual retrieval tasks, and their respective contributions to operational retrieval systems in the context of NTCIR-4. We report results of Japanese and English monolingual retrieval and results of Japanese-to-English bilingual retrieval. In monolingual retrieval analysis, we examine two special properties of the NTCIR experimental design (two levels of relevance and identical queries in multiple languages) and explore how they interact with strategies of our retrieval system, including pseudo-relevance feedback, multi-word term down-weighting, and term weight merging strategies. Our analysis shows that the choice of language (English or Japanese) does not have a significant impact on retrieval performance. Query expansion is slightly more effective with relaxed judgments than with rigid judgments. For better retrieval performance, weights of multi-word terms should be lowered. In the bilingual retrieval analysis, we aim to identify robust strategies that are effective when used alone and when used in combination with other strategies. We examine cross-lingual specific strategies such as translation disambiguation and translation structuring, as well as general strategies such as pseudo-relevance feedback and multi-word term down-weighting. For shorter title topics, pseudo-relevance feedback is a major performance enhancer, but translation structuring affects retrieval performance negatively when used alone or in combination with other strategies. All experimented strategies improve retrieval performance for the longer description topics, with pseudo-relevance feedback and translation structuring as the major contributors.
ID:460
CLASS:5
Title: Chinese OOV translation and post-translation query expansion in chinese--english cross-lingual information retrieval
Abstract: Cross-lingual information retrieval allows users to query mixed-language collections or to probe for documents written in an unfamiliar language. A major difficulty for cross-lingual information retrieval is the detection and translation of out-of-vocabulary (OOV) terms; for OOV terms in Chinese, another difficulty is segmentation. At NTCIR-4, we explored methods for translation and disambiguation for OOV terms when using a Chinese query on an English collection. We have developed a new segmentation-free technique for automatic translation of Chinese OOV terms using the web. We have also investigated the effects of distance factor and window size when using a hidden Markov model to provide disambiguation. Our experiments show these methods significantly improve effectiveness; in conjunction with our post-translation query expansion technique, effectiveness approaches that of monolingual retrieval.
ID:461
CLASS:5
Title: Implicit user modeling for personalized search
Abstract: Information retrieval systems (e.g., web search engines) are critical for overcoming information overload. A major deficiency of existing retrieval systems is that they generally lack user modeling and are not adaptive to individual users, resulting in inherently non-optimal retrieval performance. For example, a tourist and a programmer may use the same word "java" to search for different information, but the current search systems would return the same results. In this paper, we study how to infer a user's interest from the user's search context and use the inferred implicit user model for personalized search. We present a decision theoretic framework and develop techniques for implicit user modeling in information retrieval. We develop an intelligent client-side web search agent (UCAIR) that can perform eager implicit feedback, e.g., query expansion based on previous queries and immediate result reranking based on clickthrough information. Experiments on web search show that our search agent can improve search accuracy over the popular Google search engine.
ID:462
CLASS:5
Title: Semantic similarity methods in wordNet and their application to information retrieval on the web
Abstract: Semantic Similarity relates to computing the similarity between concepts which are not lexicographically similar. We investigate approaches to computing semantic similarity by mapping terms (concepts) to an ontology and by examining their relationships in that ontology. Some of the most popular semantic similarity methods are implemented and evaluated using WordNet as the underlying reference ontology. Building upon the idea of semantic similarity, a novel information retrieval method is also proposed. This method is capable of detecting similarities between documents containing semantically similar but not necessarily lexicographically similar terms. The proposed method has been evaluated in retrieval of images and documents on the Web. The experimental results demonstrated very promising performance improvements over state-of-the-art information retrieval methods.
ID:463
CLASS:5
Title: Information storage and retrieval: a survey and functional description
Abstract: Information Storage and Retrieval (IS&R) encompasses a broad scope of topics ranging from basic techniques for accessing data to sophisticated approaches for the analysis of natural language text and the deduction of information. Within the field, three general areas of investigation can be distinguished not only by their subject matter but also by the types of individuals presently interested in them:(1) Document retrieval,(2) Generalized data management, and(3) Question-answering.A functional description which applies to each of the three areas is presented together with a survey of work being conducted. The similarities and differences of the three areas of IS&R are described. Typical systems which incorporate many of the functions and techniques are described in the appendix.
ID:464
CLASS:5
Title: On the role of a user's knowledge gap in an information retrieval process
Abstract: The main problem in traditional information retrieval systems is an ad-hoc modeling of the interaction with users, which results in a very low retrieval's precision regarding a user's information need. In this paper we discuss the knowledge level of that interaction, i.e. how an analysis of a user's knowledge gap (that initiated the retrieval process) can be used for designing an efficient interaction model, especially regarding the query refinement task. Moreover, we indicate the role that the background knowledge (i.e. a domain ontology) plays in that model. We present conceptually a comprehensive query refinement process that enables a user to fulfil his need in a gradual, step-by-step querying process.This research shows that, complementary to the mainstream IR research that is focused on the improvement of retrieval algorithms, there is a lot of playroom for the improvement of the retrieval process by better modelling user's working context, especially his task and the information need that causes that task.
ID:465
CLASS:5
Title: Context-sensitive information retrieval using implicit feedback
Abstract: A major limitation of most existing retrieval models and systems is that the retrieval decision is made based solely on the query and document collection; information about the actual user and search context is largely ignored. In this paper, we study how to exploit implicit feedback information, including previous queries and clickthrough information, to improve retrieval accuracy in an interactive information retrieval setting. We propose several context-sensitive retrieval algorithms based on statistical language models to combine the preceding queries and clicked document summaries with the current query for better ranking of documents. We use the TREC AP data to create a test collection with search context information, and quantitatively evaluate our models using this test set. Experiment results show that using implicit feedback, especially the clicked document summaries, can improve retrieval performance substantially.
ID:466
CLASS:5
Title: Improving information retrieval effectiveness by assigning context to documents
Abstract: Recently we proposed a new context-based information retrieval system that has better retrieval effectiveness than the traditional key-word systems. In this system, each document is assigned context(s) based on the type of information. This paper presents context categories for documents in computing and information technology domain and offers a new methodology for assigning context to documents in a collection. This methodology showed promising results and the inter-assigner consistency quite comparable to the results reported in literature. The comparative evaluation of context-based information retrieval system with a baseline information retrieval system showed promising results.
ID:467
CLASS:5
Title: Retrieving lightly annotated images using image similarities
Abstract: Users' search needs are often represented by words and images are retrieved according to such textual queries. Annotation words assigned to the stored images are most useful to connect queries to the images. However, due to annotation cost, quite limited amount of annotation words are available in many cases. When annotations are not given at all, there needs to be some techniques that assign annotations automatically. When only a few annotation words are given to each image (lightly annotated), there need to be some enhancement techniques that best use the available annotations. We address the later problem by estimating word associations to fill in the lexical gap between queries and annotations. The model of word associations can be learned from the data. However, since images are only lightly annotated, their sparseness in computing word associations becomes crucial. To compensate the sparseness, we propose a novel data exploration technique in which image similarities contribute to the estimation of word associations on the assumption that similar images have similar semantic concepts. We experimentally show the potential benefit of our approach.
ID:468
CLASS:5
Title: Unified utility maximization framework for resource selection
Abstract: This paper presents a unified utility framework for resource selection of distributed text information retrieval. This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases. With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications. Specifically, when used for database recommendation, the selection is optimized for the goal of high-recall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents). This new model provides a more solid framework for distributed information retrieval. Empirical studies show that it is at least as effective as other state-of-the-art algorithms.
ID:469
CLASS:5
Title: Experiments with a component theory of probabilistic information retrieval based on single terms as document components
Abstract: A component theory of information retrieval using single content terms as component for queries and documents was reviewed and experimented with. The theory has the advantages of being able to (1) bootstrap itself, that is, define initial term weights naturally based on the fact that items are self relevent; (2) make use of within-item term frequencies; (3) account for query-focused and document-focused indexing and retrieval strategies cooperatively; and (4) allow for component-specific feedback if such information is available. Retrieval results with four collections support the effectiveness of all the first three aspects, except for predictive retrieval. At the initial indexing stage, the retrieval theory performed much more consistantly across collections than croft's model and   provided results comparable to Salton's tf*idf approach. An inverse collection term frequency (ICTF) formula was also tested that performed much better than the inverse document frequency (IDF). With full feedback retrospective retrieval, the component theory performed substantially better than Croft's, because of the highly specific nature of document-focused feedback. Repetitive retireval results with partial relevance feedback mirrored those for the retrospective. However, for the important case of predictive retrieval using residual ranking, results were not unequivocal.
ID:470
CLASS:5
Title: Distributed content-based visual information retrieval system on peer-to-peer networks
Abstract: With the recent advances of distributed computing, the limitation of information retrieval from a centralized image collection can be removed by allowing distributed image data sources to interact with each other for data storage sharing and information retrieval. In this article, we present our design and implementation of DISCOVIR: DIStributed COntent-based Visual Information Retrieval system using the Peer-to-Peer (P2P) Network. We describe the system architecture and detail the interactions among various system modules. Specifically, we propose a Firework Query Model for distributed information retrieval, which aims to reduce the network traffic of query passing in the network. We carry out experiments to show the distributed image retrieval system and the Firework information retrieval algorithm. The results show that the algorithm reduces network traffic while increases searching performance.
ID:471
CLASS:5
Title: Parsimonious language models for information retrieval
Abstract: We systematically investigate a new approach to estimating the parameters of language models for information retrieval, called parsimonious language models. Parsimonious language models explicitly address the relation between levels of language models that are typically used for smoothing. As such, they need fewer (non-zero) parameters to describe the data. We apply parsimonious models at three stages of the retrieval process: 1) at indexing time; 2) at search time; 3) at feedback time. Experimental results show that we are able to build models that are significantly smaller than standard models, but that still perform at least as well as the standard approaches.
ID:472
CLASS:5
Title: Belief revision for adaptive information retrieval
Abstract: Applying Belief Revision logic to model adaptive information retrieval is appealing since it provides a rigorous theoretical foundation to model partiality and uncertainty inherent in any information retrieval (IR) processes. In particular, a retrieval context can be formalised as a belief set and the formalised context is used to disambiguate vague user queries. Belief revision logic also provides a robust computational mechanism to revise an IR system's beliefs about the users' changing information needs. In addition, information flow is proposed as a text mining method to automatically acquire the initial IR contexts. The advantage of a belief-based IRsystem is that its IR behaviour is more predictable and explanatory. However, computational efficiency is often a concern when the belief revision formalisms are applied to large real-life applications. This paper describes our belief-based adaptive IR system which is underpinned by an efficient belief revision mechanism. Our initial experiments show that the belief-based symbolic IR model is more effective than a classical quantitative IR model. To our best knowledge, this is the first successful empirical evaluation of a logic-based IR model based on large IR benchmark collections.
ID:473
CLASS:5
Title: History places: A case study for relational database and information retrieval system design
Abstract: This article presents a project-based case study that was developed for students with diverse backgrounds and varied inclinations for engaging technical topics. The project, called History Places, requires that student teams develop a vision for a kind of digital library, propose a conceptual model, and use the model to derive a logical model and information retrieval specification. From these two design representations, students implement a data-driven Web site that enables users to browse content and search by exact and best-match queries. The project brief contains a set of general requirements that promote creative solutions, while also bounding the complexity of the solution space. The article includes teaching notes and a conceptual model, expressed as an enhanced entity-relationship model in UML. The model, consisting of approximately ten entities, contains binary, unary, ternary, and specialization/generalization relationships. The article concludes with some reflections based on the experiences of using this project in six classes over four years.
ID:474
CLASS:5
Title: Mobile content enrichment
Abstract: Delivering an effective mobile search service is challenging for many reasons. Certainly small-screen mobile handsets with limited text input capabilities do not make ideal search devices. In addition, the brevity of Mobile Internet content hampers effective indexing and limits retrieval opportunities. In this paper we focus on this indexing issue and describe an approach that leverages Web search engines as a source of content enrichment. We present an evaluation using a mobile news service that demonstrated significant improvements in search performance compared to a standard benchmark system.
ID:475
CLASS:5
Title: EXPRESS: an experimental interface for factual information retrieval
Abstract: The EXPRESS system has been designed and implemented in order to explore methods for user assistance in accessing complexly structured factual databases, e.g. relational product databases. Terminological support in this area has to take into account that different controlled vocabularies may be used in a variety of attributes spread over several relations. In our approach, traditional thesaurus structures are extended in order to cope with these problems and to encode further domain-specific knowledge. User support in query reformulation is based on this enriched thesaurus as well as on the local evaluation of the retrieved data sets. Concepts for the representation of retrieval strategies in the form of plans and their potential use in future systems are discussed.
ID:476
CLASS:5
Title: Video information retrieval using objects and ostensive relevance feedback
Abstract: In this paper, we present a brief overview of current approaches to video information retrieval (IR) and we highlight its limitations and drawbacks in terms of satisfying user needs. We then describe a method of incorporating object-based relevance feedback into video IR which we believe opens up new possibilities for helping users find information in video archives. Following this we describe our own work on shot retrieval from video archives which uses object detection, object-based relevance feedback and a variation of relevance feedback called ostensive RF which is particularly appropriate for this type of retrieval.
ID:477
CLASS:5
Title: Query expansion using associated queries
Abstract: Hundreds of millions of users each day use web search engines to meet their information needs. Advances in web search effectiveness are therefore perhaps the most significant public outcomes of IR research. Query expansion is one such method for improving the effectiveness of ranked retrieval by adding additional terms to a query. In previous approaches to query expansion, the additional terms are selected from highly ranked documents returned from an initial retrieval run. We propose a new method of obtaining expansion terms, based on selecting terms from past user queries that are associated with documents in the collection. Our scheme is effective for query expansion for web retrieval: our results show relative improvements over unexpanded full text retrieval of 26%--29%, and 18%--20% over an optimised, conventional expansion approach.
ID:478
CLASS:5
Title: Concept mapping vs. web page hyperlinks as an information retrieval interface: preferences of postgraduate culturally diverse learners
Abstract: The principal objective of this research project was to determine if and to what extent cultural factors prescribe interface choices by learners. Concept mapping and standard hyperlinks were offered as choices for information retrieval interfaces. The methods employed were to identify a set of culturally divisive factors, and then to test two different interfaces with a group of culturally diverse, advanced learners. Some of the results had to be ignored due to small sample sizes. The remaining results indicated that most choices, almost irrespective of culture divisive factors, were made in favour of the concept mapping interface. This finding confirmed that of another author in the field. The primary conclusion reached is that concept mapping should be considered as the interface of choice to a knowledge repository to be used by Master's students in Information management.
ID:479
CLASS:5
Title: A decision theoretic approach to information retrieval
Abstract: We present the file search problem in a decision-theoretic framework, and discuss a variation of it that we call the common index problem. The goal of the common index problem is to return the best available record in the file, where best is in terms of a class of user preferences. We use dynamic programming to construct an optimal algorithm using two different optimality criteria, and we develop sufficient conditions for obtaining complete information.
ID:480
CLASS:5
Title: Improving pseudo-relevance feedback in web information retrieval using web page segmentation
Abstract: In contrast to traditional document retrieval, a web page as a whole is not a good information unit to search because it often contains multiple topics and a lot of irrelevant information from navigation, decoration, and interaction part of the page. In this paper, we propose a VIsion-based Page Segmentation (VIPS) algorithm to detect the semantic content structure in a web page. Compared with simple DOM based segmentation method, our page segmentation scheme utilizes useful visual cues to obtain a better partition of a page at the semantic level. By using our VIPS algorithm to assist the selection of query expansion terms in pseudo-relevance feedback in web information retrieval, we achieve 27% performance improvement on Web Track dataset.
ID:481
CLASS:5
Title: The use of cluster hierarchies in hypertext information retrieval
Abstract: The graph-traversal approach to hypertext information retrieval is a conceptualization of hypertext in which the structural aspects of the nodes are emphasized. A user navigates through such hypertext systems by evaluating the semantics associated with links between nodes as well as the information contained in nodes. [Fris88] In this paper we describe an hierarchical structure which effectively supports the graphical traversal of a document collection in a hypertext system. We provide an overview of an interactive browser based on cluster hierarchies. Initial results obtained from the use of the browser in an experimental hypertext retrieval system are presented.
ID:482
CLASS:5
Title: Advances in a Bayesian decision model of user stopping behavior for scanning the output of an information retrieval system
Abstract: The formal modeling of information storage and retrieval systems has been an important element in the analysis and design of these systems. The retrieval mechanism has been viewed as a probablistic decision problem, often involving utilities. One key element is the evaluation of such retrieval systems. In this paper, we focus on the impact of the stopping rule, which determines when the user chooses to stop scanning the list of records retrieved in response to a given query. We shall first trace the evolution of the modelling and use of the stopping rule approach. Then, we shall briefly report on some recent results in our attempt to better model the generation of stopping rules.
ID:483
CLASS:5
Title: Computerised information retrieval systems for open learning
Abstract: The paper starts with a theoretical consideration of the requirements for a computerised information retrieval system to aid open learning within an educational establishment. The requirements for such a system include consideration of the need to fulfill information retrieval objectives and also educational objectives. These requirements are then considered in the context of the theoretical information retrieval work which has been carried out by Belkin and others and takes into account the representation of the user's anomalous state of knowledge. The paper then considers the practical problems of trying to implement such a system. Attention is focussed on the use of current and developing information technology to fulfill both information retrieval and educational objectives. It is shown that current systems as exemplified by PRESTEL, DIALOG, BROWSE and STAF individually will not fulfill the requirements of this system. However, in combination these types of systems should be quite suitable. Another solution is the use of an expert system. The paper also considers the use of an expert system to "replace" the traditional teacher.
ID:484
CLASS:5
Title: Rough sets and information retrieval
Abstract: The theory of rough sets was introduced [PAWLAK82]. It allows us to classify objects into sets of equivalent members based on their attributes. We may then examine any combination of the same objects (or even their attributes) using the resultant classification. The theory has direct applications in the design and evaluation of classification schemes and the selection of discriminating attributes. Pawlak's papers discuss its application in the domain of medical diagnostic systems. Here we apply it to the design of information retrieval systems accessing collections of documents. Advantages offered by the theory are: the implicit inclusion of Boolean logic; term weighting; and the ability to rank retrieved documents. In the first section we describe the theory. This is derived from the work by [PAWLAK84, PAWLAK82] and includes only the most relevant aspects of the theory. In the second we apply it to information retrieval. Specifically, we design the approximation space, search strategies as well as illustrate the application of relevance feedback to improve document indexing. Following this in section three we compare the rough set formalism to the Boolean, vector and fuzzy models of information retrieval. Finally we present a small scale evaluation of rough sets which indicates its potential in information retrieval.
ID:485
CLASS:5
Title: IR-NLI II: applying man-machine interaction and artificial intelligence conceptsto information retrieval
Abstract: This paper addresses the problem of building expert interfaces to information retrieval systems. In particular, the problem of augmenting the capabilities of such interfaces with user modeling features is discussed and the main benefits of this approach are outlined. The paper presents a prototype system called IR-NLI II, devoted to model by means of artificial intelligence techniques the human intermediary to information retrieval systems. The overall organization of the IR-NLI II system is presented, together with a short description of the two main modules implemented so far, namely the Information Retrieval Expert Subsystem and the User Modeling Subsystem. An example of interaction with IR-NLI II is described. Perspectives and future research directions are finally outlined.
ID:486
CLASS:5
Title: Some measures and procedures for evaluation of the user interface in an information retrieval system
Abstract: Planning the evaluation of an information retrieval system involves two steps: first, a determination of performance descriptors and measures appropriate to the system objectives and, secondly, a development of an evaluation design which ensures the effect of variation in components of interest will be isolated and assessed in an unbiased fashion. This paper examines the question of retrieval system evaluation from the perspective of the user. It presents evaluation procedures which are appropriate to this perspective and which can be used to isolate the effect of variation in the user interface to the system. The general procedure is exemplified by an application to evaluation of an experimental OPAC interface.
ID:487
CLASS:5
Title: Query association for effective retrieval
Abstract: We introduce a novel technique for document summarisation which we call query association. Query association is based on the notion that a query that is highly similar to a document is a good descriptor of that document. For example, the user query "richmond football club" is likely to be a good summary of the content of a document that is ranked highly in response to the query. We describe this process of defining, maintaining, and presenting the relationship between a user query and the documents that are retrieved in response to that query. We show that associated queries are an excellent technique for describing a document: for relevance judgement, associated queries are as effective as a simple online query-biased summarisation technique. As future work, we suggest additional uses for query association including relevance feedback and query expansion.
ID:488
CLASS:5
Title: Improving stemming for Arabic information retrieval: light stemming and co-occurrence analysis
Abstract: Arabic, a highly inflected language, requires good stemming for effective information retrieval, yet no standard approach to stem¿ming has emerged. We developed several light stemmers based on heuristics and a statistical stemmer based on co-occurrence for Arabic retrieval. We compared the retrieval effectiveness of our stemmers and of a morphological analyzer on the TREC-2001 data. The best light stemmer was more effective for cross-lan¿guage retrieval than a morphological stemmer which tried to find the root for each word. A repartitioning process consisting of vowel removal followed by clustering using co-occurrence analy¿sis pro¿duced stem classes which were better than no stemming or very light stemming, but still inferior to good light stemming or mor¿phological analysis.
ID:489
CLASS:5
Title: ACQUIRE: agent-based complex query and information retrieval engine
Abstract: The heterogeneous, distributive and voluminous nature of many government and corporate data sources impose severe constraints on meeting the diverse requirements of users who analyze the data. Additionally, communication bandwidth limitations, time constraints, and multiplicity of data formats impose further restrictions on users of these distributed data sources. What is required is a reliable, robust, and efficient data retrieval technique that can access data from distributed data sources while maintaining the autonomy of individual sources. In this paper, we present an Agent-based Complex QUerying and Information Retrieval Engine (ACQUIRE) for large, heterogeneous, and distributed data sources. ACQUIRE acts as a softbot or interface agent by presenting users with the appearance of a single, unified, homogenous data source, against which users can pose high-level declarative queries. ACQUIRE translates each such user query into a set of sub-queries by employing a combination of planning and traditional database query optimization techniques. For each sub-query, ACQUIRE then spawns a corresponding mobile agent, which retrieves data from the appropriate data source. These mobile agents carry with them data-processing code that can be executed at the remote site, thus reducing the size of data returned by the agent. When all mobile agents have returned, ACQUIRE filters and merges the retrieved data and presents the results to the user. Validation experiments on simulated NASA Distributed Active Archive Centers (DAACs) have demonstrated that complex queries can be effectively decomposed and retrieved by this approach, resulting in the twin benefits of improved ease of use and significantly reduced query retrieval times.
ID:490
CLASS:5
Title: Representation issues in information retrieval system design
Abstract: The representation problem confronting information retrieval system designers is outlined in terms of three issues: what to represent, forms of representation, and functions of representation. Questions raised by each of these issues are identified and selected research projects which have begun to explore these questions are described.
ID:491
CLASS:5
Title: Query term disambiguation for Web cross-language information retrieval using a search engine
Abstract: With the worldwide growth of the Internet, research on Cross-Language Information Retrieval (CLIR) is being paid much attention. Existing CLIR approaches based on query translation require parallel corpora or comparable corpora for the disambiguation of translated query terms. However, those natural language resources are not readily available. In this paper, we propose a disambiguation method for dictionary-based query translation that is independent of the availability of such scarce language resources, while achieving adequate retrieval effectiveness by utilizing Web documents as a corpus and using co-occurrence information between terms within that corpus. In the experiments, our method achieved 97% of manual translation case in terms of the average precision.
ID:492
CLASS:5
Title: Evaluation of a simple and effective music information retrieval method
Abstract: We developed, and then evaluated, a music information retrieval (MIR) system based upon the intervals found within the melodies of a collection of 9354 folksongs. The songs were converted to an interval-only representation of monophonic melodies and then fragmented t into length-n subsections called n-grams. The length of these n-grams and the degree to which we precisely represent the intervals are variables analyzed in this paper. We constructed a collection of &ldquo;musical word&rdquo; databases using the text-based, SMART information retrieval system. A group of simulated queries, some of which contained simulated errors, was run against these databases. The results were evaluated using the normalized precision and normalized recall measures. Our concept of &ldquo;musical words&rdquo; shows great merit thus implying that useful MIR systems can be constructed simply and efficiently using pre-existing text-based information retrieval software. Second, this study is a formal and comprehensive evaluation of a MIR system using rigorous statistical analyses to determine retrieval effectiveness.
ID:493
CLASS:5
Title: Using annotated video as an information retrieval interface
Abstract: The ability to deliver appropriate information to learners at the most appropriate time is an essential component of good instruction. In the best learning environments, this information is received in the context of the performance of the skills that are being acquired. This paper explores a technological approach to situated information retrieval by linking materials to segments of a video recording a skill performance. An interface is described where users navigate through a video performance and are presented with information relevant to the current video location. An approach to algorithmically generating interfaces of this type is then presented. The system takes as input annotations that describe a video recording of a performance, translates these annotations into subject terms used to catalog information resources, and then retrieves materials from online database servers using the Z39.50 information retrieval protocol. As an example application, the system was used to generate online teacher professional development materials by linking annotated video of classroom teaching with resources cataloged in the ERIC database.
ID:494
CLASS:5
Title: A general language model for information retrieval
Abstract: Statistical language modeling has been successfully used for speech recognition, part-of-speech tagging, and syntactic parsing. Recently, it has also been applied to information retrieval. According to this new paradigm, each document is viewed as a language sample, and a query as a generation process. The retrieved documents are ranked based on the probabilities of producing a query from the corresponding language models of these documents. In this paper, we will present a new language model for information retrieval, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations. Our model is conceptually simple and intuitive, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples. The experiments with the Wall Street Journal and TREC4 data sets showed that the performance of our model is comparable to that of INQUERY and better than that of another language model for information retrieval. In particular, word pairs are shown to be useful in improving the retrieval performance.
ID:495
CLASS:5
Title: Integration of text retrieval technology into formatted (conventional) information systems
Abstract: Conventional information systems are characterized by data management, which is formatted according to the characterization, prepared by a systems analyst, in conjunction with the user of the new system. The operating principles of these systems enable efficient data management and the resolution of a broad range of problems. At times, systems of this nature do not meet the complex needs of an organization, such as the management of data that is difficult to characterize precisely, or the management of occasional activities that do not justify a separate system, etc.Text retrieval technology provides management of "open" data, as well as a wide range of other data management forms. This paper presents the advantages of integrating text retrieval technology into formatted information systems in order to solve the above problems.
ID:496
CLASS:6
Title: Initial experiments with a mobile robot on cognitive mapping
Abstract: This paper shows how a mobile robot equipped with sonar sensors and an odometer is used to test ideas about cognitive mapping. The robot first explores an office environment and computes a "cognitive map" which is a network of ASRs [1]. The robot generates two networks, one for the outward journey and the other for the journey home. It is shown that both networks are different. The two networks, however, are not merged to form a single network. Instead, the robot attempts to use distance information implicit in the shape of each ASR to find its way home. At random positions in the homeward journey, the robot calculates its orientation towards home. The robot's performances for both problems are evaluated and found to be surprisingly accurate.
ID:497
CLASS:6
Title: Self-localization of home robot ApriAttenda\&trade; based on Monte Carlo approach
Abstract: A self-localization technique for home robot is developed for in-door environment. This simple and robust self-localization approach based on Monte Carlo algorithm recovers the robot from catastrophic position tracking failure or kidnapped condition. Position of the robot at different location in the map are determined by using information obtained by laser range finder attached in front of the robot and marker distance measured by stereo vision camera.
ID:498
CLASS:6
Title: Nonholonomic mobile robot formation control with kinodynamic constraints
Abstract: Formation control of multiple wheeled mobile robots with nonholonomic constraints and limited acceleration has been studied in this paper. A novel l - &phis; control strategy in Leader-Following structure is presented. The key idea of this method is that in short control periods the velocity space restricted by the limited acceleration of robot will be searched, and the velocity which minimizes a formation matching function is chosen so that the robots can achieve the specified formation as soon as possible within their abilities. Instead of using global knowledge, this method uses only local sensing information and little communication so it is very suitable for the distributed multi-robot systems. Extensive experiments have been carried out to validate our strategy both in simulation and with two real robots.
ID:499
CLASS:6
Title: Football is coming home
Abstract: Most of the robots in the ROBOCUP soccer league are made especially for the task of playing soccer. They use methods that are specifically designed for the soccer domain and would perhaps fail in other robotic testbeds such as the newly established ROBOCUP@HOME league without making fundamental changes throughout their entire software system. In contrast, our robots and the control software were designed with a broader field of application in mind. This paper sketches our way from the soccer application to the ROBOCUP@HOME league.
ID:500
CLASS:6
Title: Impact of tactical variations in the RoboCup four-legged league
Abstract: The RoboCup Four-Legged League is a robot soccer league where AIBO robots play in teams of four on a field of size 4m x 6m. In recent years the low level skills of the robots such as vision, localisation, locomotion, and ball handling have improved substantially and the games have become more exciting to watch. However, deliberate passing is extremely challenging and occurs rarely during games. This study investigates for the first time the impact of variations of global team strategies. The experiments employed the system used by the 2006 world champion team, the NUbots. The base strategy was compared against a more offensive and a more defensive variation. All test games were video recorded and evaluated using a variety of metrics including score, shots at goal, and ball position histograms. The results indicate that a team's style of play and low level skills are still the most critical parameters.
ID:501
CLASS:6
Title: User-centered approach to path planning of cleaning robots: analyzing user's cleaning behavior
Abstract: Current research on robot navigation is focused on clear recognition of the map and optimal path planning. The human cleaning path is, however, not optimal regarding time but optimal to the cleaning purpose. We have analyzed in this paper the cleaning behaviors in home environments and understood the user's path planning behaviors through usage tests of various vacuuming robots. We discovered that the actual user cleans with methods unique to specific areas of the house rather than following an optimal cleaning path. We not only suggest a path planning method for the vacuuming robot by using a layered map, but also a cleaning area designating method reflecting each area's characteristics. Based on these, we have designed a vacuuming robot's actions.
ID:502
CLASS:6
Title: Speed adaptation for a robot walking with a human
Abstract: We have taken steps towards developing a method that enables an interactive humanoid robot to adapt its speed to a walking human that it is moving together with. This is difficult because the human is simultaneously adapting to the robot. From a case study in human-human walking interaction we established a hypothesis about how to read a human's speed preference based on a relationship between humans' walking speed and their relative position in the direction of walking. We conducted two experiments to verify this hypothesis: one with two humans walking together, and one with a human subject walking with a humanoid robot, Robovie-IV. For 11 out of 15 subjects who walked with the robot, the results were consistent with the speed-position relationship of the hypothesis. We also conducted a preferred speed estimation experiment for six of the subjects. All of them were satisfied with one or more of the speeds that our algorithm estimated and four of them answered one of the speeds as the best one if the algorithm was allowed to give three options. In the paper, we also discuss the difficulties and possibilities that we learned from this preliminary trial.
ID:503
CLASS:6
Title: Robot expressionism through cartooning
Abstract: We present a new technique for human-robot interaction called robot expressionism through cartooning. We suggest that robots utilise cartoon-art techniques such as simplified and exaggerated facial expressions, stylised text, and icons for intuitive social interaction with humans. We discuss practical mixed reality solutions that allow robots to augment themselves or their surroundings with cartoon art content. Our effort is part of what we call robot expressionism, a conceptual approach to the design and analysis of robotic interfaces that focuses on providing intuitive insight into robotic states as well as the artistic quality of interaction. Our paper discusses a variety of ways that allow robots to use cartoon art and details a test bed design, implementation, and exploratory evaluation. We describe our test bed, Jeeves, which uses a Roomba, an iRobot vacuum cleaner robot, and a mixed-reality system as a platform for rapid prototyping of cartoon-art interfaces. Finally, we present a set of interaction content scenarios which use the Jeeves prototype: trash Roomba, the recycle police, and clean tracks, as well as initial exploratory evaluation of our approach.
ID:504
CLASS:6
Title: On-line behaviour classification and adaptation to human-robot interaction styles
Abstract: This paper presents a proof-of-concept of a robot that is adapting its behaviour on-line, during interactions with a human according to detected play styles. The study is part of the AuRoRa project which investigates how robots may be used to help children with autism overcome some of their impairments in social interactions. The paper motivates why adaptation is a very desirable feature of autonomous robots in human-robot interaction scenarios in general, and in autism therapy in particular. Two different play styles namely 'strong' and 'gentle', which refer to the user, are investigated experimentally. The model relies on Self-Organizing Maps, used as a classifier, and on Fast Fourier Transform to preprocess the sensor data. First experiments were carried out which discuss the performance of the model. Related work on adaptation in socially assistive and therapeutic work are surveyed. In future work, with typically developing and autistic children, the concrete choice of the robot's behaviours will be tailored towards the children's interests and abilities.
ID:505
CLASS:6
Title: Non-facial/non-verbal methods of affective expression as applied to robot-assisted victim assessment
Abstract: This work applies a previously developed set of heuristics for determining when to use non-facial/non-verbal methods of affective expression to the domain of a robot being used for victim assessment in the aftermath of a disaster. Robot-assisted victim assessment places a robot approximately three meters or less from a victim, and the path of the robot traverses three proximity zones (intimate (contact -- 0.46m), personal (0.46 -- 1.22 m), and social (1.22 -- 3.66 m)). Robot- and victim-eye views of an Inuktun robot were collected as it followed a path around the victim. The path was derived from observations of a prior robot-assisted medical reachback study. The victim's-eye views of the robot from seven points of interest on the path illustrate the appropriateness of each of the five primary non-facial/non-verbal methods of affective expression: (body movement, posture, orientation, illuminated color, and sound), offering support for the heuristics as a design aid. In addition to supporting the heuristics, the investigation identified three open research questions on acceptable motions and impact of the surroundings on robot affect.
ID:506
CLASS:6
Title: Incremental learning of gestures by imitation in a humanoid robot
Abstract: We present an approach to teach incrementally human gestures to a humanoid robot. By using active teaching methods that puts the human teacher "in the loop" of the robot's learning, we show that the essential characteristics of a gesture can be efficiently transferred by interacting socially with the robot. In a first phase, the robot observes the user demonstrating the skill while wearing motion sensors. The motion of his/her two arms and head are recorded by the robot, projected in a latent space of motion and encoded bprobabilistically in a Gaussian Mixture Model (GMM). In a second phase, the user helps the robot refine its gesture by kinesthetic teaching, i.e. by grabbing and moving its arms throughout the movement to provide the appropriate scaffolds. To update the model of the gesture, we compare the performance of two incremental training procedures against a batch training procedure. We present experiments to show that different modalities can be combined efficiently to teach incrementally basketball officials' signals to a HOAP-3 humanoid robot.
ID:507
CLASS:6
Title: Exploring adaptive dialogue based on a robot's awareness of human gaze and task progress
Abstract: When a robot provides direction--as a guide, an assistant, or as an instructor--the robot may have to interact with people of different backgrounds and skill sets. Different people require informat on adapted to their level of understanding. In this paper, we explore the use of two simple forms of awareness that a robot might use to infer that a person needs further verbal elaboration during a tool select on task. First, the robot could use an eye tracker for inferring whether the person is looking at the robot and thus in need of further elaboration. Second, the robot could monitor delays in the individual's task progress, indicating that he or she could use further elaboration. We investigated the effects of these two types of awareness on performance time, selection mistakes, and the number of questions people asked the robot. We did not observe any obvious benefits of our gaze awareness manipulation. Awareness of task delays did reduce the number of questions participants' asked compared to our control condition but did not significantly reduce the number of select on mistakes. The mixed results of our investigation suggest that more research is necessary before we can understand how awareness of gaze and awareness of task delay can be successfully implemented in human-robot dialogue.
ID:508
CLASS:6
Title: Directed stigmergy-based control for multi-robot systems
Abstract: Multi-robot systems are particularly useful in tasks that require searching large areas such as planetary science exploration, urban search and rescue, or landmine remediation. In order to overcome the inherent complexity of controlling multiple robots, the user must be able to give high-level, goal driven direction to the robot team. Since human robot interaction is a relatively new discipline, it is helpful to look to existing systems for concepts, analogies, or metaphors that might be utilized in building useful systems. Inspiration from natural decentralized systems guides the development of a computer simulation for stigmergy-based control of multi-robot system, and the interface with which an operator can interact and control mobile robots. In-depth description of the design process includes a description of a basic stigmergy-based control system and an innovative Directed Stigmergy control system that facilitates operator control of the robot team in an interesting and surprisingly effective way.
ID:509
CLASS:6
Title: "Daisy, Daisy, give me your answer do!": switching off a robot
Abstract: Robots can exhibit life like behavior, but are according to traditional definitions not alive. Current robot users are confronted with an ambiguous entity and it is important to understand the users perception of these robots. This study analyses if a robot's intelligence and its agreeableness influence its perceived animacy. The robot's animacy was measured, amongst other measurements, by the users' hesitation to switch it off. The results show that participants hesitated three times as long to switch off an agreeable and intelligent robot as compared to a non agreeable and unintelligent robot. The robots' intelligence had a significant influence on its perceived animacy. Our results suggest that interactive robots should be intelligent and exhibit an agreeable attitude to maximize its perceived animacy.
ID:510
CLASS:6
Title: Combining ubiquitous and on-board audio sensing for human-robot interaction
Abstract: This paper reports on the development of a mobile robot system for operation within a house equipped with a ubiquitous sensor network.Human robot interaction is achieved through the combination of on-robot audio and laser range sensing and additional audio sensors mounted in the ceiling of the ubiquitous environment. The ceiling mounted microphone arrays can be used to summon a mobile robot from a location outside the robot's range of hearing. After the robot autonomously navigates to the desired location, the on-board microphone array can be used to locate the sound source and to recognise a series of greetings and commands.
ID:511
CLASS:6
Title: A field experiment of autonomous mobility: operator workload for one and two robots
Abstract: An experiment was conducted on aspects of human-robot interaction in a field environment using the U.S. Army's Experimental Unmanned Vehicle (XUV). Goals of this experiment were to examine the use of scalable interfaces and to examine operator span of control when controlling one versus two autonomous unmanned ground vehicles. We collected workload ratings from two Soldiers after they had performed missions that included monitoring, downloading and reporting on simulated reconnaissance, surveillance, and target acquisition (RSTA) images, and responding to unplanned operator intervention requests from the XUV. Several observations are made based on workload data, experimenter notes, and informal interviews with operators.
ID:512
CLASS:6
Title: RSVP: an investigation of remote shared visual presence as common ground for human-robot teams
Abstract: This study presents mobile robots as a way of augmenting communication in distributed teams through a remote shared visual presence (RSVP) consisting of the robot's view. By giving all team members access to the shared visual display provided by a robot situated in a remote workspace, the robot can serve as a source of common ground for the distributed team. In a field study examining the effects of remote shared visual presence on team performance in collocated and distributed Urban Search & Rescue technical search teams, data were collected from 25 dyadic teams comprised of US&R task force personnel drawn from high-fidelity training exercises held in California (2004) and New Jersey (2005). They performed a 2 x 2 repeated measures search task entailing robot-assisted search in a confined space rubble pile. Multilevel regression analyses were used to predict team performance based upon use of RSVP (RSVP or no-RSVP) and whether or not team members had visual access to other team members. Results indicated that the use of RSVP technology predicted team performance ( &#223;= -1.24, p&lt;.05). No significant differences emerged in performance between teams with and without visual access to their team members. Findings suggest RSVP may enable distributed teams to perform as effectively as collocated teams. However, differences detected between sites suggest efficiency of RSVP may depend on the user's domain experience and team cohesion.
ID:513
CLASS:6
Title: Comparing a computer agent with a humanoid robot
Abstract: HRI researchers interested in social robots have made large investments in humanoid robots. There is still sparse evidence that peoples' responses to robots differ from their responses to computer agents, suggesting that agent studies might serve to test HRI hypotheses. To help us understand the difference between people's social interactions with an agent and a robot, we experimentally compared people's responses in a health interview with (a) a computer agent projected either on a computer monitor or life-size on a screen, (b) a remote robot projected life-size on a screen, or (c) a collocated robot in the same room. We found a few behavioral and large attitude differences across these conditions. Participants forgot more and disclosed least with the collocated robot, next with the projected remote robot, and then with the agent. They spent more time with the collocated robot and their attitudes were most positive toward that robot. We discuss tradeoffs for HRI research of using collocated robots, remote robots, and computer agents as proxies of robots.
ID:514
CLASS:6
Title: Humanoid robots as a passive-social medium: a field experiment at a train station
Abstract: This paper reports a method that uses humanoid robots as a communication medium. There are many interactive robots under development, but due to their limited perception, their interactivity is still far poorer than that of humans. Our approach in this paper is to limit robots' purpose to a non-interactive medium and to look for a way to attract people's interest in the information that robots convey. We propose using robots as a passive-social medium, in which multiple robots converse with each other. We conducted a field experiment at a train station for eight days to investigate the effects of a passive-social medium.
ID:515
CLASS:6
Title: Humanoid robots as a passive-social medium: a field experiment at a train station
Abstract: This paper reports a method that uses humanoid robots as a communication medium. There are many interactive robots under development, but due to their limited perception, their interactivity is still far poorer than that of humans. Our approach in this paper is to limit robots' purpose to a non-interactive medium and to look for a way to attract people's interest in the information that robots convey. We propose using robots as a passive-social medium, in which multiple robots converse with each other. We conducted a field experiment at a train station for eight days to investigate the effects of a passive-social medium.
ID:516
CLASS:6
Title: Humanoid robots as a passive-social medium: a field experiment at a train station
Abstract: This paper reports a method that uses humanoid robots as a communication medium. There are many interactive robots under development, but due to their limited perception, their interactivity is still far poorer than that of humans. Our approach in this paper is to limit robots' purpose to a non-interactive medium and to look for a way to attract people's interest in the information that robots convey. We propose using robots as a passive-social medium, in which multiple robots converse with each other. We conducted a field experiment at a train station for eight days to investigate the effects of a passive-social medium.
ID:517
CLASS:6
Title: Humanoid robots as a passive-social medium: a field experiment at a train station
Abstract: This paper reports a method that uses humanoid robots as a communication medium. There are many interactive robots under development, but due to their limited perception, their interactivity is still far poorer than that of humans. Our approach in this paper is to limit robots' purpose to a non-interactive medium and to look for a way to attract people's interest in the information that robots convey. We propose using robots as a passive-social medium, in which multiple robots converse with each other. We conducted a field experiment at a train station for eight days to investigate the effects of a passive-social medium.
ID:518
CLASS:6
Title: A dancing robot for rhythmic social interaction
Abstract: This paper describes a robotic system that uses dance as a form of social interaction to explore the properties and importance of rhythmic movement in general social interaction. The system consists of a small creature-like robot whose movement is controlled by a rhythm-based software system. Environmental rhythms can be extracted from auditory or visual sensory stimuli, and the robot synchronizes its movement to a dominant rhythm. The system was demonstrated, and an exploratory study conducted, with children interacting with the robot in a generalized dance task. Through a behavioral analysis of videotaped interactions, we found that the robot's synchronization with the background music had an effect on children's interactive involvement with the robot. Furthermore, we observed a number of expected and unexpected styles and modalities of interactive exploration and play that inform our discussion on the next steps in the design of a socially rhythmic robotic system.
ID:519
CLASS:6
Title: To kill a mockingbird robot
Abstract: Robots are being introduced in our society but their social status is still unclear. A critical issue is if the robot's exhibition of intelligent life-like behavior leads to the users' perception of animacy. The ultimate test for the life-likeness of a robot is to kill it. We therefore conducted an experiment in which the robot's intelligence and the participants' gender were the independent variables and the users' destructive behavior of the robot the dependent variables. Several practical and methodological problems compromised the acquired data, but we can conclude that the robot's intelligence had a significant influence on the users' destructive behavior. We discuss the encountered problems and the possible application of this animacy measuring method.
ID:520
CLASS:6
Title: Interactive robot task training through dialog and demonstration
Abstract: Effective human/robot interfaces which mimic how humans interact with one another could ultimately lead to robots being accepted in a wider domain of applications. We present a framework for interactive task training of a mobile robot where the robot learns how to do various tasks while observing a human. In addition to observation, the robot listens to the human's speech and interprets the speech as behaviors that are required to be executed. This is especially important where individual steps of a given task may have contingencies that have to be dealt with depending on the situation. Finally, the context of the location where the task takes place and the people present factor heavily into the robot's interpretation of how to execute the task. In this paper, we describe the task training framework, describe how environmental context and communicative dialog with the human help the robot learn the task, and illustrate the utility of this approach with several experimental case studies.
ID:521
CLASS:6
Title: Developing performance metrics for the supervisory control of multiple robots
Abstract: Efforts are underway to make it possible for a single operator to effectively control multiple robots. In these high workload situations, many questions arise including how many robots should be in the team (Fan-out), what level of autonomy should the robots have, and when should this level of autonomy change (i.e., dynamic autonomy). We propose that a set of metric classes should be identified that can adequately answer these questions. Toward this end, we present a potential set of metric classes for human-robot teams consisting of a single human operator and multiple robots. To test the usefulness and appropriateness of this set of metric classes, we conducted a user study with simulated robots. Using the data obtained from this study, we explore the ability of this set of metric classes to answer these questions.
ID:522
CLASS:6
Title: Managing autonomy in robot teams: observations from four experiments
Abstract: It is often desirable for a human to manage multiple robots. Autonomy is required to keep workload within tolerable ranges, and dynamically adapting the type of autonomy may be useful for responding to environment and workload changes. We identify two management styles for managing multiple robots and present results from four experiments that have relevance to dynamic autonomy within these two management styles. These experiments, which involved 80 subjects, suggest that individual and team autonomy benefit from attention management aids, adaptive autonomy, and proper information abstraction.
ID:523
CLASS:6
Title: Natural person-following behavior for social robots
Abstract: We are developing robots with socially appropriate spatial skills not only to travel around or near people, but also to accompany people side-by-side. As a step toward this goal, we are investigating the social perceptions of a robot's movement as it follows behind a person. This paper discusses our laser-based person-tracking method and two different approaches to person-following: direction-following and path-following. While both algorithms have similar characteristics in terms of tracking performance and following distances, participants in a pilot study rated the direction-following behavior as significantly more human-like and natural than the path-following behavior. We argue that the path-following method may still be more appropriate in some situations, and we propose that the ideal person-following behavior may be a hybrid approach, with the robot automatically selecting which method to use.
ID:524
CLASS:6
Title: Human control for cooperating robot teams
Abstract: Human control of multiple robots has been characterized by the average demand of single robots on human attention or the distribution of demands from multiple robots. When robots are allowed to cooperate autonomously, however, demands on the operator should be reduced by the amount previously required to coordinate their actions. The present experiment compares control of small robot teams in which cooperating robots explored autonomously, were controlled independently by an operator or through mixed initiative as a cooperating team. Mixed initiative teams found more victims and searched wider areas than either fully autonomous or manually controlled teams. Operators who switched attention between robots more frequently were found to perform better in both manual and mixed initiative conditions.
ID:525
CLASS:6
Title: Effects of anticipatory action on human-robot teamwork efficiency, fluency, and perception of team
Abstract: A crucial skill for fluent action meshing in human team activity is a learned and calculated selection of anticipatory actions. We believe that the same holds for robotic teammates, if they are to perform in a similarly fluent manner with their human counterparts.In this work, we propose an adaptive action selection mechanism for a robotic teammate, making anticipatory decisions based on the confidence of their validity and their relative risk. We predict an improvement in task efficiency and fluency compared to a purely reactive process.We then present results from a study involving untrained human subjects working with a simulated version of a robot using our system. We show a significant improvement in best-case task efficiency when compared to a group of users working with a reactive agent, as well as a significant difference in the perceived commitment of the robot to the team and its contribution to the team's uency and success. By way of explanation, we propose a number of fluency metrics that differ significantly between the two study groups.
ID:526
CLASS:6
Title: Talking robots with LEGO MindStorms
Abstract: This paper shows how talking robots can be built from off-the-shelf components, based on the Lego MindStorms robotics platform. We present four robots that students created as final projects in a seminar we supervised. Because Lego robots are so affordable, we argue that it is now feasible for any dialogue researcher to tackle the interesting challenges at the robot-dialogue interface.
ID:527
CLASS:6
Title: Where to look: a study of human-robot engagement
Abstract: This paper reports on a study of human subjects with a robot designed to mimic human conversational gaze behavior in collaborative conversation. The robot and the human subject together performed a demonstration of an invention created at our laboratory; the demonstration lasted 3 to 3.5 minutes. We briefly discuss the robot architecture and then focus the paper on a study of the effects of the robot operating in two different conditions. We offer some conclusions based on the study about the importance of engagement for 3D IUIs. We will present video clips of the subject interactions with the robot at the conference.
ID:528
CLASS:6
Title: Cooperation through self-assembly in multi-robot systems
Abstract: This article illustrates the methods and results of two sets of experiments in which a group of mobile robots, called s-bots, are required to physically connect to each other, that is, to self-assemble, to cope with environmental conditions that prevent them from carrying out their task individually. The first set of experiments is a pioneering study on the utility of self-assembling robots to address relatively complex scenarios, such as cooperative object transport. The results of our work suggest that the s-bots possess hardware characteristics which facilitate the design of control mechanisms for autonomous self-assembly. The control architecture we developed proved particularly successful in guiding the robots engaged in the cooperative transport task. However, the results also showed that some features of the robots' controllers had a disruptive effect on their performances. The second set of experiments is an attempt to enhance the adaptiveness of our multi-robot system. In particular, we aim to synthesise an integrated (i.e., not-modular) decision-making mechanism which allows the s-bot to autonomously decide whether or not environmental contingencies require self-assembly. The results show that it is possible to synthesize, by using evolutionary computation techniques, artificial neural networks that integrate both the mechanisms for sensory-motor coordination and for decision making required by the robots in the context of self-assembly.
ID:529
CLASS:6
Title: Simulating robot collective behavior using StarLogo
Abstract: Robot simulation is a very important tool to the development of novel real-world techniques for cooperation of teams of robots. One major difficulty when trying to introduce students to robotics is that the teaching of major abstractions used to coordinate group robot behavior is not easily visualized -- it is not always true that one has enough robots available to be used in real demonstrations. In this paper, we attempt to improve the situation above by implementing a robot simulator for 5 (five) of the major abstractions used in robotics. This simulator concentrates on group coordination in a scenario where robots are required to find their way out of a room or maze. This paper describes our initial version of this simulator, as well as our future plans for the simulator, both in usage and in enhancement of the feature set.
ID:530
CLASS:6
Title: Active eye contact for human-robot communication
Abstract: Eye contact is an effective means of controlling communication for humans, such as starting communication. It seems that we can make eye contact if we look at each other. However, this alone cannot complete eye contact. In addition, we need to be aware of being looked by each other. We propose a method of active eye contact for human-robot communication considering both conditions. The robot changes its facial expressions according to the observation results of the human to make eye contact. Then, we present a robot that can recognize hand gestures after making eye contact with the human to show the effectiveness of eye contact as a means of controlling communication.
ID:531
CLASS:6
Title: Fan-out: measuring human control of multiple robots
Abstract: A goal of human-robot interaction is to allow one user to operate multiple robots simultaneously. In such a scenario the robots provide leverage to the user's attention. The number of such robots that can be operated is called the fan-out of a human-robot team. Robots that have high neglect tolerance and lower interaction time will achieve higher fan-out. We define an equation that relates fan-out to a robot's activity time and its interaction time. We describe how to measure activity time and fan-out. We then use the fan-out equation to compute interaction effort. We can use this interaction effort as a measure of the effectiveness of a human-robot interaction design. We describe experiments that validate the fan-out equation and its use as a metric for improving human-robot interaction.
ID:532
CLASS:6
Title: Dogs or robots: why do children see them as robotic pets rather than canine machines?
Abstract: In the not too distant future Intelligent Creatures (robots, smart devices, smart vehicles, smart buildings, etc) will share the everyday living environment of human beings. It is important then to analyze the attitudes humans are to adopt for interaction with morphologically different devices, based on their appearance and behavior. In particular, these devices will become multi-modal interfaces, with computers or networks of computers, for a large and complex universe of applications. Our results show that children are quickly attached to the word 'dog' reflecting a conceptualization that robots that look like dogs (in particular SONY Aibo) are closer to living dogs than they are to other devices. By contrast, adults perceive Aibo as having stronger similarities to machines than to dogs (reflected by definitions of robot). Illustration of the characteristics structured in the definition of robot are insufficient to convince children Aibo is closer to a machine than to a dog.
ID:533
CLASS:6
Title: Human-robot interface based on the mutual assistance between speech and vision
Abstract: This paper presents a user interface for a service robot that can bring the objects asked by the user. Speech-based interface is appropriate for this application. However, it alone is not sufficient. The system needs a vision-based interface to recognize gestures as well. Moreover, it needs vision capabilities to obtain the real world information about the objects mentioned in the user's speech. For example, the robot needs to find the target object ordered by speech to carry out the task. This can be considered that vision assists speech. However, vision sometimes fails to detect the objects. Moreover, there are objects for which vision cannot be expected to work well. In these cases, the robot tells the current status to the user so that he/she can give advice by speech to the robot. This can be considered that speech assists vision through the user. This paper presents how the mutual assistance between speech and vision works and demonstrates promising results through experiments.
ID:534
CLASS:6
Title: Providing the basis for human-robot-interaction: a multi-modal attention system for a mobile robot
Abstract: In order to enable the widespread use of robots in home and office environments, systems with natural interaction capabilities have to be developed. A prerequisite for natural interaction is the robot's ability to automatically recognize when and how long a person's attention is directed towards it for communication. As in open environments several persons can be present simultaneously, the detection of the communication partner is of particular importance. In this paper we present an attention system for a mobile robot which enables the robot to shift its attention to the person of interest and to maintain attention during interaction. Our approach is based on a method for multi-modal person tracking which uses a pan-tilt camera for face recognition, two microphones for sound source localization, and a laser range finder for leg detection. Shifting of attention is realized by turning the camera into the direction of the person which is currently speaking. From the orientation of the head it is decided whether the speaker addresses the robot. The performance of the proposed approach is demonstrated with an evaluation. In addition, qualitative results from the performance of the robot at the exhibition part of the ICVS'03 are provided.
ID:535
CLASS:6
Title: Quantitative analysis of the effects of robots on introductory Computer Science education
Abstract: We report the results of a year-long experiment in the use of robots to teach computer science. Our data set compares results from over 800 students on identical tests from both robotics and nonrobotics-based laboratory sessions. We also examine the effectiveness of robots in encouraging students to select computer science or computer engineering as a field of study. Our results are negative: test scores were lower in the robotics sections than in the nonrobotics ones, nor did the use of robots have any measurable effect on students' choice of discipline. We believe the most significant factor that accounts for this is the lack of a simulator for our robotics programming system. Students in robotics sections must run and debug their programs on robots during assigned lab times, and are therefore deprived of both reflective time and the rapid compile-run-debug cycle outside of class that is an important part of the learning process. We discuss this and other issues, and suggest directions for future work.
ID:536
CLASS:6
Title: Java simulation and robot modeling of the Cataglyphis Bicolor
Abstract: This summer we participated in a program at a college in a town, state. For this program, we researched and experimented with simulations of the desert ant Cataglyphis Bicolor. We wrote a Java simulation and constructed a Lego robot to help us understand and emulate the behaviors that we found in our research. Our intensive research aided our efforts to understand the methods that we needed to employ to create the most accurate model.
ID:537
CLASS:6
Title: Natural methods for robot task learning: instructive demonstrations, generalization and practice
Abstract: Among humans, teaching various tasks is a complex process which relies on multiple means for interaction and learning, both on the part of the teacher and of the learner. Used together, these modalities lead to effective teaching and learning approaches, respectively. In the robotics domain, task teaching has been mostly addressed by using only one or very few of these interactions. In this paper we present an approach for teaching robots that relies on the key features and the general approach people use when teaching each other: first give a demonstration, then allow the learner to refine the acquired capabilities by practicing under the teacher's supervision, involving a small number of trials. Depending on the quality of the learned task, the teacher may either demonstrate it again or provide specific feedback during the learner's practice trial for further refinement. Also, as people do during demonstrations, the teacher can provide simple instructions and informative cues, increasing the performance of learning. Thus, instructive demonstrations, generalization over multiple demonstrations and practice trials are essential features for a successful human-robot teaching approach. We implemented a system that enables all these capabilities and validated these concepts with a Pioneer 2DX mobile robot learning tasks from multiple demonstrations and teacher feedback.
ID:538
CLASS:6
Title: Locating moving entities in indoor environments with teams of mobile robots
Abstract: This article presents an implemented multi-robot system for playing the popular game of laser tag. The object of the game is to search for and tag opponents that can move freely about the environment. The main contribution of this paper is a new particle filter algorithm for tracking the location of many opponents in the presence of pervasive occlusion. We achieve efficient tracking principally through a clever factorization of our posterior into roles that can be dynamically added and merged. When searching for opponents, the individual agents greedily maximize their information gain, using a negotiation technique for coordinating their search efforts. Experimental results are provided, obtained with a physical robot system in large-scale indoor environments and through simulation.
ID:539
CLASS:6
Title: A road map for teaching introductory programming using LEGO\&copy; mindstorms robots
Abstract: In this paper, we describe a recent trend in the introductory computer science curriculum which advocates conceptualizing computation primarily as coordinated concurrent activities [8], [9], [10]. Consistent with this philosophy is the focus on the event-driven model of computation [7]. While one can utilize these approaches with any thread and/or event supporting object-oriented language (e.g. Java) in a desktop programming environment, they become particularly worthwhile when used in conjunction with physical robots. This paper argues the case for the benefits of this approach and provides sample exercises that illustrate the use of this pedagogy using Lego Mindstorms RCX bricks programmed in Java for use in introductory programming.
ID:540
CLASS:6
Title: All robots are not created equal: the design and perception of humanoid robot heads
Abstract: This paper presents design research conducted as part of a larger project on human-robot interaction. The primary goal of this study was to come to an initial understanding of what features and dimensions of a humanoid robot's face most dramatically contribute to people's perception of its humanness. To answer this question we analyzed 48 robots and conducted surveys to measure people's perception of each robot's humanness. Through our research we found that the presence of certain features, the dimensions of the head, and the total number of facial features heavily influence the perception of humanness in robot heads. This paper presents our findings and initial guidelines for the design of humanoid robot heads.
ID:541
CLASS:6
Title: Teaching problem solving, computing, and information technology with robots
Abstract: The Electrical Engineering and Computer Science Department at the United States Military Academy uses the Lego Mindstorms robot and Java as part of the active-learning environment used to teach Information Technology (IT) and problem solving with computers. The United States Military Academy at West Point requires students to take a course on IT and problem solving with computers during their first year. This course is an important opportunity to expose undergraduate students to technology and concepts that will be a part of their daily lives and future careers. The Mindstorms robots are used in the introductory computer science course to teach problem solving skills, fundamental computer programming concepts, and to introduce the concepts of autonomous vehicles, embedded computer systems, sensors, and computer simulation. The short-term impact on the students taking the course has been very positive, and we are confident that the long-term impact will be substantial. Members of the faculty at West Point developed a computer programming environment completely in Java for the Mindstorms robot called Jago. Jago enables students to write programs in Java that will run in a graphic simulator and in the robot. Jago enables the students to visually evaluate their algorithmic solutions, which some students can more easily grasp. Based on these results we have incorporated Jago into the core IT course taught at West Point.
ID:542
CLASS:6
Title: Model-based recognition in robot vision
Abstract: This paper presents a comparative study and survey of model-based object-recognition algorithms for robot vision. The goal of these algorithms is to recognize the identity, position, and orientation of randomly oriented industrial parts. In one form this is commonly referred to as the "bin-picking" problem, in which the parts to be recognized are presented in a jumbled bin. The paper is organized according to 2-D, 2&frac12;-D, and 3-D object representations, which are used as the basis for the recognition algorithms. Three central issues common to each category, namely, feature extraction, modeling, and matching, are examined in detail. An evaluation and comparison of existing industrial part-recognition systems and algorithms is given, providing insights for progress toward future robot vision systems.
ID:543
CLASS:6
Title: Teaching neural networks using LEGO handy board robots in an artificial intelligence course
Abstract: In this paper we propose a novel method for teaching neural networks with back propagation in an undergraduate Artificial Intelligence course. We use an agent based approach in the course, as outlined in the textbook Artificial Intelligence A Modern Approach by Stuart Russell and Peter Norvig [7]. The students build a robot agent whose task is to learn path-following behavior with a neural network. Robot agents are constructed from standard LEGO pieces and use the MIT Handy Board as a controller.
ID:544
CLASS:6
Title: Extreme work teams: using SWAT teams as a model for coordinating distributed robots
Abstract: We present a field study of police SWAT teams for the purpose of enabling grounded design of a system to coordinate distributed field robots. The mission-oriented, spatially distributed SWAT environment provides a rich resource for robotics designers that mirrors field robot deployments in key ways. We highlight the processes with which SWAT team leaders create and maintain common ground among team members and coordinate action in these tightly-coupled, distributed teams. We present a system for coordinating distributed robots that we designed based on our SWAT team observations.
ID:545
CLASS:6
Title: Circle formation for oblivious anonymous mobile robots with no common sense of orientation
Abstract: This paper proposes a distributed algorithm by which a collection of mobile robots roaming on a plane move to form a circle. The algorithm operates under the premises that robots (1) are unable to recall past actions and observations (i.e., oblivious), (2) cannot be distinguished from each others (i.e., anonymous), (3) share no common sense of direction, and (4) are unable to communicate in any other ways than by observing each others position.
ID:546
CLASS:6
Title: Modeling and simulation for exploring human-robot team interaction requirements
Abstract: Small-sized and micro-robots will soon be available for deployment in large-scale forces. Consequently, the ability of a human operator to coordinate and interact with large-scale robotic forces is of great interest. This paper describes the ways in which modeling and simulation have been used to explore new possibilities for human-robot interaction. The paper also discusses how these explorations have fed implementation of a unified set of command and control concepts for robotic force deployment. Modeling and simulation can play a major role in fielding robot teams in actual missions. While live testing is preferred, limitations in terms of technology, cost, and time often prohibit extensive experimentation with physical multi-robot systems. Simulation provides insight, focuses efforts, eliminates large areas of the possible solution space, and increases the quality of actual testing.
ID:547
CLASS:6
Title: The freeze-tag problem: how to wake up a swarm of robots
Abstract: An optimization problem that naturally arises in the study of "swarm robotics" is to wake up a set of "asleep" robots, starting with only one "awake" robot. One robot can only awaken another when they are in the same location. As soon as a robot is awake, it assists in waking up other robots. The goal is to compute an optimal awakening schedule such that all robots are awake by time t*, for the smallest possible value of t*.We consider both scenarios on graphs and in geometric environments. In the graph setting, robots sleep at vertices and there is a length function on the edges. An awake robot can travel from vertex to vertex along edges, and the length of an edge determines the time it takes to travel from one vertex to the other.While this problem bears some resemblance to problems from various areas in combinatorial optimization such as routing, broadcasting, scheduling and covering, its algorithmic characteristics are surprisingly different. We prove that the problem is NP-hard, even for the special case of star graphs. We also establish hardness of approximation, showing that it is NP-hard to obtain an approximation factor better than 5/3, even for graphs of bounded degree.These lower bounds are complemented with several algorithmic results. We present a simple on-line algorithm that is O(log&Delta;)-competitive for graphs with maximum degree &Delta;. Other results include algorithms that require substantially more sophistication and development of new techniques:(1) The natural greedy strategy on star graphs has a worst-case performance of 7/3, which is tight.(2) There exists a PTAS for star graphs.(3) For the problem on ultrametrics, there is a polynomial-time approximation algorithm with performance ratio 2O(&radic;log log n).(4) There is a PTAS, running in nearly linear time, for geometrically embedded instances (e.g., Euclidean distances in any fixed dimension).
ID:548
CLASS:6
Title: How to make a self-reconfigurable robot run
Abstract: In this paper we present a multiagent based control algorithm for self-reconfigurable robots. These robots are robots made from a possibly large number of independent modules. In the proposed control algorithm all modules run identical programs, but may play different roles. The modules decide what role to play based on their local configuration and information propagated down to them through the configuration tree. A role consists of a cyclic motion, the period of this motion, and a set of delays. The delays specify the phase delay of the cyclic motions of the child modules compared to the parent. These delays are used to coordinate the motions of the individual module to obtain a coordinated global behavior. We use this general algorithm to implement locomotion in a legged self-reconfigurable robot. We demonstrate that this algorithm successfully produces quadruped and hexapod gaits in a real self-reconfigurable robot made from up to nine independent autonomous modules. We show that the control algorithm scales and argue that the algorithm is minimal, robust to module failures, to loss of communication signals, and to interchange of modules.
ID:549
CLASS:6
Title: The AGILO autonomous robot soccer team: computational principles, experiences, and perspectives
Abstract: This paper describes the computational model underlying the AGILO autonomous robot soccer team, its implementation, and our experiences with it. The most salient aspects of the AGILO control software are that it includes (1) a cooperative probabilistic game state estimator working with a simple off-the-shelf camera system; (2) a situated action selection module that makes ample use of experience-based learning and produces coherent team behavior even if inter-robot communication is perturbed; and (3) a playbook executor that can perform preprogrammed complex soccer plays in appropriate situations by employing plan-based control techniques. The use of such sophisticated state estimation and control techniques distinguishes the AGILO software from many others applied to mid-size autonomous robot soccer. The paper discusses the computational techniques and necessary extensions based on experimental data from the 2001 robot soccer world championship.
ID:550
CLASS:6
Title: On the development of cooperative behavior-based mobile manipulators
Abstract: We present an approach to the problem of cooperative object transportation to be performed by a pair of autonomous mobile robots controlled with a behavior-based architecture. Inherent characteristics of this method such as simplicity, rapid development, robustness, and low memory and processing requirements have allowed us to implement a behavioral control system in rather simple, low-processing-power mobile platforms. Robot coordination may be achieved through either implicit or explicit communication. Although implicit communication can make the system more robust to faulty communication environments, we show that the use of the explicit form can avoid some undesirable system locks. Experimental results where two robots carry a relatively large box in an environment cluttered with both easily and hardly avoidable obstacles show the flexibility and effectiveness of the proposed architecture.
ID:551
CLASS:6
Title: A hybrid mobile robot architecture with integrated planning and control
Abstract: Research in the planning and control of mobile robots has received much attention in the past two decades. Two basic approaches have emerged from these research efforts: deliberative vs.\ reactive. These two approaches can be distinguished by their different usage of sensed data and global knowledge, speed of response, reasoning capability, and complexity of computation. Their strengths are complementary and their weaknesses can be mitigated by combining the two approaches in a hybrid architecture. This paper describes a method for goal-directed, collision-free navigation in unpredictable environments that employs a behavior-based hybrid architecture with asynchronously operating behavioral modules. It differs from existing hybrid architectures in two important ways: (1) the planning module produces a sequence of checkpoints instead of a conventional complete path, and (2) in addition to obstacle avoidance, the reactive module also performs target reaching under the control of a self-organizing neural network. The neural network is trained to perform fine, smooth motor control that moves the robot through the checkpoints. These two aspects facilitate a tight integration between high-level planning and low-level control, which permits real-time performance and easy path modification even when the robot is en route to the goal position.
ID:552
CLASS:6
Title: Robots in the laboratory
Abstract: A $70 toy robot has been successfully used in Computer Science undergraduate laboratory courses in real-time programming and advanced operating systems to provide students with hands on experience.A custom designed interface card connects a Radio Shack Armatron toy mobile robot with an IBM PC. To provide sensory input and hence introduce feedback, the robot is shackled to a track filled with sensors. Extra sensors in the robot's environment allow challenging experiments such as picking up an object from a moving belt.While programming the robot and its environment in Turbo Pascal, the students learn how to write software drivers to control low level hardware that requires real-time response. This experimental design obviates the need to use sophisticated test equipment or special software development tools, and so the robot has transformed potentially routine courses into a exciting and fulfilling learning experiences.
ID:553
CLASS:6
Title: A comparison of the artistic aspects of various industrial robots
Abstract: Robot choreography has been developed to explore the aesthetic implications of robotic movement. The application of choreographic techniques to robot motion has evolved as a result of the implementation of the new technology. New materials and techniques have made changes possible in artistic forms. The widespread use of robots may significantly influence artistic trends.While pioneering in the field of robot choreography, I found that robot movement may be functionally efficient but not always aesthetically pleasing. Thus, my work has focused on the artistic design of robot motion. A method of programming industrial robots to move in a graceful and fluid fashion has been developed as a result of my research. The majority of this aesthetic exploration has been with Unimation robots (PUMA 260 and PUMA 560), although I have choreograhed compositions for other robotic systems; namely, the Spine, U.S. Robot, and the Robotics Reasearch Corporation K-2107.This paper will present a comparison of these robots from the artistic aspect of the design and movement. The following will be discussed:the aesthetic design of the manipulatorthe programming techniques used in choreographythe types of movement sequences specific to each robot systemthe control of robot motionthe safety aspects of the robots related to concert performances
ID:554
CLASS:6
Title: Entertainment robotics
Abstract: Competing teams of autonomous robot soccer players illustrate the challenges, pleasures, and promise of developing collaborative multi-robot applications.
ID:555
CLASS:6
Title: Self-reconfiguring robots
Abstract: Mimicking the adaptability of living biological cells, robot modules will reconfigure themselves toward a common purpose within the limits imposed by the local environment.
ID:556
CLASS:6
Title: Facilitating active learning with inexpensive mobile robots
Abstract: Our educational system must nurture a student's ability to acquire knowledge. Research has proven that active learning, learning promoted by interacting with one's environment, as opposed to lectures, is most effective in developing a students ability to acquire knowledge. In the computer science domain, active learning can be facilitated by using mobile robots in a collaborative setting. We believe that a context-based, collaborative approach, combined with the excitement, motivation, and real-world experiences provided by a robot, provide a nearly optimal method of teaching students how to acquire knowledge about computer science. However, this methodology has not seen wide acceptance. In order to facilitate the use of robots we have developed an inexpensive wheeled robot that uses common parts and requires only a simple set of tools for assembly. An abstracted Java interface allows students to interact with the robot from any desktop computer using the simple commands provided. Additionally, students can develop sophisticated distributed software solutions that require only simple changes to the interface, and development of software for both the embedded microcontroller and desktop machine. Our approach makes a wide variety of robot-based projects accessible to all level of courses, and for even the smallest computer science departments.
ID:557
CLASS:6
Title: Terrain coverage with ant robots: a simulation study
Abstract: In this paper, we study a simple means for coordinating teams of simple agents. In particular, we study ant robots and how they can cover terrain once or repeatedly by leaving markings in the terrain, similar to what ants do. These markings can be sensed by all robots and allow them to cover terrain even if they do not communicate with each other except via the markings, do not have any kind of memory, do not know the terrain, cannot maintain maps of the terrain, nor plan complete paths. The robots do not even need to be localized, which completely eliminates solving difficult and time-consuming localization problems. In this paper, we use real-time heuristic search methods to implement ant robots and present a simulation study with several real-time heuristic search methods to study their properties for terrain coverage. Our experiments show that all of the real-time heuristic search methods robustly cover terrain even if the robots are moved without realizing this, some robots fail, and some markings get destroyed. These results demonstrate that terrain coverage with real-time heuristic search methods is an interesting alternative to more conventional terrain coverage methods.
ID:558
CLASS:6
Title: CMPack: a complete software system for autonomous legged soccer robots
Abstract: This paper describes a completely implemented, fully autonomous software system for soccer playing quadruped ro\-bots.  The system includes real-time color vision, probabilistic localization, quadruped locomotion/motion, and a hierarchical behavior system.  Each component was based on well tested algorithms and approaches from other domains. Our design exposed strengths and weaknesses in each component, and led to improvements and extensions that made them more capable in general, as well as better suited for our testing domain.  Integrating the components revealed design assumptions that were violated.  We describe the problems that arose and how we addressed them.The integrated system was then used at the annual Robo\-Cup robotic soccer competition where we placed third, losing only a single game. We reflect on how our system addressed its goals and what was learned through implementation and testing on real robots.
ID:559
CLASS:6
Title: Teaching CS1 with karel the robot in Java
Abstract: Most current Java textbooks for CS1 (and thus most current courses) begin either with fundamentals from the procedural paradigm (assignment, iteration, selection) or with a brief introduction to using objects followed quickly with writing objects. We have found a third way to be most satisfying for both teachers and students: using interesting predefined classes to introduce the fundamentals of object-oriented programming (object instantiation, method calls, inheritance) followed quickly by the traditional fundamentals of iteration and selection, also taught using the same predefined classes.Karel the Robot, developed by Richard Pattis [6] and well-known to many computer science educators, has aged gracefully and is a vital part of our CS1 curriculum. This paper explains how Karel may be used and the advantages of doing so.
ID:560
CLASS:6
Title: Robots: a real-time systems architectural style
Abstract: This paper presents an architectural style for real-time systems, and an associated formal architectural description language, called Robots. A basic specification in Robots consists of a synchronous control task that is responsible for the dynamic reconfiguration of the system controller as a set of asynchronous observer and process tasks. The controller architecture evolves by hierarchical refinement of observers and processes into lower level control tasks each dominating a new set of observers and processes. Robots is given operational semantics by statecharts. Also, the architectural style is embedded in Robots by semantic rules that allow formal checking of the consistency and completeness of architectural specifications.
ID:561
CLASS:6
Title: Xavier: experience with a layered robot architecture
Abstract: Office delivery robots have to perform many tasks such as picking up and delivering mail or faxes, returning library books, and getting coffee. They have to determine the order in which to visit locations, plan paths to those locations, follow paths reliably, and avoid static and dynamic obstacles in the process. Reliability and efficiency are key issues in the design of such autonomous robot systems. They must deal reliably with noisy sensors and actuators and with incomplete knowledge of the environment. They must also act efficiently, in real time, to deal with dynamic situations. To achieve these objectives, we have developed a robot architecture that is composed of four layers: obstacle avoidance, navigation, path planning, and task planning. The layers are independent, communicating processes that are always active, processing sensory data and status information to update their decisions and actions. A version of our robot architecture has been in nearly daily use in our building since December 1995. As of January 1997, the robot has traveled more than 110 kilometers (65 miles) in service of over 2500 navigation requests that were specified using our World Wide Web interface.
ID:562
CLASS:6
Title: A robotics course using hero I robots
Abstract: An undergraduate Computer Science course in Robotics has been offered for the past two semesters. Supporting the course is a laboratory with six Hero I robots. Students learn how to program the robots in 6808 assembly language.
ID:563
CLASS:6
Title: The scenario and design process of childcare robot, PaPeRo
Abstract: The scenarios and design process for a childcare robot, PaPeRo, are presented. The design process mainly consists of production and evaluations. The production is composed of three observations, namely, one in the user field, one with respect to the world view, and one about the assumed users and results for the basic role of the robot, its personality, and classification of users. Our investigation of available technologies deepened our understanding of the robot performance and characteristics. Preproduction, which consisted of a scenario design, prototyping, and a short play, made the scenario details more sophisticated. A six-month demonstration at the 2005 Aichi Expo started with seven different scenarios, which were improved using additional recognition words, more attractive robot personalities, and additional scenarios through on-site observations and discussion with staff members there. We conducted an extensive study on the robot personalities and recognizable words. Our study will play an important role in the scenario design process.
ID:564
CLASS:6
Title: Optimizing robot algorithms with simulation
Abstract: Maximizing equipment throughput on multi-chambered cluster tools is an ongoing objective for semiconductor fabs. The increasing use of dual-armed robots and the need to process multiple products simultaneously complicates this objective. Typically, when a new processing technology is introduced, one chamber inside the tool is dedicated to the new process, while the other chambers are assigned to run normal production wafers. This results in multiple wafer flows or "parallel routes" within the tool. Determining and implementing optimal robot schedulers to efficiently handle the complexities within the tool is key to maximizing equipment throughput. This paper introduces the components of a multi-chambered cluster tool and discusses how simulation was used at Infineon to develop, test, and optimize efficient wafer selection rules. Several real-world cases are detailed and reported.
ID:565
CLASS:6
Title: Programming modular robots with the TOTA middleware
Abstract: Modular robots represent a perfect application scenario for multiagent coordination. The autonomous modules composing the robot must coordinate their respective activities to enforce a specific global shape or a coherent motion gait. Here we show how the TOTA ("Tuples On The Air") middleware can be effectively exploited to support agents' coordination in this context. The key idea in TOTA is to rely on spatially distributed tuples, spread across the robot, to guide the agents' activities in moving and reshaping the robot. Three simulated examples are presented to support our claims.
ID:566
CLASS:6
Title: Multi-robot learning with particle swarm optimization
Abstract: We apply an adapted version of Particle Swarm Optimization to distributed unsupervised robotic learning in groups of robots with only local information. The performance of the learning technique for a simple task is compared across robot groups of various sizes, with the maximum group size allowing each robot to individually contain and manage a single PSO particle. Different PSO neighborhoods based on limitations of real robotic communication are tested in this scenario, and the effect of varying communication power is explored. The algorithms are then applied to a group learning scenario to explore their susceptibility to the credit assignment problem. Results are discussed and future work is proposed.
ID:567
CLASS:6
Title: Division of labor in a group of robots inspired by ants' foraging behavior
Abstract: In this article, we analyze the behavior of a group of robots involved in an object retrieval task. The robots' control system is inspired by a model of ants' foraging. This model emphasizes the role of learning in the individual. Individuals adapt to the environment using only locally available information. We show that a simple parameter adaptation is an effective way to improve the efficiency of the group and that it brings forth division of labor between the members of the group. Moreover, robots that are best at retrieving have a higher probability of becoming active retrievers. This selection of the best members does not use any explicit representation of individual capabilities. We analyze this system and point out its strengths and its weaknesses.
ID:568
CLASS:6
Title: Where you point is where the robot is
Abstract: It is virtually envisioned that in the near future home-service robots will be assisting people in their daily lives. While a wide spectrum of utility of home-service robots has been proposed, i.e., cleaning, surveillance or go-and-fetch jobs, usability studies of the home-service robots have been less undertaken. This paper explores the usability issues, in particular, a map-based user interface for instructing home-service robots in the home environment. It focused on how the different map representation of the co-located environment would affect task performance of locating the home-service robots. The effectiveness of the map-based human-robot interface was thus analysed according to the dimensionality of the map, the location information of the elements in the co-located workspace. The experimental results showed that task performance was varied by the different map representation, providing a better understanding of what characteristics of the map representation were able to effectively support the human operator in instructing the home-service robots in the home environment.
ID:569
CLASS:6
Title: Evolutionary motion design for humanoid robots
Abstract: We propose a new approach to generating the motion of humanoid robots intuitively by means of Interactive Evolutionary Computation (IEC). In our system, novice users are able to design effective motions through the subjective evaluation of displayed individuals, even if they do not have any technical knowledge. The motions evolved by the IEC system are not necessarily stable nor feasible in real environments. Thus, appropriate adjustments are required to revise the motions. For this purpose, we use a real-valued GA in a dynamic simulator. We empirically show the effectiveness of our approach by designing a kick motion for a humanoid robot.
ID:570
CLASS:6
Title: SHAGE: a framework for self-managed robot software
Abstract: Behavioral, situational and environmental changes in complex software, such as robot software, cannot be completely captured in software design. To handle this dynamism, self-managed software enables its services dynamically adapted to various situations by reconfiguring its software architecture during run-time. We have developed a practical framework, called SHAGE (Self-Healing, Adaptive, and Growing SoftwarE), to support self-managed software for intelligent service robots. The SHAGE framework is composed of six main elements: a situation monitor to identify internal and external conditions of a software system, ontology-based models to describe architecture and components, brokers to find appropriate architectural reconfiguration patterns and components for a situation, a reconfigurator to actually change the architecture based on the selected reconfiguration pattern and components, a decision maker/learner to find the optimal solution of reconfiguring software architecture for a situation, and repositories to effectively manage and share architectural reconfiguration patterns, components, and problem solving strategies. We conducted an experiment of applying the framework to an infotainment robot. The result of the experiment shows the practicality and usefulness of the framework for the intelligent service robots.
ID:571
CLASS:6
Title: UML-based service robot software development: a case study
Abstract: The research field of Intelligent Service Robots, which has become more and more popular over the last years, covers a wide range of applications from climbing machines for cleaning large storefronts to robotic assistance for disabled or elderly people. When developing service robot software, it is a challenging problem to design the robot architecture by carefully considering user needs and requirements, implement robot application components based on the architecture, and integrate these components in a systematic and comprehensive way for maintainability and reusability. Furthermore, it becomes more difficult to communicate among development teams and with others when many engineers from different teams participate in developing the service robot. To solve these problems, we applied the COMET design method, which uses the industry-standard UML notation, to developing the software of an intelligent service robot for the elderly, called T-Rot, under development at Center for Intelligent Robotics (CIR). In this paper, we discuss our experiences with the project in which we successfully addressed these problems and developed the autonomous navigation system of the robot with the COMET/UML method.
ID:572
CLASS:6
Title: How contingent should a communication robot be?
Abstract: The purpose of our research is to develop lifelike behavior in a communication robot, which is expected to potentially make human-robot interaction more natural. Our earlier research demonstrated the importance of a robot's contingency for lifelikeness [1]. On the other hand, perfect contingency seems to give us a non-lifelike impression. In order to explore the appropriate contingency for communication robots, we developed a robot system that allows us to adjust its contingency to an interacting person in a simple mimic interaction. As a result of an experiment, we identified the relationships between the degree of contingency and the subjective impressions of lifelikeness, autonomy, and preference. However, the experimental result also seems to suggest the importance of the complexity of interaction for investigating the appropriate contingency of communication robots.
ID:573
CLASS:6
Title: Interactive humanoid robots for a science museum
Abstract: This paper reports on a field trial with interactive humanoid robots at a science museum where visitors are supposed to study and develop an interest in science. In the trial, each visitor wore an RFID tag while looking around the museum's exhibits. Information obtained from the RFID tags was used to direct the robots' interaction with the visitors. The robots autonomously interacted with visitors via gestures and utterances resembling the free play of children [1]. In addition, they performed exhibit-guiding by moving around several exhibits and explaining the exhibits based on sensor information. The robots were highly evaluated by visitors during the two-month trial. Moreover, we conducted an experiment in the field trial to compare the detailed effects of exhibit-guiding and free-play interaction under three operating conditions. This revealed that the combination of the free-play interaction and exhibit-guiding positively affected visitors' experiences at the science museum.
ID:574
CLASS:6
Title: The effect of head-nod recognition in human-robot conversation
Abstract: This paper reports on a study of human participants with a robot designed to participate in a collaborative conversation with a human. The purpose of the study was to investigate a particular kind of gestural feedback from human to the robot in these conversations: head nods. During these conversations, the robot recognized head nods from the human participant. The conversations between human and robot concern demonstrations of inventions created in a lab. We briefly discuss the robot hardware and architecture and then focus the paper on a study of the effects of understanding head nods in three different conditions. We conclude that conversation itself triggers head nods by people in human-robot conversations and that telling participants that the robot recognizes their nods as well as having the robot provide gestural feedback of its nod recognition is effective in producing more nods.
ID:575
CLASS:6
Title: Service robots in the domestic environment: a study of the roomba vacuum in the home
Abstract: Domestic service robots have long been a staple of science fiction and commercial visions of the future. Until recently, we have only been able to speculate about what the experience of using such a device might be. Current domestic service robots, introduced as consumer products, allow us to make this vision a reality.This paper presents ethnographic research on the actual use of these products, to provide a grounded understanding of how design can influence human-robot interaction in the home. We used an ecological approach to broadly explore the use of this technology in this context, and to determine how an autonomous, mobile robot might "fit" into such a space. We offer initial implications for the design of these products: first, the way the technology is introduced is critical; second, the use of the technology becomes social; and third, that ideally, homes and domestic service robots must adapt to each other.
ID:576
CLASS:6
Title: Children and robots learning to play hide and seek
Abstract: How do children learn how to play hide and seek? At age 3-4, children do not typically have perspective taking ability, so their hiding ability should be extremely limited. We show through a case study that a 3 1/2 year old child can, in fact, play a credible game of hide and seek, even though she does not seem to have perspective taking ability. We propose that children are able to learn how to play hide and seek by learning the features and relations of objects (e.g., containment, under) and use that information to play a credible game of hide and seek. We model this hypothesis within the ACT-R cognitive architecture and put the model on a robot, which is able to mimic the child's hiding behavior. We also take the "hiding" model and use it as the basis for a "seeking" model. We suggest that using the same representations and procedures that a person uses allows better interaction between the human and robotic system.
ID:577
CLASS:6
Title: Analysis of human behavior to a communication robot in an open field
Abstract: This paper investigates human behavior around an interactive robot at a science museum. To develop a communication robot that works in daily environments, it is important to investigate the available information from a robot about people's behavior. Such information will enable the robot to predict people's behavior so that the robot can optimize its interactive behavior. We analyzed visitor behavior toward a simple interactive robot exhibited at a science museum in relation to information from sound level and range sensors. We discovered factors that influence the way people approach, maintain distance, and interact both physically and verbally with the robot. This enabled us to extract meaningful information from the sensory information and apply it to communication robots.
ID:578
CLASS:6
Title: The utility of affect expression in natural language interactions in joint human-robot tasks
Abstract: Recognizing and responding to human affect is important in collaborative tasks in joint human-robot teams. In this paper we present an integrated affect and cognition architecture for HRI and report results from an experiment with this architecture that shows that expressing affect and responding to human affect with affect expressions can significantly improve team performance in a joint human-robot task.
ID:579
CLASS:6
Title: The advisor robot: tracing people's mental model from a robot's physical attributes
Abstract: Humanoid robots offer many physical design choices such as voice frequency and head dimensions. We used hierarchical statistical mediation analysis to trace differences in people's mental model of robots from these choices. In an experiment, a humanoid robot gave participants online advice about their health. We used mediation analysis to identify the causal path from the robot's voice and head dimensions to the participants' mental model, and to their willingness to follow the robot's advice. The male robot voice predicted impressions of a knowledgeable robot, whose advice participants said they would follow. Increasing the voice's fundamental frequency reduced this effect. The robot's short chin length (but not its forehead dimensions) predicted impressions of a sociable robot, which also predicted intentions to take the robot's advice. We discuss the use of this approach for designing robots for different roles, when people's mental model of the robot matters.
ID:580
CLASS:6
Title: Empirical results from using a comfort level device in human-robot interaction studies
Abstract: This paper describes an extensive analysis of the comfort level data of 7 subjects with respect to 12 robot behaviours as part of a human-robot interaction trial. This includes robot action, proximity and motion relative to the subjects. Two researchers coded the video material, identifying visible states of discomfort displayed by subjects in relation to the robot's behaviour. Agreement between the coders varied from moderate to high, except for more ambiguous situations involving robot approach directions. The detected visible states of discomfort were correlated with the situations where the comfort level device (CLD) indicated states of discomfort. Results show that the uncomfortable states identified by both coders, and by either of the coders corresponded with 31% and 64% of the uncomfortable states identified by the subjects' CLD data (N=58), respectively. Conversely there was 72% agreement between subjects' CLD data and the uncomfortable states identified by both coders (N=25). Results show that the majority of the subjects expressed discomfort when the robot blocked their path or was on a collision course towards them, especially when the robot was within 3 meters proximity. Other observations include that the majority of subjects experienced discomfort when the robot was closer than 3m, within the social zone reserved for human-human face to face conversation, while they were performing a task. The advantages and disadvantages of the CLD in comparison to other techniques for assessing subjects' internal states are discussed and future work concludes the paper.
ID:581
CLASS:6
Title: Interactions with a moody robot
Abstract: This paper reports on the results of a long-term experiment in which a social robot's facial expressions were changed to reflect different moods. While the facial changes in each condition were not extremely different, they still altered how people interacted with the robot. On days when many visitors were present, average interactions with the robot were longer when the robot displayed either a "happy" or a "sad" expression instead of a neutral face, but the opposite was true for low-visitor days. The implications of these findings for human-robot social interaction are discussed.
ID:582
CLASS:6
Title: How may I serve you?: a robot companion approaching a seated person in a helping context
Abstract: This paper presents the combined results of two studies that investigated how a robot should best approach and place itself relative to a seated human subject. Two live Human Robot Interaction (HRI) trials were performed involving a robot fetching an object that the human had requested, using different approach directions. Results of the trials indicated that most subjects disliked a frontal approach, except for a small minority of females, and most subjects preferred to be approached from either the left or right side, with a small overall preference for a right approach by the robot. Handedness and occupation were not related to these preferences. We discuss the results of the user studies in the context of developing a path planning system for a mobile robot.
ID:583
CLASS:6
Title: On natural language dialogue with assistive robots
Abstract: This paper examines the appropriateness of natural language dialogue (NLD) with assistive robots. Assistive robots are defined in terms of an existing human-robot interaction taxonomy. A decision support procedure is outlined for assistive technology researchers and practitioners to evaluate the appropriateness of NLD in assistive robots. Several conjectures are made on when NLD may be appropriate as a human-robot interaction mode.
ID:584
CLASS:6
Title: Encouraging physical therapy compliance with a hands-Off mobile robot
Abstract: This paper presents results toward our ongoing research program into hands-off assistive human-robot interaction [6]. Our work has focused on applications of socially assistive robotics in health care and education, where human supervision can be significantly augmented and complemented by intelligent machines. In this paper, we focus on the role of embodiment, empirically addressing the question: "In what ways can the robot's physical embodiment be used effectively to positively influence human task-related behavior?" We hypothesized that users' personalities would correlate with their preferences of robot behavior expression. To test this hypothesis, we implemented an autonomous mobile robot aimed at the role of a monitoring and encouragement system for stroke patient rehabilitation. We performed a pilot study that indicates that the presence and behavior of the robot can influence how well people comply with their physical therapy.
ID:585
CLASS:6
Title: Effects of adaptive robot dialogue on information exchange and social relations
Abstract: Human-robot interaction could be improved by designing robots that engage in adaptive dialogue with users. An adaptive robot could estimate the information needs of individuals and change its dialogue to suit these needs. We test the value of adaptive robot dialogue by experimentally comparing the effects of adaptation versus no adaptation on information exchange and social relations. In Experiment 1, a robot chef adapted to novices by providing detailed explanations of cooking tools; doing so improved information exchange for novice participants but did not influence experts. Experiment 2 added incentives for speed and accuracy and replicated the results from Experiment 1 with respect to information exchange. When the robot's dialogue was adapted for expert knowledge (names of tools rather than explanations), expert participants found the robot to be more effective, more authoritative, and less patronizing. This work suggests adaptation in human-robot interaction has consequences for both task performance and social cohesion. It also suggests that people may be more sensitive to social relations with robots when under task or time pressure.
ID:586
CLASS:6
Title: Teaching robots by moulding behavior and scaffolding the environment
Abstract: Programming robots to carry out useful tasks is both a complex and non-trivial exercise. A simple and intuitive method to allow humans to train and shape robot behaviour is clearly a key goal in making this task easier. This paper describes an approach to this problem based on studies of social animals where two teaching strategies are applied to allow a human teacher to train a robot by moulding its actions within a carefully scaffolded environment. Within these enviroments sets of competences can be built by building stateslash action memory maps of the robot's interaction within that environment. These memory maps are then polled using a k-nearest neighbour based algorithm to provide a generalised competence. We take a novel approach in building the memory models by allowing the human teacher to construct them in a hierarchical manner. This mechanism allows a human trainer to build and extend an action-selection mechanism into which new skills can be added to the robot's repertoire of existing competencies. These techniques are implemented on physical Khepera miniature robots and validated on a variety of tasks.
ID:587
CLASS:6
Title: Changing shape: improving situation awareness for a polymorphic robot
Abstract: Polymorphic, or shape-shifting, robots can normally tackle more types of tasks than non-polymorphic robots due to their flexible morphology. Their versatility adds to the challenge of designing a human interface, however. To investigate the utility of providing awareness information about the robot's physical configuration (or "pose"), we performed a within-subjects experiment with presence or absence of pose information being the independent variable. We found that participants were more likely to tip the robot or have it ride up on obstacles when they used the display that lacked pose information and also more likely to move the robot to the highest position to become oriented. There was no significant difference in the number of times that participants bumped into obstacles, however, indicating that having more awareness of the robot's state does not affect awareness of the robots' immediate surroundings. Participants thought the display with pose information was easier to use, helped their performance and was more enjoyable than having no pose information. Future research directions point toward providing recommendations to robot operators for which pose they should change to given the terrain to be traversed.
ID:588
CLASS:6
Title: Interaction debugging: an integral approach to analyze human-robot interaction
Abstract: Along with the development of interactive robots, controlled experiments and field trials are regularly conducted to stage human-robot interaction. Experience in this field has shown that analyzing human-robot interaction for evaluation purposes fosters the development of improved systems and the generation of new knowledge. In this paper, we present the interaction debugging approach, which is based on the collection and analysis of data from robots and their environment. Considering the multimodality of robotic technology, often only audio and video are insufficient for detailed analysis of human-robot interaction. Therefore, in our analysis we integrate multimodal information using audio, video, sensory data, and intermediate variables. An important aspect of the interaction debugging approach is using a tool called Interaction Debugger to analyze data. By supporting user-friendly data presentation, annotation and navigation, Interaction Debugger enables fine-grained inspection of human-robot interaction. The main goal of this paper is to address how an integral approach to the analysis of human-robot interaction can be adopted. This is demonstrated by three case studies.
ID:589
CLASS:6
Title: Developer oriented visualisation of a robot program
Abstract: Robot programmers are faced with the challenging problem of understanding the robot's view of its world, both when creating and when debugging robot software. As a result tools are created as needed in different laboratories for different robots and different applications. We discuss the requirements for effective interaction under these conditions, and propose an augmented reality approach to visualising robot input, output and state information, including geometric data such as laser range scans, temporal data such as the past robot path, conditional data such as possible future robot paths, and statistical data such as localisation distributions. The visualisation techniques must scale appropriately as robot data and complexity increases. Our current progress in developing a robot visualisation toolkit is presented.
ID:590
CLASS:6
Title: The human-robot interaction operating system
Abstract: In order for humans and robots to work effectively together, they need to be able to converse about abilities, goals and achievements. Thus, we are developing an interaction infrastructure called the ``Human-Robot Interaction Operating System'' (HRI/OS). The HRI/OS provides a structured software framework for building human-robot teams, supports a variety of user interfaces, enables humans and robots to engage in task-oriented dialogue, and facilitates integration of robots through an extensible API.
ID:591
CLASS:6
Title: Common metrics for human-robot interaction
Abstract: This paper describes an effort to identify common metrics for task-oriented human-robot interaction (HRI). We begin by discussing the need for a toolkit of HRI metrics. We then describe the framework of our work and identify important biasing factors that must be taken into consideration. Finally, we present suggested common metrics for standardization and a case study. Preparation of a larger, more detailed toolkit is in progress.
ID:592
CLASS:6
Title: Development of a test bed for evaluating human-robot performance for explosive ordnance disposal robots
Abstract: This paper discusses the development of a test bed to evaluate the combined performance of the human operator and an explosive ordnance disposal robot. We have other means of evaluating the capabilities of the robots but for the robots to be truly useful it is necessary to understand how effectively and efficiently operators will be able to use these robots in critical situations. In this paper we discuss the tasks developed for the test bed and how we are going about development of the metrics for assessing the human-robot performance and, more specifically, the human-robot user interface.
ID:593
CLASS:6
Title: Task planning for human-robot interaction
Abstract: Human-robot interaction requires explicit reasoning on the human environment and on the robot capacities to achieve its tasks in a collaborative way with a human partner.This paper focuses on organization of the robot decisional abilities and more particularly on the management of human interaction as an integral part of the robot control architecture. Such an architecture should be the framework that will allow the robot to accomplish its tasks but also produce behaviors that support its engagement vis-a-vis its human partner and interpret similar behaviors from him.Together and in coherence with this framework, we intend to develop and experiment various task planners and interaction schemes, that will allow the robot to select and perform its tasks while taking into account explicitly the constraints imposed by the presence of humans, their needs and preferences.We have considered a scheme where the robot plans for itself and for the human in order not only (1) to assess the feasibility of the task (at a certain level) before performing it, but also (2) to share the load between the robot and the human and (3) to explain/illustrate a possible course of action.
ID:594
CLASS:6
Title: Designing robot applications for everyday environments
Abstract: We report from the workshop "Designing robot applications for everyday use". This event gathered robot researchers and interaction designers from several countries in order to push robot application domains in novel directions. This article presents the methods we used for breaking out of limited views of robots, and our process for refining ideas to more realistic product opportunities. Based on the results of the workshop, we discuss current challenges of extending the design space of novel robot product ideas.
ID:595
CLASS:6
Title: Everyday robotics: robots as everyday objects
Abstract: Why are we not living yet with robots? If robots are not common everyday objects, it is maybe because we have looked for robotic applications without considering with sufficient attention what could be the experience of interacting with a robot. This article introduces the idea of a value profile, a notion intended to capture the general evolution of our experience with different kinds of objects. After discussing value profiles of commonly used objects, it offers a rapid outline of the challenging issues that must be investigated concerning immediate, short-term and long-term experience with robots. Beyond science-fiction classical archetypes, the picture emerging from this analysis is the one of versatile everyday robots, autonomously developing in interaction with humans, communicating with one another, changing shape and body in order to be adapted to their various context of use. To become everyday objects, robots will not necessary have to be useful, but they will have to be at the origins of radically new forms of experiences.
ID:596
CLASS:7
Title: Assigning document identifiers to enhance compressibility of Web Search Engines indexes
Abstract: Granting efficient accesses to the index is a key issue for the performances of Web Search Engines (WSE). In order to enhance memory utilization and favor fast query resolution, WSEs use Inverted File (IF) indexes where the posting lists are stored as sequences of d_gaps (i.e. differences among successive document identifiers) compressed using variable length encoding methods. This paper describes the use of a lightweight clustering algorithm aimed at assigning the identifiers to documents in a way that minimizes the average values of d_gaps. The simulations performed on a real dataset, i.e. the Google contest collection, show that our approach allows to obtain an IF index which is, depending on the d_gap encoding chosen, up to 23% smaller than the one built over randomly assigned document identifiers. Moreover, we will show, both analytically and empirically, that the complexity of our algorithm is linear in space and time.
ID:597
CLASS:7
Title: Inverted files for text search engines
Abstract: The technology underlying text search engines has advanced dramatically in the past decade. The development of a family of new index representations has led to a wide range of innovations in index storage, index construction, and query evaluation. While some of these developments have been consolidated in textbooks, many specific techniques are not widely known or the textbook descriptions are out of date. In this tutorial, we introduce the key techniques in the area, describing both a core implementation and how the core can be enhanced through a range of extensions. We conclude with a comprehensive bibliography of text indexing literature.
ID:598
CLASS:7
Title: Optimizing result prefetching in web search engines with segmented indices
Abstract: We study the process in which search engines with segmented indices serve queries. In particular, we investigate the number of result pages that search engines should prepare during the query processing phase.Search engine users have been observed to browse through very few pages of results for queries that they submit. This behavior of users suggests that prefetching many results upon processing an initial query is not efficient, since most of the prefetched results will not be requested by the user who initiated the search. However, a policy that abandons result prefetching in favor of retrieving just the first page of search results might not make optimal use of system resources either.We argue that for a certain behavior of users, engines should prefetch a constant number of result pages per query. We define a concrete query processing model for search engines with segmented indices, and analyze the cost of such prefetching policies. Based on these costs, we show how to determine the constant that optimizes the prefetching policy. Our results are mostly applicable to local index partitions of the inverted files, but are also applicable to processing short queries in global index architectures.
ID:599
CLASS:7
Title: Experiences with selecting search engines using metasearch
Abstract: Search engines are among the most useful and high-profile resources on the Internet. The problem of finding information on the Internet has been replaced with the problem of knowing where search engines are, what they are designed to retrieve, and how to use them. This article describes and evaluates SavvySearch, a metasearch engine designed to intelligently select and interface with multiple remote search engines. The primary metasearch issue examined is the importance of carefully selecting and ranking remote search engines for user queries. We studied the efficacy of SavvySearch's incrementally acquired metaindex approach to selecting search engines by analyzing the effect of time and experience on performance. We also compared the metaindex approach to the  simpler categorical approach and showed how much experience is required to surpass the simple scheme.
ID:600
CLASS:7
Title: Boosting the performance of Web search engines: Caching and prefetching query results by exploiting historical usage data
Abstract: This article discusses efficiency and effectiveness issues in caching the results of queries submitted to a Web search engine (WSE). We propose SDC (Static Dynamic Cache), a new caching strategy aimed to efficiently exploit the temporal and spatial locality present in the stream of processed queries. SDC extracts from historical usage data the results of the most frequently submitted queries and stores them in a static, read-only portion of the cache. The remaining entries of the cache are dynamically managed according to a given replacement policy and are used for those queries that cannot be satisfied by the static portion. Moreover, we improve the hit ratio of SDC by using an adaptive prefetching strategy, which anticipates future requests by introducing a limited overhead over the back-end WSE. We experimentally demonstrate the superiority of SDC over purely static and dynamic policies by measuring the hit ratio achieved on three large query logs by varying the cache parameters and the replacement policy used for managing the dynamic part of the cache. Finally, we deploy and measure the throughput achieved by a concurrent version of our caching system. Our tests show how the SDC cache can be efficiently exploited by many threads that concurrently serve the queries of different users.
ID:601
CLASS:7
Title: Incorporating agent based neural network model for adaptive meta-search
Abstract: In the current information age, the web is increasing at a very rapid pace, while the indexes of the current Search Engines are not scaling up at the same pace resulting in the loss of access to a good fraction of documents on the web. An intriguing alternative is a Meta-Search Engine, which provides a unified access to several Search Engines thereby increasing the coverage of the web. Though using Meta-Search Engines, the coverage of the web is increased, maintaining a good precision can be a problem especially if one or more of the Search Engine's returns irrelevant documents for certain user queries. This paper proposes a novel, intelligent, and adaptive approach to improve the precision of the meta-search results. This approach uses an adaptive agent based neural network model to improve the quality of the search results by incorporating user relevance feedback in to the system.
ID:602
CLASS:7
Title: A semisupervised learning method to merge search engine results
Abstract: The proliferation of searchable text databases on local area networks and the Internet causes the problem of finding information that may be distributed among many disjoint text databases (distributed information retrieval). How to merge the results returned by selected databases is an important subproblem of the distributed information retrieval task. Previous research assumed that either resource providers cooperate to provide normalizing statistics or search clients download all retrieved documents and compute normalized scores without cooperation from resource providers.This article presents a semisupervised learning solution to the result merging problem. The key contribution is the observation that information used to create resource descriptions for resource selection can also be used to create a centralized sample database to guide the normalization of document scores returned by different databases. At retrieval time, the query is sent to the selected databases, which return database-specific document scores, and to a centralized sample database, which returns database-independent document scores. Documents that have both a database-specific score and a database-independent score serve as training data for learning to normalize the scores of other documents. An extensive set of experiments demonstrates that this method is more effective than the well-known CORI result-merging algorithm under a variety of conditions.
ID:603
CLASS:7
Title: Optimal crawling strategies for web search engines
Abstract: Web Search Engines employ multiple so-called crawlers to maintain local copies of web pages. But these web pages are frequently updated by their owners, and therefore the crawlers must regularly revisit the web pages to maintain the freshness of their local copies. In this paper, we propose a two-part scheme to optimize this crawling process. One goal might be the minimization of the average level of staleness over all web pages, and the scheme we propose can solve this problem. Alternatively, the same basic scheme could be used to minimize a possibly more important search engine embarrassment level metric: The frequency with which a client makes a search engine query and then clicks on a returned url only to find that the result is incorrect. The first part our scheme determines the (nearly) optimal crawling frequencies, as well as the theoretically optimal times to crawl each web page. It does so within an extremely general stochastic framework, one which supports a wide range of complex update patterns found in practice. It uses techniques from probability theory and the theory of resource allocation problems which are highly computationally efficient -- crucial for practicality because the size of the problem in the web environment is immense. The second part employs these crawling frequencies and ideal crawl times as input, and creates an optimal achievable schedule for the crawlers. Our solution, based on network flow theory, is exact as well as highly efficient. An analysis of the update patterns from a highly accessed and highly dynamic web site is used to gain some insights into the properties of page updates in practice. Then, based on this analysis, we perform a set of detailed simulation experiments to demonstrate the quality and speed of our approach.
ID:604
CLASS:7
Title: Parallel programming in modern web search engines
Abstract: When a Search Engine responds to your query, thousands of machines from around the world have cooperated to produce your result. With a global reach of hundreds-of-millions of users, Search Engines are arguably the most commonly used massively-parallel computing systems on the planet.In this talk, we examine Web Search Engines as a case study of parallel programming in a practical context. We focus primarily on the practice of parallel programming, reviewing many ways in which parallel programming is used in a modern Search Engine. We also discuss briefly the principles of parallel programming, listing some of the principles that guide our use of parallelism and speculating a bit on how the mechanics of parallelism might better be automated in our context.
ID:605
CLASS:7
Title: Modeling search engine effectiveness for federated search
Abstract: Federated search links multiple search engines into a single, virtual search system. Most prior research of federated search focused on selecting search engines that have the most relevant contents, but ignored the retrieval effectiveness of individual search engines. This omission can cause serious problems when federating search engines of different qualities.This paper proposes a federated search technique that uses utility maximization to model the retrieval effectiveness of each search engine in a federated search environment. The new algorithm ranks the available resources by explicitly estimating the amount of relevant material that each resource can return, instead of the amount of relevant material that each resource contains. An extensive set of experiments demonstrates the effectiveness of the new algorithm.
ID:606
CLASS:7
Title: A module-based integration of information retrieval into undergraduate curricula
Abstract: As more and more digital information available online, the ability to conduct proficient information retrieval (IR) is becoming a key factor for success in almost any career. However, integration of IR into undergraduate curricula has not kept up with this demand. To address primary problems in the current college-level IR education, this paper proposes a module-based curricular model, which facilitates the development of an array of IR modules that enable flexible adoption and integration. These modules, including general IR modules mainly designed for IT/CS programs, and discipline-oriented IR modules based on specific disciplines such as Biology and Psychology, can be adopted based on the specific needs of different universities and programs.
ID:607
CLASS:7
Title: Comparison of two approaches to building a vertical search tool: a case study in the nanotechnology domain
Abstract: As the Web has been growing exponentially, it has become increasingly difficult to search for desired information. In recent years, many domain-specific (vertical) search tools have been developed to serve the information needs of specific fields. This paper describes two approaches to building a domain-specific search tool. We report our experience in building two different tools in the nanotechnology domain -- (1) a server-side search engine, and (2) a client-side search agent. The designs of the two search systems are presented and discussed, and their strengths and weaknesses are compared. Some future research directions are also discussed.
ID:608
CLASS:7
Title: Efficient and effective metasearch for a large number of text databases
Abstract: Metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources (text databases). In a metasearch engine, the contents of each local database is represented by a representative. Each user query is evaluated against the set of representatives of all databases in order to determine the appropriate databases to search. When the number of databases is very large, say in the order of tens of thousands or more, then a traditional metasearch engine may become inefficient as each query needs to be evaluated against too many database representatives. Furthermore, the storage requirement on the site containing the metasearch engine can be very large. In this paper, we propose to use a hierarchy of database representatives to improve the efficiency. We provide an algorithm to search the hierarchy. We show that the retrieval effectiveness of our algorithm is the same as that of evaluating the user query against all database representatives. We also show that our algorithm is efficient. In addition, we propose an alternative way of allocating representatives to sites so that the storage burden on the site containing the metasearch engine is much reduced.
ID:609
CLASS:7
Title: Web based information for product ranking in e-business: a fuzzy approach
Abstract: In this paper we have introduced a methodology to rank the available products in the Internet market. These rankings are based on the customers' own preferences and also on the information in the different search engines about the products. Linguistically defined customers' preferences about the products or product attributes are directly collected from the customers. Where as the search engines are used to accumulate the web based information of other customers' about the product. The aggregation of buyer's preferences and search engines' information is interpreted here as a measure of relevance of the search engines in providing the need based information to the buyer. Weighted average of the products across the search engines with weights as the relevance degrees of the search engines help us to obtain the product rankings in Internet market. The methodology of ordering of fuzzy subsets in the unit interval assists to obtain the preference ranking of the products in the e-business site.
ID:610
CLASS:7
Title: Shuffling a stacked deck: the case for partially randomized ranking of search engine results
Abstract: In-degree, PageRank, number of visits and other measures of Web page popularity significantly influence the ranking of search results by modern search engines. The assumption is that popularity is closely correlated with quality, a more elusive concept that is difficult to measure directly. Unfortunately, the correlation between popularity and quality is very weak for newly-created pages that have yet to receive many visits and/or in-links. Worse, since discovery of new content is largely done by querying search engines, and because users usually focus their attention on the top few results, newly-created but high-quality pages are effectively "shut out," and it can take a very long time before they become popular.We propose a simple and elegant solution to this problem: the introduction of a controlled amount of randomness into search result ranking methods. Doing so offers new pages a chance to prove their worth, although clearly using too much randomness will degrade result quality and annul any benefits achieved. Hence there is a tradeoff between exploration to estimate the quality of new pages and exploitation of pages already known to be of high quality. We study this tradeoff both analytically and via simulation, in the context of an economic objective function based on aggregate result quality amortized over time. We show that a modest amount of randomness leads to improved search results.
ID:611
CLASS:7
Title: User-centric Web crawling
Abstract: Search engines are the primary gateways of information access on the Web today. Behind the scenes, search engines crawl the Web to populate a local indexed repository of Web pages, used to answer user search queries. In an aggregate sense, the Web is very dynamic, causing any repository of Web pages to become out of date over time, which in turn causes query answer quality to degrade. Given the considerable size, dynamicity, and degree of autonomy of the Web as a whole, it is not feasible for a search engine to maintain its repository exactly synchronized with the Web.In this paper we study how to schedule Web pages for selective (re)downloading into a search engine repository. The scheduling objective is to maximize the quality of the user experience for those who query the search engine. We begin with a quantitative characterization of the way in which the discrepancy between the content of the repository and the current content of the live Web impacts the quality of the user experience. This characterization leads to a user-centric metric of the quality of a search engine's local repository. We use this metric to derive a policy for scheduling Web page (re)downloading that is driven by search engine usage and free of exterior tuning parameters. We then focus on the important subproblem of scheduling refreshing of Web pages already present in the repository, and show how to compute the priorities efficiently. We provide extensive empirical comparisons of our user-centric method against prior Web page refresh strategies, using real Web data. Our results demonstrate that our method requires far fewer resources to maintain same search engine quality level for users, leaving substantially more resources available for incorporating new Web pages into the search repository.
ID:612
CLASS:7
Title: Website navigation architectures and their effect on website visibility: a literature survey
Abstract: Search engines hold a promise of delivering relevant and useful information to the human user. The primary objective of this research project is to compare and report on different types of navigation schemes, their advantages and disadvantages, and the impact they have on the visibility of a webpage to a search engine crawler. The method employed was to review relevant literature, compare the advantages and disadvantages of navigation architectures and to reach a conclusion. It was found that a number of options are offered to the designer of a website, including text-based links, navigation buttons, image maps, JavaScript, Flash elements, hidden menus and frames. All of these elements appeared to have a question mark over their positive contribution to the visibility of a webpage. The primary conclusion reached is that navigation architecture used on a website does impact its visibility to a search engine crawler. The webpage designer should exercise care in choosing a navigation scheme. One option is to duplicate navigation schemes to please both human and crawler visitors, which could add to clutter and information overload. Finally, some areas for further research are identified.
ID:613
CLASS:7
Title: Primarily history: historians and the search for primary source materials
Abstract: This paper describes the first phase of an international project that is exploring how historians locate primary resource materials in the digital age, what they are teaching their Ph.D. students about finding research materials, and what archivists are doing to facilitate access to these materials. Preliminary findings are presented from a survey of 300 historians studying American History from leading institutions of higher education in the U.S. Tentative conclusions indicate the need to provide multiple pathways of access to historical research materials including paper-based approaches and newer digital ones. The need for user education, especially in regard to electronic search methodologies is indicated.
ID:614
CLASS:7
Title: Incremental clustering for profile maintenance in information gathering web agents
Abstract: User profiles are the central component of most personalized Web information agents. They consist of a set of models representing the various topics of interest to the user. Often the agent learns the user's preferences from examples of documents deemed relevant to the user. The topic of the document can either be supplied by the user (active modeling), or it must be guessed by the agent (passive modeling), which is more convenient but is expected to diminish the agent's accuracy. We present an empirical study assessing the trade-offs in passive versus active document classification. We compare a manual profile maintenance technique in which the user supplies the document topic, and two incremental clustering methods (greedy and the doubling algorithm) for automated maintenance of the user profile components. The study is performed using our SurfAgent, a testbed information gathering Web agent. Our evaluation methodology exploits the strong parallel between Web information agents and text filtering; we use text filtering benchmarks from the information retrieval community (TREC disk \#5) to simulate user behavior and thus speed up data collection, exert additional experimental control and improve the objectivity of our results.
ID:615
CLASS:7
Title: Efficient and effective metasearch for text databases incorporating linkages among documents
Abstract: Linkages among documents have a significant impact on the importance of documents, as it can be argued that important documents are pointed to by many documents or by other important documents. Metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources (text databases). There is a search engine associated with each database. In a large-scale metasearch engine, the contents of each local database is represented by a representative. Each user query is evaluated against he set of representatives of all databases in order to determine the appropriate databases (search engines) to search (invoke) In previous word, the linkage information between documents has not been utilized in determining the appropriate databases to search. In this paper, such information is employed to determine the degree of relevance of a document with respect to a given query. Specifically, the importance (rank) of each document as determined by the linkages is integrated in each database representative to facilitate the selection of databases for each given query. We establish a necessary and sufficient condition to rank databases optimally, while incorporating the linkage information. A method is provided to estimate the desired quantities stated in the necessary and sufficient condition. The estimation method runs in time linearly proportional to the number of query terms. Experimental results are provided to demonstrate the high retrieval effectiveness of the method.
ID:616
CLASS:7
Title: Tools and approaches for developing data-intensive Web applications: a survey
Abstract: The exponential growth and capillar diffusion of the Web are nurturing a novel generation of applications, characterized by a direct business-to-customer relationship. The development of such applications is a hybrid between traditional IS development and Hypermedia authoring, and challenges the existing tools and approaches for software production. This paper investigates the current situation of Web development tools, both in the commercial and research fields, by identifying and characterizing different categories of solutions, evaluating their adequacy to the requirements of Web application development, enlightening open problems, and exposing possible future trends.
ID:617
CLASS:7
Title: Identifying redundant search engines in a very large scale metasearch engine context
Abstract: For a given set of search engines, a search engine is redundant if its searchable contents can be found from other search engines in this set. In this paper, we propose a method to identify redundant search engines in a very large-scale metasearch engine context. The general problem is equivalent to an NP hard problem -- the set-covering problem. Due to the large number of search engines that need to be considered and the large sizes of these search engines, approximate solutions must be developed. In this paper, we propose a general methodology to tackle this problem and within the context of this methodology, we propose several new heuristic algorithms for solving the set-covering problem.
ID:618
CLASS:7
Title: Automated gathering of Web information: An in-depth examination of agents interacting with search engines
Abstract: The Web has become a worldwide repository of information which individuals, companies, and organizations utilize to solve or address various information problems. Many of these Web users utilize automated agents to gather this information for them. Some assume that this approach represents a more sophisticated method of searching. However, there is little research investigating how Web agents search for online information. In this research, we first provide a classification for information agent using stages of information gathering, gathering approaches, and agent architecture. We then examine an implementation of one of the resulting classifications in detail, investigating how agents search for information on Web search engines, including the session, query, term, duration and frequency of interactions. For this temporal study, we analyzed three data sets of queries and page views from agents interacting with the Excite and AltaVista search engines from 1997 to 2002, examining approximately 900,000 queries submitted by over 3,000 agents. Findings include: (1) agent sessions are extremely interactive, with sometimes hundreds of interactions per second (2) agent queries are comparable to human searchers, with little use of query operators, (3) Web agents are searching for a relatively limited variety of information, wherein only 18&percnt; of the terms used are unique, and (4) the duration of agent-Web search engine interaction typically spans several hours. We discuss the implications for Web information agents and search engines.
ID:619
CLASS:7
Title: Efficient query processing in geographic web search engines
Abstract: Geographic web search engines allow users to constrain and order search results in an intuitive manner by focusing a query on a particular geographic region. Geographic search technology, also called local search, has recently received significant interest from major search engine companies. Academic research in this area has focused primarily on techniques for extracting geographic knowledge from the web. In this paper, we study the problem of efficient query processing in scalable geographic search engines. Query processing is a major bottleneck in standard web search engines, and the main reason for the thousands of machines used by the major engines. Geographic search engine query processing is different in that it requires a combination of text and spatial data processing techniques. We propose several algorithms for efficient query processing in geographic search engines, integrate them into an existing web search query processor, and evaluate them on large sets of real data and query traces.
ID:620
CLASS:7
Title: Finding the search engine that works for you
Abstract: A search engine evaluation model that considers over seventy performance and feature parameters is presented. The design of a web-based system that allows the user to tailor the model to his/her own preference, and to evaluate search engines of interest, is introduced. The results presented to the user identify the most suitable search engine that suits his/her needs.
ID:621
CLASS:7
Title: Internet search engines: past and future
Abstract: I will review the short history of Internet Search Engines from early first generation systems to the current crop of stock market darlings. Many of the underlying technology problems remain the same, but the business has become significantly more sophisticated and high-powered. I will touch on some of the economics driving the remarkable success of these services and make some predictions about future trends.
ID:622
CLASS:7
Title: Fully automatic wrapper generation for search engines
Abstract: When a query is submitted to a search engine, the search engine returns a dynamically generated result page containing the result records, each of which usually consists of a link to and/or snippet of a retrieved Web page. In addition, such a result page often also contains information irrelevant to the query, such as information related to the hosting site of the search engine and advertisements. In this paper, we present a technique for automatically producing wrappers that can be used to extract search result records from dynamically generated result pages returned by search engines. Automatic search result record extraction is very important for many applications that need to interact with search engines such as automatic construction and maintenance of metasearch engines and deep Web crawling. The novel aspect of the proposed technique is that it utilizes both the visual content features on the result page as displayed on a browser and the HTML tag structures of the HTML source file of the result page. Experimental results indicate that this technique can achieve very high extraction accuracy.
ID:623
CLASS:7
Title: Site-to-site (s2s) searching using the p2p framework with cgi
Abstract: Peer-To-Peer (P2P) networks like Gnutella improve some shortcomings of Conventional Search Engines (CSE) such as centralized and outdated indexing by distributing the search engines over the peers, which maintain their updated local contents. But they are designed for sharing and searching the contents in personal computers instead of websites. In this work, we propose a novel web information retrieval method called Site-To-Site (S2S) searching, which uses the P2P framework with CGI as protocol. It helps the site owners to turn their websites into autonomous search engines without extra hardware and software costs. In this paper, we introduce S2S searching with some related work. We also describe the system architecture and communication protocol. Finally, we summarize the experimental results, and show that S2S searching works well in one thousand sites.
ID:624
CLASS:7
Title: Assigning identifiers to documents to enhance the clustering property of fulltext indexes
Abstract: Web Search Engines provide a large-scale text document retrieval service by processing huge Inverted File indexes. Inverted File indexes allow fast query resolution and good memory utilization since their d-gaps representation can be effectively and efficiently compressed by using variable length encoding methods. This paper proposes and evaluates some algorithms aimed to find an assignment of the document identifiers which minimizes the average values of d-gaps, thus enhancing the effectiveness of traditional compression methods. We ran several tests over the Google contest collection in order to validate the techniques proposed. The experiments demonstrated the scalability and effectiveness of our algorithms. Using the proposed algorithms, we were able to sensibly improve (up to 20.81%) the compression ratios of several encoding schemes.
ID:625
CLASS:7
Title: WebKDD 2006: web mining and web usage analysis post-workshop report
Abstract: In this report, we summarize the contents and outcomes of the recent WebKDD 2006 workshop on Web Mining and Web Usage Analysis that was held in conjunction with the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2006), August 20-23, 2006, in Philadelphia, Pennsylvania. In 2006, WebKDD was organized for the eighth time. It solicited papers on the broad overview subject "Knowledge Discovery on the Web" and on new directions for Web mining research. We reflect on the results of the workshop, as captured in presentations and plenary discussions.
ID:626
CLASS:7
Title: Interest-based personalized search
Abstract: Web search engines typically provide search results without considering user interests or context. We propose a personalized search approach that can easily extend a conventional search engine on the client side. Our mapping framework automatically maps a set of known user interests onto a group of categories in the Open Directory Project (ODP) and takes advantage of manually edited data available in ODP for training text classifiers that correspond to, and therefore categorize and personalize search results according to user interests. In two sets of controlled experiments, we compare our personalized categorization system (PCAT) with a list interface system (LIST) that mimics a typical search engine and with a nonpersonalized categorization system (CAT). In both experiments, we analyze system performances on the basis of the type of task and query length. We find that PCAT is preferable to LIST for information gathering types of tasks and for searches with short queries, and PCAT outperforms CAT in both information gathering and finding types of tasks, and for searches associated with free-form queries. From the subjects' answers to a questionnaire, we find that PCAT is perceived as a system that can find relevant Web pages quicker and easier than LIST and CAT.
ID:627
CLASS:7
Title: Building domain-specific web collections for scientific digital libraries: a meta-search enhanced focused crawling method
Abstract: Collecting domain-specific documents from the Web using focused crawlers has been considered one of the most important strategies to build digital libraries that serve the scientific community. However, because most focused crawlers use local search algorithms to traverse the Web space, they could be easily trapped within a limited sub-graph of the Web that surrounds the starting URLs and build domain-specific collections that are not comprehensive and diverse enough to scientists and researchers. In this study, we investigated the problems of traditional focused crawlers caused by local search algorithms and proposed a new crawling approach, meta-search enhanced focused crawling, to address the problems. We conducted two user evaluation experiments to examine the performance of our proposed approach and the results showed that our approach could build domain-specific collections with higher quality than traditional focused crawling techniques.
ID:628
CLASS:7
Title: Can e-learning replace classroom learning?
Abstract: In an e-learning environment that emphasizes learner-centered activity and system interactivity, remote learners can outperform traditional classroom students.
ID:629
CLASS:7
Title: Web mining in search engines
Abstract: Given the rate of growth of the Web, scalability of search engines is a key issue, as the amount of hardware and network resources needed is large, and expensive. In addition, search engines are popular tools, so they have heavy constraints on query answer time. So, the efficient use of resources can improve both scalability and answer time. One tool to achieve these goals is Web mining. Web mining has three branches: link mining, usage mining, and content mining. One important analysis in all these cases is the dynamic behavior. Here we give examples of link and usage mining related to search engines, as well as the related Web dynamics.
ID:630
CLASS:7
Title: Using titles and category names from editor-driven taxonomies for automatic evaluation
Abstract: Evaluation of IR systems has always been difficult because of the need for manually assessed relevance judgments. The advent of large editor-driven taxonomies on the web opens the door to a new evaluation approach. We use the ODP (Open Directory Project) taxonomy to find sets of pseudo-relevant documents via one of two assumptions: 1) taxonomy entries are relevant to a given query if their editor-entered titles exactly match the query, or 2) all entries in a leaf-level taxonomy category are relevant to a given query if the category title exactly matches the query. We compare and contrast these two methodologies by evaluating six web search engines on a sample from an America Online log of ten million web queries, using MRR measures for the first method and precision-based measures for the second. We show that this technique is stable with respect to the query set selected and correlated with a reasonably large manual evaluation.
ID:631
CLASS:7
Title: Advertising, profits, switching costs, and the Internet
Abstract: In this paper, I model the online media market. There are three types of players in the market: advertisers, publishers, and users. The advertising side of the market is competitive and publishers are price takers. To draw users, they compete in quality. I find that without any frictions, publishers will earn zero profits; however, if users face switching costs, publishers earn positive profits because they deteriorate quality to the locked-in users. I provide empirical evidence for this predicted correlation between financial performance and switching costs in advertising-supported online markets.
ID:632
CLASS:7
Title: Coverage, relevance, and ranking: The impact of query operators on Web search engine results
Abstract: Research has reported that about 10&percnt; of Web searchers utilize advanced query operators, with the other 90&percnt; using extremely simple queries. It is often assumed that the use of query operators, such as Boolean operators and phrase searching, improves the effectiveness of Web searching. We test this assumption by examining the effects of query operators on the performance of three major Web search engines. We selected one hundred queries from the transaction log of a Web search service. Each of these original queries contained query operators such as AND, OR, MUST APPEAR (+), or PHRASE (" "). We then removed the operators from these one hundred advanced queries. We submitted both the original and modified queries to three major Web search engines; a total of 600 queries were submitted and 5,748 documents evaluated. We compared the results from the original queries with the operators to the results from the modified queries without the operators. We examined the results for changes in coverage, relative precision, and ranking of relevant documents. The use of most query operators had no significant effect on coverage, relative precision, or ranking, although the effect varied depending on the search engine. We discuss implications for the effectiveness of searching techniques as currently taught, for future information retrieval system design, and for future research.
ID:633
CLASS:7
Title: eBizSearch: a niche search engine for e-business
Abstract: Niche Search Engines offer an efficient alternative to traditional search engines when the results returned by general-purpose search engines do not provide a sufficient degree of relevance. By taking advantage of their domain of concentration they achieve higher relevance and offer enhanced features. We discuss a new niche search engine, eBizSearch, based on the technology of CiteSeer and dedicated to e-business and e-business documents. We present the integration of CiteSeer in the framework of eBizSearch and the process necessary to tune the whole system towards the specific area of e-business. We also discuss how using machine learning algorithms we generate metadata to make eBizSearch Open Archives compliant. eBizSearch is a publicly available service and can be reached at [3].
ID:634
CLASS:7
Title: Mining topic-specific concepts and definitions on the web
Abstract: Traditionally, when one wants to learn about a particular topic, one reads a book or a survey paper. With the rapid expansion of the Web, learning in-depth knowledge about a topic from the Web is becoming increasingly important and popular. This is also due to the Web's convenience and its richness of information. In many cases, learning from the Web may even be essential because in our fast changing world, emerging topics appear constantly and rapidly. There is often not enough time for someone to write a book on such topics. To learn such emerging topics, one can resort to research papers. However, research papers are often hard to understand by non-researchers, and few research papers cover every aspect of the topic. In contrast, many Web pages often contain intuitive descriptions of the topic. To find such Web pages, one typically uses a search engine. However, current search techniques are not designed for in-depth learning. Top ranking pages from a search engine may not contain any description of the topic. Even if they do, the description is usually incomplete since it is unlikely that the owner of the page has good knowledge of every aspect of the topic. In this paper, we attempt a novel and challenging task, mining topic-specific knowledge on the Web. Our goal is to help people learn in-depth knowledge of a topic systematically on the Web. The proposed techniques first identify those sub-topics or salient concepts of the topic, and then find and organize those informative pages, containing definitions and descriptions of the topic and sub-topics, just like those in a book. Experimental results using 28 topics show that the proposed techniques are highly effective.
ID:635
CLASS:7
Title: A multi-paradigm querying approach for a generic multimedia database management system
Abstract: To truly meet the requirements of multimedia database (MMDB) management, an integrated framework for modeling, managing and retrieving various kinds of media data in a uniform way is necessary. MediaLand is an experimental MMDB platform being developed at Microsoft Research Asia for users with different levels of experiences and expertise to manage and search multimedia repositories easily, efficiently, and cooperatively. Key features of MediaLand include a uniform data model for describing all kinds of media objects and their relationships, and a 4-tier architecture based on this data model. In this paper, a multi-paradigm querying approach of MediaLand is presented, in which multimedia queries are processed based on a seamless integration of various existing search approaches. In doing so, MediaLand also offers the feature of "media independence" which is analogous to the notion of "data independence" from the classic ANSI SPARC standard. By incorporating a rich set of facilities and techniques, MediaLand lays down a good foundation for addressing further research issues, such as multimedia query rewriting, optimization, and presentation.
ID:636
CLASS:7
Title: Using sampled data and regression to merge search engine results
Abstract: This paper addresses the problem of merging results obtained from different databases and search engines in a distributed information retrieval environment. The prior research on this problem either assumed the exchange of statistics necessary for normalizing scores (cooperative solutions) or is heuristic. Both approaches have disadvantages. We show that the problem in uncooperative environments is simpler when viewed as a component of a distributed IR system that uses query-based sampling to create resource descriptions. Documents sampled for creating resource descriptions can also be used to create a sample centralized index, and this index is a source of training data for adaptive results merging algorithms. A variety of experiments demonstrate that this new approach is more effective than a well-known alternative, and that it allows query-by-query tuning of the results merging function.
ID:637
CLASS:7
Title: Using navigation data to improve IR functions in the context of web search
Abstract: As part of the process of delivering content, devices like proxies and gateways log valuable information about the activities and navigation patterns of users on the Web. In this study, we consider how this navigation data can be used to improve Web search. A query posted to a search engine together with the set of pages accessed during a search task is known as a search session. We develop a mixture model for the observed set of search sessions, and propose variants of the classical EM algorithm for training. The model itself yields a type of navigation-based query clustering. By implicitly borrowing strength between related queries, the mixture formulation allows us to identify the "highly relevant" URLs for each query cluster. Next, we explore methods for incorporating existing labeled data (the Yahoo! directory, for example) to speed convergence and help resolve low-traffic clusters. Finally, the mixture formulation also provides for a simple, hierarchical display of search results based on the query clusters. The effectiveness of our approach is evaluated using proxy access logs for the outgoing Lucent proxy.
ID:638
CLASS:7
Title: Merging techniques for performing data fusion on the web
Abstract: Data fusion on the Web refers to the merging, into a unified single list, of the ranked document lists, which are retrieved in response to a user query by more than one Web search engine. It is performed by metasearch engines and their merging algorithms utilise the information present in the ranked lists of retrieved documents provided to them by the underlying search engines, such as the rank positions of the retrieved documents and their retrieval scores. In this paper, merging techniques are introduced that take into account not only the rank positions, but also the title and the summary accompanying the retrieved documents. Furthermore, the data fusion process is viewed as being similar to the combination of belief in uncertain reasoning and is modelled using Dempster-Shafer's theory of evidence. Our evaluation experiments indicate that the above merging techniques yield improvements in the effectiveness and that their effectiveness is comparable to that of the approach that merges the ranked lists by downloading and analysing the Web documents.
ID:639
CLASS:7
Title: Adaptively constructing the query interface for meta-search engines
Abstract: With the exponential growth of information on the Internet, current information integration systems have become more and more unsuitable for this "Internet age" due to the great diversity among sources. This paper presents a constraint-based query user interface model, which can be applied to the construction of dynamically generated adaptive user interfaces for meta-search engines.
ID:640
CLASS:7
Title: Topical locality in the Web
Abstract: Most web pages are linked to others with related content. This idea, combined with another that says that text in, and possibly around, HTML anchors describe the pages to which they point, is the foundation for a usable World-Wide Web. In this paper, we examine to what extent these ideas hold by empirically testing whether topical locality mirrors spatial locality of pages on the Web. In particular, we find that the likelihood of linked pages having similar textual content to be high; the similarity of sibling pages increases when the links from the parent are close together; titles, descriptions, and anchor text represent at least part of the target page; and that anchor text may be a useful discriminator among unseen child pages. These results show the foundations necessary for the success of many web systems, including search engines, focused crawlers, linkage analyzers, and intelligent web agents.
ID:641
CLASS:7
Title: Constructing, organizing, and visualizing collections of topically related Web resources
Abstract: For many purposes, the Web page is too small a unit of interaction and analysis. Web sites are structured multimedia documents consisting of many pages, and users often are interested in obtaining and evaluating entire collections of topically related sites.  Once such a collection is obtained, users face the challenge of exploring, comprehending and organizing the items. We report four innovations that address these user needs: (1) we replaced the Web page with the Web site as the basic unit of interaction and analysis;(2) we defined a new informationstructure, the clan graph, that groups together sets of related sites; (3) we augment the representation of a site with a site profile, information about site structure and content that helps inform  user evaluation of a site; and (4) we invented a new graph visualization, the auditorium visualization, that reveals important structural and content properties of sites within a clan graph. Detailed analysis and user studies document the utility of this approach. The clan graph construction algorithm tends to filter out irrelevant sites and discover additional relevant items. The auditorium visualization, augmented with drill-down capabilities to explore site profile data, helps users to find high-quality sites as well as sites that serve a particular function.
ID:642
CLASS:7
Title: CiteSeer: an autonous Web agent for automatic retrieval and identification of interesting publications
Abstract: Research papers available on the World Wide Web (WWW or Web) areoften poorly organized, often exist in forms opaque to searchengines (e.g. Postscript), and increase in quantity daily.Significant amounts of time and effort are typically needed inorder to find interesting and relevant publications on the Web. Wehave developed a Web based information agent that assists the userin the process of performing a scientific literature search. Givena set of keywords, the agent uses Web search engines and heuristicsto locate and download papers. The papers are parsed in order toextract information features such as the abstract and individuallyidentified citations. The agents Web interface can be used to findrelevant papers in the database using keyword searches, or bynavigating the links between papers formed by the citations. Linksto both citing and cited publications can be followed. In additionto simple browsing and keyword searches, the agent can find paperswhich are similar to a given paper using word information and byanalyzing common citations made by the papers.
ID:643
CLASS:7
Title: A multi-similarity algebra
Abstract: The need to automatically extract and classify the contents of multimedia data archives such as images, video, and text documents has led to significant work on similarity based retrieval of data. To date, most work in this area has focused on the creation of index structures for similarity based retrieval. There is very little work on developing formalisms for querying multimedia databases that support similarity based computations and optimizing such queries, even though it is well known that feature extraction and identification algorithms in media data are very expensive. We introduce a similarity algebra that brings together relational operators and results of multiple similarity implementations in a uniform language. The algebra can be used to specify complex queries that combine different interpretations of similarity values and multiple algorithms for computing these values. We prove equivalence and containment relationships between similarity algebra expressions and develop query rewriting methods based on these results. We then provide a generic cost model for evaluating cost of query plans in the similarity algebra and query optimization methods based on this model. We supplement the paper with experimental results that illustrate the use of the algebra and the effectiveness of query optimization methods using the Integrated Search Engine (I.SEE) as the testbed.
ID:644
CLASS:7
Title: Automatic extraction of dynamic record sections from search engine result pages
Abstract: A search engine returned result page may contain search results that are organized into multiple dynamically generated sections in response to a user query. Furthermore, such a result page often also contains information irrelevant to the query, such as information related to the hosting site of the search engine. In this paper, we present a method to automatically generate wrappers for extracting search result records from all dynamic sections on result pages returned by search engines. This method has the following novel features: (1) it aims to explicitly identify all dynamic sections, including those that are not seen on sample result pages used to generate the wrapper, and (2) it addresses the issue of correctly differentiating sections and records. Experimental results indicate that this method is very promising. Automatic search result record extraction is critical for applications that need to interact with search engines such as automatic construction and maintenance of metasearch engines and deep Web crawling.
ID:645
CLASS:7
Title: The impact of search engine optimization on online advertising market
Abstract: Online advertising market is becoming a popular area of academic research. Among other types of advertising, search engine advertising is leading the growth in terms of revenue. In general, there are two types of search engine advertising: paid placement and search engine optimization (SEO). This study aims to analyze the condition under which SEO exist and further, its impact on the advertising market. With an analytical model, several interesting insights are generated. The results of the study fill the gap of SEO in academic research and help managers in online advertising make informed advertising decisions.
ID:646
CLASS:7
Title: Implementation and evaluation of a quality-based search engine
Abstract: In this paper, an approach for the implementation of a quality-based Web search engine is proposed. Quality retrieval is introduced and an overview on previous efforts to implement such a service is given. Machine learning approaches are identified as the most promising methods to determine the quality of Web pages. Features for the most appropriate characterization of Web pages are determined. A quality model is developed based on human judgments. This model is integrated into a meta search engine which assesses the quality of all results at run time. The evaluation results show that quality based ranking does lead to better results concerning the perceived quality of Web pages presented in the result set. The quality models are exploited to identify potentially important features and characteristics for the quality of Web pages.
ID:647
CLASS:7
Title: Report on the 7<sup>th</sup> ACM International Workshop on Web Information and Data Management (WIDM 2005)
Abstract: The 7th ACM International Workshop on Web Information and Data Management (WIDM 2005) was held at the Hilton Bremen Hotel in Bremen (Germany), on November 5, 2005, in conjunction with the 14th ACM International Conference on Information and Knowledge Management (CIKM 2005). The main objective of the workshop was to bring together researchers, industrial practitioners, and developers to discuss how Web information can be extracted, stored, analyzed, and processed to provide useful knowledge to the end users for various advanced database and Web applications. WIDM 2005 was sponsored by ACM SIGIR and by GI (Gesellshaft f&uuml;r Informatik) and was in cooperation with ACM SIGMOD.
ID:648
CLASS:7
Title: Bibliometric impact measures leveraging topic analysis
Abstract: Measurements of the impact and history of research literature provide a useful complement to scientific digital library collections. Bibliometric indicators have been extensively studied, mostly in the context of journals. However, journal-based metrics poorly capture topical distinctions in fast-moving fields, and are increasingly problematic with the rise of open-access publishing. Recent developments in latent topic models have produced promising results for automatic sub-field discovery. The fine-grained, faceted topics produced by such models provide a clearer view of the topical divisions of a body of research literature and the interactions between those divisions. We demonstrate the usefulness of topic models in measuring impact by applying a new phrase-based topic discovery model to a collection of 300,000 Computer Science publications, collected by the Rexa automatic citation indexing system.
ID:649
CLASS:7
Title: Efficient query subscription processing for prospective search engines
Abstract: Current web search engines are retrospective in that they limit users to searches against already existing pages. Prospective search engines, on the other hand, allow users to upload queries that will be applied to newly discovered pages in the future. We study and compare algorithms for efficiently matching large numbers of simple keyword queries against a stream of newly discovered pages.
ID:650
CLASS:7
Title: Searching with context
Abstract: Contextual search refers to proactively capturing the information need of a user by automatically augmenting the user query with information extracted from the search context; for example, by using terms from the web page the user is currently browsing or a file the user is currently editing.We present three different algorithms to implement contextual search for the Web. The first, it query rewriting (QR), augments each query with appropriate terms from the search context and uses an off-the-shelf web search engine to answer this augmented query. The second,  rank-biasing (RB), generates a representation of the context and answers queries using a custom-built search engine that exploits this representation. The third, iterative filtering meta-search (IFM), generates multiple subqueries based on the user query and appropriate terms from the search context, uses an off-the-shelf search engine to answer these subqueries, and re-ranks the results of the subqueries using rank aggregation methods.We extensively evaluate the three methods using 200 contexts and over 24,000 human relevance judgments of search results. We show that while QR works surprisingly well, the relevance and recall can be improved using RB and substantially more using IFM. Thus, QR, RB, and IFM represent a cost-effective design spectrum for contextual search.
ID:651
CLASS:7
Title: Detecting spam web pages through content analysis
Abstract: In this paper, we continue our investigations of "web spam": the injection of artificially-created pages into the web in order to influence the results from search engines, to drive traffic to certain pages for fun or profit. This paper considers some previously-undescribed techniques for automatically detecting spam pages, examines the effectiveness of these techniques in isolation and when aggregated using classification algorithms. When combined, our heuristics correctly identify 2,037 (86.2%) of the 2,364 spam pages (13.8%) in our judged collection of 17,168 pages, while misidentifying 526 spam and non-spam pages (3.1%).
ID:652
CLASS:7
Title: SIGIR workshop report: the SIGIR heterogeneous and distributed information retrieval workshop
Abstract: In the last few years there have been the explosion in the use of heterogeneous distributed systems. Ranging from simple Network of Workstations to the more modern and complex Grid systems, the adoption of distributed systems instead of massively parallel supercomputers has been preferred due to their reduced cost of ownership. These kinds of systems pose many challenges in terms of information access, storage and retrieval. Usually, in fact, instead of having collections stored at a single site they are collected, and sometimes managed, at different sites (possibly owned by different institutions). Particular interest has been expressed on architectures and specifications for information retrieval in the context of heterogeneous distributed computing systems.
ID:653
CLASS:7
Title: DirectoryRank: ordering pages in web directories
Abstract: Web Directories are repositories of Web pages organized in a hierarchy of topics and sub-topics. In this paper, we present DirectoryRank, a ranking framework that orders the pages within a given topic according to how informative they are about the topic. Our method works in three steps: first, it processes Web pages within a topic in order to extract structures that are called lexical chains, which are then used for measuring how informative a page is for a particular topic. Then, it measures the relative semantic similarity of the pages within a topic. Finally, the two metrics are combined for ranking all the pages within a topic before presenting them to the users.
ID:654
CLASS:7
Title: Report on the 6th ACM international workshop on web information and data management (WIDM 2004) held at CIKM 2004
Abstract: The 6th ACM International Workshop on Web Information and Data Management (WIDM 2004) was held at the Hyatt Arlington Hotel, in Washington DC, on November 12-13, 2004, in conjunction with the 13th ACM International Conference on Information and Knowledge Management (CIKM 2004). The main objective of the workshop was to bring together researchers, industrial practitioners, and developers to discuss how Web information can be extracted, stored, analyzed, and processed to provide useful knowledge to the end users for various advanced database and Web applications. As for previous years, WIDM 2004 was sponsored by ACM SIGIR.
ID:655
CLASS:7
Title: SpidersRUs: automated development of vertical search engines in different domains and languages
Abstract: In this paper we discuss the architecture of a tool designed to help users develop vertical search engines in different domains and different languages. The design of the tool is presented and an evaluation study was conducted, showing that the system is easier to use than other existing tools.
ID:656
CLASS:7
Title: Downloading textual hidden web content through keyword queries
Abstract: An ever-increasing amount of information on the Web today is available only through search interfaces: the users have to type in a set of keywords in a search form in order to access the pages from certain Web sites. These pages are often referred to as the Hidden Web or the Deep Web. Since there are no static links to the Hidden Web pages, search engines cannot discover and index such pages and thus do not return them in the results. However, according to recent studies, the content provided by many Hidden Web sites is often of very high quality and can be extremely valuable to many users.In this paper, we study how we can build an effective Hidden Web crawler that can autonomously discover and download pages from the Hidden Web. Since the only "entry point" to a Hidden Web site is a query interface, the main challenge that a Hidden Web crawler has to face is how to automatically generate meaningful queries to issue to the site. Here, we provide a theoretical framework to investigate the query generation problem for the Hidden Web and we propose effective policies for generating queries automatically. Our policies proceed iteratively, issuing a different query in every iteration. We experimentally evaluate the effectiveness of these policies on 4 real Hidden Web sites and our results are very promising. For instance, in one experiment, one of our policies downloaded more than 90% of a Hidden Web site (that contains 14 million documents) after issuing fewer than 100 queries.
ID:657
CLASS:7
Title: An analysis of search engine switching behavior using click streams
Abstract: In this paper, we propose a simple framework to characterize the switching behavior between search engines based on click streams. We segment users into a number of categories based on their search engine usage during two adjacent time periods and construct the transition probability matrix across these usage categories. The principal eigenvector of the transposed transition probability matrix represents the limiting probabilities, which are proportions of users in each usage category at steady state. We experiment with this framework using click streams focusing on two search engines: one with a large market share and the other with a small market share. The results offer interesting insights into search engine switching. The limiting probabilities provide empirical evidence that small engines can still retain its fair share of users over time.
ID:658
CLASS:7
Title: Using a web-based categorization approach to generate thematic metadata from texts
Abstract: Conventional tools for automatic metadata creation mostly extract named entities or text segments from texts and annotate them with information about persons, locations, dates, and so on. However, this kind of entity type information is often insufficient for machines to understand the facts contained in the texts, thus precluding the possibility of implementing more advanced, intelligent applications, such as concept-based search. In this work, we try to create more refined thematic metadata inherent in texts. Based on Web resource mining, our approach acquires training corpora necessary to describe both the thematic categories and the metadata extracted from the texts. The approach then finds the corresponding relationships among them by means of categorization and thus generates thematic metadata for the textual data. Experimental results confirm the potential and wide adaptability of our approach.
ID:659
CLASS:7
Title: A practical web-based approach to generating topic hierarchy for text segments
Abstract: It is crucial in many information systems to organize short text segments, such as keywords in documents and queries from users, into a well-formed topic hierarchy. In this paper, we address the problem of generating topic hierarchies for diverse text segments with a general and practical approach that uses the Web as an additional knowledge source. Unlike long documents, short text segments typically do not contain enough information to extract reliable features. This work investigates the possibilities of using highly ranked search-result snippets to enrich the representation of text segments. A hierarchical clustering algorithm is then applied to create the hierarchical topic structure of text segments. Different from traditional clustering algorithms, which tend to produce cluster hierarchies with a very unnatural shape, the approach tries to produce a more natural and comprehensive hierarchy. Extensive experiments were conducted on different domains of text segments. The obtained results have shown the potential of the proposed approach, which is believed able to benefit many information systems.
ID:660
CLASS:7
Title: What Americans like about being online
Abstract: A study of AOL users finds they are particularly gratified using the Net as a source of information, communication, and socializing---results that may be helpful to ISPs in their efforts to attract and retain users.
ID:661
CLASS:7
Title: Autonomous visual model building based on image crawling through internet search engines
Abstract: In this paper, we propose an autonomous learning scheme to automatically build visual semantic concept models from the output data of Internet search engines without any manual labeling work. First of all, images are gathered by crawling through the Internet using a search engine such as Google. Then, we model the search results as "Quasi-Positive Bags" in the Multiple-Instance Learning (MIL) framework. We call this generalized MIL (GMIL). We propose an algorithm called "Bag K-Means" to find the maximum Diverse Density (DD) without the existence of negative bags. A cost function is found as K-Means with special "Bag Distance". We also propose a solution called "Uncertain Labeling Density" (ULD) which describes the target density distribution of instances in the case of quasi-positive bags. A "Bag Fuzzy K-Means" is presented to get the maximum of ULD. By this generalized MIL with ULD, the model for a particular concept is learned from the crawled images of the Internet search engines. Experiments show that our algorithm can get correct models for the concepts we are interested in. Compared to the original Google Image Search, our algorithm shows improved accuracy.
ID:662
CLASS:7
Title: Clustering e-commerce search engines
Abstract: In this paper, we sketch a method for clustering e-commerce search engines by the type of products/services they sell. This method utilizes the special features of interface pages of such search engines. We also provide an analysis of different types of ESE interface pages.
ID:663
CLASS:7
Title: Towards a more natural and intelligent interface with embodied conversation agent
Abstract: Conversational agent also known as chatterbots are computer programs which are designed to converse like a human as much as their intelligent allows. In many ways, they are the embodiment of Turing's vision. The ability for computers to converse with human users using natural language would arguably increase their usefulness. Recent advances in Natural Language Processing (NLP) and Artificial Intelligence (AI) in general have advances this field in realizing the vision of a more humanoid interactive system. This paper presents and discusses the use of embodied conversation agent (ECA) for the imitation games. This paper also presents the technical design of our ECA and its performance. In the interactive media industry, it can also been observed that the ECA are getting popular.
ID:664
CLASS:7
Title: Modeling user behavior using a search-engine
Abstract: A model of user-search-engine interaction is developed using the ACT-R cognitive architecture. We test, using an empirical evaluation, the model across different result orderings and relevance distributions, demonstrating that across a number of trials, the model approximates the characteristics of large numbers of users interacting with search-engines. These results are discussed in terms of their practical implications for search interfaces and ranking algorithms.
ID:665
CLASS:7
Title: Mining context specific similarity relationships using the world wide web
Abstract: We have studied how context specific web corpus can be automatically created and mined for discovering semantic similarity relationships between terms (words or phrases) from a given collection of documents (target collection). These relationships between terms can be used to adjust the standard vectors space representation so as to improve the accuracy of similarity computation between text documents in the target collection. Our experiments with a standard test collection (Reuters) have revealed the reduction of similarity errors by up to 50%, twice as much as the improvement by using other known techniques.
ID:666
CLASS:7
Title: Discretization based learning approach to information retrieval
Abstract: We approached the problem as learning how to order documents by estimated relevance with respect to a user query. Our support vector machines based classifier learns from the relevance judgments available with the standard test collections and generalizes to new, previously unseen queries. For this, we have designed a representation scheme, which is based on the discrete representation of the local (lw) and global (gw) weighting functions, thus is capable of reproducing and enhancing the properties of such popular ranking functions as tf.idf, BM25 or those based on language models. Our tests with the standard test collections have demonstrated the capability of our approach to achieve the performance of the best known scoring functions solely from the labeled examples and without taking advantage of knowing those functions or their important properties or parameters.
ID:667
CLASS:7
Title: Learning a spelling error model from search query logs
Abstract: Applying the noisy channel model to search query spelling correction requires an error model and a language model. Typically, the error model relies on a weighted string edit distance measure. The weights can be learned from pairs of misspelled words and their corrections. This paper investigates using the Expectation Maximization algorithm to learn edit distance weights directly from search query logs, without relying on a corpus of paired words.
ID:668
CLASS:7
Title: Using random walks for question-focused sentence retrieval
Abstract: We consider the problem of question-focused sentence retrieval from complex news articles describing multi-event stories published over time. Annotators generated a list of questions central to understanding each story in our corpus. Because of the dynamic nature of the stories, many questions are time-sensitive (e.g. "How many victims have been found?") Judges found sentences providing an answer to each question. To address the sentence retrieval problem, we apply a stochastic, graph-based method for comparing the relative importance of the textual units, which was previously used successfully for generic summarization. Currently, we present a topic-sensitive version of our method and hypothesize that it can outperform a competitive baseline, which compares the similarity of each sentence to the input question via IDF-weighted word overlap. In our experiments, the method achieves a TRDR score that is significantly higher than that of the baseline.
ID:669
CLASS:7
Title: Web graph analyzer tool
Abstract: We present the software tool "Web Graph Analyzer". This tool is designed to perform a comprehensive analysis of the Web Graph structure. By Web Graph we mean a graph whose vertices are Web pages and whose edges are hyper-links. With the help of the Web Graph Analyzer we can study the local graph characteristics such as numbers and sets of incoming/outgoing links to/from a given page, the page level relative to a given root page, and the global graph characteristics such as PageRank, Giant Strongly Connected Component, the number of dangling nodes. The Web Graph Analyzer has a user friendly GUI that allows an easy collection of a part of WWW and its thorough investigation. The Web Graph Analyzer is based on the Oracle DBMS which scales well with the large volumes of data.
ID:670
CLASS:7
Title: Learning to find answers to questions on the Web
Abstract: We introduce a method for learning to find documents on the Web that contain answers to a given natural language question. In our approach, questions are transformed into new queries aimed at maximizing the probability of retrieving answers from existing information retrieval systems. The method involves automatically learning phrase features for classifying questions into different types, automatically generating candidate query transformations from a training set of question/answer pairs, and automatically evaluating the candidate transformations on target information retrieval systems such as real-world general purpose search engines. At run-time, questions are transformed into a set of queries, and reranking is performed on the documents retrieved. We present a prototype search engine, Tritus, that applies the method to Web search engines. Blind evaluation on a set of real queries from a Web search engine log shows that the method significantly outperforms the underlying search engines, and outperforms a commercial search engine specializing in question answering. Our methodology cleanly supports combining documents retrieved from different search engines, resulting in additional improvement with a system that combines search results from multiple Web search engines.
ID:671
CLASS:7
Title: Web-scale information extraction in knowitall: (preliminary results)
Abstract: Manually querying search engines in order to accumulate a large bodyof factual information is a tedious, error-prone process of piecemealsearch. Search engines retrieve and rank potentially relevantdocuments for human perusal, but do not extract facts, assessconfidence, or fuse information from multiple documents. This paperintroduces KnowItAll, a system that aims to automate the tedious process ofextracting large collections of facts from the web in an autonomous,domain-independent, and scalable manner.The paper describes preliminary experiments in which an instance of KnowItAll, running for four days on a single machine, was able to automatically extract 54,753 facts. KnowItAll associates a probability with each fact enabling it to trade off precision and recall. The paper analyzes KnowItAll's architecture and reports on lessons learned for the design of large-scale information extraction systems.
ID:672
CLASS:7
Title: Impact of search engines on page popularity
Abstract: Recent studies show that a majority of Web page accesses are referred by search engines. In this paper we study the widespread use of Web search engines and its impact on the ecology of the Web. In particular, we study how much impact search engines have on the popularity evolution of Web pages. For example, given that search engines return currently popular" pages at the top of search results, are we somehow penalizing newly created pages that are not very well known yet? Are popular pages getting even more popular and new pages completely ignored? We first show that this unfortunate trend indeed exists on the Web through an experimental study based on real Web data. We then analytically estimate how much longer it takes for a new page to attract a large number of Web users when search engines return only popular pages at the top of search results. Our result shows that search engines can have an immensely worrisome impact on the discovery of new Web pages.
ID:673
CLASS:7
Title: What's new on the web?: the evolution of the web from a search engine perspective
Abstract: We seek to gain improved insight into how Web search engines shouldcope with the evolving Web, in an attempt to provide users with themost up-to-date results possible. For this purpose we collectedweekly snapshots of some 150 Web sites over the course of one year,and measured the evolution of content and link structure. Our measurements focus on aspects of potential interest to search engine designers: the evolution of link structure over time, the rate ofcreation of new pages and new distinct content on the Web, and the rate of change of the content of existing pages under search-centric measures of degree of change.Our findings indicate a rapid turnover rate of Web pages, i.e.,high rates of birth and death, coupled with an even higher rate ofturnover in the hyperlinks that connect them. For pages that persistover time we found that, perhaps surprisingly, the degree of contentshift as measured using TF.IDF cosine distance does not appear to beconsistently correlated with the frequency of contentupdating. Despite this apparent non-correlation, the rate of content shift of a given page is likely to remain consistent over time. That is, pages that change a great deal in one week will likely change by a similarly large degree in the following week. Conversely, pages that experience little change will continue to experience little change. We conclude the paper with a discussion of the potential implications ofour results for the design of effective Web search engines.
ID:674
CLASS:7
Title: Search result exploration: a preliminary study of blind and sighted users' decision making and performance
Abstract: We conducted a preliminary study to examine sighted and blind users' decision-making behavior and performance during the search process. We manipulated the search result's relevance to a task, the search result presentation, and the effort required to process the corresponding web page. We found that users leveraged page features to gauge the amount of effort that is required to explore search pages and made exploration decisions accordingly. Users' desire to know additional page details varied based on their visual ability and the results' relevance. We quantified the cost/benefit tradeoff of additional page features and suggest ways to better support diverse Web searchers.
ID:675
CLASS:7
Title: Toward an ontology-enhanced information filtering agent
Abstract: Whereas search engines assist users in locating initial information sources, often an overwhelmingly large number of ULRs is returned, and the task of browsing websites rests heavily on users. The contribution of this work is developing an information filtering agent (IFA) that assists users in identifying out-of-context web pages and rating the relevance of web pages. An IFA determines the relevance of web pages by adopting three heuristics: (i) detecting evidence phrases (EP) constructed from WORDNET's ontology, (ii) counting the frequencies of EP and (iii) considering the nearness among keywords. Favorable experimental results show that the IFA's ratings of web pages are generally close to human ratings in many instances. The strength and weaknesses of the IFA are also discussed.
ID:676
CLASS:7
Title: An analysis of multimedia searching on AltaVista
Abstract: Web searching is a significant activity for many people seeking multimedia information. Major Web search engines, such as Alta Vista, are essential tools in the quest to locate relevant online information. As such, it is important that we understand how searchers utilize these Web information systems. This paper presents research that examines characteristics of multimedia Web searching on Alta Vista. More specifically, the research questions driving this study are: (1) What are the characteristics of multimedia searching on Alta Vista? and (2) How does this multimedia searching compare to Web searching in general? The results of our research show that multimedia searching is complex relative to general Web searching and that searching specific multimedia collections does not necessarily reduce the searching complexity. We discuss the implications of the findings for the development of online multimedia retrieval systems.
ID:677
CLASS:7
Title: The influence of semantics in IR using LSI and K-means clustering techniques
Abstract: In this paper we study the influence of semantics in the information retrieval preprocessing. We concretely compare the reached performance with stemming and semantic lemmatization as preprocessing. Three techniques are used in the study: the direct use of a weighted matrix, the SVD technique in the LSI model and the bisecting spherical k-means clustering technique. although the results seem not to be very promising, we believe that they should be improved in the future.
ID:678
CLASS:7
Title: Automated index management for distributed web search
Abstract: Distributed heterogeneous search systems are an emerging phenomenon in Web search, in which independent topic-specific search engines provide search services, and metasearchers distribute user's queries to only the most suitable search engines. Previous research has investigated methods for engine selection and merging of search results (i.e. performance improvements from the user's perspective). We focus instead on performance from the service provider's point of view (e.g, income from queries processed vs. resources used to answer them). We consider a scenario in which individual search engines compete for user queries by choosing which documents (topics) to index. The difficulty here stems from the fact that the utilities of local engine actions should depend on the uncertain actions of competitors. Thus, naive strategies (e.g, blindly indexing lots of popular documents) are ineffective. We model the competition between search engines as a stochastic game, and propose a reinforcement learning approach to managing search index contents. We evaluate our approach using a large log of user queries to 47 real search engines.
ID:679
CLASS:7
Title: An adaptive nearest neighbor search for a parts acquisition ePortal
Abstract: One of the major hurdles in maintaining long-lived electronic systems is that electronic parts become obsolete, no longer available from the original suppliers. When this occurs, an engineer is tasked with resolving the problem by finding a replacement that is "as similar as possible" to the original part. The current approach involves a laborious manual search through several electronic portals and data books. The search is difficult because potential replacements may differ from the original and from each other by one or more parameters. Worse still, the cumbersome nature of this process may cause the engineers to miss appropriate solutions amid the many thousands of parts listed in industry catalogs.In this paper, we address this problem by introducing the notion of a parametric "distance" between electronic components. We use this distance to search a large parts data set and recommend likely replacements. Recommendations are based on an adaptive nearest-neighbor search through the parametric data set. For each user, we learn how to scale the axes of the feature space in which the nearest neighbors are sought. This allows the system to learn each user's judgment of the phrase "as similar as possible."
ID:680
CLASS:7
Title: A practical SVM-based algorithm for ordinal regression in image retrieval
Abstract: Most current learning algorithms for image retrieval are based on dichotomy relevance judgement (relevant and non-relevant), though this measurement of relevance is too coarse. To better identify the user needs and preference, a good retrieval system should be able to handle multilevel relevance judgement. In this paper, we focus on relevance feedback with multilevel relevance judgment, where the relevance feedback is considered as an ordinal regression problem. Herbrich has proposed a support vector learning algorithm for ordinal regression based on the Linear Utility Model. His algorithm is intrinsically to train a SVM on a new derived training set, whose size increases rapidly when the original training set gets bigger. This property limits its applicability in relevance feedback, due to real-time requirement of the interactive process. By thoroughly analyzing Herbrich's algorithm, we first propose a new model for ordinal regression, called Cascade Linear Utility Model, then a practical SVM-based algorithm for image retrieval upon it. Our new algorithm is tested on a real-world image database, and compared with other three algorithms capable to handle multilevel relevance judgment. The experimental results show that the retrieval performance of our algorithm is comparable with that of Herbrich's algorithm but with only a fraction of its computational time, and apparently outperform the other methods.
ID:681
CLASS:7
Title: Design method of interaction techniques for large information spaces
Abstract: Our work focuses on the design of interaction techniques for large information spaces. Our goal is not to define yet another visualization technique but to provide insights for the design of such techniques. Our design approach is based on ergonomic criteria that arose from a study of how the user perceives and manipulates a large information space. We then provide design rules that should help the designer in devising an interaction technique that verifies the ergonomic criteria. After the design of the interaction technique, the next step is software design. We establish links between our design rules/ergonomic criteria and the software architecture model. By applying our PAC-Amodeus model, we show how the software architecture model helps to either verify or assess the ergonomic criteria. We therefore adopt a predictive evaluation approach to the design of interaction techniques for large information spaces. We illustrate our design approach and results through our VITESSE system.
ID:682
CLASS:7
Title: Inferring relative popularity of internet applications by actively querying DNS caches
Abstract: In this work, we propose a novel methodology that can be used to assess the relative popularity for any Internet application based on the data servers it uses. The basic idea is to infer popularity of data servers by periodically "poking" at local Domain Name servers (LDNSs) that service Domain Name System requests from a set of users running Internet applications and determining if LDNSs have cached resource records for the data servers. This approach allows us to measure the relative percentage of pokes that result in a cache hit as a coarse measure of the relative popularity of a particular data server among the users of a given LDNS. In addition, the time-to-live (TTL) of cached DNS resource records can be used to measure the gaps in time when a resource record for a data server is not cached. The cache gaps can be used to infer request interarrivals for more popular data servers.The methodology can be applied to any Internet application that uses distinguished server names and performs DNS lookups on these names as part of application use. The methodology can be used to collect usage information from any LDNS that accepts DNS queries. As example applications of the methodology, we evaluate the relative popularity of selected Web sites and the relative popularity of different Web servers serving content at a given Web site. We also apply the methodology to servers providing multimedia content, data servers for grid computing, and network game servers. We use data gathered from LDNSs of commercial and educational sites as well as Internet Service Providers serving both commercial and home customers.
ID:683
CLASS:7
Title: Comparison of allocation rules for paid placement advertising in search engines
Abstract: Web sites such as Internet search engines, web portals, and comparison shopping services, aim to provide information or recommendations to users who might be searching for information or trying to make a purchase decision. Paid placement advertising has established itself as an important revenue resource for such information-oriented web sites, which often deliberately bias their recommendations (or sequence of results) in return for a fee from providers who wish to get preferential placement on the results page. This article examines the paid-placement ranking strategies of the two dominant firms in this industry, and compares their revenues under different scenarios via computational simulation. We find that ranking paid placement links by the product of willingness to pay and relevance is better, in most cases, than ranking by willingness to pay alone, which performs best only when the correlation between the provider's relevance and willingness to pay is large. We also analyze the impact of the competition for placement slots on placement revenues under these mechanisms.
ID:684
CLASS:7
Title: ONTOLOGER: a system for usage-driven management of ontology-based information portals
Abstract: In this paper we present the conceptual architecture of ONTOLOGER, a system for usage-driven management of querying for information in ontology-based information portals. The system is based on the analysis of users' activities that are acquired in the so-called semantic log file, which contains information about the content of a visited page as well. These analyses are used for managing (i) the underlying ontology, (ii) querying mechanism and (iii) the content of the information repository, in order to ensure that (a) the structure of the vocabulary reflects the needs of users, (b) the highly-relevant results can be found by users although they make short queries and read only top-ranked results and (c) the information repository contains (only) resources users are interested in. The focus of the paper is on improving querying (retrieval) mechanism based on the usage data. We present a case study, which shows how a portal can benefit from implementing presented management system.
ID:685
CLASS:7
Title: Comparing the performance of collection selection algorithms
Abstract: The proliferation of online information resources increases the importance of effective and efficient information retrieval in a multicollection environment. Multicollection searching is cast in three parts: collection selection (also referred to as database selection), query processing and results merging. In this work, we focus our attention on the evaluation of the first step, collection selection.In this article, we present a detailed discussion of the methodology that we used to evaluate and compare collection selection approaches, covering both test environments and evaluation measures. We compare the CORI, CVV and gGLOSS collection selection approaches using six test environments utilizing three document testbeds. We note similar trends in performance among the collection selection approaches, but the CORI approach consistently outperforms the other approaches, suggesting that effective collection selection can be achieved using limited information about each collection.The contributions of this work are both the assembled evaluation methodology as well as the application of that methodology to compare collection selection approaches in a standardized environment.
ID:686
CLASS:7
Title: Which semantic web?
Abstract: Through scenarios in the popular press and technical papers in the research literature, the promise of the Semantic Web has raised a number of different expectations. These expectations can be traced to three different perspectives on the Semantic Web. The Semantic Web is portrayed as: (1) a universal library, to be readily accessed and used by humans in a variety of information use contexts; (2) the backdrop for the work of computational agents completing sophisticated activities on behalf of their human counterparts; and (3) a method for federating particular knowledge bases and databases to perform anticipated tasks for humans and their agents. Each of these perspectives has both theoretical and pragmatic entailments, and a wealth of past experiences to guide and temper our expectations. In this paper, we examine all three perspectives from rhetorical, theoretical, and pragmatic viewpoints with an eye toward possible outcomes as Semantic Web efforts move forward.
ID:687
CLASS:7
Title: XRANK: ranked keyword search over XML documents
Abstract: We consider the problem of efficiently producing ranked results for keyword search queries over hyperlinked XML documents. Evaluating keyword search queries over hierarchical XML documents, as opposed to (conceptually) flat HTML documents, introduces many new challenges. First, XML keyword search queries do not always return entire documents, but can return deeply nested XML elements that contain the desired keywords. Second, the nested structure of XML implies that the notion of ranking is no longer at the granularity of a document, but at the granularity of an XML element. Finally, the notion of keyword proximity is more complex in the hierarchical XML data model. In this paper, we present the XRANK system that is designed to handle these novel features of XML keyword search. Our experimental results show that XRANK offers both space and performance benefits when compared with existing approaches. An interesting feature of XRANK is that it naturally generalizes a hyperlink based HTML search engine such as Google. XRANK can thus be used to query a mix of HTML and XML documents.
ID:688
CLASS:7
Title: Peer-to-peer information retrieval using self-organizing semantic overlay networks
Abstract: Content-based full-text search is a challenging problem in Peer-to-Peer (P2P) systems. Traditional approaches have either been centralized or use flooding to ensure accuracy of the results returned.In this paper, we present pSearch, a decentralized non-flooding P2P information retrieval system. pSearch distributes document indices through the P2P network based on document semantics generated by Latent Semantic Indexing (LSI). The search cost (in terms of different nodes searched and data transmitted) for a given query is thereby reduced, since the indices of semantically related documents are likely to be co located in the network.We also describe techniques that help distribute the indices more evenly across the nodes, and further reduce the number of nodes accessed using appropriate index distribution as well as using index samples and recently processed queries to guide the search.Experiments show that pSearch can achieve performance comparable to centralized information retrieval systems by searching only a small number of nodes. For a system with 128,000 nodes and 528,543 documents (from news, magazines, etc.), pSearch searches only 19 nodes and transmits only 95.5KB data during the search, whereas the top 15 documents returned by pSearch and LSI have a 91.7% intersection.
ID:689
CLASS:7
Title: Word sense disambiguation in information retrieval revisited
Abstract: Word sense ambiguity is recognized as having a detrimental effect on the precision of information retrieval systems in general and web search systems in particular, due to the sparse nature of the queries involved. Despite continued research into the application of automated word sense disambiguation, the question remains as to whether less than 90% accurate automated word sense disambiguation can lead to improvements in retrieval effectiveness. In this study we explore the development and subsequent evaluation of a statistical word sense disambiguation system which demonstrates increased precision from a sense based vector space retrieval model over traditional TF*IDF techniques.
ID:690
CLASS:7
Title: Query-independent evidence in home page finding
Abstract: Hyperlink recommendation evidence, that is, evidence based on the structure of a web's link graph, is widely exploited by commercial Web search systems. However there is little published work to support its popularity. Another form of query-independent evidence, URL-type, has been shown to be beneficial on a home page finding task. We compared the usefulness of these types of evidence on the home page finding task, combined with both content and anchor text baselines. Our experiments made use of five query sets spanning three corpora---one enterprise crawl, and the WT10g and VLC2 Web test collections.We found that, in optimal conditions, all of the query-independent methods studied (in-degree, URL-type, and two variants of PageRank) offered a better than random improvement on a content-only baseline. However, only URL-type offered a better than random improvement on an anchor text baseline. In realistic settings, for either baseline, only URL-type offered consistent gains. In combination with URL-type the anchor text baseline was more useful for finding popular home pages, but URL-type with content was more useful for finding randomly selected home pages. We conclude that a general home page finding system should combine evidence from document content, anchor text, and URL-type classification.
ID:691
CLASS:7
Title: Utilizing hyperlink transitivity to improve web page clustering
Abstract: The rapid increase of web complexity and size makes web searched results far from satisfaction in many cases due to a huge amount of information returned by search engines. How to find intrinsic relationships among the web pages at a higher level to implement efficient web searched information management and retrieval is becoming a challenge problem. In this paper, we propose an approach to measure web page similarity. This approach takes hyperlink transitivity and page importance into consideration. From this new similarity measurement, an effective hierarchical web page clustering algorithm is proposed. The primary evaluations show the effectiveness of the new similarity measurement and the improvement of web page clustering. The proposed page similarity, as well as the matrix-based hyperlink analysis methods, could be applied to other web-based research areas.
ID:692
CLASS:7
Title: Experiential computing
Abstract: Seeking insight from data, users experience, explore, and experiment, no longer limited to just generating lists of possible answers to simplistic queries.
ID:693
CLASS:1
Title: Rappit: framework for synthesis of host-assisted scripting engines for adaptive embedded systems
Abstract: Scripting is a powerful, high-level, cross-platform, dynamic, easy way of composing software modules as black boxes. Unfortunately, the high runtime overhead has prevented scripting from being widely adopted in embedded applications. This work proposes to overcome these obstacles by synthesizing light-weight, host-assisted scripting engines for embedded systems. The result is dramatically shortened development cycle due to the much higher-level abstraction, interactive access and dynamic reconfigurability, robust in-field software upgradability, and compact code size. This framework has been successfully applied to ultra low-power sensor nodes with under 10KB of program memory to high-performance platforms with fast Ethernet.
ID:694
CLASS:1
Title: Facial modeling and animation
Abstract: In this course we present an overview of the concepts and current techniques in facial modeling and animation. We introduce this research area by its history and applications. As a necessary prerequisite for facial modeling, data acquisition is discussed in detail. We describe basic concepts of facial animation and present different approaches including parametric models, performance-, physics-, and learning-based methods. State-of-the-art techniques such as muscle-based facial animation, mass-spring networks for skin models, and morphable models are part of these approaches. We furthermore discuss texturing of head models and rendering of skin, addressing problems related to texture synthesis and bump mapping with graphics hardware. Typical applications for facial modeling and animation such as medical and forensic applications (craniofacial surgery simulation, facial reconstruction from skull data, virtual aging) and animation techniques for movie production (case study of The Matrix sequels) are presented and explained.
ID:695
CLASS:1
Title: ICOS: an intelligent concurrent object-oriented synthesis methodology for multiprocessor systems
Abstract: The design of multiprocessor architectures differs from uniprocessor systems in that the number of processors and their interconnection must be considered. This leads to an enormous increase in the design-space exploration time, which is exponential in the total number of system components. The methodology proposed here, called Intelligent Concurrent Object-Oriented Synthesis (ICOS) methodology, makes feasible the synthesis of complex multiprocessor systems through the application of several techiques that speed up the design process. ICOS is based on Performance Synthesis Methodology (PSM), a recently proposed object-oriented system-level design methodology. Four major techniques: object-oriented design, fuzzy design-space exploration, concurrent   design, and intelligent reuse of complete subsystems are integrated in ICOS. First, object-oriented modeling and design, through the use of object-oriented relationships and operators, make the whole design process manageable and maintainable in ICOS. Second, fuzzy comparison applied to the specializations or instances of components reduces the exponential growth of design-space exploration in ICOS. Third, independent components from different design alternatives are synthesized in parallel; this design concurrency shortens the overall design time. Lastly, the resynthesis of complete subsystems can be avoided through the application of learning, thus making the methodology intelligent enough to reuse previous design configurations. Experiments show that all these applied techniques   contribute to the synthesis efficiency and the degree of automation in ICOS.
ID:696
CLASS:1
Title: Embedded system education: a new paradigm for engineering schools?
Abstract: Embedded systems are emerging as an essential component of modern electronic products. Embedded system design problems are posing challenges that involve entirely new skills for engineers. These skills are related to the combination of traditionally disjoint engineering disciplines. There is a shared concern that today's educational systems are not providing the appropriate foundations for embedded systems. We believe a new education paradigm is needed.We will argue this point using the example of an emerging curriculum on embedded systems at the University of California at Berkeley. This curriculum is the result of a distillation process of more than ten years of intense research work. We will present the considerations that are driving the curriculum development and we review our undergraduate and graduate program. In particular, we describe in detail a graduate class (EECS249: Design of Embedded Systems: Modeling, Validation and Synthesis) that has been taught for six years. A common feature of our education agenda is the search for fundamentals of embedded system science rather than embedded system design techniques, an approach that today is rather unique.
ID:697
CLASS:1
Title: An Efficient ILP-Based Scheduling Algorithm for Control-Dominated VHDL Descriptions
Abstract: In this paper, we present for the first time a mathematical framework for solving a special instance of the scheduling problem in control-flow dominated behavioral VHDL descriptions given that the timing of I/O signals has been completely or partially specified. It is based on a code-transformational approach which fully preserves the VHDL semantics. The scheduling problem is mapped onto an integer linear program (ILP) which can be constrained to be solvable in polynomial time, but still permits optimizing the statement sequence across basic block boundaries.
ID:698
CLASS:1
Title: Scheduling and Mapping of Conditional Task Graphs for the Synthesis of Low Power Embedded Systems
Abstract: This paper describes a new Dynamic Voltage Scaling (DVS) technique for embedded systems expressed as Conditional Task Graphs (CTGs). The idea is to identify and exploit the available worst case slack time, taking into account the conditional behaviour of CTGs. Also we examine the effect of combining a genetic algorithm based mapping with the DVS technique for CTGs and show that further energy reduction can be obtained. The techniques have been tested on a number of CTGs including a real-life example. The results show that the DVS technique can be applied to CTGs with energy saving up to 24%. Furthermore it is shown that savings of up to 51% are achieved by considering DVS during the mapping.
ID:699
CLASS:1
Title: Reducing power while increasing performance with supercisc
Abstract: Multiprocessor Systems on Chips (MPSoCs) have become a popular architectural technique to increase performance. However, MPSoCs may lead to undesirable power consumption characteristics for computing systems that have strict power budgets, such as PDAs, mobile phones, and notebook computers. This paper presents the super-complex instruction-set computing (SuperCISC) Embedded Processor Architecture and, in particular, investigates performance and power consumption of this device compared to traditional processor architecture-based execution. SuperCISC is a heterogeneous, multicore processor architecture designed to exceed performance of traditional embedded processors while maintaining a reduced power budget compared to low-power embedded processors. At the heart of the SuperCISC processor is a multicore VLIW (Very Large Instruction Word) containing several homogeneous execution cores/functional units. In addition, complex and heterogeneous combinational hardware function cores are tightly integrated to the core VLIW engine providing an opportunity for improved performance and reduced energy consumption. Our SuperCISC processor core has been synthesized for both a 90-nm Stratix II Field Programmable Gate Aray (FPGA) and a 160-nm standard cell Application-Specific Integrated Circuit (ASIC) fabrication process from OKI, each operating at approximately 167 MHz for the VLIW core. We examine several reasons for speedup and power improvement through the SuperCISC architecture, including predicated control flow, cycle compression, and a reduction in arithmetic power consumption, which we call power compression. Finally, testing our SuperCISC processor with multimedia and signal-processing benchmarks, we show how the SuperCISC processor can provide performance improvements ranging from 7X to 160X with an average of 60X, while also providing orders of magnitude of power improvements for the computational kernels. The power improvements for our benchmark kernels range from just over 40X to over 400X, with an average savings exceeding 130X. By combining these power and performance improvements, our total energy improvements all exceed 1000X. As these savings are limited to the computational kernels of the applications, which often consume approximately 90&percnt; of the execution time, we expect our savings to approach the ideal application improvement of 10X.
ID:700
CLASS:1
Title: CORAL II: linking behavior and structure in an IC design system
Abstract: This paper describes a technique for maintaining very fine grained links between a behavioral specification and an automatically generated VLSI structural implementation. CORAL II execedes previous systems in the scope of design representations involved and the complexity of the relationships handled. The design representations used are described, as are the behavioral transformations that may be applied and the types of design choices that may be made. The complications introduced by these transformations and design decisions are discussed. Some possible applications of this work are outlined and an existing graphical interface for examining the synthesized design and its relationship to the behavioral specification is described.
ID:701
CLASS:1
Title: <italic>Wavesched</italic>: a novel scheduling technique for control-flow intensive behavioral descriptions
Abstract: In this paper, we present a novel scheduling algorithm targeted towards minimizing the average execution time of control-flow intensive behavioral descriptions. Our algorithm uses a CDFG model, which preserves the parallelism inherent in the application. It explores previously unexplored regions of the solution space by its ability to overlap the schedules of independent iterative constructs, whose bodies share resources. It also incorporates well known optimization techniques like loop unrolling in a natural fashion. This is made possible by a general loop-handling technique, which we have devised. Application of the algorithm to several common benchmarks demonstrates up to 4.8-fold improvement in expected schedule length over existing scheduling algorithms, without paying a price in terms of the best- and worst-case schedule lengths required to execute the behavioral description (in fact, frequently, the best/worst-case schedule lengths are also better for our algorithm).
ID:702
CLASS:1
Title: Synthesis of fault-tolerant concurrent programs
Abstract: Methods for mechanically synthesizing concurrent programs from temporal logic specifications obviate the need to manually construct a program and compose a proof of its correctness. A serious drawback of extant synthesis methods, however, is that they produce concurrent programs for models of computation that are often unrealistic. In particular, these methods assume completely fault-free operation, that is, the programs they produce are fault-intolerant. In this paper, we show how to mechanically synthesize fault-tolerant concurrent programs for various fault classes. We illustrate our method by synthesizing fault-tolerant solutions to the mutual exclusion and barrier synchronization problems.
ID:703
CLASS:1
Title: Symbolic synthesis of clock-gating logic for power optimization of control-oriented synchronous networks
Abstract: Recent results have shown that clock-gating techniques are effective in reducing the total power consumption of sequential circuits. Unfortunately, such techniques assume the availability of the state transition graph of the target system, and rely on explicit algorithms whose complexity is polynomial in the number of states, that is, exponential in the number of state variables. This assumption poses serious limitations on the size of the circuits for which automatic gated-clock generation is feasible. In this paper we propose fully symbolic algorithms for the automatic extraction and synthesis of the clock-gating circuitry for large control-oriented sequential designs. Our techniques leverage the compact BDD-based representation of Boolean and pseudo-Boolean functions to extend the applicability of gated-clock architectures to designs implemented by synchronous networks. As a result, we can deal with circuits for which the explicit state transition graph is too large to be generated and/or manipulated. Moreover, symbolic manipulation techniques allow accurate probabilistic computations; in particular, they enable the use of non-equiprobable primary input distributions, a key step in the construction of models that match the behavior of real hardware devices with a high degree of fidelity. The results are encouraging, since power savings of up to 36% have been obtained on controllers containing up to 21 registers.
ID:704
CLASS:1
Title: Instruction generation and regularity extraction for reconfigurable processors
Abstract: The increasing demand for complex and specialized embedded hardware must be met by processors which are optimized for performance, yet are also extremely flexible. In our work, we explore the tradeoff between flexibility and performance in the domain of reconfigurable processor design. Specifically, we seek to identify regularly occurring, computation-heavy patterns in an application or set of applications. These patterns become candidates for hard-logic implementation, potentially embedded in the flexible reconfigurable fabric as special optimized instructions. In this work we present an extension to previous work in instruction generation: an algorithm that identifies parallel templates. We discuss the advantages of parallel templates, and prove the correctness of our algorithm. We introduce an All-Pairs Common Slack Graph (APCSG) as an effective tool for parallel template generation. Finally, we demonstrate the effectiveness of our algorithm on several applicationse dataflow graphs, reducing latency on average by 51.98%, without unreasonably increasing chip area.
ID:705
CLASS:1
Title: ACUMEN: amplifying control and understanding of multiple entities
Abstract: In virtual environments, the control of numerous entities in multiple dimensions can be difficult and tedious. In this paper, we present a system for synthesizing and recognizing aggregate movements in a virtual environment with a high-level (natural language) interface. The principal components include: an interactive interface for aggregate control based on a collection of parameters extending an existing movement quality model, a feature analysis of aggregate motion verbs, recognizers to detect occurrences of features in a collection of simulated entities, and a clustering algorithm that determines subgroups. Results based on simulations and a sample instruction application are shown.
ID:706
CLASS:1
Title: HAL: a multi-paradigm approach to automatic data path synthesis
Abstract: A novel approach to automatic data path synthesis is presented. This approach features innovations in the synthesis process as well as in the system implementation.The synthesis process exhibits three new features. The first relates to a subtask that performs an expert analysis of the input data flow graph and attempts to evenly distribute operations requiring similar resources. This is done using a novel &ldquo;load balancing&rdquo; technique. The second consists of a global preselection of operator cells to fulfill an explicit speed constraint. Finally, the third deals with new techniques for register and multiplexer optimization. These features support extended design space search by taking an explicit performance specification into account.The system implementation is based on the LOOPS multiparadigm programming system. In this approach the overall task can be partitioned into complementary subtasks requiring different programming paradigms. These subtasks will be realized using an object-based paradigm, a knowledge-based expert system paradigm, a functional paradigm, or combinations of all three.Two complete examples are given to demonstrate the functionality of the system and to allow comparison with existing systems.
ID:707
CLASS:1
Title: Hardware/software partitioning of operating systems: a behavioral synthesis approach
Abstract: In this paper we propose a hardware real time operating system(HW-RTOS) solution that makes use of a dedicated hardware in order to replace the standard support provided by the POSIX layer of a general purpose RTOS for implementing task synchronization and scheduling. By redefining only the I/O APIs of the tasks, the HW-RTOS then takes care of the communication requirements of the original application and also implements the task scheduling algorithm. The new software application can then be compiled without any need for POSIX support. The main advantages are smaller and faster executables. We present results that show how a small hardware area, less than 10K gates, can result in a 15X performance improvement when the original software scheduler is replaced by a dedicated HW-RTOS.
ID:708
CLASS:1
Title: Resource-constrained loop scheduling in high-level synthesis
Abstract: Today electronic design automation software plays an important role in modern VLSI design technology. High-Level Synthesis (HLS) translates the behavioral specification of a digital system to a register transfer level structure. In this research we will focus scheduling loop constructs under fixed hardware constraints. We proposed a new two-phase algorithm for loop scheduling based on the Force-Directed Scheduling (FDS) algorithm. The algorithm also employs a local priority function called the 'mobility' of an operation to select the best operation to be rescheduled when resource violation is detected. In the first phase of the algorithm, the FDS algorithm is used to generate an initial schedule of the system that balances the distribution of the operations and optimizes the system hardware utilization. The second phase of the algorithm iteratively modifies the initial FDS schedule in order to resolve any hardware constraint violations. The performance of the proposed algorithm was evaluated using the differential equation and elliptical wave filter HLS benchmarks. The algorithm was found to significantly reduce the execution time under relaxed hardware constraints and yield results similar to the traditional sequential scheduler under tight hardware constraints.
ID:709
CLASS:1
Title: A unified scheduling model for high-level synthesis and code generation
Abstract: Scheduling is an essential task both in high-level synthesis and in code generation for programmable processors. In this paper we discuss the impact of the controller model on the scheduling task for DSP applications. Existing techniques in high-level synthesis mostly assume a simple controller model in the form of a single FSM. However, in reality more complex controller architectures are often used. On the other hand, in the case of programmable processors, the controller architecture is largely defined by the available control-flow instructions in the instruction set. In this paper, a unified scheduling model is presented to handle a wide range of controller architectures,from the application-specific to programmable processor solutions. The impact of choosing a certain controller architecture on the scheduling phase is investigated. Finally, the tasks of controller generation and code assembly are discussed, which will generate the FSM or machine code description from the correct schedule.
ID:710
CLASS:1
Title: Transistor placement for noncomplementary digital VLSI cell synthesis
Abstract: There is an increasing need in modern VLSI designs for circuits implemented in high-performance logic families such as Cascode Voltage Switch Logic (CVSL), Pass Transistor Logic (PTL), and domino CMOS. Circuits designed in these noncomplementary ratioed logic families can be highly irregular, with complex diffusion sharing and nontrivial routing. Traditional digital cell layout synthesis tools derived from the highly stylized "functional cell" style break down when confronted with such circuit topologies. These cells require a full-custom, two-dimensional layout style which currently requires skilled manual design. In this work we propose a methodology for the synthesis of such complex noncomplementary digital cell layouts. We describe a new algorithm which permits the concurrent optimization of transistor chain placement and the ordering of the transistors within these diffusion-sharing chains. The primary mechanism for supporting this concurrent optimization is the placement of transistor subchains, diffusion-break-free components of the full transistor chains. When a chain is reordered, transistors may move from one subchain (and therefore one placement component) to another. We will demonstrate how this permits the chain ordering to be optimized for both intra-chain and inter-chain routing. We combine our placement algorithms with third-party routing and compaction tools, and present the results of a series of experiments which compare our technique with a commercial cell synthesis tool. These experiments make use of a new set of benchmark circuits which provide a rich sample of representative examples in several noncomplementary digital logic families.
ID:711
CLASS:1
Title: Modeling real-world control systems: beyond hybrid systems
Abstract: Hybrid system modeling refers to the construction of system models combining both continuous and discrete dynamics. These models can greatly reduce the complexity of a physical system model by abstracting some of the continuous dynamics of the system into discrete dynamics. Hybrid system models are also useful for describing the interaction between physical processes and computational processes, such as in a digital feedback control system. Unfortunately, hybrid system models poorly capture common software architecture design patterns, such as threads, mobile code, safety, and hardware interfaces. Dealing effectively with these practical software issues is crucial when designing real-world systems. This paper presents a model of a complex control system that combines continuous-state physical system models with rich discrete-state software models in a disciplined fashion. We show how expressive modeling using multiple semantics can be used to address the design difficulties in such a system.
ID:712
CLASS:1
Title: Predicate abstraction for reachability analysis of hybrid systems
Abstract: Embedded systems are increasingly finding their way into a growing range of physical devices. These embedded systems often consist of a collection of software threads interacting concurrently with each other and with a physical, continuous environment. While continuous dynamics have been well studied in control theory, and discrete and distributed systems have been investigated in computer science, the combination of the two complexities leads us to the recent research on hybrid systems. This paper addresses the formal analysis of such hybrid systems. Predicate abstraction has emerged to be a powerful technique for extracting finite-state models from infinite-state discrete programs. This paper presents algorithms and tools for reachability analysis of hybrid systems by combining the notion of predicate abstraction with recent techniques for approximating the set of reachable states of linear systems using polyhedra. Given a hybrid system and a set of predicates, we consider the finite discrete quotient whose states correspond to all possible truth assignments to the input predicates. The tool performs an on-the-fly exploration of the abstract system. We present the basic techniques for guided search in the abstract state-space, optimizations of these techniques, implementation of these in our verifier, and case studies demonstrating the promise of the approach. We also address the completeness of our abstraction-based verification strategy by showing that predicate abstraction of hybrid systems can be used to prove bounded safety.
ID:713
CLASS:2
Title: XML \&lt;and semi-structured data\&gt;
Abstract: XML, as defined by the World Wide Web Consortium in 1998, is a method of marking up a document or character stream to identify structural or other units within the data. XML makes several contributions to solving the problem of semi-structured data, the term database theorists use to denote data that exhibits any of the following characteristics:&bull;Numerous repeating fields and structures in a naive hierarchical representation of the data, which lead to large numbers of tables in a second- or third-normal form representation&bull;Wide variation in structure&bull;Sparse tablesXML provides a natural representation for hierarchical structures and repeating fields or structures. Further, XML document type definitions (DTDs) and schemas allow fine-grained control over how much variation to allow in the data: Vocabulary designers can require XML data to be perfectly regular, or they can allow a little variation, or a lot. In the extreme case, an XML vocabulary can effectively say that there are no rules at all beyond those required of all well-formed XML. Because XML syntax records only what is present, not everything that might be present, sparse data does not make the XML representation awkward; XML storage systems are typically built to handle sparse data gracefully.The most important contribution XML makes to the problem of semi-structured data, however, is to call into question the nature and existence of the problem. As the description makes clear, semi-structured data is just data that does not fit neatly into the relational model. Referring to "the problem of semi-structured data" suggests subliminally that the problem lies in the failure of the data to live up fully to the relational model, rather than in the model and its failure fully to support the natural structure of the data.In the wild (that is, in documents, reports, and program data structures as they are encountered in daily life), information takes forms rather different from third normal form. XML arose from efforts to represent documents in a device- and application-independent way, and it reflects the complexity of documents and their stubborn refusal to fit into tabular form.In XML, data can have an elaborate and intricate structure that is significantly richer and more complex than a table of rows and columns. Calling this semi-structured is misleading, just as it would be to describe DNA molecules as semi-structured because they are less simply regular than those of table salt. XML seeks to make possible capturing and expressing the structure of the data as we understand it, without forcing it into a too-simple structure.
ID:714
CLASS:2
Title: Managing semi-structured data
Abstract: I vividly remember during my first college class my fascination with the relational database--an information oasis that guaranteed a constant flow of correct, complete, and consistent information at our disposal. In that class I learned how to build a schema for my information, and I learned that to obtain an accurate schema there must be a priori knowledge of the structure and properties of the information to be modeled. I also learned the ER (entity-relationship) model as a basic tool for all further data modeling, as well as the need for an a priori agreement on both the general structure of the information and the vocabularies used by all communities producing, processing, or consuming this information.Several years later I was working with an organization whose goal was to create a large repository of food recipes. The intent was to include recipes from around the world and their nutritional information, as well as the historical and cultural aspects of food creation.I was involved in creating the database schema to hold this information. Suddenly the axioms I had learned in school collapsed. There was no way we could know in advance what kind of schema was necessary to describe French, Chinese, Indian, and Ethiopian recipes. The information that we had to model was practically unbound and unknown. There was no common vocabulary. The available information was contained mostly in natural language descriptions; even with significant effort, modeling it using entities and relationships would have been impossible. Asking a cook to enter the data in tables, rows, objects, or XML elements was unthinkable, and building an entry form for such flexible and unpredictable information structures was difficult, if not impossible. The project stopped. Years later I believe we still do not have such information available to us in the way we envisioned it.Many projects of this kind are all around us. While the traditional data modeling and management techniques are useful and appropriate for a wide range of applications--as the huge success of relational databases attests--in other cases those traditional techniques do not work. A large volume of information is still unavailable and unexploited because the existing data modeling and management tools are not adapted to the reality of such information.
ID:715
CLASS:2
Title: Semantic querying of tree-structured data sources using partially specified tree patterns
Abstract: Nowadays, huge volumes of data are organized or exported in a tree-structured form. Querying capabilities are provided through queries that are based on branching path expression. Even for a single knowledge domain structural differences raise difficulties for querying data sources in a uniform way. In this paper, we present a method for semantically querying tree-structured data sources using partially specified tree patterns. Based on dimensions which are sets of semantically related nodes in tree structures, we define dimension graphs. Dimension graphs can be automatically extracted from trees and abstract their structural information. They are semantically rich constructs that support the formulation of queries and their efficient evaluation. We design a tree-pattern query language to query multiple tree-structured data sources. A central feature of this language is that the structure can be specified fully, partially, or not at all in the queries. Therefore, it can be used to query multiple trees with structural differences. %and We study the derivation of structural expressions in queries by introducing a set of inference rules for structural expressions. We define two types of query unsatisfiability and we provide necessary and sufficient conditions for checking each of them. Our approach is validated through experimental evaluation.
ID:716
CLASS:2
Title: The irregular Z-buffer: Hardware acceleration for irregular data structures
Abstract: The classical Z-buffer visibility algorithm samples a scene at regularly spaced points on an image plane. Previously, we introduced an extension of this algorithm called the irregular Z-buffer that permits sampling of the scene from arbitrary points on the image plane. These sample points are stored in a two-dimensional spatial data structure. Here we present a set of architectural enhancements to the classical Z-buffer acceleration hardware which supports efficient execution of the irregular Z-buffer. These enhancements enable efficient parallel construction and query of certain irregular data structures, including the grid of linked lists used by our algorithm. The enhancements include flexible atomic read-modify-write units located near the memory controller, an internal routing network between these units and the fragment processors, and a MIMD fragment processor design. We simulate the performance of this new architecture and demonstrate that it can be used to render high-quality shadows in geometrically complex scenes at interactive frame rates. We also discuss other uses of the irregular Z-buffer algorithm and the implications of our architectural changes in the design of chip-multiprocessors.
ID:717
CLASS:2
Title: LH*<sub>RS</sub>---a highly-available scalable distributed data structure
Abstract: LH&ast;RS is a high-availability scalable distributed data structure (SDDS). An LH&ast;RS file is hash partitioned over the distributed RAM of a multicomputer, for example, a network of PCs, and supports the unavailability of any k &geq; 1 of its server nodes. The value of k transparently grows with the file to offset the reliability decline. Only the number of the storage nodes potentially limits the file growth. The high-availability management uses a novel parity calculus that we have developed, based on Reed-Salomon erasure correcting coding. The resulting parity storage overhead is about the lowest possible. The parity encoding and decoding are faster than for any other candidate coding we are aware of. We present our scheme and its performance analysis, including experiments with a prototype implementation on Wintel PCs. The capabilities of LH&ast;RS offer new perspectives to data intensive applications, including the emerging ones of grids and of P2P computing.
ID:718
CLASS:2
Title: Algorithms and data structures for flash memories
Abstract: Flash memory is a type of electrically-erasable programmable read-only memory (EEPROM). Because flash memories are nonvolatile and relatively dense, they are now used to store files and other persistent objects in handheld computers, mobile phones, digital cameras, portable music players, and many other computer systems in which magnetic disks are inappropriate. Flash, like earlier EEPROM devices, suffers from two limitations. First, bits can only be cleared by erasing a large block of memory. Second, each block can only sustain a limited number of erasures, after which it can no longer reliably store data. Due to these limitations, sophisticated data structures and algorithms are required to effectively use flash memories. These algorithms and data structures support efficient not-in-place updates of data, reduce the number of erasures, and level the wear of the blocks in the device. This survey presents these algorithms and data structures, many of which have only been described in patents until now.
ID:719
CLASS:2
Title: Concurrent cache-oblivious b-trees
Abstract: This paper presents concurrent cache-oblivious (CO) B-trees. We extend the cache-oblivious model to a parallel or distributed setting and present three concurrent CO B-trees. Our first data structure is a concurrent lock-based exponential CO B-tree. This data structure supports insertions and non-blocking searches/successor queries. The second and third data structures are lock-based and lock-free variations, respectively, on the packed-memory CO B-tree. These data structures support range queries and deletions in addition to the other operations. Each data structure achieves the same serial performance as the original data structure on which it is based. In a concurrent setting, we show that these data structures are linearizable, meaning that completed operations appear to an outside viewer as though they occurred in some serialized order. The lock-based data structures are also deadlock free, and the lock-free data structure guarantees forward progress by at least one process.
ID:720
CLASS:2
Title: Skip-webs: efficient distributed data structures for multi-dimensional data sets
Abstract: We present a framework for designing efficient distributed data structures for multi-dimensional data. Our structures, which we call skip-webs, extend and improve previous randomized distributed data structures, including skipnets and skip graphs. Our framework applies to a general class of data querying scenarios, which include linear (one-dimensional) data, such as sorted sets, as well as multi-dimensional data, such as d-dimensional octrees and digital tries of character strings defined over a fixed alphabet.We show how to perform a query over such a set of n items spread among n hosts using O(log n/log log n) messages for one-dimensional data, or O(log n) messages for fixed-dimensional data, while using only O(log n) space per host. We also show how to make such structures dynamic so as to allow for insertions and deletions in O(log n) messages for quadtrees, octrees, and digital tries, and O(log n/log log n) messages for one-dimensional data. Finally, we show how to apply a blocking strategy to skip-webs to further improve message complexity for one-dimensional data when hosts can store more data.
ID:721
CLASS:2
Title: Managing structure in bits \& pieces: the killer use case for XML
Abstract: This paper asserts that for databases to manage a significantly greater percentage of the world's data, managing structural information must get significantly easier. XML technologies provide a widely accepted basis for significant advances in managing data structure. Topics include schema design, evolution, and versioning; managing related applications; and application architecture.
ID:722
CLASS:2
Title: Similarity evaluation on tree-structured data
Abstract: Tree-structured data are becoming ubiquitous nowadays and manipulating them based on similarity is essential for many applications. The generally accepted similarity measure for trees is the edit distance. Although similarity search has been extensively studied, searching for similar trees is still an open problem due to the high complexity of computing the tree edit distance. In this paper, we propose to transform tree-structured data into an approximate numerical multidimensional vector which encodes the original structure information. We prove that the L1 distance of the corresponding vectors, whose computational complexity is O(|T1| + |T2|), forms a lower bound for the edit distance between trees. Based on the theoretical analysis, we describe a novel algorithm which embeds the proposed distance into a filter-and-refine framework to process similarity search on tree-structured data. The experimental results show that our algorithm reduces dramatically the distance computation cost. Our method is especially suitable for accelerating similarity query processing on large trees in massive datasets.
ID:723
CLASS:2
Title: Automatic pool allocation: improving performance by controlling data structure layout in the heap
Abstract: This paper describes Automatic Pool Allocation, a transformation framework that segregates distinct instances of heap-based data structures into seperate memory pools and allows heuristics to be used to partially control the internal layout of those data structures. The primary goal of this work is performance improvement, not automatic memory management, and the paper makes several new contributions. The key contribution is a new compiler algorithm for partitioning heap objects in imperative programs based on a context-sensitive pointer analysis, including a novel strategy for correct handling of indirect (and potentially unsafe) function calls. The transformation does not require type safe programs and works for the full generality of C and C++. Second, the paper describes several optimizations that exploit data structure partitioning to further improve program performance. Third, the paper evaluates how memory hierarchy behavior and overall program performance are impacted by the new transformations. Using a number of benchmarks and a few applications, we find that compilation times are extremely low, and overall running times for heap intensive programs speed up by 10-25% in many cases, about 2x in two cases, and more than 10x in two small benchmarks. Overall, we believe this work provides a new framework for optimizing pointer intensive programs by segregating and controlling the layout of heap-based data structures.
ID:724
CLASS:2
Title: Space efficient dynamic orthogonal range reporting
Abstract: In this paper we present new space e cient dynamic data structures for orthogonal range reporting.The described data structures support planar range reporting queries in time O (log n +k log log(4 n/(k +1)))and space O (n log log n ), or in time O (log n+k )and space O (n log e n )for any e &gt; 0. Both data structures can be constructed in O (n log n )time and support insert and delete operations in amortized time O (log 2 n )and O (log n log log n )respectively. These results match the corresponding upper space bounds of Chazelle [6] for the static case. We also present a dynamic data structure for d -dimensional range reporting with search time O (log d .1 n +k ),update time O (log d n ),and space O (n log d .2+e n )for any e &gt; 0.
ID:725
CLASS:2
Title: The skip quadtree: a simple dynamic data structure for multidimensional data
Abstract: We present a new multi-dimensional data structure, which we call the skip quadtree (for point data in R2) or the skip octree (for point data in Rd, with constant d &gt; 2). Our data structure combines the best features of two well-known data structures, in that it has the well-defined "box"-shaped regions of region quadtrees and the logarithmic-height search and update hierarchical structure of skip lists. Indeed, the bottom level of our structure is exactly a region quadtree (or octree for higher dimensional data). We describe efficient algorithms for inserting and deleting points in a skip quadtree, as well as fast methods for performing point location, approximate range, and approximate nearest neighbor queries.
ID:726
CLASS:2
Title: Data structure repair using goal-directed reasoning
Abstract: Data structure repair is a promising technique for enabling programs to execute successfully in the presence of otherwise fatal data structure corruption errors. Previous research in this field relied on the developer to write a specification to explicitly translate model repairs into concrete data structure repairs, raising the possibility of 1) incorrect translations causing the supposedly repaired concrete data structures to be inconsistent, and 2) repaired models with no corresponding concrete data structure representation.We present a new repair algorithm that uses goal-directed reasoning to automatically translate model repairs into concrete data structure repairs. This new repair algorithm eliminates the possibility of incorrect translations and repaired models with no corresponding representation as concrete data structures.
ID:727
CLASS:2
Title: Nonblocking memory management support for dynamic-sized data structures
Abstract: Conventional dynamic memory management methods interact poorly with lock-free synchronization. In this article, we introduce novel techniques that allow lock-free data structures to allocate and free memory dynamically using any thread-safe memory management library. Our mechanisms are lock-free in the sense that they do not allow a thread to be prevented from allocating or freeing memory by the failure or delay of other threads. We demonstrate the utility of these techniques by showing how to modify the lock-free FIFO queue implementation of Michael and Scott to free unneeded memory. We give experimental results that show that the overhead introduced by such modifications is moderate, and is negligible under low contention.
ID:728
CLASS:2
Title: A data structure for non-manifold simplicial <i>d</i>-complexes
Abstract: We propose a data structure for d-dimensional simplicial complexes, that we call the Simplified Incidence Graph (SIG). The simplified incidence graph encodes all simplices of a simplicial complex together with a set of boundary and partial co-boundary topological relations. It is a dimension-independent data structure in the sense that it can represent objects of arbitrary dimensions. It scales well to the manifold case, i.e. it exhibits a small overhead when applied to simplicial complexes with a manifold domain, Here, we present efficient navigation algorithms for retrieving all topological relations from a SIG, and an algorithm for generating a SIG from a representation of the complex as an incidence graph. Finally, we compare the simplified incidence graph with the incidence graph, with a widely-used data structure for d-dimensional pseudo-manifold simplicial complexes, and with two data structures specific for two-and three-dimensional simplicial complexes.
ID:729
CLASS:2
Title: Types for describing coordinated data structures
Abstract: Coordinated data structures are sets of (perhaps unbounded) data structures where the nodes of each structure may share abstract types with the corresponding nodes of the other structures. For example, consider a list of arguments, and a separate list of functions, where the n-th function of the second list should be applied only to the n-th argument of the first list. We can guarantee that this invariant is obeyed by coordinating the two lists, such that the type of the n-th argument is existentially Quantified and identical to the argument type of the n-th function. In this paper, we describe a minimal set of features sufficient for a type system to support coordinated data structures. We also demonstrate that two known type systems (Crary and Weirich's LX [6] and Xi, Chen and Chen's guarded recursive datatypes [24]) have these features, even though the systems were developed for other purposes. We illustrate the power of coordinated data structures as a programming idiom with three examples: (1) a list of function closures stored as a list of environments and a separate list of code pointers, (2) a "tagless" list, and (3) a red-black tree where the values and colors are stored in separate trees that are guaranteed to have the same shape.
ID:730
CLASS:2
Title: Discovering frequently changing structures from historical structural deltas of unordered XML
Abstract: Recently, a large amount of work has been done in XML data mining. However, we observed that most of the existing works focus on the snapshot XML data, while XML data is dynamic in real applications. To the best of our knowledge, none of the existing works has addressed the issue of mining the history of changes to XML documents. Such mining results can be useful in many applications such as XML change detection, XML indexing, association rule mining, and classification etc. In this paper, we propose a novel approach to discover the &#60;i>frequently changing structures&#60;/i> from the sequence of historical &#60;i>structural deltas&#60;/i> of unordered XML. To make the structure discovering process efficient, an expressive and compact data model, &#60;b>H&#60;/b>istorical-&#60;b>D&#60;/b>ocument &#60;b>O&#60;/b>bject &#60;b>M&#60;/b>odel (&#60;b>H-DOM&#60;/b>), is proposed. Using this model, two basic algorithms, which can discover all the &#60;i>frequently changing structures&#60;/i> with only two scans of the XML sequence, are presented. Experimental results show that our algorithms, together with the optimization techniques, are efficient and scalable.
ID:731
CLASS:2
Title: The dimensions of variation in the teaching of data structures
Abstract: The current debate about the teaching of data structures is hampered because, as a community, we usually debate specifics about data structure implementations and libraries, when the real level of disagreement remains implicit -- the intent behind our teaching. This paper presents a phenomenographic study of the intent of CS educators for teaching data structures in CS2. Based on interviews with Computer Science educators and analysis of CS literature, we identified five categories of intent: developing transferable thinking, improving students' programming skills, knowing "what's under the hood", knowledge of software libraries, and component thinking. The CS community needs to first debate at the level of these categories before moving to more specific issues. This study also serves as an example of how phenomenographic analysis can be used to inform debate on syllabus design in general.
ID:732
CLASS:2
Title: Parallel algorithms for mining frequent structural motifs in scientific data
Abstract: Discovery of important substructures from molecules is an important data mining problem. The basic motivation is that the structure of a molecule has a role to play in its biochemical function. There is interest in finding important, often recurrent, substructures both within a single molecule and across a class of molecules.Recently, we have developed a general purpose suite of algorithms -- the MotifMiner Toolkit -- that can mine for structural motifs in a wide area of biomolecular datasets. While the algorithms have proven to be extremely useful in their ability to identify novel substructures, the algorithms themselves are quite time consuming. There are two reasons for this: i) inherently the algorithm suffers from the curse of subgraph isomorphism; and ii) handling noise effects (e.g. protein structure data) results in a significant slowdown.To address this problem in this paper we propose parallelization strategies in a cluster environment for the above algorithms. We identify key optimizations that handle load imbalance, scheduling, and communication overheads. Results show that the optimizations are quite effective and that we are able to obtain good speedup on moderate sized clusters.
ID:733
CLASS:3
Title: Software error analysis: a real case study involving real faults and mutations
Abstract: The paper reports on a first experimental comparison of software errors generated by real faults and by 1st-order mutations. The experiments were conducted on a program developed by a student from the industrial specification of a critical software from the civil nuclear field. Emphasis was put on the analysis of errors produced upon activation of 12 real faults by focusing on the mechanisms of error creation, masking, and propagation up to failure occurrence, and on the comparison of these errors with those created by 24 mutations. The results involve a total of 3730 errors recorded from program execution traces: 1458 errors were produced by the real faults, and the 2272 others by the mutations. They are in favor of a suitable consistency between errors generated by mutations and by real faults: 85% of the 2272 errors due to the mutations were also produced by the real faults. Moreover, it was observed that although the studied mutations were simple faults, they can create erroneous behaviors as complex as those identified for the real faults. This lends support to the representativeness of errors due to mutations.
ID:734
CLASS:3
Title: Segregated failures model for availability evaluation of fault-tolerant systems
Abstract: This paper presents a method of estimating the availability of fault-tolerant computer systems with several recovery procedures. A segregated failures model has been proposed recently for this purpose. This paper provides further analysis and extension of this model. The segregated failures model is compared with a Markov chain model and is extended for the situation when the coverage factor is unknown and failure escalation rates must be used instead. This situation is illustrated in detail by estimating availability of a Lucent Technologies Reliable Clustered Computing architecture. For this example, numeric values are provided for availability indexes and the contribution of each recovery procedure to total system availability is analysed.
ID:735
CLASS:3
Title: Experimental program analysis: a new program analysis paradigm
Abstract: Program analysis techniques are used by software engineers to deduce and infer characteristics of software systems. Recent research has suggested that a new form of program analysis technique can be created by incorporating characteristics of experimentation into analyses. This paper reports the results of research exploring this suggestion. Building on principles and methodologies underlying the use of experimentation in other fields, we provide descriptive and operational definitions of experimental program analysis, illustrate them by example, and describe several differences between experimental program analysis and experimentation in other fields. We show how the use of an experimental program analysis paradigm can help researchers identify limitations of analysis techniques, improve existing experimental program analysis techniques, and create new experimental program analysis techniques.
ID:736
CLASS:3
Title: Achieving sub-50 milliseconds recovery upon BGP peering link failures
Abstract: We first show by measurements that BGP peering links fail as frequently as intradomain links and usually for short periods of time. We propose a new fast-reroute technique where routers are prepared to react quickly to interdomain link failures. For each of its interdomain links, each router precomputes a protection tunnel, i.e. an IP tunnel to an alternate nexthop which can reach the same destinations as via the protected link. We propose a BGP-based auto-discovery technique that allows each router to learn the candidate protection tunnels for its links. Each router selects the best protection tunnels for its links and when it detects an interdomain link failure, it immediately encapsulates the packets to send them through the protection tunnel. Our solution is applicable for the links between large transit ISPs and also for the links between multi-homed stub networks and their providers. Furthermore, we show that transient forwarding loops (and thus the corresponding packet losses) can be avoided during the routing convergence that follows the deactivation of a protection tunnel in BGP/MPLS VPNs and in IP networks using encapsulation.
ID:737
CLASS:3
Title: Experiences with a continuous network tracing infrastructure
Abstract: One of the most pressing problems in network research is the lack of long-term trace data from ISPs. The Internet carries an enormous volume and variety of data; mining this data can provide valuable insight into the design and development of new protocols and applications. Although capture cards for high-speed links exist today, actually making the network traffic available for analysis involves more than just getting the packets off the wire, but also handling large and variable traffic loads, sanitizing and anonymizing the data, and coordinating access by multiple users. In this paper we discuss the requirements, challenges, and design of an effective traffic monitoring infrastructure for network research. We describe our experience in deploying and maintaining a multi-user system for continuous trace collection at a large regional ISP@. We evaluate the performance of our system and show that it can support sustained collection and processing rates of over 160--300Mbits/s.
ID:738
CLASS:3
Title: Specification, Safety and Reliability Analysis Using Stochastic Petri Net Models
Abstract: In this study, we focus on the specification and assessment of Stochastic Petri net (SPN) models to evaluate the design of an embedded system for reliability and availability. The system provides dynamic driving regulation (DDR) to improve vehicle derivability (anti-skid, -slip and steering assist). A functional SPN abstraction was developed for each of three subsystems that incorporate mechanics, failure modes/effects and model parameters. The models are solved in terms of the subsystem and overall system reliability and availability. Four sets of models were developed. The first three sets include subsystem representations for the TC (Traction Control), AB (Antilock Braking) and ESA (Electronic Steering Assistance) systems. The last set combines these systems into one large model. We summarize the general approach and provide sample Petri net graphs and reliability charts that were used to evaluate the design of the DDR in parts and as a whole.
ID:739
CLASS:3
Title: Modelling erroneous operator behaviours for an air-traffic control task
Abstract: This paper introduces a new approach to formalising analysis of human errors in human-computer interaction. The approach takes account of the cognitive processes involved in a task, and how mistakes arise and how errors propagate through the task. It argues for modelling errors as behaviours rather than as events (the usual approach), at least for tasks involving highly interleaved concurrent, ongoing activities. The models are formalised using a combination of CSP and temporal logic, and the approach is illustrated on a case study from Air Traffic Control. By providing a richer modelling framework and being more expressive, the approach overcomes significant limitations of existing human-error identification techniques.
ID:740
CLASS:3
Title: Thermal aware cell-based full-chip electromigration reliability analysis
Abstract: A hierarchical scheme with cells and modules is crucial for managing design complexity during a large integrated circuit design. We present a methodology for thermal aware cell-based electromigration analysis suitable for integrating electromigration reliability analysis into a conventional IC design flow. A block or cell is characterized for reliability while it is characterized for power and timing. Reusing cell characterization data significantly reduces computational load while analyzing a full-chip layout. During full-chip analysis, we compute a layout-level temperature profile from cell power dissipations using a Fast Fourier Transform based algorithm. The described full-chip reliability assessment methodology has been implemented in an interconnect reliability CAD tool. We have exercised the tool to demonstrate performance-reliability tradeoff and the significance of thermal-aware reliability analysis for true reliability aware design.
ID:741
CLASS:3
Title: Manufacturing 1: simulation-based analysis of a complex printed circuit board testing process
Abstract: This paper describes a simulation-based analysis of a printed circuit board (PCB) testing process. The PCBs are used in a defense application and the testing process is fairly complex. Boards are mounted on a test unit in batches and go through three thermal test cycles. As boards fail testing during the thermal cycling, operators can either replace the failed boards at fixed points during the cycling or can allow the test unit to complete the testing cycle before removing failed boards. The primary objective of the simulation study is to select an operating strategy for a given set of operating parameters. A secondary objective is to identify the operating factors to which the strategy selection is sensitive. Initial testing indicated that failed boards should be replaced as soon as possible under the current operating configuration of the sponsor's facility. Secondary testing is also described.
ID:742
CLASS:3
Title: Parallel execution of prolog programs: a survey
Abstract: Since the early days of logic programming, researchers in the field realized the potential for exploitation of parallelism present in the execution of logic programs. Their high-level nature, the presence of nondeterminism, and their referential transparency, among other characteristics, make logic programs interesting candidates for obtaining speedups through parallel execution. At the same time, the fact that the typical applications of logic programming frequently involve irregular computations, make heavy use of dynamic data structures with logical variables, and involve search and speculation, makes the techniques used in the corresponding parallelizing compilers and run-time systems potentially interesting even outside the field. The objective of this article is to provide a comprehensive survey of the issues arising in parallel execution of logic programming languages along with the most relevant approaches explored to date in the field. Focus is mostly given to the challenges emerging from the parallel execution of Prolog programs. The article describes the major techniques used for shared memory implementation of Or-parallelism, And-parallelism, and combinations of the two. We also explore some related issues, such as memory management, compile-time analysis, and execution visualization.
ID:743
CLASS:3
Title: On scalable and efficient distributed failure detectors
Abstract: Process groups in distributed applications and services rely on failure detectors to detect process failures completely, and as quickly, accurately, and scalably as possible, even in the face of unreliable message deliveries. In this paper, we look at quantifying the optimal scalability, in terms of network load, (in messages per second, with messages having a size limit) of distributed, complete failure detectors as a function of application-specified requirements. These requirements are 1) quick failure detection by some non-faulty process, and 2) accuracy of failure detection. We assume a crash-recovery (non-Byzantine) failure model, and a network model that is probabilistically unreliable (w.r.t. message deliveries and process failures). First, we characterize, under certain independence assumptions, the optimum worst-case network load imposed by any failure detector that achieves an application's requirements. We then discuss why traditional heart beating schemes are inherently unscalable according to the optimal load. We also present a randomized, distributed, failure detector algorithm that imposes an equal expected load per group member. This protocol satisfies the application defined constraints of completeness and accuracy, and speed of detection on an average. It imposes a network load that differs frown the optimal by a sub-optimality factor that is much lower than that for traditional distributed heartbeating schemes. Moreover, this sub-optimality factor does not vary with group size (for large groups).
ID:744
CLASS:3
Title: A probability analysis for candidate-based frequent itemset algorithms
Abstract: This paper explores the generation of candidates, which is an important step in frequent itemset mining algorithms, from a theoretical point of view. Important notions in our probabilistic analysis are success (a candidate that is frequent), and failure (a candidate that is infrequent). For a selection of candidate-based frequent itemset mining algorithms, the probabilities of these events are studied for the shopping model where all the shoppers are independent and each combination of items has its own probability, so any correlation between items is possible. The Apriori Algorithm is considered in detail; for AIS, Eclat, FP-growth and the Fast Completion Apriori Algorithm, the main principles are sketched. The results of the analysis are used to compare the behaviour of the algorithms for a variety of data distributions.
ID:745
CLASS:3
Title: Coping with network failures: routing strategies for optimal demand oblivious restoration
Abstract: Link and node failures in IP networks pose a challenge for network control algorithms. Routing restoration, which computes new routes that avoid failed links, involves fundamental tradeoffs between efficient use of network resources, complexity of the restoration strategy and disruption to network traffic. In order to achieve a balance between these goals, obtaining routings that provide good performance guarantees under failures is desirable.In this paper, building on previous work that provided performance guarantees under uncertain (and potentially unknown) traffic demands, we develop algorithms for computing optimal restoration paths and a methodology for evaluating the performance guarantees of routing under failures. We then study the performance of route restoration on a diverse collection of ISP networks. Our evaluation uses a competitive analysis type framework, where performance of routing with restoration paths under failures is compared to the best possible performance on the failed network. We conclude that with careful selection of restoration paths one can obtain restoration strategies that retain nearly optimal performance on the failed network while minimizing disruptions to traffic flows that did not traverse the failed parts of the network.
ID:746
CLASS:3
Title: Synchronous path analysis in MOS circuit simulator
Abstract:  For verifying the timing performance of synchronous MOS circuits a path analysis facility has been developed in the MOTIS (MOS Timing Simulator) system. This path analysis traces the clock signals to the latches in the circuit, computes the clock skews and then performs a path search analysis between all latches. For the paths between clocked latches, the timing constraints are determined using the clock skews and the operating frequency. The paths that do not satisfy these constraints are identified as problem paths. Such an analysis does not require a prior generation of circuit stimuli that are necessary for simulation. In terms of complexity also, it is simpler than simulation. 
ID:747
CLASS:3
Title: Critical slicing for software fault localization
Abstract: Developing effective debugging strategies to guarantee the reliability of software is important. By analyzing the debugging process used by experienced programmers, we have found that four distinct tasks are consistently performed: (1) determining statements involved in program failures, (2) selecting suspicious statements that might contain faults, (3) making hypotheses about suspicious faults (variables and locations), and (4) restoring program state to a specific statement for verification. This research focuses support for the second task, reducing the search domain for faults, which we refer to as fault localization.We explored a new approach to enhancing the process of fault localization based on dynamic program slicing and mutation-based testing. In this new approach, we have developed the technique of Critical Slicing to enable debuggers to highlight suspicious statements and thus to confine the search domain to a small region. The Critical Slicing technique is partly based on "statement deletion" mutant operator of the mutation-based testing methodology. We have explored properties of Critical Slicing, such as the relationship among Critical Slicing, Dynamic Program Slicing, and Executable Static Program Slicing; the cost to construct critical slices; and the effectiveness of Critical Slicing. Results of experiments support our conjecture as to the effectiveness and feasibility of using Critical Slicing for fault localization.This paper explains our technique and summarizes some of our findings. From these, we conclude that a debugger equipped with our proposed fault localization method can reduce human interaction time significantly and aid in the debugging of complex software.
ID:748
CLASS:3
Title: A new component concept for fault trees
Abstract: The decomposition of complex systems into manageable parts is an essential principle when dealing with complex technical systems. However, many safety and reliability modelling techniques do not support hierarchical decomposition in the desired way. Fault Tree Analysis (FTA) offers decomposition into modules, a breakdown with regard to the hierarchy of failure influences rather than to the system architecture. In this paper we propose a compositional extension of the FTA technique. Each technical component is represented by an extended Fault Tree. Besides the internal basic events and gates, each component can have input and output ports. By connecting these ports, components can be integrated into a higher-level system model. All components can be developed independently and stored in separate files or component libraries. Mathematically, each Component Fault Tree represents a logical function from its input ports and internal events to its output ports. As in traditional FTA, both qualitative and quantitative analyses are possible. Known algorithms e.g. based on Binary Decision Diagrams (BDDs) can still be applied. The Windows based safety analysis tool UWG3 has been developed to prove this concept in practice. It allows creating component libraries in an exchangeable XML format. We have carried out some case studies in order to show that the new concept improves clearness and intuitive modelling while maintaining the same results as traditional FTA.
ID:749
CLASS:3
Title: Tailoring the software process to project goals and environments
Abstract: This paper presents a methodology for improving the software process by tailoring it to the specific project goals and environment. This improvement process is aimed at the global software process model as well as methods and tools supporting that model. The basic idea is to use defect profiles to help characterize the environment and evaluate the project goals and the effectiveness of methods and tools in a quantitative way. The improvement process is implemented iteratively by setting project improvement goals, characterizing those goals and the environment, in part, via defect profiles in a quantitative way, choosing methods and tools fitting those characteristics, evaluating the actual behavior of the chosen set of methods and tools, and refining the project goals based on the evaluation results. All these activities require analysis of large amounts of data and, therefore, support by an automated tool. Such a tool &mdash; TAME (Tailoring A Measurement Environment) &mdash; is currently being developed.
ID:750
CLASS:3
Title: Compositional verification of concurrent systems using Petri-net-based condensation rules
Abstract: The state-explosion problem of formal verification has obstructed its  application to large-scale software systems. In this article, we introduce a set of new condensation theories: IOT-failure equivalence, IOT-state equivalence, and firing-dependence theory to cope with this problem. Our condensation theories are much weaker than current theories used for the compositional verification of Petri nets. More significantly, our new condensation theories can eliminate the interleaved behaviors caused by asynchronously sending actions. Therefore, our technique provides a much more powerful means for the compositional verification of asynchronous processes. Our technique can efficiently analyze several state-based properties: boundedness, reachable markings, reachable submarkings, and  deadlock states. Based on the notion of our new theories, we develop a set of condensation rules for efficient verification of large-scale software systems. The experimental results show a significant improvement in the analysis large-scale concurrent systems.
ID:751
CLASS:3
Title: HDD: hierarchical delta debugging
Abstract: Inputs causing a program to fail are usually large and often contain information irrelevant to the failure. It thus helps debugging to simplify program inputs. The Delta Debugging algorithm is a general technique applicable to minimizing all failure-inducing inputs for more effective debugging. In this paper, we present HDD, a simple but effective algorithm that significantly speeds up Delta Debugging and increases its output quality on tree structured inputs such as XML. Instead of treating the inputs as one flat atomic list, we apply Delta Debugging to the very structure of the data. In particular, we apply the original Delta Debugging algorithm to each level of a program's input, working from the coarsest to the finest levels. We are thus able to prune the large irrelevant portions of the input early. All the generated input configurations are syntactically valid, reducing the number of inconclusive configurations that need to be tested and accordingly the amount of time spent simplifying. We have implemented HDD and evaluated it on a number of real failure-inducing inputs from the GCC and Mozilla bugzilla databases. Our Hierarchical Delta Debugging algorithm produces simpler outputs and takes orders of magnitude fewer test cases than the original Delta Debugging algorithm. It is able to scale to inputs of considerable size that the original Delta Debugging algorithm cannot process in practice. We argue that HDD is an effective tool for automatic debugging of programs expecting structured inputs.
ID:752
CLASS:3
Title: An analytical study of peer-to-peer media streaming systems
Abstract: Recent research efforts have demonstrated the great potential of building cost-effective media streaming systems on top of peer-to-peer (P2P) networks. A P2P media streaming architecture can reach a large streaming capacity that is difficult to achieve in conventional server-based streaming services. Hybrid streaming systems that combine the use of dedicated streaming servers and P2P networks were proposed to build on the advantages of both paradigms. However, the dynamics of such systems and the impact of various factors on system behavior are not totally clear. In this article, we present an analytical framework to quantitatively study the features of a hybrid media streaming model. Based on this framework, we derive an equation to describe the capacity growth of a single-file streaming system. We then extend the analysis to multi-file scenarios. We also show how the system achieves optimal allocation of server bandwidth among different media objects. The unpredictable departure/failure of peers is a critical factor that affects the performance of P2P systems. We utilize the concept of peer lifespan to model peer failures. The original capacity growth equation is enhanced with coefficients generated from peer lifespans that follow an exponential distribution. We also propose a failure model under arbitrarily distributed peer lifespan. Results from large-scale simulations support our analysis.
ID:753
CLASS:4
Title: Robust document image understanding technologies
Abstract: No existing document image understanding technology, whether experimental or commercially available, can guarantee high accuracy across the full range of documents of interest to industrial and government agency users. Ideally, users should be able to search, access, examine, and navigate among document images as effectively as they can among encoded data files, using familiar interfaces and tools as fully as possible. We are investigating novel algorithms and software tools at the frontiers of document image analysis, information retrieval, text mining, and visualization that will assist in the full integration of such documents into collections of textual document images as well as "born digital" documents. Our approaches emphasize &#60;i>versatility first&#60;/i>: that is, methods which work reliably across the broadest possible range of documents.
ID:754
CLASS:4
Title: An APL pattern-directed module for bidimensional data analysis
Abstract: We suggest that APL may effectively be used for the implementation of pattern directed production systems (PDPS) for the analysis of bidimensional data. These systems look for interesting or important situations occurring as patterns in their input or memory data. These patterns cause the execution of suitable functions. Moreover APL makes it possible to carry out in a natural manner the synthetic and expressive definition of the patterns and then allows to verify the efficacy and power of such definition. We face these problems through MIPS (Module Interpreter for Production Systems). In the work described, simple idioms and skeletons are used to set up the definitions and the instruments. The system is described through two examples of application to the analysis of digital images from different experiments.
ID:755
CLASS:4
Title: Detection and segmentation of tables and math-zones from document images
Abstract: We propose an algorithm to separate out tables and math-zones from document images. The algorithm relies on the spatial characteristics of tables and math-zones in a document. It has been observed that tables have distinct columns which imply that gaps between the fields are substantially larger than the gaps between the words in text lines and in math-zones the characters and symbols are less dense in comparison to normal text lines. These deceptively simple observations have led us to design a simple but powerful table and math-zone detection system with low computation cost.
ID:756
CLASS:4
Title: Diagnosis of lung nodule using Gini coefficient and skeletonization in computerized tomography images
Abstract: This paper uses the Gini coefficient and a set of skeleton measures, with the purposes, with the purpose of characterizing lung nodules as malignant or benign in computerized tomography images.Based on a sample of 31 nodules, 25 benign and 6 malignant, these methods are first analyzed individually and then jointly, with classfication and analysis techniques (linear stepwise discriminant analysis, leave-one-out and ROC curve). We have concluded that the individual measures and their combinations produce good results in the diagnosis of lung nodules.
ID:757
CLASS:4
Title: High dynamic range imaging
Abstract: Current display devices can display only a limited range of contrast and colors, which is one of the main reasons that most image acquisition, processing, and display techniques use no more than eight bits per color channel. This course outlines recent advances in high-dynamic-range imaging, from capture to display, that remove this restriction, thereby enabling images to represent the color gamut and dynamic range of the original scene rather than the limited subspace imposed by current monitor technology. This hands-on course teaches how high-dynamic-range images can be captured, the file formats available to store them, and the algorithms required to prepare them for display on low-dynamic-range display devices. The trade-offs at each stage, from capture to display, are assessed, allowing attendees to make informed choices about data-capture techniques, file formats, and tone-reproduction operators. The course also covers recent advances in image-based lighting, in which HDR images can be used to illuminate CG objects and realistically integrate them into real-world scenes. Through practical examples taken from photography and the film industry, it shows the vast improvements in image fidelity afforded by high-dynamic-range imaging.
ID:758
CLASS:4
Title: Efficient edge detection and object segmentation using Gabor filters
Abstract: Gabor filter is a widely used feature extraction method, especially in image texture analysis. The selection of optimal filter parameters is usually problematic and unclear. This study analyzes the filter design essentials and proposes two different methods to segment the Gabor filtered multi-channel images. The first method integrates Gabor filters with labeling algorithm for edge detection and object segmentation. The second method uses the K-means clustering with simulated annealing for image segmentation of a stack of Gabor filtered multi-channel images. Various experiments with real images demonstrate the effectiveness of these approaches.
ID:759
CLASS:4
Title: Image-based 3D acquisition of archaeological heritage and applications
Abstract: In this paper an approach is presented that obtains virtual models from sequences of images. The system can deal with uncalibrated image sequences acquired with a hand-held camera. Based on tracked or matched features the relations between multiple views are computed. From this both the structure of the scene and the motion of the camera are retrieved. The ambiguity on the reconstruction is restricted from projective to metric through auto-calibration. A flexible multi-view stereo matching scheme is used to obtain a dense estimation of the surface geometry. From the computed data virtual models can be constructed or, inversely, virtual models can be included in the original images.
ID:760
CLASS:4
Title: To split or to conjoin: the question in image computation
Abstract: Image computation is the key step in fixpoint computations that are extensively used in model checking. Two techniques have been used for this step: one based on conjunction of the terms of the transition relation, and the other based on recursive case splitting. We discuss when one technique outperforms the other, and consequently formulate a hybrid approach to image computation. Experimental results show that the hybrid algorithm is much more robust than the &ldquo;pure&rdquo; algorithms and outperforms both of them in most cases. Our findings also shed light on the remark of several researchers that splitting is especially effective in approximate reachability analysis.
ID:761
CLASS:4
Title: Video tomography: an efficient method for camerawork extraction and motion analysis
Abstract: This paper proposes a new, efficient and practical way to extract lens zoom, camera pan and camera tilt information using modified motion analysis. The proposed method is called the Video Tomography Method (VTM), in which tomographic techniques are introduced into a motion estimation algorithm. By using the VTM, one is able to visualize motion as a spatiotemporal flow for motion analysis. The VTM is an extremely robust [resistant to noise] method for estimating camera operation due to its tomographic nature. The practicality of this type of motion estimation and analysis is confirmed by the results of our simulations and experiments in testing the prototype platform with a low quality video source. Other possible applications that might use extracted motion data are discussed. This method is targeted towards video handling applications that attribute extracted motion data into a video index. It will enhance the process of editing and browsing structured video, and will allow the visualization of scenes spatiotemporally so that video may be accessed intuitively and spatially in relation to its temporal location. This type of access is a new interface for structured video. Scene reconstruction techniques can be extended to apply to the problem of reconstructing occluded images and resolution enhancement.
ID:762
CLASS:4
Title: Learning image manifolds by semantic subspace projection
Abstract: In many image retrieval applications, the mapping between high-level semantic concept and low-level features is obtained through a learning process. Traditional approaches often assume that images with same semantic label share strong visual similarities and should be clustered together to facilitate modeling and classification. Our research indicates this assumption is inappropriate in many cases. Instead we model the images as lying on non-linear image subspaces embedded in the high-dimensional space and find that multiple subspaces may correspond to one semantic concept.
ID:763
CLASS:4
Title: Image annotations by combining multiple evidence \& wordNet
Abstract: The development of technology generates huge amounts of non-textual information, such as images. An efficient image annotation and retrieval system is highly desired. Clustering algorithms make it possible to represent visual features of images with finite symbols. Based on this, many statistical models, which analyze correspondence between visual features and words and discover hidden semantics, have been published. These models improve the annotation and retrieval of large image databases. However, current state of the art including our previous work produces too many irrelevant keywords for images during annotation. In this paper, we propose a novel approach that augments the classical model with generic knowledge-based, WordNet. Our novel approach strives to prune irrelevant keywords by the usage of WordNet. To identify irrelevant keywords, we investigate various semantic similarity measures between keywords and finally fuse outcomes of all these measures together to make a final decision using Dempster-Shafer evidence combination. We have implemented various models to link visual tokens with keywords based on knowledge-based, WordNet and evaluated performance using precision, and recall using benchmark dataset. The results show that by augmenting knowledge-based with classical model we can improve annotation accuracy by removing irrelevant keywords.
ID:764
CLASS:4
Title: Texture detection for segmentation of iris images
Abstract: The idea of using the distinct spatial distribution of patterns in the human iris for person authentication is now a widely developing technology. Current systems rely on a set of basic assumptions in order to improve the accuracy and running time of the recognition process. The advent of a robust system implies a viable solution to a number of general problems. This paper focuses on a common yet difficult problem - the segmentation of eyelashes from iris texture. Tests give promising results when using grey level co-occurrence matrix (GLCM) approach.
ID:765
CLASS:4
Title: IRIS - a system for image and video retrieval
Abstract: The abundance of available multimedia information (e.g.videos, audio, images) requires efficient and effective annotation and retrieval methods. The IRIS system is designed for content-based retrieval of single images. Techniques and methods from computer vision and AI are combined in a new way within IRIS. The system has been tested with single images on several domains (e.g. landscape images, technical drawings) covering a wide range of applications As videos become a more important role in the frame of multimedia, we want to include them into the IRIS system. This paper describes the methods to divide the video into shots (scenes with a common content) and to create still images for each scene with the mosaicing technique. These images can be analyzed with the IRIS system. The first results of this approach are also shown in this paper.
ID:766
CLASS:4
Title: Disjunctive image computation for embedded software verification
Abstract: Finite state models generated from software programs have unique characteristics that are not exploited by existing model checking algorithms. In this paper, we propose a novel disjunctive image computation algorithm and other simplifications based on these characteristics. Our algorithm divides an image computation into a disjunctive set of easier ones that can be performed in isolation. Hypergraph partitioning is used to minimize the number of live variables in each disjunctive component. We use the live variables to simplify transition relations and reachable state subsets. Our experiments on a set of real-world C programs show that the new algorithm achieves orders-of-magnitude performance improvement over the best known conjunctive image computation algorithm.
ID:767
CLASS:4
Title: A Frequency-Sensitive Point Hierarchy for Images and Volumes
Abstract: This paper introduces a method for converting an image or volume sampled on a regular grid into a space-efficient irregular point hierarchy. The conversion process retains the original frequency characteristics of the dataset by matching the spatial distribution of sample points with the required frequency. To achieve good blending, the spherical points commonly used in volume rendering are generalized to ellipsoidal point primitives. A family of multiresolution, oriented Gabor wavelets provide the frequency-space analysis of the dataset. The outcome of this frequency analysis is the reduced set of points, in which the sampling rate is decreased in originally oversampled areas. During rendering, the traversal of the hierarchy can be controlled by any suitable error metric or quality criteria. The local level of refinement is also sensitive to the transfer function. Areas with density ranges mapped to high transfer function variability are rendered at higher point resolution than others. Our decomposition is flexible and can be used for iso-surface rendering, alpha compositing and X-ray rendering of volumes. We demonstrate our hierarchy with an interactive splatting volume renderer, in which the traversal of the point hierarchy for rendering is modulated by a user-specified frame rate.
ID:768
CLASS:4
Title: A pipelined pseudoparallel system architecture for motion analysis
Abstract: Parallel processing has mostly been applied to well-defined and a priori partitioned problems, and not much has been done to introduce parallelism into serial algorithms. This paper introduces a new concept of pseudoparallelism in which the serial algorithm is partitioned into several noninteractive independent subtasks so that parallelism can be used within each subtask level. This novel approach is illustrated by taking motion analysis as an example. Complete details of such a pseudoparallel architecture with a distributed operating system (no master control) have been worked out. Problems encountered in the course of designing such a system are outlined, and necessary justifications provided. A detailed scheme indicating various memory modules, processing elements, and their data-path requirements is included, and ways to provide continuous flow of partitioned information in the form of a synchronized pipeline are described. Finally, the performance of the proposed scheme is evaluated. The basic strategy appears to be useful in designing such parallel systems for other unexplored complex problems.
ID:769
CLASS:4
Title: Context constraints for compositional reachability analysis
Abstract: Behavior analysis of complex distributed systems has led to the search for enhanced reachability analysis techniques which support modularity and which control the state explosion problem. While modularity has been achieved, state explosion in still a problem. Indeed, this problem may even be exacerbated, as a locally minimized subsystem may contain many states and transitions forbidden by its environment or context. Context constraints, specified as interface processes, are restrictions imposed by the environment on subsystem behavior. Recent research has suggested that the state explosion problem can be effectively controlled if context constraints are incorporated in compositional reachability analysis (CRA). Although theoretically very promising, the approach has rarely been  used in practice because it generally requires a more complex computational model and does not contain a mechanism to derive context constraints automatically. This article presents a technique to automate the approach while using a similar computational model to that of CRA. Context constraints are derived automatically, based on a set of sufficient conditions for these constraints to be transparently included when building reachability graphs. As a result, the global reachability graph generated using the derived constraints is shown to be observationally equivalent to that generated by CRA without the inclusion of context constraints. Constraints can also be specified explicitly by users, based on their application knowledge. Erroneous constraints which contravene transparency can be  identified together with an indication of the error sources. User-specified constraints can be combined with those generated automatically. The technique is illustrated using a clients/server system and other examples.
ID:770
CLASS:4
Title: Segmentation, categorization, and identification of commercial clips from TV streams using multimodal analysis
Abstract: TV advertising is ubiquitous, perseverant, and economically vital. Millions of people's living and working habits are affected by TV commercials. In this paper, we present a multimodal ("visual + audio + text") commercial video digest scheme to segment individual commercials and carry out semantic content analysis within a detected commercial segment from TV streams.Two challenging issues are addressed. Firstly, we propose a multimodal approach to robustly detect the boundaries of individual commercials. Secondly, we attempt to classify a commercial with respect to advertised products/services. For the first, the boundary detection of individual commercials is reduced to the problem of binary classification of shot boundaries via the mid-level features derived from two concepts: Image Frames Marked with Product Information (FMPI) and Audio Scene Change Indicator (ASCI). Moreover, the accurate individual boundary enables us to perform commercial identification by clip matching via a spatial-temporal signature. For the second, commercial classification is formulated as the task of text categorization by expanding sparse texts from ASR/OCR with external knowledge. Our boundary detection has achieved a good result of F1 = 93.7% on the dataset comprising 499 individual commercials from TRECVID'05 video corpus. Commercial classification has obtained a promising accuracy of 80.9% on 141 distinct ones. Based on these achievements, various applications such as an intelligent digital TV set-top box can be accomplished to enhance the TV viewer's capabilities in monitoring and managing commercials from TV streams.
ID:771
CLASS:4
Title: TensorTextures: multilinear image-based rendering
Abstract: This paper introduces a tensor framework for image-based rendering. In particular, we develop an algorithm called TensorTextures that learns a parsimonious model of the bidirectional texture function (BTF) from observational data. Given an ensemble of images of a textured surface, our nonlinear, generative model explicitly represents the multifactor interaction implicit in the detailed appearance of the surface under varying photometric angles, including local (per-texel) reflectance, complex mesostructural self-occlusion, interreflection and self-shadowing, and other BTF-relevant phenomena. Mathematically, TensorTextures is based on multilinear algebra, the algebra of higher-order tensors, hence its name. It is computed through a decomposition known as the N-mode SVD, an extension to tensors of the conventional matrix singular value decomposition (SVD). We demonstrate the application of TensorTextures to the image-based rendering of natural and synthetic textured surfaces under continuously varying viewpoint and illumination conditions.
ID:772
CLASS:4
Title: Color quantization by dynamic programming and principal analysis
Abstract: Color quantization is a process of choosing a set of K representative colors to approximate the N colors of an image, K &lt; N, such that the resulting K-color image looks as much like the original N-color image as possible. This is an optimization problem known to be NP-complete in K. However, this paper shows that by ordering the N colors along their principal axis and partitioning the color space with respect to this ordering, the resulting constrained optimization problem can be solved in O(N + KM2) time by dynamic programming (where M is the intensity resolution of the   device).Traditional color quantization algorithms recursively bipartition the color space. By using the above dynamic-programming algorithm, we can construct a globally optimal K-partition, K&gt;2, of a color space in the principal direction of the input data. This new partitioning strategy leads to smaller quantization error and hence better image quality. Other algorithmic issues in color quantization such as efficient statistical computations and nearest-neighbor searching are also studied. The interplay between luminance and chromaticity in color quantization with and without color dithering is investigated. Our color quantization method allows the user to choose a balance between the image smoothness and hue accuracy for a given   K.
ID:773
CLASS:5
Title: Visualizing implicit queries for information management and retrieval
Abstract: In this paper, we describe the use of similarity metrics in anovel visual environment for storing and retrieving favorite webpages. The similarity metrics, called Implicit Queries, areused to automatically highlight stored web pages that are relatedto the currently selected web page. Two experiments explored howusers manage their personal web information space with and withoutthe Implicit Query highlighting and later retrieve their stored webpages. When storing and organizing web pages, users with ImplicitQuery highlighting generated slightly more categories. ImplicitQueries also led to faster web page retrieval time, although theresults were not statistically significant.
ID:774
CLASS:5
Title: Recent trends in automatic information retrieval
Abstract: Substantial successes were achieved in the early years in automatic indexing and retrieval using single term indexing theories with term weight assignments based on frequency considerations. The development of more refined indexing systems using thesaurus aids and automatically constructed term association maps changed the retrieval effectiveness only slightly. The recent introduction of the relevance concept in the form of probabilistic retrieval models provided a firm basis for term weighting and document ranking practices. However, the probabilistic methods were not helpful in substantially enhancing the retrieval effectiveness.At the present time, attempts are made to add artificial intelligence concepts to the document retrieval environment in the form of fancy graphics interfaces, learning systems for query and document indexing and for collection searching, extended logic models relating documents and information requests, and analysis methods based on the use of semantic maps and other kinds of knowledge structures. Using the earlier developments and evaluation results as guidelines, an attempt is made to outline the information retrieval environment of the future and to assess the usefulness of some of the currently proposed search and retrieval methods.
ID:775
CLASS:5
Title: A new theoretical framework for information retrieval
Abstract: A new framework based on a non-classical logic is proposed for investigating IR. The paper motivates the use of a particular conditional logic as the 'right' logic for IR. A new principle, the logical uncertainty principle, is proposed, to deal with the inherent uncertainty associated with applicable inferences.
ID:776
CLASS:5
Title: Fast evaluation of structured queries for information retrieval
Abstract: Information retrieval systems are being challenged to managelarger and larger document collections. In an effort to providebetter retrieval performance on large collections, moresophisticated retrieval techniques have been developed that supportrich, structured queries. Structured queries are not amenable topreviously proposed optimization techniques. Optimizing execution,however, is even more important in the context of large documentcollections. We present a new structured query optimizationtechnique which we have implemented in an inference network-basedinformation retrieval system. Experimental results show that queryevaluation time can be reduced by more than half with little impacton retrieval effectiveness.
ID:777
CLASS:5
Title: SPIDER: a multiuser information retrieval system for semistructured and dynamic data
Abstract: The access structure, the retrieval model, and the system architecture of the SPIDER information retrieval system are described. The access structure provides efficient weighted retrieval on dynamic data collections. It is based on signatures and non-inverted item descriptions. The signatures provide upper bounds for the exact retrieval status values such that only a small number of exact retrieval status values have to be computed. SPIDER's retrieval model is a probabilistic retrieval model that is capable to exploit the database scheme of semistructured data collections. This model can be considred as a further development of the Binary Independence Indexing (BII) model. The system architecture was derived systematically from a given set of requirements such as effective and efficient retrieval on dynamic data collections, exploitation of the database scheme, computed views, and the integration of information retrieval functionality and database functionality.
ID:778
CLASS:5
Title: Towards task models for embedded information retrieval
Abstract: This paper investigates to what extent task-oriented user support based on plan recognition is feasible in a highly situation-driven domain like information retrieval (IR) and discusses requirements for appropriate task models. It argues that information seeking tasks which are embedded in some higher-level external task context (e.g. travel planning) often exhibit procedural dependences; that these dependences are mainly due to external task; and that they can be exploited for inferring the users' goals and plans. While there is a clear need for task models in IR to account for situational determinants of user behaviour, what is required are hybrid models that take account of both is &ldquo;planned&rdquo; and &ldquo;situated&rdquo; aspects. Empirical evidence for the points made is reported from a probabilistic analysis of retrieval sessions with a fact database and from experience with plan-based and state-based methods for user support in an experimental travel planning system
ID:779
CLASS:5
Title: The information grid: a framework for information retrieval and retrieval-centered applications
Abstract: The Information Grid (InfoGrid) is a framework for building information access applications that provides a user interface design and an interaction model. It focuses on retrieval of application objects as its top level mechanism for accessing user information, documents, or services. We have embodied the InfoGrid design in an object-oriented application framework that supports rapid construction of applications. This application framework has been used to build a number of applications, some that are classically characterized as information retrieval applications, others that are more typically viewed as personal work tools.
ID:780
CLASS:5
Title: Image retrieval by end-users and intermediaries in a journalistic work context
Abstract: This paper describes a study on the image searching behavior of end-users (journalists) and intermediaries (archivists) in a newspaper editorial office. Image queries by end-users and requests to intermediaries were analyzed, compared and categorized according to typologies from literature. The process of image selection was modeled and selection criteria were studied based on interviews, observation and a survey. The results indicate that most image queries and requests dealt with specific entities, but that object types were also common. Thematic image needs seem to be fulfilled by end-user searching and browsing rather than by requests. Image retrieval tasks were highly influenced by contextual factors. Relevance assessments were made at situational level using several types of criteria, including abstract and affective factors. Several types of collaborative searches were observed. Richer research and analysis methods are needed to characterize journalists' image needs and searching behavior.
ID:781
CLASS:5
Title: Keyphrase extraction-based query expansion in digital libraries
Abstract: In pseudo-relevance feedback, the two key factors affecting the retrieval performance most are the source from which expansion terms are generated and the method of ranking those expansion terms. In this paper, we present a novel unsupervised query expansion technique that utilizes keyphrases and POS phrase categorization. The keyphrases are extracted from the retrieved documents and weighted with an algorithm based on information gain and co-occurrence of phrases. The selected keyphrases are translated into Disjunctive Normal Form (DNF) based on the POS phrase categorization technique for better query refomulation. Furthermore, we study whether ontologies such as WordNet and MeSH improve the retrieval performance in conjunction with the keyphrases. We test our techniques on TREC 5, 6, and 7 as well as a MEDLINE collection. The experimental results show that the use of keyphrases with POS phrase categorization produces the best average precision.
ID:782
CLASS:5
Title: Catenaccio: interactive information retrieval system through drawing
Abstract: The Catenaccio system integrates information retrieval with sketch manipulations. The system is designed especially for pen-based computing and allows users to retrieve information by simple pen manipulations such as drawing a picture. When a user draws a circle and writes a keyword, information nodes related to the keyword are collected automatically inside the circle. In addition, the user can create a Venn diagram by repeatedly drawing circles and keywords to form more complex queries. Thus, the user can retrieve information both interactively and visually without complex manipulations. Moreover, the sketch interaction is so simple that it is possible to combine it with other types of data such as images and real-world information for information retrieval. In this paper, we describe our Catenaccio system and how it can be effectively applied.
ID:783
CLASS:5
Title: BRIDJE over a language barrier: cross-language information access by integrating translation and retrieval
Abstract: This paper describes two new features of the BRIDJE system for cross-language information access. The first feature is the partial disambiguation function of the Bi-directional Retriever, which can be used for search request translation in cross-language IR. Its advantage over a "black-box" machine translation approach is consistent across five test collections and across two language permutations: English-Japanese and Japanese-English. The second new feature is the Information Distiller, which performs interactive summarisation of retrieved documents based on Semantic Role Analysis. Our examples illustrate the usefulness of this feature, and our evaluation results show that the precision of Semantic Role Analysis is very high.
ID:784
CLASS:5
Title: Word sense disambiguation in queries
Abstract: This paper presents a new approach to determine the senses of words in queries by using WordNet. In our approach, noun phrases in a query are determined first. For each word in the query, information associated with it, including its synonyms, hyponyms, hypernyms, definitions of its synonyms and hyponyms, and its domains, can be used for word sense disambiguation. By comparing these pieces of information associated with the words which form a phrase, it may be possible to assign senses to these words. If the above disambiguation fails, then other query words, if exist, are used, by going through exactly the same process. If the sense of a query word cannot be determined in this manner, then a guess of the sense of the word is made, if the guess has at least 50% chance of being correct. If no sense of the word has 50% or higher chance of being used, then we apply a Web search to assist in the word sense disambiguation process. Experimental results show that our approach has 100% applicability and 90% accuracy on the most recent robust track of TREC collection of 250 queries. We combine this disambiguation algorithm to our retrieval system to examine the effect of word sense disambiguation in text retrieval. Experimental results show that the disambiguation algorithm together with other components of our retrieval system yield a result which is 13.7% above that produced by the same system but without the disambiguation, and 9.2% above that produced by using Lesk's algorithm. Our retrieval effectiveness is 7% better than the best reported result in the literature.
ID:785
CLASS:5
Title: A maximum coherence model for dictionary-based cross-language information retrieval
Abstract: One key to cross-language information retrieval is how to efficiently resolve the translation ambiguity of queries given their short length. This problem is even more challenging when only bilingual dictionaries are available, which is the focus of this paper. In the previous research of cross-language information retrieval using bilingual dictionaries, the word co-occurrence statistics is used to determine the most likely translations of queries. In this paper, we propose a novel statistical model, named ``maximum coherence model'', which estimates the translation probabilities of query words that are consistent with the word co-occurrence statistics. Unlike the previous work, where a binary decision is made for the selection of translations, the new model maintains the uncertainty in translating query words when their sense ambiguity is difficult to resolve. Furthermore, this new model is able to estimate translations of multiple query words simultaneously. This is in contrast to many previous approaches where translations of individual query words are determined independently. Empirical studies with TREC datasets have shown that the maximum coherence model achieves a relative 10% - 40% improvement in cross-language information retrieval, comparing to other approaches that also use word co-occurrence statistics for sense disambiguation.
ID:786
CLASS:5
Title: Iterative translation disambiguation for cross-language information retrieval
Abstract: Finding a proper distribution of translation probabilities is one of the most important factors impacting the effectiveness of a cross-language information retrieval system. In this paper we present a new approach that computes translation probabilities for a given query by using only a bilingual dictionary and a monolingual corpus in the target language. The algorithm combines term association measures with an iterative machine learning approach based on expectation maximization. Our approach considers only pairs of translation candidates and is therefore less sensitive to data-sparseness issues than approaches using higher n-grams. The learned translation probabilities are used as query term weights and integrated into a vector-space retrieval system. Results for English-German cross-lingual retrieval show substantial improvements over a baseline using dictionary lookup without term weighting.
ID:787
CLASS:5
Title: Gravitation-based model for information retrieval
Abstract: This paper proposes GBM (gravitation-based model), a physical model for information retrieval inspired by Newton's theory of gravitation. A mapping is built in this model from concepts of information retrieval (documents, queries, relevance, etc) to those of physics (mass, distance, radius, attractive force, etc). This model actually provides a new perspective on IR problems. A family of effective term weighting functions can be derived from it, including the well-known BM25 formula. This model has some advantages over most existing ones: First, because it is directly based on basic physical laws, the derived formulas and algorithms can have their explicit physical interpretation. Second, the ranking formulas derived from this model satisfy more intuitive heuristics than most of existing ones, thus have the potential to behave empirically better and to be used safely on various settings. Finally, a new approach for structured document retrieval derived from this model is more reasonable and behaves better than existing ones.
ID:788
CLASS:5
Title: A utility theoretic approach to determining optimal wait times in distributed information retrieval
Abstract: Distributed IR systems query a large number of IR servers, merge the retrieved results and display them to users. Since different servers handle collections of different sizes, have different processing and bandwidth capacities, there can be considerable heterogeneity in their response times. The broker in the distributed IR system thus has to make decisions regarding terminating searches based on perceived value of waiting -- retrieving more documents -- and the costs imposed on users by waiting for more responses. In this paper, we apply utility theory to formulate the broker's decision problem. The problem is a stochastic nonlinear program. We use Monte Carlo simulations to demonstrate how the optimal wait time may be determined in the context of a comparison shopping engine that queries multiple store websites for price and product information. We use data gathered from 30 stores for a set of 60 books. Our research demonstrates how a broker can leverage information about past retrievals regarding distributions of server response time and relevance scores to optimize its performance. Our main contribution is the formulation of the decision model for optimal wait time and proposal of a solution method. Our results suggest that the optimal wait time is highly sensitive to the manner in which users value from a set of retrieved results differs from the sum of user value from each result evaluated independently. We also find that the optimal wait time increases with the size of the distributed collections, but only if user utility from a set of results is nearly equal to the sum of utilities from each result.
ID:789
CLASS:5
Title: Complementary information retrieval for cross-media news content
Abstract: In this paper, we propose a new way of integrating cross-media news content, such as television programs and web pages. We search cross-media news content to find complementary items which can provide additional information to users interested in a particular topic. The complementary news items searched for are not just similar to the item the user is interested in, but also provide information in more detail or from a different perspective. First, we propose a novel content representation model called the "topic structure" model. Intuitively, a topic structure is made up of a pair of subject and content terms. Subject terms denote the dominant terms of a news item. A content term is a term having strong co-occurrence relationships with the subject terms. Based on the topic structure, we search for information related to a given news item (e. g. , one in which the user is interested) from content, context, and media complementation perspectives. We also describe an application system which concurrently presents a television news program along with complementary news articles to help users understand news topics in greater detail and from multiple perspectives.
ID:790
CLASS:5
Title: Bringing natural language information retrieval out of the closet
Abstract: A prototype information retrieval system was developed that gives users fast and easy access to textual information. This system uses a statistical ranking methodology that allows a user to input a query using only natural language, such as a sentence or a noun phrase, with no special syntax required. The system returns a set of text titles or descriptions, ranked in order of likely relevance to the query. The user can then select one or more titles for further examination of the corresponding text. The prototype was tested by over forty users, all proficient in doing manual research in the subject area, but few proficient in doing online research. The system was very fast, providing response times on the order of one second for searching a gigabyte of data and was also very effective, retrieving at least one relevant record within the first ten records retrieved for 53 out of 68 test queries. All users were able to get satisfactory results within a short time after seeing a demonstration, and those that had never used an online retrieval system did as well as those with experience. This is in sharp contrast to Boolean based retrieval systems where continual use is necessary to obtain consistently good results.
ID:791
CLASS:5
Title: Information retrieval using word senses: root sense tagging approach
Abstract: Information retrieval using word senses is emerging as a good research challenge on semantic information retrieval. In this paper, we propose a new method using word senses in information retrieval: root sense tagging method. This method assigns coarse-grained word senses defined in WordNet to query terms and document terms by unsupervised way using co-occurrence information constructed automatically. Our sense tagger is crude, but performs consistent disambiguation by considering only the single most informative word as evidence to disambiguate the target word. We also allow multiple-sense assignment to alleviate the problem caused by incorrect disambiguation.Experimental results on a large-scale TREC collection show that our approach to improve retrieval effectiveness is successful, while most of the previous work failed to improve performances even on small text collection. Our method also shows promising results when is combined with pseudo relevance feedback and state-of-the-art retrieval function such as BM25.
ID:792
CLASS:5
Title: Dependence language model for information retrieval
Abstract: This paper presents a new dependence language modeling approach to information retrieval. The approach extends the basic language modeling approach based on unigram by relaxing the independence assumption. We integrate the linkage of a query as a hidden variable, which expresses the term dependencies within the query as an acyclic, planar, undirected graph. We then assume that a query is generated from a document in two stages: the linkage is generated first, and then each term is generated in turn depending on other related terms according to the linkage. We also present a smoothing method for model parameter estimation and an approach to learning the linkage of a sentence in an unsupervised manner. The new approach is compared to the classical probabilistic retrieval model and the previously proposed language models with and without taking into account term dependencies. Results show that our model achieves substantial and significant improvements on TREC collections.
ID:793
CLASS:6
Title: A robot ontology for urban search and rescue
Abstract: The goal of this Robot Ontology effort is to develop and begin to populate a neutral knowledge representation (the data structures) capturing relevant information about robots and their capabilities to assist in the development, testing, and certification of effective technologies for sensing, mobility, navigation, planning, integration and operator interaction within search and rescue robot systems. This knowledge representation must be flexible enough to adapt as the robot requirements evolve. As such, we have chosen to use an ontological approach to representing these requirements. This paper describes the Robot Ontology, how it fits in to the overall Urban Search and Rescue effort, how we will be proceeding in the future.
ID:794
CLASS:6
Title: Interactive vision to detect target objects for helper robots
Abstract: An effective human-robot interaction is essential for wide penetration of service robots into the market. Such robots need vision systems to recognize objects. It is, however, difficult to realize vision systems that can work in various conditions. More robust techniques of object recognition and image segmentation are essential. Thus, we have proposed to use the human user's assistance for object recognition through speech. The robot asks a question to which the user can easily answer and whose answer can efficiently reduce the number of candidate objects even if there are occluded objects and/or objects composed of multicolor parts in the scene. It considers the characteristics of features used for object recognition such as the easiness for humans to specify them by word, thus generating a user-friendly and efficient sequence of questions. Experimental results show that the robot can detect target objects by asking the questions generated by the method.
ID:795
CLASS:6
Title: An intelligent agent approach for teaching neural networks using LEGO\&\#174; handy board robots
Abstract: In this article we describe a project for an undergraduate artificial intelligence class. The project teaches neural networks using LEGO&reg; handy board robots. Students construct robots with two motors and two photosensors. Photosensors provide readings that act as inputs for the neural network. Output values power the motors and maintain the robot along the designated path. In doing this project, students come to realize the difference between training a neural network and the trained neural network. The fun factor associated with this project has encouraged students to elect artificial intelligence as part of their course of study.
ID:796
CLASS:6
Title: Three years of using robots in an artificial intelligence course: lessons learned
Abstract: We have been using robots in our artificial intelligence course since fall 2000. We have been using the robots for open-laboratory projects. The projects are designed to emphasize high-level knowledge-based AI algorithms. After three offerings of the course, we paused to analyze the collected data and to see if we could answer the following questions: (i) Are robot projects effective at helping students learn AI concepts? (ii) What advantages, if any, can be attributed to using robots for AI projects? (iii) What are the downsides of using robots for traditional projects in AI? In this article we discuss the results of our evaluation and list the lessons learned.
ID:797
CLASS:6
Title: \&\#220;berSim: a multi-robot simulator for robot soccer
Abstract: A robot simulation engine can be a powerful tool for decreasing development time for robot control systems provided it captures the features of robot-environment interactions relevant to robot control. In this paper, we describe a new multi-robot simulation engine, called &#220;berSim, specifically targeted for simulating games of robot soccer. For robot soccer, dynamics, friction, and collisions are integral to robot control thus our simulator must capture these aspects of the environment. Our approach builds on top of a physics simulation engine in order to capture these effects. We describe the current implementation of &#220;berSim, which is a work in progress, and present some empirical comparisons of its performance compared to some real robots.
ID:798
CLASS:6
Title: Interactive robot theatre
Abstract: Engaging a human audience through sight, sound, scent, and touch while following a loosely constrained storyline, the Public Anemone and fellow autonomous characters are let loose to entertain---sociably.
ID:799
CLASS:6
Title: Toward interactive humanoid robots: a constructive approach to developing intelligent robots
Abstract: Many robotics researchers are exploring new possibilities of intelligent robots in our everyday life. Interactive robots, which have various modalities, can communicate with humans as new information media. In this talk, we argue recent research activities of intelligent interactive robots in Japan. Then, we consider a constructive approach to developing interactive humanoid robots in ATR Media Information Science Laboratories. Cognitive scientists and robotics engineers work together in ATR.
ID:800
CLASS:6
Title: Controlling the robots of Web search engines
Abstract: Robots are deployed by a Web search engine for collecting information from different Web servers in order to maintain the currency of its data base of Web pages. In this paper, we investigate the number of robots to be used by a search engine so as to maximize the currency of the data base without putting an unnecessary load on the network. We adopt a finite-buffer queueing model to represent the system. The arrivals to the queueing system are Web pages brought by the robots; service corresponds to the indexing of these pages. Good performance requires that the number of robots, and thus the arrival rate of the queueing system, be chosen so that the indexing queue is rarely starved or saturated. Thus, we formulate a multi-criteria stochastic optimization problem with the loss rate and empty-buffer probability being the criteria. We take the common approach of reducing the problem to one with a single objective that is a linear function of the given criteria. Both static and dynamic policies can be considered. In the static setting the number of robots is held fixed; in the dynamic setting robots may be re-activated/de-activated as a function of the state. Under the assumption that arrivals form a Poisson process and that service times are independent and exponentially distributed random variables, we determine an optimal decision rule for the dynamic setting, i.e., a rule that varies the number of robots in such a way as to minimize a given linear function of the loss rate and empty-buffer probability. Our results are compared with known results for the static case. A numerical study indicates that substantial gains can be achieved by dynamically controlling the activity of the robots.
ID:801
CLASS:6
Title: GestureMan: a mobile robot that embodies a remote instructor's actions
Abstract: When designing systems that support remote instruction on physical tasks, one must consider four requirements: 1) participants should be able to use non-verbal expressions, 2) they must be able to take an appropriate body arrangement to see and show gestures, 3) the instructor should be able to monitor operators and objects, 4) they must be able to organize the arrangement of bodies and tools and gestural expression sequentially and interactively. GestureMan was developed to satisfy these four requirements by using a mobile robot that embodies a remote instructors actions. The mobile robot mounts a camera and a remote control laser pointer on it. Based on the experiments with the system we discuss the advantage and disadvantage of the current implementation. Also, some implications to improve the system are described.
ID:802
CLASS:6
Title: Working with robots and objects: revisiting deictic reference for achieving spatial common ground
Abstract: Robust joint visual attention is necessary for achieving a common frame of reference between humans and robots interacting multimodally in order to work together on real-world spatial tasks involving objects. We make a comprehensive examination of one component of this process that is often otherwise implemented in an ad hoc fashion: the ability to correctly determine the object referent from deictic reference including pointing gestures and speech. From this we describe the development of a modular spatial reasoning framework based around decomposition and resynthesis of speech and gesture into a language of pointing and object labeling. This framework supports multimodal and unimodal access in both real-world and mixed-reality workspaces, accounts for the need to discriminate and sequence identical and proximate objects, assists in overcoming inherent precision limitations in deictic gesture, and assists in the extraction of those gestures. We further discuss an implementation of the framework that has been deployed on two humanoid robot platforms to date.
ID:803
CLASS:6
Title: Human-style interaction with a robot for cooperative learning of scene objects
Abstract: In research on human-robot interaction the interest is currently shifting from uni-modal dialog systems to multi-modal interaction schemes. We present a system for human-style interaction with a robot that is integrated on our mobile robot BIRON. To model the dialog we adopt an extended grounding concept with a mechanism to handle multi-modal in- and output where object references are resolved by the interaction with an object attention system (OAS). The OAS integrates multiple input from, e.g., the object and gesture recognition systems and provides the information for a common representation. This representation can be accessed by both modules and combines symbolic verbal attributes with sensor-based features. We argue that such a representation is necessary to achieve a robust and efficient information processing.
ID:804
CLASS:6
Title: A mobile robot for corridor navigation: a multi-agent approach
Abstract: This project focuses on building an autonomous vehicle as the test bed for the future development of an intelligent wheelchair, by proposing a framework for designing and implementing a mobile robot control program that is easily expandable and portable to other robotic platforms. Using a robot equipped with a minimal set of sensors such as a camera and infrared sensors, our multi-agent based control system is built to tackle various problems encountered during corridor navigation. The control system consists of four agents: an agent responsible for handling sensor inputs, an agent which identifies a corridor using machine vision techniques, an agent which avoids collisions by applying fuzzy logic decision making to proximity data, and an agent responsible for locomotion. In the experiments, the robot's performance demonstrates the feasibility of a multi-agent approach.
ID:805
CLASS:6
Title: Provably good approximation algorithms for optimal kinodynamic planning for Cartesian robots and open chain manipulators
Abstract: We consider the following problem: given a robot system, find a minimal-time trajectory from a start state to a goal state, while avoiding obstacles by a speed-dependent safety margin and respecting dynamics bounds. In [CDRX] we developed a provably good approximation algorithm for the minimum-time trajectory problem for a robot system with decoupled dynamics bounds. This algorithm differed from previous work in three ways: it is possible (1) to bound the goodness of the approximation by an error term &egr; (2) to polynomially bound the running time (complexity) of our algorithm; and (3) to express the complexity as a polynomial function of the error term.We extend these results to d-link, revolute-joint 3D robots will full rigid body dynamics. Specifically, we first prove a generalized trajectory-tracking lemma for robots with coupled dynamics bounds. Using this result we describe polynomial-time approximation algorithms for Cartesian robots obeying L2 dynamics bounds and open kinematic chain manipulators with revolute and prismatic joints; the latter class includes most industrial manipulators. We obtain a general &Ogr;(n2 (log n)(1/&egr;6d-1) algorithm, where n is the geometric complexity. The algorithm is simple, but the new game-theoretic proof techniques we introduce are subtle, and employ tools from disparate parts of computational geometry, robotics, and dynamical systems.
ID:806
CLASS:6
Title: Dual ecologies of robot as communication media: thoughts on coordinating orientations and projectability
Abstract: The aim of our study is to investigate systems for supporting remote instruction via a mobile robot. In the real world, instructions are typically given through words and body orientations such as head movements, which make it possible to project others' actions. Projectability is an important resource in organizing multiple actions among multiple participants in co-ordination with one another. It can likewise be said that in the case of robot-human collaboration, it is necessary to design a robot's head so that a local participant can project the robot's (and remote person's) actions. GestureMan is a robot that is designed to support such projectability properties. It is argued that a remote controlled mobile robot, designed as a communication medium, makes relevant dual ecologies: ecology at a remote (robot operator's) site and at a local participant's (robot's) site. In order to design a robot as a viable communication medium, it is essential to consider how these ecologies can be mediated and supported.
ID:807
CLASS:6
Title: Humanoid robots
Abstract: The future promises lots of robots in our everyday lives; some, perhaps many, of them could look and behave like people but only if being humanoid represents a technological advantage over their relatively utilitarian counterparts.
ID:808
CLASS:6
Title: Human-Robot dialogue for joint construction tasks
Abstract: We describe a human-robot dialogue system that allows a human to collaborate with a robot agent on assembling construction toys. The human and the robot are fully equal peers in the interaction, rather than simply partners. Joint action is supported at all stages of the interaction: the participants agree on a construction task, jointly decide how to proceed to proceed with the task, and also implement the selected plans jointly. The symmetry provides novel challenges for a dialogue system, and also makes it possible for findings from human-human joint-action dialogues to be easily implemented and tested.
ID:809
CLASS:6
Title: Ontology based object categorisation for robots
Abstract: Ontologies are a powerful means for expressing and haring knowledge in a meaningful way, and are becoming accepted as a viable modelling approach. The purpose of this paper is to enhance the representations used by robots by incorporating ontologies and implementing reasoning services that can exploit the information inherent within ontology based representations. Our objective is to explore the use of ontological concepts for object categorisation in agents and issues related to the grounding of ontology based representations. The research is driven by the need to make progress towards the development of a generalised solution for the grounding problem which would allow intelligent agents to achieve more adaptive behaviours.Object categorisation is important for intercommunication between agents because it plays an important role in supporting problem solving and the achievement of goals. Ontologies allow concepts to be easily shared meaningfully between agents and this can enable interoperability between multiple heterogenous systems. In order to illustrate our ideas we focus on the robot soccer domain because it is an application where agents, namely robots, must make decision, communicate and collaborate in a complex and dynamic environment.
ID:810
CLASS:6
Title: Sociality of robots: do robots construct or collapse human relations?
Abstract: With developments in robotics, robots "living" with people will become a part of daily life in the near future. However, there are many problems with social robots. In particular, the behavior of robots can influence human relations, and societies have not yet clarified this. In this paper, we report on an experiment we conducted to verify the influence of robot behavior on human relations using the "balance theory." The results show that robots can have both good and bad influence on human relations. One person's impression of another can undergo changes because of a robot. In other words, robots can construct or collapse human relations.
ID:811
CLASS:6
Title: Experiencing the flow: design issues in human-robot interaction
Abstract: The experience of "emotional tuning" with artefacts that are not merely static (a teapot), nor merely reactive (a VCR), but that are autonomous, physical objects with decision-making abilities, pro-active, dynamic and designed with the general purpose of engaging users in social interaction, is an intriguing issue for interaction design.This paper is a reflection about the compelling yet difficult nature of interaction dynamics among humans and robots, and a special category among them: robots capable of mediating social interaction.Supporting such experiences means providing intensive embedding in the situation, motivating the users through a sense of engagement, similarly to what Csikszentmihalyi (1990) defines "optimal flow", the absolute absorption in the activity where the experience is guided by the personal feeling of the external worlds. The objective of our investigation is to analyze and try to understand if and when robotic devices can engage humans in activities likely to result in "being in the flow". We will try to analyze the different dimensions of flow in relation to different kinds of robotic devices: the seal robot Paro, used both for company and for therapeutic activities, the Intelligent Building Blocks, a robotic construction kit often used in educational activities, LEGO Mindstorms, the popular construction kit developed by LEGO.The perspective that will be presented is connected to the quality of interaction and to the personal significance that every human being creates by getting involved and involving its own life experience in the interaction with the robot.
ID:812
CLASS:6
Title: Dreaming of robots: an interview with Bruce Sterling
Abstract: Bruce Sterling is a 50-year-old cyberpunk who is currently "Provocateur-in-Residence" at the Art Center College of Design in Pasadena. His many works of science fiction include a number of robots, especially the novel and short-story cycle Schismatrix Plus.
ID:813
CLASS:7
Title: A taxonomy of web search
Abstract: Classic IR (information retrieval) is inherently predicated on users searching for information, the so-called "information need". But the need behind a web search is often not informational -- it might be navigational (give me the url of the site I want to reach) or transactional (show me sites where I can perform a certain transaction, e.g. shop, download a file, or find a map). We explore this taxonomy of web searches and discuss how global search engines evolved to deal with web-specific needs.
ID:814
CLASS:7
Title: Re-ranking search results using network analysis a case study with google: a case study with Google
Abstract: In this paper we review methods of structured search for information on the World Wide Web. We propose new methods based on co-citation and network analysis. We describe a set of 21 measures based on these methods and examine the factor structure of those measures. We then report on a recent study that we have conducted at the University of Toronto. Human judges rated the relevance of a selection of Web pages returned by the Google search engine for each of seven queries. We compared the average judged relevance of the top 20 search results selected by Google vs. the top 20 results as selected by each of the 21 network analysis measures. All but one of the network analysis measures ("inlink") showed significantly (p&lt;.05) better (as compared to Google) average judged relevance amongst their top 20 selections. Stepwise regression analysis was then used to identify a linear model with three network analysis measures as predictors, which accounted for roughly 17% of the variance in relevance judgments. While these results need to be extended with more detailed analysis of a wide range of queries and topics, they suggest that network analysis of search output adjacency matrices (where adjacency/proximity is based on web-wide co-citations) may significantly improve search engine rankings.
ID:815
CLASS:7
Title: Rethinking the digital divide
Abstract: African-American students are all too aware that the digital divide is not merely about Internet access. Rather, it involves access to the social networks that ease the path to success in high-tech careers.
ID:816
CLASS:7
Title: Snippet based relevance calculations
Abstract: The purpose of Web Host Access Tool (WHAT) project is to formulate a search based on a user's present needs, and past search results. The tool gathers information from the user, sends out a query to various search engines, ranks the responses, and then presents the results to the user. In the past, it has been assumed information returned by a search tool, such as the WHAT tool, or a search engine, provides enough information to judge the relevance of a web page. Katie and Lee Ann are partaking in a research project that examines the quality of information returned by a search engine.
ID:817
CLASS:7
Title: A new paradigm for ranking pages on the world wide web
Abstract: This paper describes a new paradigm for modeling traffic levels on the world wide web (WWW) using a method of entropy maximization. This traffic is subject to the conservation conditions of a circulation flow in the entire WWW, an aggregation of the WWW, or a subgraph of the WWW (such as an intranet or extranet). We specifically apply the primal and dual solutions of this model to the (static) ranking of web sites. The first of these uses an imputed measure of total traffic through a web page, the second provides an analogy of local "temperature", allowing us to quantify the "HOTness" of a page.
ID:818
CLASS:7
Title: Predictive caching and prefetching of query results in search engines
Abstract: We study the caching of query result pages in Web search engines. Popular search engines receive millions of queries per day, and efficient policies for caching query results may enable them to lower their response time and reduce their hardware requirements. We present PDC (probability driven cache), a novel scheme tailored for caching search results, that is based on a probabilistic model of search engine users. We then use a trace of over seven million queries submitted to the search engine AltaVista to evaluate PDC, as well as traditional LRU and SLRU based caching schemes. The trace driven simulations show that PDC outperforms the other policies. We also examine the prefetching of search results, and demonstrate that prefetching can increase cache hit ratios by 50% for large caches, and can double the hit ratios of small caches. When integrating prefetching into PDC, we attain hit ratios of over 0.53.
ID:819
CLASS:7
Title: Optimizing search engines using clickthrough data
Abstract: This paper presents an approach to automatically optimizing the retrieval quality of search engines using clickthrough data. Intuitively, a good information retrieval system should present relevant documents high in the ranking, with less relevant documents following below. While previous approaches to learning retrieval functions from examples exist, they typically require training data generated from relevance judgments by experts. This makes them difficult and expensive to apply. The goal of this paper is to develop a method that utilizes clickthrough data for training, namely the query-log of the search engine in connection with the log of links the users clicked on in the presented ranking. Such clickthrough data is available in abundance and can be recorded at very low cost. Taking a Support Vector Machine (SVM) approach, this paper presents a method for learning retrieval functions. From a theoretical perspective, this method is shown to be well-founded in a risk minimization framework. Furthermore, it is shown to be feasible even for large sets of queries and features. The theoretical results are verified in a controlled experiment. It shows that the method can effectively adapt the retrieval function of a meta-search engine to a particular group of users, outperforming Google in terms of retrieval quality after only a couple of hundred training examples.
ID:820
CLASS:7
Title: Using the web as a collaborative tool
Abstract: This paper describes how the Internet was used as a collaborative tool in a "real-world" website development project, a group project assigned to students taking an Advanced Computer Applications course. Students worked collaboratively in small groups and coordinated with area businesses, both in person and online, to develop interactive websites for the businesses. The ability to temporarily publish the websites to the university's E-learning server meant that students could work on the project at their convenience and the corresponding businesses were able to review the websites and test the interactive forms at their convenience. E-mail also provided a means of communication among group members, as well as between groups and the corresponding organizations for which the groups were developing websites.As the websites were developed, the sponsoring organizations were asked to provide keywords, words that clients would be expected to use in a search for their type of organization or products and services, as well as a short, one-sentence description of the organization. These were sent to the students via e-mail and the students incorporated the keywords and description into the website through the use of META tags. When the websites were approved by the sponsoring organizations, they were published to actual sites and the sites were submitted to search engines. The students also incorporated interactive forms into the websites, thus allowing the organizations to use the Web to communicate with customers and potential clients. As the world becomes more global in nature, the ability to communicate and collaborate without regard to time or place becomes increasingly important. The Web is an inexpensive and easily accessible tool that can be used effectively to enhance collaborative efforts, in both educational applications and commercial and service settings, especially when schedules or logistics are such that in-person meetings are difficult, if not impossible.
ID:821
CLASS:7
Title: In-home access to multimedia content
Abstract: This paper discusses the problem of content overload in the home environment. We describe the various domains where the problem manifests itself, and we analyse which user activities could benefit from software assistance. In particular, we focus on the problem of searching for information, and we introduce the concept of Conversational Search as the means to tackle it. We also introduce a framework to test several heuristics related to Conversational Search, and show how using a model of the user can significantly improve the results of purely information-theoretical heuristics.
ID:822
CLASS:7
Title: Smart simulation using collaborative formal and simulation engines
Abstract: We present Ketchum, a tool that was developed to improve the productivity of simulation-based functional verification by providing two capabilities: (1) automatic test generation and (2) unreachability analysis. Given a set of "interesting" signals in the design under test (DUT), automatic test generation creates input stimuli that drive the DUT through as many different combinations (called coverage states) of these signals as possible to thoroughly exercise the DUT. Unreachability analysis identifies as many unreachable coverage states as possible.Ketchum differs from the previous published results for several reasons. First, Ketchum provides 10x higher capacity than previous published results. The higher capacity is achieved by carefully orchestrating simulation and multiple formal methods including symbolic simulation, SAT-based BMC, symbolic fixpoint computation and automatic abstraction. Second, Ketchum performs not only automatic test generation but also unreachability analysis, which enables the test generation effort to be focused on coverage states that are not unreachable. Third, the backbone of Ketchum is an off-the-shelf commercial simulator. It enables Ketchum to reach deep states of the design quickly and supports simulation monitors through the standard API of the simulator during test generation.We applied Ketchum to several industrial designs, including the picoJava microprocessor from SUN and the DW8051 microcontroller from Synopsys and obtained very promising results. The experiments show that Ketchum can (1) handle design blocks containing more than 4500 latches and 170K gates, (2) reach up to 6x more coverage states than random simulation and (3) identify a majority of the unreachable coverage states.
ID:823
CLASS:7
Title: Affinity-based management of main memory database clusters
Abstract: We study management strategies for main memory database clusters that are interposed between Internet applications and back-end databases as content caches. The task of management is to allocate data across individual cache databases and to route queries to the appropriate databases for execution. The goal is to maximize effective cache capacity and to minimize synchronization cost. We propose an affinity-based management system for main memory database cLUsters (ALBUM). ALBUM executes each query in two stages in order to take advantage of the query affinity that is observed in a wide range of applications. We evaluate the data/query distribution strategy in ALBUM with a set of trace-based simulations. The results show that ALBUM reduces cache miss ratio by a factor of 1.7 to 9 over alternative strategies. We have implemented a prototype of ALBUM, and compare its performance to that of an existing infrastructure: a fully replicated database with large buffer cache. The results show that ALBUM outperforms the existing infrastructure with the same number of server machines by a factor of 2 to 7, and that ALBUM with only 1/3 to 1/2 of the server machines achieves the same throughput as the existing infrastructure.
ID:824
CLASS:7
Title: A survey of Web metrics
Abstract: The unabated growth and increasing significance of the World Wide Web has resulted in a flurry of research activity to improve its capacity for serving information more effectively. But at the heart of these efforts lie implicit assumptions about "quality" and "usefulness" of Web resources and services. This observation points towards measurements and models that quantify various attributes of web sites. The science of measuring all aspects of information, especially its storage and retrieval or informetrics has interested information scientists for decades before the existence of the Web. Is Web informetrics any different, or is it just an application of classical informetrics to a new medium? In this article, we examine this issue by classifying and discussing a wide ranging set of Web metrics. We present the origins, measurement functions, formulations and comparisons of well-known Web metrics for quantifying Web graph properties, Web page significance, Web page similarity, search and retrieval, usage characterization and information theoretic properties. We also discuss how these metrics can be applied for improving Web information access and use.
ID:825
CLASS:7
Title: The catacomb project: building a user-centered portal the conversational way
Abstract: Enterprise computing is marked by large-scale information systems, such as databases, document management, and groupware that present significant obstacles to consistent cross-application use: dissimilar user interfaces, incompatible security schemes, and the undesirable property of serving only parts of the user community (islands of use) and accessing only some of the enterprise knowledge assets (islands of information).World Wide Web (WWW) architectures do not solve this problem directly. WWW software components are combinable in many clever ways but until recently there were no specific efforts to solve the enterprise computing problems of islands of use and information.The situation is changing now with nascent efforts to architect Web Portal systems with small software modules, for example with Java "portlets" or the Python-based Zope framework [15]. These are program-centric approaches to coalesce information sources and unify the query interface without inherent user modeling. This paper discusses in detail an alternative: a user-centered, conversational portal which extends the ALICE chatbot technology platform and links the user conveniently to information resources, such as Web Services, with specialized query routing. The approach offers scalability, extensibility, and coordination between end-users and developers. A university Intranet implementation, the Catacomb system, is presented and discussed to illustrate the advantages of the conversational Web portal approach.
ID:826
CLASS:7
Title: Personalized web search by mapping user queries to categories
Abstract: Current web search engines are built to serve all users, independent of the needs of any individual user. Personalization of web search is to carry out retrieval for each user incorporating his/her interests. We propose a novel technique to map a user query to a set of categories, which represent the user's search intention. This set of categories can serve as a context to disambiguate the words in the user's query. A user profile and a general profile are learned from the user's search history and a category hierarchy respectively. These two profiles are combined to map a user query into a set of categories. Several learning and combining algorithms are evaluated and found to be effective. Among the algorithms to learn a user profile, we choose the Rocchio-based method for its simplicity, efficiency and its ability to be adaptive. Experimental results indicate that our technique to personalize web search is both effective and efficient.
ID:827
CLASS:7
Title: Information retrieval on the semantic web
Abstract: We describe an approach to retrieval of documents that contain of both free text and semantically enriched markup. In particular, we present the design and implementation prototype of a framework in which both documents and queries can be marked up with statements in the DAML+OIL semantic web language. These statements provide both structured and semi-structured information about the documents and their content. We claim that indexing text and semantic markup together will significantly improve retrieval performance. Our approach allows inferencing to be done over this information at several points: when a document is indexed, when a query is processed and when query results are evaluated.
ID:828
CLASS:7
Title: A language modeling framework for resource selection and results merging
Abstract: Statistical language models have been proposed recently for several information retrieval tasks, including the resource selection task in distributed information retrieval. This paper extends the language modeling approach to integrate resource selection, ad-hoc searching, and merging of results from different text databases into a single probabilistic retrieval model. This new approach is designed primarily for Intranet environments, where it is reasonable to assume that resource providers are relatively homogeneous and can adopt the same kind of search engine. Experiments demonstrate that this new, integrated approach is at least as effective as the prior state-of-the-art in distributed IR.
ID:829
CLASS:7
Title: Categorizing information objects from user access patterns
Abstract: Many web sites have dynamic information objects whose topics change over time. Classifying these objects automatically and promptly is a challenging and important problem for site masters. Traditional content-based and link structure based classification techniques have intrinsic limitations for this task. This paper proposes a framework to classify an object into an existing category structure by analyzing the users' traversals in the category structure. The key idea is to infer an object's topic from the predicted preferences of users when they access the object. We compare two approaches using this idea. One analyzes collective user behavior and the other each user's accesses. We present experimental results on actual data that demonstrate a much higher prediction accuracy and applicability with the latter approach. We also analyze the correlation between classification quality and various factors such as the number of users accessing the object. To our knowledge, this work is the first effort in combining object classification with user access prediction.
ID:830
CLASS:7
Title: An investigation into search engines as a form of targeted advert delivery
Abstract: The process of marketing encompasses three functions: to inform,remind and persuade. The growth of the Internet signified a dawn ofa new age of marketing; a low cost form of marketing that presentedvast economies of scale. It can further be said that using theInternet provided a business advantage [Aldridge et al. 1997].However, the Internet's business advantage is not as great anymoreas most medium and large sized companies have a web site. In themid and late 1990's, e-commerce companies were willing to spendvast amounts of money on advertising, apparently without concernfor the returns on the investment. Companies who were keen toharness the potential of the Internet soon became disillusionedwith over-hyped expectations. E-commerce companies are now reducingexpenses and investment in online marketing and technology[Aldridge et al. 1997:162, Bonanos 2001, Mollison 2002, Joseph&amp;amp; Poon 2001].
ID:831
CLASS:7
Title: Disclosive computer ethics
Abstract: This essay provides a critique of mainstream computer ethics and argues for the importance of a complementary approach called disclosive computer ethics, which is concerned with the moral deciphering of embedded values and norms in computer systems, applications and practices. Also, four key values are proposed as starting points for disclosive studies in computer ethics: justice, autonomy, democracy and privacy. Finally, it is argued that research in disclosive computer ethics should be multi-level and interdisciplinary, distinguishing between a disclosure level, a theoretical level, and an an application level.
ID:832
CLASS:7
Title: Knowledge management and XML: derivation of synthetic views over semi-structured data
Abstract: One of the effects of the expansion of the World Wide Web is theproduction of a huge amount of data, differentiated for type,available to a large number of different users. Furthermore, theconstant progress of computer hardware technology in the past threedecades has led to the availability of powerful computers, datacollection equipments, and storage media; this technology providesa great boost to the database and information industry by allowingtransaction management, information retrieval, and data analysisover massive amounts of heterogeneous data. Moreover, the explosionof Internet increases the availability of data in differentformats: structured (e.g. relational), semistructured (e.g. HTML,XML) and unstructured (e.g. plain text, audio/video) data [2].Thus, new data management systems, able to take advantage of theseheterogeneous data, are emerging and will play a vital role in theinformation industry. Thus, heterogeneous database systems emergeand play a vital role in the information industry.
ID:833
CLASS:4
Title: Probabilistic cost-effectiveness comparison of screening strategies for colorectal cancer
Abstract: A stochastic discrete-event simulation model of the natural history of Colorectal Cancer (CRC) is augmented with screening technology representations to create a base for simulating various screening strategies for CRC. The CRC screening strategies recommended by the American Gastroenterological Association (AGA) and the newest screening strategies for which clinical efficacy has been established are simulated. In addition to verification steps, validation of screening is pursued by comparison with the Minnesota Colon Cancer Control Study. The model accumulates discounted costs and quality-adjusted life-years. The natural variability in the modeled random variables for natural history is conditioned using a probabilistic sensitivity analysis through a two-stage sampling process that adds other random variables representing parametric uncertainty. The analysis of the screening alternatives in a low-risk population explores both deterministic and stochastic dominance to eliminate some screening alternatives. Net benefit analysis, based on willingness to pay for quality-adjusted life-years, is used to compare the most cost-effective strategies through acceptability curves and to make a screening recommendation. Methodologically, this work demonstrates how variability from the natural variation in the development, screening, and treatment of a disease can be combined with the variation in parameter uncertainty. Furthermore, a net benefit analysis that characterizes cost-effectiveness alternatives can explicitly depend on variation from all sources producing a probabilistic cost-effectiveness analysis of decision alternatives.
ID:834
CLASS:4
Title: STAC: software tuning panels for autonomic control
Abstract: One aspect of autonomic computing is the ability to identify, separate and automatically tune parameters related to performance, security, robustness and other properties of a software system. Often the response to events affecting these properties consists of adjusting tuneable system parameters such as table sizes, timeout limits, restart checks and so on. In many ways these tuneable parameters correspond to the switches and potentiometers on the control panel of many hardware devices. While modern software systems designed for autonomic control may make these parameters easily accessible, in legacy systems they are often scattered or deeply hidden in the software source.In this paper we introduce Software Tuning Panels for Autonomic Control (STAC), a system for automatically re-architecting legacy software systems to facilitate autonomic control. STAC works to isolate tuneable system parameters into one visible area of a system, producing a resulting architecture that can be used in conjunction with an autonomic controller for self-maintenance and tuning. A proof-of-concept implementation of STAC using source transformation is presented along with its application to the automatic re-architecting of two open source Java programs. Use of the new architecture in monitoring and autonomic control is demonstrated on these examples.
ID:835
CLASS:5
Title: Attack resistant collaborative filtering
Abstract: The widespread deployment of recommender systems has lead to user feedback of varying quality. While some users faithfully express their true opinion, many provide noisy ratings which can be detrimental to the quality of the generated recommendations. The presence of noise can violate modeling assumptions and may thus lead to instabilities in estimation and prediction. Even worse, malicious users can deliberately insert attack profiles in an attempt to bias the recommender system to their benefit.  While previous research has attempted to study the robustness of various existing Collaborative Filtering (CF) approaches, this remains an unsolved problem. Approaches such as Neighbor Selection algorithms, Association Rules and Robust Matrix Factorization have produced unsatisfactory results. This work describes a new collaborative algorithm based on SVD which is accurate as well as highly stable to shilling. This algorithm exploits previously established SVD based shilling detection algorithms, and combines it with SVD based-CF. Experimental results show a much diminished effect of all kinds of shilling attacks. This work also offers significant improvement over previous Robust Collaborative Filtering frameworks.
ID:836
CLASS:5
Title: A privacy-preserving technique for Euclidean distance-based mining algorithms using Fourier-related transforms
Abstract: Privacy preserving data mining has become increasingly popular because it allows sharing of privacy-sensitive data for analysis purposes. However, existing techniques such as random perturbation do not fare well for simple yet widely used and efficient Euclidean distance-based mining algorithms. Although original data distributions can be pretty accurately reconstructed from the perturbed data, distances between individual data points are not preserved, leading to poor accuracy for the distance-based mining methods. Besides, they do not generally focus on data reduction. Other studies on secure multi-party computation often concentrate on techniques useful to very specific mining algorithms and scenarios such that they require modification of the mining algorithms and are often difficult to generalize to other mining algorithms or scenarios. This paper proposes a novel generalized approach using the well-known energy compaction power of Fourier-related transforms to hide sensitive data values and to approximately preserve Euclidean distances in centralized and distributed scenarios to a great degree of accuracy. Three algorithms to select the most important transform coefficients are presented, one for a centralized database case, the second one for a horizontally partitioned, and the third one for a vertically partitioned database case. Experimental results demonstrate the effectiveness of the proposed approach.
ID:837
CLASS:7
Title: A quantitative measure for telecommunications networks topology design
Abstract: This paper proposes a new measure for network performance evaluation called topology lifetime. The measure provides insight into which one of a set of topologies is likely to last the longest before more capacity must be installed. The lifetime measure is not single valued, but considers growth as a function of a set of demand shifts (perturbation). One network may be better able to support a uniform growth in the traffic, while another may support more growth when unexpected shifts in the load occur. The ability of a network to support unexpected changes in load is becoming more important because of: 1) current practices for installing fiber optics cables; 2) recent advances in dense wavelength division multiplexing; and 3) the increasing popularity of the Internet. The lifetime measure is applied to several topologies; a dual ring, a chordal ring, a Manhattan Street network and an hierarchical network. We also apply the measure to a realistic US IP Backbone network. In this paper, our objective is to show how to apply the measure to different networks, and to explain certain implications for comparisons between networks. We expect this measure to be useful both in the construction of new networks and in selecting between new links that may be added to an existing network.
ID:838
CLASS:7
Title: Adaptive execution of variable-accuracy functions
Abstract: Many analysis applications require the ability to repeatedly execute sophisticated modeling functions, which can each take minutes or even hours to produce a single answer. Because of this expense, such applications have largely been unable to directly use such models in queries, with either on-demand or continuous query processing technology. Query processors are hindered in their ability to optimize expensive modeling functions due to the "black box" nature of existing user-defined function (UDF) interfaces. In this paper, we address the problem of querying over sophisticated models with the development of VAOs (Variable-Accuracy Operators). VAOs use a new function interface that exposes the trade-off between compute time and accuracy that exists in many modeling functions. Using this interface, VAOs adaptively run each function call in a query only to an accuracy needed to answer the query, thus eliminating unneeded work. In this paper, we present the design of VAOs for a set of common query operations. We show the effectiveness of VAOs using a prototype implementation running financial queries over real bond market data.
ID:839
CLASS:5
Title: Temporal causal modeling with graphical granger methods
Abstract: The need for mining causality, beyond mere statistical correlations, for real world problems has been recognized widely. Many of these applications naturally involve temporal data, which raises the challenge of how best to leverage the temporal information for causal modeling. Recently graphical modeling with the concept of "Granger causality", based on the intuition that a cause helps predict its effects in the future, has gained attention in many domains involving time series data analysis. With the surge of interest in model selection methodologies for regression, such as the Lasso, as practical alternatives to solving structural learning of graphical models, the question arises whether and how to combine these two notions into a practically viable approach for temporal causal modeling. In this paper, we examine a host of related algorithms that, loosely speaking, fall under the category of graphical Granger methods, and characterize their relative performance from multiple viewpoints. Our experiments show, for instance, that the Lasso algorithm exhibits consistent gain over the canonical pairwise graphical Granger method. We also characterize conditions under which these variants of graphical Granger methods perform well in comparison to other benchmark methods. Finally, we apply these methods to a real world data set involving key performance indicators of corporations, and present some concrete results.
ID:840
CLASS:7
Title: Dancing on quicksand gracefully: instructional design for rapidly evolving technology courses
Abstract: One of the challenges of higher education in technology, especially in computer disciplines, is the rapid change of technical content. In technology disciplines with a tradition of experiential learning instructors need to continually redesign courses to ensure the learning experience for the students is current and relevant. This paper discusses an approach to course design that has been applied to a class in Information Technology that experiences significant changes in course content on a regular basis. The design approach allows many aspects of the course design to remain constant, including much of the class presentation and assessment, while including the latest technology developments and applications in the course. Students collaborate in seeking out new applications in technology and in sharing them with the class.  The paper discusses a course module developed using these principles. Different aspects of the design were analyzed. Firstly how successful is this as a mechanism for maintaining course currency with current technological developments, secondly to what extent is this successful for instructors to collaborate with students in acquiring new knowledge in the discipline and thirdly does the instructional design approach adequately support student learning of new application areas in the discipline.
