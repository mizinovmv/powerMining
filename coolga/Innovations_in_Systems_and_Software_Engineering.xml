<?xml version="1.0" encoding="UTF-8" standalone="no"?><documents><document><year>2009</year><authors>Angelo Furfaro1  | Libero Nigro1 </authors><title>A development methodology for embedded systems based on RT-DEVS      </title><content>This work is concerned with modelling, analysis and implementation of embedded control systems using RT-DEVS, i.e. a specialization         of classic discrete event system specification (DEVS) for real-time. RT-DEVS favours model continuity, i.e. the possibility         of using the same model for property analysis (by simulation or model checking) and for real time execution. Special case         tools are reported in the literature for RT-DEVS model analysis and design. In this work, temporal analysis of a model exploits         a translation in Uppaal timed automata for exhaustive verification. For large models a simulator was realized in Java which directly stems from RT-DEVS         operational semantics. The same concerns are at the basis of a real-time executive. The paper describes the proposed RT-DEVS         development methodology and clarifies its implementation status. The approach is demonstrated by applying it to an embedded         system example which is analyzed through model checking and implemented in Java. Finally, research directions which deserve         further work are indicated.      </content></document><document><year>2009</year><authors>Robert Bucholz1  | Phillip A. Laplante1 </authors><title>A dynamic capture&amp;#8211;recapture model for software defect prediction      </title><content>Several models have been developed that attempt to predict the total number of defects in a software product. One such approach         uses the capture&amp;#8211;recapture model, a technique employed by biologists for predicting wildlife populations. In this method once         the software is built and defects begin to be identified a prediction can be made for the total number of software defects         present. But capture&amp;#8211;recapture models rely on expert inspectors and the technique cannot be employed once the software has         been released. The work reported here extends the capture&amp;#8211;recapture technique to the post-inspection phase and to where inspection         data is unavailable, by using user defect reports. The proposed technique does not rely on expert inspectors and is particularly         suitable for open source software.      </content></document><document><year>2009</year><authors>Ondrej Rysavy1  | Jaroslav Rab1 </authors><title>A formal model of composing components: the TLA+ approach      </title><content>In this paper, a method for writing composable TLA+ specifications that conform to the formal model called Masaccio is introduced. Specifications are organized in TLA+ modules that correspond to Masaccio components by means of a trace-based semantics. Hierarchical TLA+ specifications are built from atomic component specifications by parallel and serial composition that can be arbitrary nested.         While the rule of parallel composition is a variation of the classical joint-action composition, the authors do not know about         a reuse method for the TLA+ that systematically employs the presented kind of a serial composition. By combining these two composition rules and assuming         only the noninterleaving synchronous mode of an execution, the concurrent, sequential, and timed compositionality is achieved.      </content></document><document><year>2009</year><authors>Alex|re BraganГ§a1  | Ricardo J. Machado2 </authors><title>A model-driven approach for the derivation of architectural requirements of software product lines      </title><content>The alignment of the software architecture and the functional requirements of a system is a demanding task because of the         difficulty in tracing design elements to requirements. The four-step rule set (4SRS) is a unified modeling language (UML)-based         model-driven method for single system development which provides support to the software architect in this task. This paper         presents an evolution of the 4SRS method aimed at software product lines. In particular, we describe how to address the transformation         of functional requirements (use cases) into component-based requirements for the product line architecture. The result is         a UML-based model-driven method that can be applied in combination with metamodeling tools such as the eclipse modeling framework         (EMF) to derive the architecture of software product lines. We present our approach in a practical way and illustrate it with         an example. We also discuss how our proposals are related to the work of other authors.      </content></document><document><year>2009</year><authors>Roman Gumzej1  | Wolfgang A. Halang2 </authors><title>A safety shell for UML-RT projects structure and methods of the corresponding UML pattern      </title><content>A safety shell pattern was defined based on a re-configuration management pattern and inspired by the architectural specifications         in Specification PEARL. It is meant to be used for real-time applications to be developed with UML-RT as described. The implementation         of the safety shell features as defined in Kornecki and Zalewski (Software Development for Real-Time Safety&amp;#8212;Critical Applications.         Software Engineering Workshop&amp;#8212;Tutorial Notes, 29th Annual IEEE/NASA 03, pp 1&amp;#8211;95, 2005), namely, its timing and state guards         as well as I/O protection and exception handling mechanisms, is explained. The pattern is parameterised by defining the properties         of its components as well as by defining the mapping between software and hardware architectures. Initial and alternative         execution scenarios as well as the method for switching between them are defined. The goal pursued with the safety shell is         to obtain clearly specified operation scenarios with well-defined transitions between them. To achieve safe and timely operation,         the pattern must provide safety shell mechanisms for an application designed, i.e., enable its predictable deterministic and         temporally predictable operation now and in the future.      </content></document><document><year>2009</year><authors>Roy Sterritt1  | Mike Hinchey2 </authors><title>Adaptive reflex autonomicity for real-time systems      </title><content>It may appear that for software systems that require strict real-time behavior, the idea of incorporating self-management         (and specifically concepts from Autonomic Computing) may add the burden of excessive additional functionality and overhead.         However, our experience is that, not only does real-time software benefit from autonomicity, but also the Autonomic Computing         initiative (like other initiatives aiming at self-management) requires the expertise of the real-time community in order to         achieve its overarching vision. In particular, there are emerging classes of real-time systems for which incorporation of         self-management is absolutely essential in order to implement all of the requirements of the system, and in particular the         timing requirements.      </content></document><document><year>2009</year><authors>Ahmed Rahni1 | Emmanuel Grolleau1  | MichaГ«l Richard1 </authors><title>An efficient response-time analysis for real-time transactions with fixed priority assignment      </title><content>In the general context of tasks with offsets (general transactions), only exponential methods are known to calculate the exact         worst-case response time (WCRT) of a task. The known pseudo-polynomial techniques give an upper bound of the WCRT. In this         paper, we present a new worst-case response-time analysis technique (mixed method) for transactions scheduled by fixed priorities,         and executing on a uniprocessor system. This new analysis technique gives a better (i.e. lower) pseudo-polynomial upper bound         of the WCRT. The main idea is to combine the principle of exact calculation and the principle of approximation calculation,         in order to decrease the pessimism of WCRT analysis, thus allowing improving the upper bound of the response time provided         while preserving a pseudo-polynomial complexity. Then we define the Accumulative Monotonic property on which a necessary condition         of feasibility is discussed. We also propose, to speed up the exact and mixed analysis, the dominated candidate task concept         that allows reducing significantly the number of critical instants to study in an analysis.      </content></document><document><year>2009</year><authors>Norman F. Schneidewind1 </authors><title>Analysis of object-oriented software reliability model development      </title><content>Can object-oriented methods be applied to mathematical software? Apparently, according to Beall and Shepard (An object-oriented         framework for reliable numerical simulations, object-oriented software. Addison Wesley, Reading, 1994) who say: &amp;#8220;It has been         recognized in a number of different fields that object-oriented programming, in general, and software frameworks, in particular,         provide a means to allow the efficient construction and maintenance of large scale software systems. Since general purpose         numerical analysis codes certainly qualify as large-scale software it makes sense for us to see how these methodologies can         be applied to this field.&amp;#8221;      </content></document><document><year>2009</year><authors>Lidia Fuentes1 | Nadia GГЎmez1  | Pablo SГЎnchez1 </authors><title>Aspect-oriented design and implementation of context-aware pervasive applications      </title><content>Pervasive applications must be aware of the contexts where they are executed. These contexts may vary greatly and change quickly.         Two main problems are associated with this issue: (1) context-awareness is a crosscutting concern that cannot be well-encapsulated         in a single module using traditional technologies, thus hindering software maintenance and reusability; and (2) reasoning         about application design correctness can be complex due to the number and diversity of potential contexts where a pervasive         application could be executed. In order to overcome these problems, we present a process for the design and implementation         of context-aware pervasive applications that uses aspect-orientation and executable modelling in order to overcome these shortcomings.         Aspect-oriented techniques contribute to the encapsulation of crosscutting concerns, such as context-awareness, into well-localized         modules. Executable modelling helps engineers to reason about application design by executing the design models in different         contexts and situations. Pervasive applications are modelled using the aspect-oriented executable modelling UML 2.0 profile,         executed at the modelling level for testing purposes, and then mapped into an aspect-oriented middleware platform for pervasive         applications. This process is illustrated using a location-aware intelligent transportation system consisting of a set of         cooperating sentient vehicles.      </content></document><document><year>2009</year><authors>Avi Soffer1  | Dov Dori2 </authors><title>Bridging the requirements&amp;#8211;implementation modeling gap with object&amp;#8211;process methodology      </title><content>A model-based system development cycle involves two semantically distinct aspects: the requirements specification and the         implementation model. Due to the conceptual and semantic differences between these two major system lifecycle stages, the         transition from requirements to implementation is inherently a noncoherent process. Consequently, the system requirements         are not faithfully transformed into the working system. This paper introduces an effective solution via an Integrated Modeling         Paradigm (IMP) that combines the requirements and implementation domain models into a unified system model that continuously         represents the system as it evolves. The IMP was implemented in an Object&amp;#8211;Process Methodology (OPM) development environment.         This implementation reinforces OPM with the capability to bridge the significant conceptual gap that lies right at the heart         of the development process. A user survey has shown that this OPM-based solution is easy to use and can indeed help bridge         the information gap, yielding a better match between the required and implemented systems than the currently accepted practice.      </content></document><document><year>2009</year><authors>Andrew Kornecki1  | Janusz Zalewski2 </authors><title>Certification of software for real-time safety-critical systems: state of the art      </title><content>This paper presents an overview and discusses the role of certification in safety-critical computer systems focusing on software,         and partially hardware, used in the civil aviation domain. It discusses certification activities according to RTCA DO-178B         &amp;#8220;Software Considerations in Airborne Systems and Equipment Certification&amp;#8221; and touches on tool qualification according to RTCA         DO-254 &amp;#8220;Design Assurance Guidance for Airborne Electronic Hardware.&amp;#8221; Specifically, certification issues as related to real-time         operating systems and programming languages are reviewed, as well as software development tools and complex electronic hardware         tool qualification processes are discussed. Results of an independent industry survey done by the authors are also presented.      </content></document><document><year>2009</year><authors>Jens B. JГёrgensen1 | Simon Tjell2  | JoГЈo M. Fern|es3 </authors><title>Formal requirements modelling with executable use cases and coloured Petri nets      </title><content>This paper presents executable use cases (EUCs), which constitute a model-based approach to requirements engineering. EUCs         may be used as a supplement to model-driven development (MDD) and can describe and link user-level requirements and more technical         software specifications. In MDD, user-level requirements are not always explicitly described, since usually it is sufficient         that one provides a specification, or platform-independent model, of the software that is to be developed. Therefore, a combination         of EUCs and MDD may have potential to cover the path from user-level requirements via specifications to implementations of         computer-based systems.      </content></document><document><year>2009</year><authors>Ricardo J. Machado1 | FlГЎvio R. Wagner2  | Rick Kazman3 </authors><title>Introduction to special issue: model-based development methodologies      </title><content>Without Abstract</content></document><document><year>2009</year><authors>Drago&amp;#351  Tru&amp;#351 can1 | TorbjГ¶rn Lundkvist1 | Marcus Alanen2 | Kim S|strГ¶m3 | Ivan Porres1  | Johan Lilius1 </authors><title>MDE for SoC design      </title><content>We employ the principles of model-driven engineering to assist the design of system-on-chip (SoC) architectures. As a concrete         example, we look at the MICAS architecture, for which we propose a graphical specification language, defined via metamodeling         techniques, that models the architecture at different abstraction levels. Model transformations are defined to support the         refinement of MICAS specification towards implementation. In addition, several libraries are put in place, to enable reuse         and automation throughout the design process. Tool support for editing the specifications, enforcing their consistency, and         for running the transformations is provided via the Coral modeling framework. The approach shows that model-driven engineering         can be seen as an enabler in providing computer-aided software engineering (CASE) tool support and automation for the development         of SoC architectures.      </content></document><document><year>2009</year><authors>Elvinia Riccobene1  | Patrizia Sc|urra2 </authors><title>Model transformations in the UPES/UPSoC development process for embedded systems      </title><content>Model-based development (MBD) aims at combining modeling languages with model transformers and code generators. Modeling languages,         like profiles of the Unified Modeling Language (UML), are increasingly being adopted for specific domains of interest to alleviate         the complexity of platforms and express domain concepts effectively. Moreover, system development processes based on automatic         model transformations are widely required to improve the productivity and quality of the developed systems. In this paper,         we show how MBD principles and automatic model transformations provide the basis for the unified process for embedded systems         (UPES) development process and its unified process for system-on-chip (SoC) (UPSoC) subprocess. They have been defined to         foster in a systematic and seamless manner a model-based design methodology based on the UML2 and UML profiles for the C/SystemC         programming languages, which we developed to improve the current industrial system design flow in the embedded systems and         system-on-chip area.      </content></document><document><year>2009</year><authors>Martin Kot1 </authors><title>Modeling selected real-time database concurrency control protocols in Uppaal      </title><content>Real-time database management systems (RTDBMS) are recently subject of an intensive research. Model checking algorithms and         verification tools are of great concern as well. In this paper, we show some possibilities of using a verification tool Uppaal         on some variants of pessimistic and optimistic concurrency control protocols used in real-time database management systems.         We present some possible models of such protocols expressed as nets of timed automata, which are a modeling language of Uppaal.      </content></document><document><year>2009</year><authors>Huibiao Zhu1 | Shengchao Qin2 | Jifeng He1  | Jonathan P. Bowen3 </authors><title>PTSC: probability, time and shared-variable concurrency      </title><content>Complex software systems typically involve features like time, concurrency and probability, where probabilistic computations         play an increasing role. It is challenging to formalize languages comprising all these features. In this paper, we integrate         probability, time and concurrency in one single model, where the concurrency feature is modelled using shared-variable-based         communication. The probability feature is represented by a probabilistic nondeterministic choice, probabilistic guarded choice         and a probabilistic version of parallel composition. We formalize an operational semantics for such an integration. Based         on this model we define a bisimulation relation, from which an observational equivalence between probabilistic programs is         investigated and a collection of algebraic laws are explored. We also implement a prototype of the operational semantics to         animate the execution of probabilistic programs.      </content></document><document><year>2009</year><authors>Hui Liang1| Jin Song Dong1| Jing Sun2  | W. Eric Wong3</authors><title>Software monitoring through formal specification animation      </title><content>This paper presents a formal specification-based software monitoring approach that can dynamically and continuously monitor         the behaviors of a target system and explicitly recognize undesirable behaviors in the implementation with respect to its         formal specification. The key idea of our approach is in building a monitoring module that connects a specification animator         with a program debugger. The requirements information about expected dynamic behaviors of the target system are gathered from         the formal specification animator, while the actual behaviors of concrete implementations of the target system are obtained         through the program debugger. Based on the information obtained from both sides, the judgement on the conformance of the concrete         implementation with respect to the formal specification is made timely while the target system is running. Furthermore, the         proposed formal specification-based software monitoring technique does not embed any instrumentation codes to the target system         nor does it annotate the target system with any formal specifications. It can detect implementation errors in a real-time         manner, and help the developers and users of the system to react to the problems before critical failure occurs.      </content></document><document><year>2009</year><authors>Janusz Zalewski1 </authors><title>Special section on real-time safety-critical systems      </title><content>Without Abstract</content></document><document><year>2009</year><authors>Marco Bakera1| Tiziana Margaria1 | Clemens D. Renner2 | Bernhard Steffen2 </authors><title>Tool-supported enhancement of diagnosis in model-driven verification      </title><content>We show on a case study from an autonomous aerospace context how to apply a game-based model-checking approach as a powerful         technique for the verification, diagnosis, and adaptation of system behaviors based on temporal properties. This work is part         of our contribution within the SHADOWS project, where we provide a number of enabling technologies for model-driven self-healing.         We propose here to use GEAR, a game-based model checker, as a user-friendly tool that can offer automatic proofs of critical         properties of such systems. Although it is a model checker for the full modal;&amp;#956;-calculus, it also supports derived, more user-oriented logics. With GEAR, designers and engineers can interactively investigate         automatically generated winning strategies for the games, by this way exploring the connection between the property, the system,         and the proof.      </content></document><document><year>2009</year><authors>FrГ©dГ©ric Jouault1 | Jean BГ©zivin1  | MikaГ«l Barbero1 </authors><title>Towards an advanced model-driven engineering toolbox      </title><content>Model-driven engineering (MDE) is frequently presented as an important change in software development practice. However, behind         this new trend, one may recognize a lot of different objectives and solutions. This paper first studies the multiple facets         of MDE and its evolution in the recent period. Several new usage scenarios (i.e., reverse engineering, and models at runtime),         which have appeared following the initial forward engineering scenario, i.e., platform-independent model (PIM) to platform-specific         model (PSM), are described. Not surprisingly, new applications trigger the need for new tools and the requirement for model         engineering platforms evolves correspondingly. We have adapted the AmmA toolbox to these new usages and the result is described         herein together with some illustrative examples.      </content></document><document><year>2009</year><authors>Fern|o Valles-Barajas1 </authors><title>Use of a lightweight formal method to model the static aspects of state machines      </title><content>In this paper the author uses Alloy as a modeling language to model the elements that form a state machine and the rules that         govern how they can be connected. In particular, the model proposed in this paper focuses in the static aspects of state machines.         To analyze and detect design errors in the model, the Alloy analyzer was used. With the use of this tool, design errors can         be detected very quickly. This tool can also generate instances of the model without making a line of code. The paper presents         two models based on the formal approach: a graphical model and a textual model. The graphical model is used as an overview         of the system while the textual model is used to establish further constraints on the graphical model.      </content></document><document><year>2009</year><authors>Lydie du Bousquet1 | Masahide Nakamura2 | Ben Yan3  | Hiroshi Igaki2 </authors><title>Using formal methods to increase confidence in a home network system implementation: a case study      </title><content>A home network system consists of multiple networked appliances, intended to provide more convenient and comfortable living         for home users. Before being deployed, one has to guarantee the correctness, the safety, and the security of the system. Here,         we present the approach chosen to validate the Java implementation of a home network system. We rely on the Java Modelling         Language to formally specify and validate an abstraction of the system.      </content></document><document><year>2009</year><authors>FrГ©dГ©ric Boniol2 | JГ©rГґme Ermont1  | Claire Pagetti1| 2 </authors><title>Verification of real-time systems with preemption: negative and positive results      </title><content>The aim of this article is to explore the problem of verification of preemptible real-time systems, i.e. systems composed         of tasks which can be suspended and resumed by an on-line scheduler. The first contribution of the article is to show that         this problem is unfortunately undecidable. To overcome this negative result, we restrict the real-time tasks to be periodic         and the implementation to be functionally deterministic, meaning that the preemptions do not affect the functional behaviour and preserve some temporal properties satisfied by the         specification. We prove that the verification problem of functional determinism is decidable. This outlines a verification         strategy: (1) prove that the scheduled real-time system is deterministic, (2) consider a deterministic non preemptible behaviour         which is functionally equivalent to the executions and (3) verify the properties on this behaviour.      </content></document><document><year>2008</year><authors>David Delahaye1 | Jean-FrГ©dГ©ric Г‰tienne1  | VГ©ronique ViguiГ© Donzeau-Gouge1 </authors><title>A formal and sound transformation from          Focal to UML: an application to airport security regulations      </title><content>We propose an automatic transformation of Focal specifications to UML class diagrams. The main motivation for this work lies within the framework of the EDEMOI project, which aims to integrate and apply several requirements engineering and formal methods techniques to analyze airport         security regulations. The idea is to provide a graphical documentation of formal models for developers, and in the long-term,         for certification authorities. The transformation is formally described and an implementation has been designed. We also show         how the soundness of our approach can be achieved.      </content></document><document><year>2008</year><authors>Doron Drusinsky1 | James Bret Michael1  | Man-Tak Shing1 </authors><title>A framework for computer-aided validation      </title><content>This paper presents a framework for augmenting independent validation and verification (IV&amp;amp;V) of software systems with computer-based         IV&amp;amp;V techniques. The framework allows an IV&amp;amp;V team to capture its own understanding of the application as well as the expected         behavior of any proposed system for solving the underlying problem by using an executable system reference model, which uses formal assertions to specify mission- and safety-critical behaviors. The framework uses execution-based model         checking to validate the correctness of the assertions and to verify the correctness and adequacy of the system under test.      </content></document><document><year>2008</year><authors>Christophe Sibertin-Blanc1 | Nabil Hameurlain2| 3  | Omar Tahir1 </authors><title>Ambiguity and structural properties of basic sequence diagrams      </title><content>Sequence Diagrams (SDs) are one of the most popular elements of the UML notation to model the dynamics of systems. However,         the graphical representation of basic SDs suffers from an inherent ambiguity that has led to different definitions in UML         1.x and in UML 2.0. This ambiguity paves the way for the consideration of several semantics for basic SDs. The paper studies         four of these semantics and shows to what extent their differences for a given SD (that is the amount of ambiguity of this         diagram) comes from its structural properties (linearity, local control and local causality). The fulfilment of these properties         can serve as a measure of the ambiguity of a SD, and thus the attention to be paid at its validation.      </content></document><document><year>2008</year><authors>Tian Zhang1 | FrГ©dГ©ric Jouault2 | Jean BГ©zivin2  | Xu|ong Li1 </authors><title>An MDE-based method for bridging different design notations      </title><content>Different communities have developed plenty of design notations for software engineering in support of practical (via UML)         and rigorous (via formal methods) approaches. Hence the problem of bridging these notations rises. Model-driven engineering         (MDE) is a new paradigm in software engineering, which treats models and model transformations as first class citizens. Furthermore,         it is seen as a promising method for bridging heterogeneous platforms. In this paper, we provide an MDE-based approach to         build bridges between informal, semi-formal and formal notations: Firstly, different notations are viewed as different domain         specification languages (DSLs) and introduced into MDE, especially into the ATLAS Model Management Architecture (AMMA) platform,         by metamodeling. Then, ATL transformation rules are built for semantics mapping. At last, TCS-based model-to-text syntax rules         are developed, allowing one to map models to programs. Consequently, different design notations in both graphical style and         grammatical style are bridged. A case study of bridging OMG SysML&amp;#8482; to LOTOS is also illustrated showing the validity and practicability         of our approach.      </content></document><document><year>2008</year><authors>Peter T. Breuer1  | Simon Pickin2 </authors><title>Approximate verification in an open source world      </title><content>This article details advances in a lightweight technology we have evolved to handle post hoc verification in the very large,         uncontrolled and rapidly evolving code-bases exemplified by C language open source projects such as the Linux kernel. Successful         operation in this context means timeliness, and we are currently treating millions of lines of unrestricted mixed C and assembler         source code in a few hours on very modest platforms. The technology is soundly based, in that it delivers false alarms (in         a ratio of about 8 to 1 in practice), rather than misses true alarms. Speed of operation is traded off against accuracy via         configuration of a program logic tailored to each analysis. The program logic specification language and the theory behind         it will be described here.      </content></document><document><year>2008</year><authors>Irfan Hamid1 | Bechir Zalila1| Elie Najm1 | JГ©rГґme Hugues1</authors><title>Automatic framework generation for hard real-time applications      </title><content>The communication and tasking infrastructure of a real-time application makes up a significant portion of any modern embedded         control system. Traditionally, the tasking and communication constructs are provided by a full-fledged real-time operating         system. We present an approach to automatically generate a robust framework for an application from its architectural description.         This framework sits atop a Ravenscar-compliant runtime as opposed to a standard real-time operating system. We also present         an extension of our approach to support code generation for high-integrity distributed applications.      </content></document><document><year>2008</year><authors>Ana Sofia C. MarГ§al1| 2 | Bruno Celso C. de Freitas2 | Felipe S. Furtado Soares2 | Maria Elizabeth S. Furtado1 | Teresa M. Maciel2  | Arnaldo D. Belchior1</authors><title>Blending Scrum practices and CMMI project management process areas      </title><content>Software development organizations that have been employing capability maturity models, such as SW-CMM or CMMI for improving         their processes are now increasingly interested in the possibility of adopting agile development methods. In the context of         project management, what can we say about Scrum&amp;#8217;s alignment with CMMI? The aim of our paper is to present the mapping between         CMMI and the agile method Scrum, showing major gaps between them and identifying how organizations are adopting complementary         practices in their projects to make these two approaches more compliant. This is useful for organizations that have a plan-driven         process based on the CMMI model and are planning to improve the agility of processes or to help organizations to define a         new project management framework based on both CMMI and Scrum practices.      </content></document><document><year>2008</year><authors>FrГ©dГ©ric Mallet1 </authors><title>Clock constraint specification language: specifying clock constraints with UML/MARTE      </title><content>The Object Management Group (OMG) unified modeling language (UML) profile for modeling and analysis of real-time and embedded         systems (MARTE) aims at using the general-purpose modeling language UML in the domain of real-time and embedded (RTE) systems.         To achieve this goal, it is absolutely required to introduce inside the mainly untimed UML an unambiguous time structure which         MARTE model elements can rely on to build precise models amenable to formal analysis. The MARTE Time model has defined such         a structure. We have also defined a non-normative concrete syntax called the clock constraint specification language (CCSL)         to demonstrate what can be done based on this structure. This paper gives a brief overview of this syntax and its formal semantics,         and shows how existing UML model elements can be used to apply this syntax in a graphical way and benefit from the semantics.      </content></document><document><year>2008</year><authors>Bernd J. KrГ¤mer1 </authors><title>Component meets service: what does the mongrel look like?      </title><content>Computing with services has attracted much attention as a promising approach for developing distributed applications. The         approach is often advertised as being superior to distributed component-based software engineering (CBSE), because it provides         a higher potential to bridge heterogeneous IT application and infrastructure landscapes. It facilitates cross-institutional         cooperation, lets services run over all kinds of ubiquitous communication infrastructure, scales better and simplifies legacy         software integration. If this were absolutely true, there would be no reason for a consortium of major vendors of service         and Java EE technology to come up with a new specification, called service component architecture (SCA). This emerging standard         tries to leverage service-oriented architecture (SOA) principles with component-based software development techniques. In         this article we discuss some commonalities and fundamental differences of the CBS and SOA worlds. We illustrate SCA briefly         using snippets of an ongoing case study based on an e-university federation. Then we elaborate on the qualities and current         deficits of SCA in the light of CBSE findings and related works.      </content></document><document><year>2008</year><authors>J. Carter1  | W. B. Gardner1 </authors><title>Converting scenarios to CSP traces with Mise en Scene for requirements-based programming      </title><content>The Requirements-to-Design-to-Code (R2D2C) project of NASA&amp;#8217;s Software Engineering Laboratory is based on inferring a formal         specification expressed in Communicating Sequential Processes (CSP) from system requirements supplied in the form of CSP traces.         The traces, in turn, are to be derived from scenarios, a user-friendly medium used to describe the required behavior of computer         systems under development. An extensive survey of the &amp;#8220;scenario&amp;#8221; concept and an overview of scenario-based approaches to system         engineering are presented. This work, called Mise en Scene, defines a new scenario medium (scenario notation language, SNL)         suitable for control-dominated systems, coupled with a two-stage process for automatic translation of scenarios to a new trace         medium (trace notation language, TNL), which encompasses CSP traces. Notes on progress toward a &amp;#8220;smart&amp;#8221; scenario authoring         tool are provided, as well as a detailed case study.      </content></document><document><year>2008</year><authors>Gabriela Robiolo1  | Ricardo Orosco2 </authors><title>Employing use cases to early estimate effort with simpler metrics      </title><content>In order to calculate size and to estimate effort in applications, the standard method most usually used is function points,         which has been used with good results in the development of industrial software for some time. However, some aspects should         be improved, namely: the time at which the estimation of effort is performed and the margin of error in the effort estimation.         Consequently, another size metric which could be used to obtain more accurate estimations should be found. This article presents         two other size metrics for projects based on use cases: transactions and entity objects. Effort is estimated by using the         technique mean productivity value. There is also a description of two case studies, one which involved four academic projects         and the other one which involved four industrial projects. They were developed in order to compare the estimations obtained         with each method. The result shows that the current way of estimating effort can be improved by using the number of transactions         as a size metric and the technique mean productivity value.      </content></document><document><year>2008</year><authors>Ronald T. Kneusel1 </authors><title>Extending interactive data language with higher-order functions      </title><content>The interactive data language (IDL) is a dynamically typed array processing language widely used for the analysis of images         and other scientific data. It operates in two basic modes. The first is a command line mode for interactive analysis and visualization         of scientific data. The second is as a development platform for end-user applications which process scientific data. This         paper details the introduction of higher-order functions to the core IDL. The purpose of these constructs is to increase the         productivity of the interactive IDL user. Historically, interactive users of IDL have been scientists and engineers engaged         in the exploration of new data. The addition of functional constructs aids these users by allowing them to accomplish, in         a few lines of code, what might otherwise require writing a custom function which is then compiled and used. The constructs         described were implemented as C language extensions to core IDL. The IDL extensions themselves are available for download         at http://www.ittvis.com/idl/hof/.      </content></document><document><year>2008</year><authors>Marc Frappier1 | FrГ©dГ©ric Gervais2 | RГ©gine Laleau2 | BenoГ®t Fraikin1  | Richard St-Denis1 </authors><title>Extending statecharts with process algebra operators      </title><content>This paper describes an adaptation of statecharts to take advantage of process algebra operators like those found in CSP and         EB3. The resulting notation is called algebraic state transition diagrams (ASTDs). The process algebra operators considered include sequence, iteration, parallel composition, and quantified synchronization.         Quantification is one of the salient features of ASTDs, because it provides a powerful mechanism to precisely and explicitly         define cardinalities in a dynamic model. The formal semantics of ASTDs is expressed using the operational style typically         used in process algebras. The target application domain is the specification and implementation of information systems.      </content></document><document><year>2008</year><authors>Antonio Cerone1  | Paul Curzon2 </authors><title>Formal methods for interactive systems      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Judy Bowen1  | Steve Reeves1</authors><title>Formal models for user interface design artefacts      </title><content>There are many different ways of building software applications and of tackling the problems of understanding the system to         be built, designing that system and finally implementing the design. One approach is to use formal methods, which we can generalise         as meaning we follow a process which uses some formal language to specify the behaviour of the intended system, techniques         such as theorem proving or model-checking to ensure the specification is valid (i.e., meets the requirements and has been         shown, perhaps by proof or other means of inspection, to have the properties the client requires of it) and a refinement process         to transform the specification into an implementation. Conversely, the approach we take may be less structured and rely on         informal techniques. The design stage may involve jotting down ideas on paper, brainstorming with users etc. We may use prototyping         to transform these ideas into working software and get users to test the implementation to find problems. Formal methods have         been shown to be beneficial in describing the functionality of systems, what we may call application logic, and underlying         system behaviour. Informal techniques, however, have also been shown to be useful in the design of the user interface to systems.         Given that both styles of development are beneficial to different parts of the system we would like to be able to use both         approaches in one integrated software development process. Their differences, however, make this a challenging objective.         In this paper we describe models and techniques which allow us to incorporate informal design artefacts into a formal software         development process.      </content></document><document><year>2008</year><authors>Isabelle Perseil1  | Laurent Pautet1</authors><title>Foundations of a new software engineering method for real-time systems      </title><content>The design of a fault-tolerant distributed, real-time, embedded system with safety-critical concerns requires the use of formal         languages. In this paper, we present the foundations of a new software engineering method for real-time systems that enables         the integration of semiformal and formal notations. This new software engineering method is mostly based upon the &amp;#8221;COntinuuM&amp;#8221;         co-modeling methodology that we have used to integrate architecture models of real-time systems (Perseil and Pautet in 12th         International conference on engineering of complex computer systems, ICECCS, IEEE Computer Society, Auckland, pp 371&amp;#8211;376,         2007) (so we call it &amp;#8220;Method C&amp;#8221;), and a model-driven development process (ISBN 978-0-387-39361-2 in: From model-driven design         to resource management for distributed embedded systems, Springer, chap. MDE benefits for distributed, real time and embedded         systems, 2006). The method will be tested in the design and development of integrated modular avionics (IMA) frameworks, with         DO178, DO254, DO297, and MILS-CC requirements.      </content></document><document><year>2008</year><authors>Huibiao Zhu1 | Jifeng He1 | Jonathan P. Bowen2 </authors><title>From algebraic semantics to denotational semantics for Verilog      </title><content>This paper considers how the algebraic semantics for Verilog relates with its denotational semantics. Our approach is to derive         the denotational semantics from the algebraic semantics. We first present the algebraic laws for Verilog. Every program can         be expressed as a guarded choice that can model the execution of a program. In order to investigate the parallel expansion         laws, a sequence is introduced, indicating which instantaneous action is due to which exact parallel component. A head normal         form is defined for each program by using a locality sequence. We provide a strategy for deriving the denotational semantics         based on head normal form. Using this strategy, the denotational semantics for every program can be calculated. Program equivalence         can also be explored by using the derived denotational semantics.      </content></document><document><year>2008</year><authors>Stefan Gruner1 </authors><title>From use cases to test cases via meta model-based reasoning         Position paper: work in progress</title><content>In Use cases considered harmful, Simons has analyzed the logical weaknesses of the UML use case notation and has recommended to &amp;#8220;fix the faulty notion of         dependency&amp;#8221; (Simons: Use cases considered harmful. 29th Conference on Techn. of OO Lang. and Syst., pp 194&amp;#8211;203, 1999). The         project sketched in this position paper is inspired by Simons&amp;#8217; critique. The main contribution of this paper is a detailed         meta model of possible relations between use cases. Later in the project this meta model is then to be formalized in a natural         deduction calculus which shall be implemented in the Prolog. As a result of such formalization a use case specification can be queried for inconsistencies as well as for test cases         which must be observable after a software system is implemented based on such a use case specification. Software tool support         for this method is also under development.      </content></document><document><year>2008</year><authors>Sven JГ¶rges1 | Tiziana Margaria2  | Bernhard Steffen1 </authors><title>Genesys: service-oriented construction of property conform code generators      </title><content>This paper presents Genesys, a framework for the high-level construction of property conform code generators. Genesys is an         integral part of jABC, a flexible framework designed to enable a systematic model-driven development of systems and applications         on the basis of an (extensible) library of well-defined, reusable building blocks. Within jABC, Genesys manages the construction         of code generators for jABC&amp;#8217;s models. So far, Genesys has been used to develop code generators for a variety of different         target platforms, like a number of Java-based platforms, mobile devices, BPEL engines, etc. Since the code generators are         themselves built within the jABC in a model-driven way, concepts like bootstrapping and reuse of existing components enable         a fast evolution of Genesys&amp;#8217; code generation library, and a high degree of self-application. Due to its increasing complexity         and its high degree of reuse, Genesys profits from model checking-based verification. This way, jABC&amp;#8217;s models of code generators         can be automatically checked wrt. well-formedness properties, to ensure that the models do indeed only consist of building         blocks which are suitable for the considered target platforms, and whose versions are mutually compatible. It can be also         be verified that the code generation process only starts after a successful initialization phase, and that the generated code         is always packaged with all the required libraries. We will illustrate the ease of extension and flexibility of the Genesys         framework by describing the intended coverage of diversity while highlighting the high potential of reuse.      </content></document><document><year>2008</year><authors>Boulbaba Ben Ammar1| 2 | Mohamed Tahar Bhiri2  | Jeanine SouquiГЁres1 </authors><title>Incremental development of UML specifications using operation refinements      </title><content>In an incremental specification development process, operations are used to model dynamic aspects and can be refined gradually.         We propose four kinds of operation refinement in order to control modifications when developing and refactoring UML specifications.         Each refinement is described with its properties and illustrated by an example, showing which verifications can be done, using         the B formal method.      </content></document><document><year>2008</year><authors>Jean-Michel Bruel1 | Agusti Canals2 | SГ©bastien GГ©rard3  | Isabelle Perseil4 </authors><title>Introduction to special issue: papers from UML&amp;amp;FM      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Jean-Michel Bruel1 | Agusti Canals2 | SГ©bastien GГ©rard3  | Isabelle Perseil4 </authors><title>Introduction to special issue: papers from UML&amp;amp;FM      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Michael G. Hinchey1 </authors><title>Introduction to special issue: selected papers from SEW-31      </title><content>Without Abstract</content></document><document><year>2008</year><authors>Tim Menzies1 | Markl| Benson2 | Ken Costello2 | Christina Moats2 | Melissa Northey2  | Julian Richardson2| 3 </authors><title>Learning better IV&amp;amp;V practices      </title><content>After data mining National Aeronautics and Space Administration (NASA) independent verification and validation (IV&amp;amp;V) data,         we offer (a) an early life cycle predictor for project issue frequency and severity; (b) an IV&amp;amp;V task selector (that used         the predictor to find the appropriate IV&amp;amp;V tasks); and (c) pruning heuristics describing what tasks to ignore, if the budget         cannot accommodate all selected tasks. In ten-way cross-validation experiments, the predictor performs very well indeed: the         average f-measure for predicting four classes of issue severity was over 0.9. This predictor is built using public-domain data and         software. To the best of our knowledge, this is the first reproducible report of a predictor for issue frequency and severity         that can be applied early in the life cycle.      </content></document><document><year>2008</year><authors>Luiz AndrГ© P. Leme1 | Daniela F. Brauner2 | Karin K. Breitman1 | Marco A. Casanova1  | Alex|re Gazola1 </authors><title>Matching object catalogues      </title><content>A catalogue holds information about a set of objects, typically classified using terms taken from a given thesaurus, and described         with the help of a set of attributes. Matching a pair of catalogues means to find a relationship between the terms of their         thesauri and a relationship between their attributes. This paper first introduces a matching approach, based on the notion         of similarity, that applies to both thesauri and attribute matching. It then describes matchings based on mutual information         and introduces variations that explore certain heuristics. Finally, it discusses experimental results that evaluate the precision         of the matchings and that measure the influence of the heuristics.      </content></document><document><year>2008</year><authors>Kristian Bisgaard Lassen1  | Simon Tjell1 </authors><title>Model-based requirements analysis for reactive systems with UML sequence diagrams and coloured petri nets      </title><content>In this paper, we describe a formal foundation for a specialized approach to automatically check traces against real-time         requirements. The traces are obtained from simulation of coloured petri net (CPN) models of reactive systems. The real-time         requirements are expressed in terms of a derivative of UML 2.0 high-level sequence diagrams. The automated requirement checking         is part of a bigger tool framework in which VDM++ is applied to automatically generate initial CPN models based on problem         diagrams. These models are manually enhanced to provide behavioral descriptions of the environment and the system itself.      </content></document><document><year>2008</year><authors>Rimvydas Ruk&amp;#353 &amp;#279 nas1 | Paul Curzon1  | Ann Bl|ford2 </authors><title>Modelling and analysing cognitive causes of security breaches      </title><content>In this paper we are concerned with security issues that arise in the interaction between user and system. We focus on cognitive         processes that affect security of information flow from the user to the computer system and the resilience of the whole system         to intruder attacks. For this, we extend our framework developed for the verification of usability properties by introducing         two kinds of intruder models, an observer and an active intruder, with the associated security properties. Finally, we consider         small examples to illustrate the ideas and approach. These examples demonstrate how our framework can be used (a) to detect         confidentiality leaks, caused by a combination of an inappropriate design and certain aspects of human cognition, and (b)         to identify designs more susceptible to cognitively based intruder attacks.      </content></document><document><year>2008</year><authors>Neelam Soundarajan1 | Jason O. Hallstrom2 | Guoqiang Shu1  | Adem Delibas1 </authors><title>Patterns: from system design to software testing      </title><content>Design patterns are used extensively in the design of software systems. Patterns codify effective solutions for recurring         design problems and allow software engineers to reuse these solutions, tailoring them appropriately to their particular applications,         rather than reinventing them from scratch. In this paper, we consider the following question: How can system designers and         implementers test whether their systems, as implemented, are faithful to the requirements of the patterns used in their design? A key consideration         underlying our work is that the testing approach should enable us, in testing whether a particular pattern P has been correctly implemented in different systems designed using P, to reuse the common parts of this effort rather than having to do it from scratch for each system. Thus in the approach         we present, corresponding to each pattern P, there is a set of pattern test case templates (PTCTs). A PTCT codifies a reusable test case structure designed to identify defects associated with applications of P in all systems designed using P. Next we present a process using which, given a system designed using P, the system tester can generate a test suite from the PTCTs for P that can be used to test the particular system for bugs in the implementation of P in that system. This allows the tester to tailor the PTCTs for P to the needs of the particular system by specifying a set of specialization rules that are designed to reflect the scenarios in which the defects codified in this set of PTCTs are likely to manifest         themselves in the particular system. We illustrate the approach using the Observer pattern.      </content></document><document><year>2008</year><authors>Ralf BuschermГ¶hle1  | JГ¶rg Oelerink1 </authors><title>Rich meta object facility formal integration platform: syntax, semantics and implementation      </title><content>Traditionally formal papers bridge interpretation gaps of informal OMG specifications. The papers are often not holistic but         rather concentrate on certain aspects of the original specification. This increases often the gap between research and practice         because it is difficult to understand and combine different semantic methodologies. This paper formalizes and extends the         Meta Object Facility of the Object Management Group towards a platform to explore and combine formal methodologies. The extension         focuses primarily on algorithms with an action language to define all kinds of normative and desired behavior. The methodology         includes mechanisms to support arbitrary dependent language layers. Exemplary syntax and semantics of the methodology is introduced         on base of the original specifications. This is complemented by an implementation supporting the graphical definition and         simulation of instantiated models. The platform binding is supported by code generators allowing the easy connection of powerful         analysis techniques. The platform binding can be done on all layers finding the optimal mixture between independent semantic         variants currently in the discussion and established dependent semantic variants strengthening the bridge between research         and practice.      </content></document><document><year>2008</year><authors>Huafeng Yu1 | Abdoulaye GamatiГ©1| Г‰ric Rutten2  | Jean-Luc Dekeyser1</authors><title>Safe design of high-performance embedded systems in an MDE framework      </title><content>In this paper, we use the UML MARTE profile to model high-performance embedded systems (HPES) in the GASPARD2 framework. We address the design correctness issue on the UML model by using the formal validation tools associated with         synchronous languages, i.e., the SIGALI model checker, etc. This modeling and validation approach benefits from the advantages of UML as a standard, and from the         number of validation tools built around synchronous languages. In our context, model transformations act as a bridge between         UML and the chosen validation technologies. They are implemented according to a model-driven engineering approach. The modeling         and validation are illustrated using the multimedia functionality of a new-generation cellular phone.      </content></document><document><year>2008</year><authors>R. PlГ¶sch1 | H. Gruber1 | A. Hentschel2 | Ch. KГ¶rner2 | G. Pomberger1 | S. Schiffer1 | M. Saft2  | S. Storck3 </authors><title>The EMISQ method and its tool support-expert-based evaluation of internal software quality      </title><content>There is empirical evidence that internal software quality, e.g., the quality of source code, has great impact on the overall         quality of software. Besides well-known manual inspection and review techniques for source code, more recent approaches utilize         tool-based static code analysis for the evaluation of internal software quality. Despite the high potential of code analyzers         the application of tools alone cannot replace well-founded expert opinion. Knowledge, experience and fair judgment are indispensable         for a valid, reliable quality assessment, which is accepted by software developers and managers. The EMISQ method (Evaluation         Method for Internal Software Quality), guides the assessment process for all stakeholders of an evaluation project. The method         is supported by the Software Product Quality Reporter (SPQR), a tool which assists evaluators with their analysis and rating         tasks and provides support for generating code quality reports. The application of SPQR has already proved its usefulness         in various code assessment projects around the world. This paper introduces the EMISQ method and describes the tool support         needed for an efficient and effective evaluation of internal software quality.      </content></document><document><year>2008</year><authors>Iulian Ober1 | Susanne Graf2 | Yuri Yushtein3| 4  | Ileana Ober1 </authors><title>Timing analysis and validation with UML: the case of the embedded MARS bus manager      </title><content>This paper presents a case study in UML-based modeling and validation of the intricate timing aspects arising in a small but         complex component of the airborne Medium-Altitude Reconnaissance System produced by the Netherlands National Aerospace Laboratory.         The purpose is to show how automata-based timing analysis and verification tools can be used by field engineers for solving         isolated hard points in a complex real-time design, even if the press-button verification of entire systems remains a remote         goal. We claim that the accessibility of such tools is largely improved by the use of an UML profile with intuitive features         for modeling timing and related properties.      </content></document><document><year>2008</year><authors>Hung Le Dang1 | Hubert Dubois1  | SГ©bastien GГ©rard1 </authors><title>Towards a traceability model in a MARTE-based methodology for real-time embedded systems      </title><content>This paper addresses the traceability management in the context of Accord|UML, a MARTE-based methodology for designing distributed real-time embedded systems. Our contribution is two fold: on the one         hand, we propose to include directly requirements in the modeling process; on the other hand, we identify potential traceability         links that we model by using the SysML requirement profile. We also present the toolbox that supports our contribution.      </content></document><document><year>2008</year><authors>Yann Thierry-Mieg1  | Lom-Messan Hillah1</authors><title>UML behavioral consistency checking using instantiable Petri nets      </title><content>Model-driven engineering (MDE) development methods are gaining increasing attention from industry. In MDE, the model is the         primary artifact and serves several goals, including code generation, requirements traceability, and model-based testing.         MDE thus enables cost-effective building of models versus direct coding of an application. Thus model-based formal verification         of behavioral consistency is desirable as it helps improve model quality. Our approach is based on translation of a UML model         to instantiable Petri nets (IPN). This formalism is based on the semantics of Petri nets, but introduces the concepts of type and instance. This allows one to accurately capture these concepts in UML models. IPN support hierarchical descriptions natively, and use the notion of transition synchronization for composition of behaviors. This is a general and powerful mechanism borrowed from process algebra. We show that IPN allow         one to adequately address the challenges of translation from UML for analysis purposes. The approach has been implemented         and experimental results are presented.      </content></document><document><year>2008</year><authors>Dan Li1| 2| 3 | Xiaoshan Li2 | Jicong Liu1  | Zhiming Liu1 </authors><title>Validation of requirement models by automatic prototyping      </title><content>Prototyping is an efficient and effective way to understand and validate system requirements at the early stage of software         development. In this paper, we present an approach for transforming UML system requirement models with OCL specifications         into executable prototypes with the function of checking multiplicity and invariant constraints. Generally, a use case in         UML can be described as a sequence of system operations. A system operation can be formally defined by a pair of preconditions         and postconditions specified using OCL in the context of the conceptual class model. By analyzing the semantics of the preconditions         and postconditions, the execution of the operation can be prototyped as a sequence of primitive actions which first check         the precondition, and then enforce the postcondition by transferring the system from a pre-state to a post-state step by step.         The primitive actions are basic manipulations of the system state (an object diagram), including find objects and links, create and remove objects and links, and check and set attribute values. Based on this approach, we have developed a tool of automatic prototype generation and analysis: AutoPA3.0.      </content></document><document><year>2006</year><authors>Norman Schneidewind1 </authors><title>Allocation and analysis of reliability: multiple levels: system, subsystem, and module</title><content>We model the reliability allocation and prediction process across a hierarchical software system comprised of modules, subsystems, and system. We experiment in modeling complex reliability software systems using several software reliability models to test the feasibility of the process and to evaluate the accuracy of the models for this application. This is a subject deserving research and experimentation because this type of system is implemented in safety-critical projects, such as National Aeronautics and Space Administration (NASA) flight software modules, that we use in our experiments. Given the reliability requirement of a software system in the software planning or design stage, we predict each module&amp;#8217;s reliability and their relationships (e.g., reliability interactions among modules, subsystems, and system), Our critical interfaces and components are failure-mode sequences and the modules that comprise these sequences, respectively. In addition, we evaluate how sensitive the achievement of reliability goals is to predicted component reliabilities that do not meet expectations.</content></document><document><year>2006</year><authors>Antonio Mendes da Silva Filho1  | Ivanilton Polato2 </authors><title>Component behavior-based adaptation in embedded software</title><content>Several models of computation have been used in software development approaches. The specialization of the existing models makes them suitable to specific application domains. Nevertheless, when there is no solution for applications at hand, heterogeneous models have been used. Within this context, this paper discusses a heterogeneous model called extended dataflow with a focus on component-based design. The emphasis lies on the dynamics of the components, including the way they interact with each other, their behavioral modeling, and flow of control. The main objective is to provide mechanisms for supporting both the ability of the run-time environment to safely dispatch tasks and the ability of components to adapt their interfaces. This paper focuses on embedded software. The purpose of the mechanisms we have been working on is to improve robustness while promoting component-based design. An adaptive application involving digital filters is used to illustrate our approach.</content></document><document><year>2006</year><authors>Kalpesh Kapoor1 </authors><title>Formal Analysis of Coupling Hypothesis for Logical Faults</title><content>Fault-based testing focuses on detecting faults in a software. Test data is typically generated considering the presence of a single fault at a time, under the assumption of coupling hypothesis. Fault-based testing approach, in particular mutation testing, assumes that the coupling hypothesis holds. According to the hypothesis, a test set that can detect presence of single faults in an implementation, is also likely to detect presence of multiple faults. In this paper it is formally shown that the hypothesis is guaranteed to hold for a large number of logical fault classes.</content></document><document><year>2006</year><authors>Sergiy A. Vilkomir1 | Jonathan P. Bowen2  | Aditya K. Ghose3 </authors><title>Formalization and assessment of regulatory requirements for safety-critical software</title><content>Regulatory requirements, as opposed to requirements for a particular system, have a generic nature, are applicable to a wide range of systems and are the basis for certification or licensing process. The important tasks in requirement engineering are resolving requirements inconsistencies between regulators and developers of safety-critical computer systems and the validation of regulatory requirements. This paper proposes a new approach to the regulatory process, including the use of formal regulatory requirements as a basis for the development of software assessment methods. We address the differences between prescriptive and nonprescriptive regulation, and suggest a middle approach. The notion of a normative package is introduced as the collection of documents to be used by a regulator and provided to a developer. It is argued that the normative package should include not only regulatory requirements, but also methods of their assessment. This approach is illustrated with examples of requirements for protecting computer control systems against unauthorized access and against common-mode software failures, using the Z notation for the formalization.</content></document><document><year>2006</year><authors>Shengchao Qin1 | Wei-Ngan Chin2 | Jifeng He3  | Zongyan Qiu4 </authors><title>From Statecharts to Verilog: a formal approach to hardware/software co-specification      </title><content>Hardware/software co-specification is a critical phase in co-design. Our co-specification process starts with a high level         graphical description in Statecharts and ends with an equivalent parallel composition of hardware and software descriptions         in Verilog. In this paper, we first investigate the Statecharts formalism by providing it a formal syntax and a compositional         operational semantics. Based on that, a semantics-preserving linking function is designed to compile specifications written         in Statecharts into Verilog. The obtained Verilog specifications are then passed to a partitioning process to generate hardware         and software subspecifications, where the correctness is guaranteed by algebraic laws of Verilog.      </content></document><document><year>2006</year><authors>Sue Black1 </authors><title>Is ripple effect intuitive? A pilot study</title><content>The computation of ripple effect is based on the effect that a change to a single variable will have on the rest of a program; it determines the scope of the change and provides a measure of the program&amp;#8217;s complexity. The original algorithm used to compute ripple effect has been reformulated mainly to provide clarity in the operations involved. The reformulation involved some approximation which was shown not to affect the measures produced. The reformulated, approximated algorithm has been implemented as the software tool: Ripple Effect and Stability Tool (REST). This paper uses a software development project as a case study to look at the relationship between the approximated ripple effect and a programmer&amp;#8217;s intuitive idea of ripple effect. Four versions of a mutation testing software tool were written in C over a period of several months. After the completion of each version the programmer was asked to detail his predicted/intuitive ripple effect for each module of code. The predictions are compared with the approximated ripple effect measures for each module and some surprising conclusions drawn.</content></document><document><year>2006</year><authors>Markus Bajohr1  | Tiziana Margaria2 </authors><title>MaTRICS: A service-based management tool for remote intelligent configuration of systems</title><content>With MaTRICS, we describe a service-oriented architecture that allows remotely connected users to modify the configuration of any service provided by a specific (application) server, like email-, news- or web-servers. Novel to our approach is that the system can manage configuration processes on heterogeneous software- and hardware-platforms, which are performed from a variety of peripherals unmatched in today&amp;#8217;s practice, where devices like mobile phones, faxes, PDAs are enabled to be used by system managers as remote system configuration and management tools.</content></document><document><year>2006</year><authors>Manfred Broy1 </authors><title>Model-driven architecture-centric engineering of (embedded) software intensive systems: modeling theories and architectural         milestones      </title><content>Today, in general, embedded software is distributed onto networks and structured into logical components that interact asynchronously         by exchanging messages. The software system is connected to sensors, actuators, human machine interfaces and networks. In         this paper we study fundamental models of composed embedded software systems and their properties, identify and describe various         basic views, and show how they are related. We consider, in particular, models of data, states, interfaces, functionality,         hierarchically composed systems, and processes. We study relationships by abstraction and refinement as well as forms of composition         and modularity. In particular, we introduce a comprehensive mathematical model and a corresponding mathematical theory for         composed systems, its essential views and their relationships. We introduce two methodologically essential, complementary         and orthogonal concepts for the structured modeling of multifunctional embedded systems in software and systems engineering         and their scientific foundation. One approach addresses mainly tasks in requirements engineering and the specification of         the comprehensive user functionality of multifunctional systems in terms of their functions, features and services. The other         approach essentially addresses the design phase with its task to develop logical architectures formed by networks of interactive         components that are specified by their interface behavior.      </content></document><document><year>2006</year><authors>Ali Mili1| 2 | Frederick Sheldon3| Lamia Labed Jilani4| Alex Vinokurov1| Alex Thomasian1 | Rahma Ben Ayed5</authors><title>Modeling security as a dependability attribute: a refinement-based approach      </title><content>As distributed, networked computing systems become the dominant computing platform in a growing range of applications, they         increase opportunities for security violations by opening hitherto unknown vulnerabilities. Also, as systems take on more         critical functions, they increase the stakes of security by acting as custodians of assets that have great economic or social         value. Finally, as perpetrators grow increasingly sophisticated, they increase the threats on system security. Combined, these         premises place system security at the forefront of engineering concerns. In this paper, we introduce and discuss a refinement-based         model for one dimension of system security, namely survivability.      </content></document><document><year>2006</year><authors>Karine Arnout1  | Bertr| Meyer2| 3 </authors><title>Pattern Componentization: The Factory Example</title><content>Can Design Patterns be turned into reusable components? To help answer this question, we have performed a systematic study of the standard design patterns. One of the most interesting is Abstract Factory, for which we were indeed able to build a reusable component fulfilling the same needs as the original pattern. This article presents the component&amp;#8217;s design and its lessons for the general issue of pattern componentization.</content></document><document><year>2006</year><authors>Norman Schneidewind1 </authors><title>Software reliability engineering process</title><content>Without Abstract</content></document><document><year>2006</year><authors>Leo Freitas1 | Jim Woodcock1  | Ana Cavalcanti1 </authors><title>State-rich model checking      </title><content>In this paper we survey the area of formal verification techniques, with emphasis on model checking due to its wide acceptance         by both academia and industry. The major approaches and their characteristics are presented, together with the main problems         faced while trying to apply them. With the increased complexity of systems, as well as interest in software correctness, the         demand for more powerful automatic techniques is pushing the theories and tools towards integration. We discuss the state         of the art in combining formal methods tools, mainly model checking with theorem proving and abstract interpretation. In particular,         we present our own recent contribution on an approach to integrate model checking and theorem proving to handle state-rich         systems specified using a combination of Z and CSP.      </content></document><document><year>2006</year><authors>Peter T. Breuer1  | Simon Pickin1 </authors><title>Symbolic approximation: an approach to verification in the large</title><content>This article describes symbolic approximation, a theoretical foundation for techniques evolved for large-scale verification &amp;#8211; in particular, for post hoc verification of the C code in large-scale open-source projects such as the Linux kernel. The corresponding toolset&amp;#8217;s increasing maturity means that it is now capable of treating millions of lines of C code source in a few hours on very modest support platforms. In order to explicitly manage the state-space-explosion problem that bedevils model-checking approaches, we work with approximations to programs in a symbolic domain where approximation has a well-defined meaning. A more approximate program means being able to say less about what the program does, which means weaker logic for reasoning about the program. So we adjust the approximation by adjusting the applied logic. That guarantees a safe approximation (one which may generate false alarms but no false negatives) provided the logic used is weaker than the exact logic of C. We choose the logic to suit the analysis.</content></document><document><year>2006</year><authors>Albert Elcock1  | Phillip A. Laplante1 </authors><title>Testing software without requirements: using development artifacts to develop test cases</title><content>SourceForge, the world&amp;#8217;s largest development and download repository of open-source code and applications, hosts more than 114,500 projects with 1.3 million registered users at this writing. These applications are downloaded and used by millions across the globe. However, there is little evidence that any of these applications have been tested against a set of rigorous requirements and indeed, for most of these applications, it is likely that no requirements specification exists. How then, can one test this software? In this work an approach is presented to create a set of behavioral requirements to be used to develop test cases. These behavioral requirements were developed for a small number of projects derived from SourceForge using artifacts such as help files and software trails such as open and closed bug reports. These artifacts highlight implemented product features as well as expected behavior, which form the basis for what we term behavioral requirements. The behavioral requirements were then used to create test cases, which were exercised against the software. The contribution of this work is to demonstrate a viable technique for testing software when traditional requirements are unavailable or incomplete.</content></document><document><year>2006</year><authors>Federico Pecora1| 2 | Riccardo Rasconi1| 3| Gabriella Cortellessa1 | Amedeo Cesta1</authors><title>User-oriented problem abstractions in scheduling      </title><content>In this paper we describe a modeling framework aimed at facilitating the customization and deployment of artificial intelligence         (AI) scheduling technology in real-world contexts. Specifically, we describe an architecture aimed at facilitating software         product line development in the context of scheduling systems. The framework is based on two layers of abstraction: a first         layer providing an interface with the scheduling technology, on top of which we define a formalism to abstract domain-specific         concepts. We show how this two-layer modeling framework provides a versatile formalism for defining user-oriented problem         abstractions, which is pivotal for facilitating interaction between domain experts and technologists. Moreover, we describe         a graphical user interface (GUI)-enhanced tool which allows the domain expert to interact with the underlying core scheduling         technology in domain-specific terms. This is achieved by automatically instantiating an abstract GUI template on top of the second modeling layer.      </content></document><document><year>2007</year><authors>Ahmed Sidky1 | James Arthur1  | Shawn Bohner1 </authors><title>A disciplined approach to adopting agile practices: the agile adoption framework      </title><content>Many organizations aspire to adopt agile processes to take advantage of the numerous benefits that they offer to an organization.         Those benefits include, but are not limited to, quicker return on investment, better software quality, and higher customer         satisfaction. To date, however, there is no structured process (at least that is published in the public domain) that guides         organizations in adopting agile practices. To address this situation, we present the agile adoption framework and the innovative         approach we have used to implement it. The framework consists of two components: an agile measurement index, and a four-stage         process, that together guide and assist the agile adoption efforts of organizations. More specifically, the Sidky Agile Measurement         Index (SAMI) encompasses five agile levels that are used to identify the agile potential of projects and organizations. The         four-stage process, on the other hand, helps determine (a) whether or not organizations are ready for agile adoption, and         (b) guided by their potential, what set of agile practices can and should be introduced. To help substantiate the &amp;#8220;goodness&amp;#8221;         of the Agile Adoption Framework, we presented it to various members of the agile community, and elicited responses through         questionnaires. The results of that substantiation effort are encouraging, and are also presented in this paper.      </content></document><document><year>2007</year><authors>Fern|o Valles-Barajas1 </authors><title>A requirements engineering process for control engineering software      </title><content>This paper presents a novel approach for the specification of control engineering software. The proposal is based on software         engineering techniques which have improved the performance and quality of software processes. Some of the best practices for         the building of software products are included. An example of the proposal is given for the requirements specification of         the 1992 ACC robust control benchmark.      </content></document><document><year>2007</year><authors>Andres S. Orrego1  | Gregory E. Mundy1 </authors><title>A study of software reuse in NASA legacy systems      </title><content>Software reuse is regarded as a highly important factor in reducing development overheads for new software projects; however,         much of the literature is concerned with cost and labor savings that reuse brings to industrial software development and little         is known about the inherent risks associated with reuse, particularly in the case of mission and safety-critical software         systems. We present the preliminary findings of a research project geared toward assessing the impact of risk in National         Aeronautics and Space Administration (NASA) legacy software in flight control systems. We introduce the concept of context variables and the impact they have on reuse within these legacy systems as well as the genealogy classification models, which provide a simple, concise method of mapping reuse between families of software projects.      </content></document><document><year>2007</year><authors>Abilio Fern|es1 | Angelo E. M. Ciarlini2 | Antonio L. Furtado1 | Michael G. Hinchey3 | Marco A. Casanova1  | Karin K. Breitman1 </authors><title>Adding flexibility to workflows through incremental planning      </title><content>Workflow management systems usually interpret a workflow definition rigidly. However, there are real life situations where         users should be allowed to deviate from the prescribed static workflow definition for various reasons, including lack of information,         unavailability of the required resources and unanticipated situations. Furthermore, workflow complexity may grow exponentially         if all possible combinations of anticipated scenarios must be compiled into the workflow definition. To flexibilize workflow         execution and help reduce workflow complexity, this paper proposes a dual strategy that combines a library of predefined typical         workflows with a planner mechanism capable of incrementally synthesizing new workflows, at execution time. This dual strategy         is motivated by the difficulty of designing emergency plans, modeled as workflows, which account for real-life complex crisis         or accident scenarios.      </content></document><document><year>2007</year><authors>Christian Van der Velden1 | Cees Bil1 | Xinghuo Yu2  | Adrian Smith3 </authors><title>An intelligent system for automatic layout routing in aerospace design      </title><content>This paper discusses the development of an intelligent routing system for automating design of electrical wiring harnesses         and pipes in aircraft. The system employs knowledge based engineering (KBE) methods and technologies for capturing and implementing         rules and engineering knowledge relating to the routing process. The system reads a mesh of three dimensional structure and         obstacles falling within a given search space and connects source and target terminals satisfying a knowledge base of design         rules and best practices. Routed paths are output as computer aided design (CAD) readable geometry, and a finite element (FE)         mesh consisting of geometry, routed paths and a knowledge layer providing detail of the rules and knowledge implemented in         the process. Use of this intelligent routing system provides structure to the routing design process and has potential to         deliver significant savings in time and cost.      </content></document><document><year>2007</year><authors>Joaquin PeГ±a1 | Michael G. Hinchey2 | Roy Sterritt3  | Antonio Ruiz-CortГ©s1 </authors><title>Building and implementing policies in autonomous and autonomic systems using MaCMAS         A case study based on a NASA concept mission</title><content>Autonomic Computing, self-management based on high level guidance from humans, is increasingly being accepted as a means forward         in designing reliable systems that both hide complexity from the user and control IT management costs. Effectively, AC may         be viewed as policy-based self-management. We look at ways of achieving this, with particular focus on agent-oriented software         engineering. We propose utilizing MaCMAS, an AOSE methodology for specifying autonomic and autonomous properties of the system         independently. Later, by means of composition of these specifications, guided by a policy specification, we construct a specification         for the policy and its subsequent deployment. We illustrate this by means of a case study based on a NASA concept mission         and describe future work on a support toolkit.      </content></document><document><year>2007</year><authors>Dawn Lawrie1 | Christopher Morrell2 | Henry Feild1  | David Binkley1 </authors><title>Effective identifier names for comprehension and memory      </title><content>Readers of programs have two main sources of domain information: identifier names and comments. When functions are uncommented,         as many are, comprehension is almost exclusively dependent on the identifier names. Assuming that writers of programs want         to create quality identifiers (e.g., identifiers that include relevant domain knowledge), one must ask how should they go         about it. For example, do the initials of a concept name provide enough information to represent the concept? If not, and         a longer identifier is needed, is an abbreviation satisfactory or does the concept need to be captured in an identifier that         includes full words? What is the effect of longer identifiers on limited short term memory capacity? Results from a study         designed to investigate these questions are reported. The study involved over 100 programmers who were asked to describe 12         different functions and then recall identifiers that appeared in each function. The functions used three different levels         of identifiers: single letters, abbreviations, and full words. Responses allow the extent of comprehension associated with         the different levels to be studied along with their impact on memory. The functions used in the study include standard computer         science textbook algorithms and functions extracted from production code. The results show that full-word identifiers lead         to the best comprehension; however, in many cases, there is no statistical difference between using full words and abbreviations.         When considered in the light of limited human short-term memory, well-chosen abbreviations may be preferable in some situations         since identifiers with fewer syllables are easier to remember.      </content></document><document><year>2007</year><authors>Norman F. Schneidewind1 </authors><title>Finding the optimal parameters for a software reliability model      </title><content>Failure rate parameters exert a great deal of influence on the accuracy of software reliability predictions. Thus, it behooves         us to examine the methods that are used to estimate the parameters. The value of parameters is determined by the pattern of         historical failure data and the amount of data used in the estimates. Conventional wisdom suggests that all the data be used.         However, this could be counterproductive if all the data are not representative of the current and future failure process.         Therefore, we can analyze these factors to obtain the optimal parameter estimates, where &amp;#8220;optimal&amp;#8221; means parameter values         that will result in minimum prediction error. We examine one software reliability model that does not use all the failure         data if a priori analysis indicates that optimal parameter values could not be obtained. Historically, this model has used         a mean square error criterion (MSE) of the difference between actual and predicted reliability in the parameter estimation range. However, it has been observed that this policy does not always result in minimum error predictions in the prediction range. Therefore, we investigated the linear interpolation estimation method as an alternative to MSE for three NASA shuttle flight         software releases comprising forty-two reliability predictions. Interestingly, linear interpolation provided better prediction         accuracy, overall. We believe our analysis can be applied to other software and failure data. In order to systematize our         thinking on developing new models using linear interpolation, we employed object-oriented design concepts as guides to implementing C++ programs for evaluating linear interpolation with respect to MSE.      </content></document><document><year>2007</year><authors>Fei Xue1 | Lu Yan2 </authors><title>Formal approach to fault diagnosis in distributed discrete event systems with OBDD      </title><content>In this paper, we study the fault diagnosis problem for distributed discrete event systems. The model assumes that the system         is composed of distributed components which are modeled in labeled Petri nets and interact with each other via sets of common         resources (places). Further, a component&amp;#8217;s own access to a common resource is an observable event. Based on the diagnoser         approach proposed by Sampath et;al., a distributed fault diagnosis algorithm with communication is presented. The distributed         algorithm assumes that the local diagnosis process can exchange messages upon the occurrence of observable events. We prove         the distributed diagnosis algorithm is correct in the sense that it recovers the same diagnostic information as the centralized         diagnosis algorithm. Furthermore, we introduce the ordered binary decision diagrams (OBDD) in order to manage the state explosion         problem in state estimation of the system.      </content></document><document><year>2007</year><authors>David Bustard1 | Sa&amp;#8217 adah Hassan1 | David McSherry1  | Steven Walmsley1</authors><title>GRAPHIC illustrations of autonomic computing concepts      </title><content>Illustrations are general aids to communication, with those that are particularly effective becoming standard exemplars. In         the computing domain, for instance, frequent reference is made to lift controllers, automated bank teller machines, and car         parks. So far, there are few equivalent illustrations for autonomic systems. The purpose of this paper is to clarify the requirements         for standard exemplars, propose an autonomic computing exemplar for consideration, and assess it against the criteria identified.         The exemplar described, an autonomic printing service, aims to be GRAPHIC: general, realistic, appealing, powerful, holistic,         informative, and comprehensible. Experience in implementing a demonstration printing service with autonomic characteristics         is also briefly discussed.      </content></document><document><year>2007</year><authors>C. D. Peters-Lidard1 | P. R. Houser2| Y. Tian1| 3| S. V. Kumar1| 3| J. Geiger4| S. Olden4| L. Lighty4| B. Doty5| P. Dirmeyer5| J. Adams5| K. Mitchell6| E. F. Wood7 | J. Sheffield7</authors><title>High-performance Earth system modeling with NASA/GSFC&amp;#8217;s Land Information System      </title><content>The Land Information System software (LIS; http://lis.gsfc.nasa.gov/, 2006) has been developed to support high-performance         land surface modeling and data assimilation. LIS integrates parallel and distributed computing technologies with modern land         surface modeling capabilities, and establishes a framework for easy interchange of subcomponents, such as land surface physics,         input/output conventions, and data assimilation routines. The software includes multiple land surface models that can be run         as a multi-model ensemble on global or regional domains with horizontal resolutions ranging from 2.5В° to 1;km. The software         may execute serially or in parallel on various high-performance computing platforms. In addition, the software has well-defined,         standard-conforming interfaces and data structures to interface and interoperate with other Earth system models. Developed         with the support of an Earth science technology office (ESTO) computational technologies project round~3 cooperative agreement,         LIS has helped advance NASA&amp;#8217;s Earth&amp;#8211;Sun division&amp;#8217;s software engineering principles and practices, while promoting portability,         interoperability, and scalability for Earth system modeling. LIS was selected as a co-winner of NASA&amp;#8217;s 2005 software of the         year award.      </content></document><document><year>2007</year><authors>Akio Shiibashi1 | Naoki Mizoguchi1  | Kinji Mori2 </authors><title>High-speed processing in wired-and-wireless integrated autonomous decentralized system and its application to IC card ticket         system      </title><content>The automatic fare collection systems need both high performance and high reliability. High performance is one of the most         expected functions on the automatic fare collection gates (AFCGs) handling the highly dense passengers during rush hours.         Reliability is also indispensable because the tickets are equivalent to money. For a wireless IC card ticket system, expected         to improve the passengers&amp;#8217; convenience and to reduce the maintenance cost, is difficult to meet these two requirements because         of wireless communications between an IC card and an AFCG. This paper introduces the autonomous decentralized system as the         solution and how it is applied to the system. Then, two models are prepared and simulated to evaluate the efficiency, especially         high-speed processing. The technologies are implemented into the &amp;#8220;Suica&amp;#8221; system at East Japan Railway Company and have proven         the effectiveness.      </content></document><document><year>2007</year><authors>Joakim FrГ¶berg1 | Mikael Г…kerholm2 | Kristian S|strГ¶m2 | Christer NorstrГ¶m2</authors><title>Key factors for achieving project success in integration of automotive mechatronics      </title><content>In this paper, we present a multiple case study on integration of automotive mechatronic components. Based on the findings,         we identify that the root causes of problems in integration are largely related to decisions omitted in electronic strategy.         We present and recommend use of checklists defining key factors to address in order to achieve successful integration projects         in terms of cost and quality. Our recommendations are defined by checklists for critical decisions in areas: functionality,         platform, integration, and stakeholder involvement. The recommendations are established based on practitioner experience and         then validated in a multiple case study. Five cases of integration are studied for different heavy vehicles in one company,         and the fulfillment of our recommendations is measured. Finally we define project success criteria and we compare the level         of fulfillment with the project success in terms of time plan and resource consumption. The main contribution of this study         is the validated recommendations, each including a set of checkpoints that defines recommendation fulfillment. We also present         defining characteristics to identify a high-risk project. We provide a set of observable project properties and show how they         affect project risk.      </content></document><document><year>2007</year><authors>Shawn Bohner1 | Ramya Ravich|ar1  | James Arthur1 </authors><title>Model-based engineering for change-tolerant systems      </title><content>Developing and evolving today&amp;#8217;s systems are often stymied by the sheer size and complexity of the capabilities being developed         and integrated. At one end of the spectrum, we have sophisticated agent-based software with hundreds of thousands of collaborating         nodes. These require modeling abstractions relevant to their complex workflow tasks as well as predictable transforms and         mappings for the requisite elaborations and refinements that must be accomplished in composing these systems. At the other         end of the spectrum, we have ever-increasing capabilities of reconfigurable hardware devices such as field-programmable gate         arrays to support the emerging adaptability and flexibility needs of these systems. From a model-based engineering perspective,         these challenges are very similar; both must move their abstraction and reuse levels up to meet growing productivity and quality         objectives. Model-based engineering and software system variants such as the model-driven architecture (MDA) are increasingly         being applied to systems development as the engineering community recognizes the benefits of managing complexity, separating         key concerns, and automating transformations from high-level abstract requirements down through the implementation. However,         there are challenges when it comes to establishing the correct boundaries for change-tolerant parts of the system. Capabilities         engineering (CE) is a promising approach for defining long-lived components of a system to ensure some sense of change tolerance.         For innovative initiatives such as the National Aeronautics and Space Administration (NASA)&amp;#8217;s autonomous nanotechology swarms         (ANTS), the development and subsequent evolution of such systems are of considerable importance as their missions involve         complex, collaborative behaviors across distributed, reconfigurable satellites. In this paper, we investigate the intersection         of these two technologies as they support the development of complex, change-tolerant systems. We present an effective approach         for bounding computationally independent models so that, as they transition to the architecture, capabilities-based groupings         of components are relevant to the change-tolerant properties that must convey in the design solution space. The model-based         engineering approach is validated via a fully functional prototype and verified by generating nontrivial multiagent systems         and reusing components in subsequent systems. We build off of this research completed on the collaborative agent architecture,         discuss the CE approach for the transition to architecture, and then examine how this will be applied in the reconfigurable         computing community with the new National Science Foundation Center for High-Performance Reconfigurable Computing. Based on         this work and extrapolating from similar efforts, the model-based approach shows promise to reduce the complexities of software         evolution and increase productivity&amp;#8212;particularly as the model libraries are populated with canonical components.      </content></document><document><year>2007</year><authors>G. P. Briggs1 | Tapabrata Ray1 | J. F. Milthorpe1</authors><title>Optimal design of an Australian medium launch vehicle      </title><content>Conceptual design of a satellite launch vehicle is a multidisciplinary task which must take into account interactions of disciplines         such as propulsion, aerodynamics, structures, guidance and orbital mechanics. We discuss the initial modelling of a clean         sheet design for a putative Australian medium launch vehicle capable of placing an Ariane-44L equivalent payload into geostationary         transfer orbit. While the Ariane-44L vehicle design is a three and a half stage vehicle, the alternative design is for a straight         three stage vehicle. The &amp;#8220;ideal velocity&amp;#8221; or delta-V capability of the AR44L is first derived from published data. The proposed         design is then modeled using a spreadsheet. The gross lift-off weight of the vehicle is then minimised while still providing         the same delta-V as Ariane. Various differences between the two vehicles are discussed. The initial design of a launch vehicle         as presented here is based on a simple stack model optimised automatically using an evolutionary algorithm. The efficiency         of the proposed approach and the reasons for using evolutionary algorithms is discussed along with future developments in         the areas of multi-objective formulations of the design optimisation problem as well as the vehicle model from the standpoint         of a number of system considerations.      </content></document><document><year>2007</year><authors>Sven Apel1 | Christian KГ¤stner2 | Martin Kuhlemann2  | Thomas Leich3 </authors><title>Pointcuts, advice, refinements, and collaborations: similarities, differences, and synergies      </title><content>Aspect-oriented programming (AOP) is a novel programming paradigm that aims at modularizing complex software. It embraces         several mechanisms including (1) pointcuts and advice as well as (2) refinements and collaborations. Though all these mechanisms         deal with crosscutting concerns, i.e., a special class of design and implementation problems that challenge traditional programming         paradigms, they do so in different ways. In this article we explore their relationship and their impact on modularity, which         is an important prerequisite for reliable and maintainable software. Our exploration helps researchers and practitioners to         understand their differences and exposes which mechanism is best used for which problem.      </content></document><document><year>2007</year><authors>Norman F. Schneidewind1 </authors><title>Predicting shuttle software reliability with parameter evaluation      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Roman Gumzej1  | Wolfgang A. Halang2 </authors><title>QoS-oriented design of embedded systems with specification PEARL      </title><content>Only recently have methodical tools adequate to design real-time systems been formally introduced in design methodologies.         Naturally, they were present from the beginning, but due to the large diversity of embedded systems&amp;#8217; areas of deployment,         specially dedicated formalisms have been developed and used. High-level language programming and integration of modeling formalisms         into design methods eased the development of more complex real-time applications. With the emerging object-oriented programming         languages and design methods, their integration into larger information systems has become more transparent. It was the UML         methodology, however, which eventually merged also the design methods and concepts of real-time systems into a consistent         whole. It took a large consortium and a long process to persuade industry of the benefits the new integral methodology can         offer. On the other hand, there are some trade-offs, and there are some features not completely covered, yet. Here, a different,         more straightforward approach to program and design (embedded) real-time systems is presented. Since it emerged from the real-time         community, it includes most features relevant there. Independent of the UML profile for schedulability, performance and time         specification, a profile was devised for use in PEARL-oriented UML design. The strengths of the mentioned language and design         methods for QoS-oriented design of (embedded) real-time systems are emphasised throughout this article.      </content></document><document><year>2007</year><authors>Norita Ahmad1  | Phillip A. Laplante1 </authors><title>Reasoning about software using metrics and expert opinion      </title><content>When comparing software programs on the basis of more than one metric a difficulty arises when the metrics are contradictory         or if there are no standard acceptance thresholds. An appealing solution in such cases is to incorporate expert opinion to         resolve the inconsistencies. A rigorous framework, however, is essential when fusing metrics and expert opinion in this decision-making         process. Fortunately, the analytical hierarchy process (AHP) can be used to facilitate rigorous decision-making in this particular         problem. In this work a combination of expert opinion and tool-collected measures are used to reason about software programs         using AHP. The methodology employed can be adapted to other decision-making problems in software engineering when both metrics         data and expert opinion are available, some of which are described.      </content></document><document><year>2007</year><authors>Jane Huffman Hayes1 | Alex Dekhtyar2 | Senthil Karthikeyan Sundaram1 | E. Ashlee Holbrook1 | Sravanthi Vadlamudi1  | Alain April3 </authors><title>REquirements TRacing On target (RETRO): improving software maintenance through traceability recovery      </title><content>A number of important tasks in software maintenance require an up-to-date requirements traceability matrix (RTM): change impact         analysis, determination of test cases to execute for regression testing, etc. The generation and maintenance of RTMs are tedious         and error-prone, and they are hence often not done. In this paper, we present REquirements TRacing On-target (RETRO), a special-         purpose requirements tracing tool. We discuss how RETRO automates the generation of RTMs and present the results of a study         comparing manual RTM generation to RTM generation using RETRO. The study showed that RETRO found significantly more correct         links than manual tracing and took only one third of the time to do so.      </content></document><document><year>2007</year><authors>Sam Lightstone1 </authors><title>Seven software engineering principles for autonomic computing development      </title><content>The complexity of modern middleware and software solutions is growing at an exponential rate. Only self-managing, or autonomic         computing technology can reasonably stem the confusion this complexity brings to bear on human administrators. While much         has been published on &amp;#8220;architecture&amp;#8221; and &amp;#8220;function&amp;#8221; for producing such systems, little has been written about the engineering         of self-managing systems as a distinct paradigm. In this short article we suggest a set of software engineering principles         for engineering of autonomic systems that should guide the planning of autonomic systems and their interfaces, with the intent         to guide the thinking of R&amp;amp;D organizations pursuing the development of autonomic computing capability.      </content></document><document><year>2007</year><authors>Elena Sitnikova1 | Trent Kroeger1 | Stephen Cook1</authors><title>Software and systems engineering process capability in the South Australian defence industry      </title><content>It is well recognised that that there is a correlation between process maturity in large organisations and project success.         In response to this, a number of process models and standards have been developed for the large-project environment. The Australian         defence industry, unlike many overseas countries, relies to a much greater extent on small and medium-sized enterprises to         supply equipment and services. Hence, the question has arisen about the scalability of overseas concepts to the Australian         defence industry situation. To address this question, a research project has been undertaken to identify the current baseline         of process capability for the South Australian defence software and systems engineering industry. This paper presents findings         from the research project, including a general characterisation of the industry process capability and a discussion of the         common perceived strengths and challenges of organisations within the industry. The project&amp;#8217;s objectives, research design         and findings are then compared and contrasted with similar research activities conducted in different parts of the world.         Finally, the paper draws conclusions based on the body of work presented and suggests areas for future research and development         to address industry needs.      </content></document><document><year>2007</year><authors>Ji Zhang1 | Zhinan Zhou1| 2 | Betty H. C. Cheng1  | Philip K. McKinley1 </authors><title>Specifying real-time properties in autonomic systems      </title><content>Increasingly, computer software must adapt dynamically to changing conditions. The correctness of adaptation cannot be rigorously         addressed without precisely specifying the requirements for adaptation. In many situations, these requirements involve absolute         time, in addition to a logical ordering of events. This paper introduces an approach to formally specifying such timing requirements         for adaptive software. We introduce TA-LTL, a timed adaptation-based extension to linear temporal logic, and use this logic         to specify three timing properties associated with the adaptation process: safety, liveness, and stability. A dynamic adaptation scenario involving interactive audio streaming software is used to illustrate the timed temporal logic.      </content></document><document><year>2007</year><authors>Roy Sterritt1 </authors><title>Systems and software engineering of autonomic and autonomous systems      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Lars Grunske1  | Roger McCowan2</authors><title>Systems engineering, test and evaluation: maximising customer satisfaction      </title><content>Without Abstract</content></document><document><year>2007</year><authors>Abhishek Dubey1 | Steve Nordstrom1 | Turker Keskinpala1 | S|eep Neema1 | Ted Bapty1  | Gabor Karsai1 </authors><title>Towards a verifiable real-time, autonomic, fault mitigation framework for large scale real-time systems      </title><content>Designing autonomic fault responses is difficult, particularly in large-scale systems, as there is no single &amp;#8216;perfect&amp;#8217; fault         mitigation response to a given failure. The design of appropriate mitigation actions depend upon the goals and state of the         application and environment. Strict time deadlines in real-time systems further exacerbate this problem. Any autonomic behavior         in such systems must not only be functionally correct but should also conform to properties of liveness, safety and bounded         time responsiveness. This paper details a real-time fault-tolerant framework, which uses a reflex and healing architecture         to provide fault mitigation capabilities for large-scale real-time systems. At the heart of this architecture is a real-time         reflex engine, which has a state-based failure management logic that can respond to both event- and time-based triggers. We         also present a semantic domain for verifying properties of systems, which use this framework of real-time reflex engines.         Lastly, a case study, which examines the details of such an approach, is presented.      </content></document><document><year>2007</year><authors>Kevin Adams1  | Denis Gra&amp;#269 anin2 </authors><title>Using adaptive scheduling for increased resiliency in passive asynchronous replication      </title><content>Distributed systems commonly replicate data to enhance system dependability. In such systems, a logical update on a data item         results in a physical update on a number of copies. The synchronization and communication required to keep the copies of replicated         data consistent introduce a delay when operations are performed. In systems distributed over a bandwidth-constrained area,         such operational delays generally prove unacceptable and passive asynchronous replication is often used to mitigate the delays.         The research described in this paper looks to develop a new methodology for passive asynchronous replication that includes         the introduction of an adaptive data replication scheduler to increase the data redundancy of the most valued data objects         in overloaded situations. The approach we describe relies on the batch processing nature of passive asynchronous replication         to make update decisions based on a defined policy. The methodology uses an adaptive scheduling algorithm providing near real-time         selection of which objects to replicate during overloaded conditions based on a trained multilayer perceptron neural network.         Historical replication logs are used for the initial training of the network and supervised training continues as replication         logs are created providing periodic improvement through a feedback mechanism. This paper presents summary results of data         replication scheduling simulations with an emphasis on the design selection of the adaptive distributed scheduler.      </content></document><document><year>2005</year><authors>Jean Hartmann1 | Marlon Vieira2 | Herbert Foster2 | Axel Ruder2</authors><title>A UML-based approach to system testing</title><content>Abstract.;;This article describes an approach for automatically generating and executing system tests, which can be used to improve the validation of an application. Tests are automatically generated from behavioural models of the application using the unified modelling language (UML) and then executed using a suitable test execution environment. For this paper, we demonstrate our approach by means of an application that interacts with its users via a graphical user interface. Thus, we discuss the test- execution phase with respect to a commercial user interface (UI) or capture&amp;#x2013;replay tool.In this article, we show how, in the first step, a test designer manually annotates the UML models, which may have been semiautomatically extracted from existing, textual-use case documentation, with test requirements. In the second step, the test- generation tool automatically creates a set of textual test procedures (test cases) or executable test scripts. In the third step, a test executor runs these against the system under test using a commercial UI testing tool.</content></document><document><year>2005</year><authors>Mikael Lindvall2 | Ioana Rus2 | Forrest Shull2 | Marvin Zelkowitz1| 2 | Paolo Donzelli1 | Atif Memon1| Victor Basili1| 2 | Patricia Costa2 | Roseanne Tvedt2 | Lorin Hochstein1 | Sima Asgari1| Chris Ackermann2  | Dan Pech2 </authors><title>An evolutionary testbed for software technology evaluation</title><content>Abstract.;;Empirical evidence and technology evaluation are needed to close the gap between the state of the art and the state of the practice in software engineering. However, there are difficulties associated with evaluating technologies based on empirical evidence: insufficient specification of context variables, cost of experimentation, and risks associated with trying out new technologies. In this paper, we propose the idea of an evolutionary testbed for addressing these problems. We demonstrate the utility of the testbed in empirical studies involving two different research technologies applied to the testbed, as well as the results of these studies. The work is part of NASAs High Dependability Computing Project (HDCP), in which we are evaluating a wide range of new technologies for improving the dependability of NASA mission-critical systems.</content></document><document><year>2005</year><authors>Roy Sterritt1 </authors><title>Autonomic computing</title><content>Abstract.;;Autonomic computing (AC) has as its vision the creation of self-managing systems to address todays concerns of complexity and total cost of ownership while meeting tomorrows needs for pervasive and ubiquitous computation and communication. This paper reports on the latest autonomic systems research and technologies to influence the industry; it looks behind AC, summarising what it is, the current state-of-the-art research, related work and initiatives, highlights research and technology transfer issues and concludes with further and recommended reading.</content></document><document><year>2005</year><authors>Phan C. Vinh1  | Jonathan P. Bowen1 </authors><title>Continuity aspects of embedded reconfigurable computing</title><content>Abstract.;;In embedded systems, dynamically reconfigurable computing can be partially modified at runtime without stopping the operation of the whole system. In this paper, we consider a reorganization mechanism for dynamically reconfigurable computing in embedded systems to guarantee that invariants of the design are respected. This reorganization is considered as a visual transformation of the logical configuration by the formulated rules. The invariant is recognized under the restructuring of the configuration using reconfiguration rules.</content></document><document><year>2005</year><authors>Rajesh Mathew1 | Mohamed Younis1  | Sameh M. Elsharkawy2 </authors><title>Energy-efficient bootstrapping for wireless sensor networks</title><content>Wireless sensor networks are poised for increasingly wider uses in many military and civil applications. Such applications has stimulated research in a number of research areas related to energy conservation in such networks. Most such research focuses on energy saving in tasks after the network has been organized. Very little attention has been paid to network bootstrapping as a possible phase where energy can be saved. Bootstrapping is the phase in which the entities in a network are made aware of the presence of all or some of the other entities in the network. This paper describes a bootstrapping protocol for a class of sensor networks consisting of a mix of low-energy sensor nodes and a small number of high-energy entities called gateways. We propose a new approach, namely the slotted sensor bootstrapping (SSB) protocol, which focuses on avoiding collisions in the bootstrapping phase and emphasizes turning off node radio circuits whenever possible to save energy. Our mechanism synchronizes the sensor nodes to the gateway&amp;#x2019;s clock so that time-based communication can be used. The proposed SSB protocol tackles the issue of node coverage in scenarios, when physical device limitations and security precautions prevent some sensor nodes from communicating with the gateways. Additionally, we present an extension of the bootstrapping protocol, which leverages possible gateway mobility.</content></document><document><year>2005</year><authors>Andrew J. Kornecki1  | Janusz Zalewski2 </authors><title>Experimental evaluation of software development tools for safety-critical real-time systems</title><content>Since the early years of computing, programmers, systems analysts, and software engineers have sought ways to improve development process efficiency. Software development tools are programs that help developers create other programs and automate mundane operations while bringing the level of abstraction closer to the application engineer. In practice, software development tools have been in wide use among safety-critical system developers. Typical application areas include space, aviation, automotive, nuclear, railroad, medical, and military. While their use is widespread in safety-critical systems, the tools do not always assure the safe behavior of their respective products. This study examines the assumptions, practices, and criteria for assessing software development tools for building safety-critical real-time systems. Experiments were designed for an avionics testbed and conducted on six industry-strength tools to assess their functionality, usability, efficiency, and traceability. The results some light on possible improvements in the tool evaluation process that can lead to potential tool qualification for safety-critical real-time systems.</content></document><document><year>2005</year><authors>Marcel Oliveira1 | Ana Cavalcanti1 | Jim Woodcock1</authors><title>Formal development of industrial-scale systems in Circus</title><content>Circus is a new notation that may be used to specify both data and behavioural aspects of a system, and has an associated refinement calculus. In this work, we present rules to translate Circus programs to Java programs that use JCSP, a library that implements Communicating Sequential Processes constructs. These rules can be used as a complement to the Circus algebraic refinement technique, or as a guideline for implementation. They are a link between the results on refinement in the context of Circus and a practical programming language in current use. The rules can also be used as the basis for a tool that mechanises the translation. Although a few case studies are already available in the literature, the industrial fire control system, whose refinement and implementation is discussed in this paper, is, as far as we know, the largest case study on the Circus refinement strategy.</content></document><document><year>2005</year><authors>Introduction to Innovations in System and Software Engineering</authors><title>Without Abstract</title><content/></document></documents>